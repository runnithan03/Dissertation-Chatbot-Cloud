chunk_id,source,preview_400chars
0,unknown,"Multivariate Multiple Regression Bias caused by a misspecified model. Suppose some important predictor variables are omitted from the proposed regression model. That is, suppose the true model has with rank and (7-20) where and However, the investigator unknowingly fits a model using only the first q predictors by minimizing the error sum of squares The least squares estimator of is = Then, unlike"
1,unknown,"Then, unlike the situation when the model is correct, (7-21) That is, is a biased estimator of unless the columns of are perpendicular to those of (that is, ). If important variables are missing from the model, the least squares estimates may be misleading. 7.7 Multivariate Multiple Regression In this section, we consider the problem of modeling the relationship between m responses and a single se"
2,unknown,"response is assumed to follow its own regression model, so that (7-22) The error term = has and Thus, the error terms associated with different responses may be correlated. To establish notation conforming to the classical linear regression model, let denote the values of the predictor variables for the jth trial, let = be the responses, and let = be the errors. In matrix notation, the design matr"
3,unknown,"Z1n*1r+122 = D z1 0 z1 1 Á z1 r z2 0 z2 1 Á z2 r oo ∞ o zn 0 zn 1 Á zn r T 7ej 1 , ej 2 , Á , ej m8Ej œ7Yj 1 , Yj 2 , Á , Yj m8Yj œ 7zj 0 , zj 1 , Á , zj r8 Var1E2 = /H9018.E1E2 = 07e1 , e2 , Á , em8E¿ Ym = b0 m + b1 m z1 + Á + br m zr + em oo Y2 = b0 2 + b1 2 z1 + Á + br 2 zr + e2 Y1 = b0 1 + b1 1 z1 + Á + br 1 zr + e1 z1 , z2 , Á , zr.Y1 , Y2 , Á , Ym Bn 112 Zœ 1 Z2 = 0Z2 Z1B112Bn 112 = B112 + 1"
4,unknown,Z1B112Bn 112 = B112 + 1Zœ 1 Z12-1 Zœ 1 Z2 B122 E1Bn 1122 = 1Zœ 1 Z12-1 Zœ 1 E1Y2 = 1Zœ 1 Z12-1 Zœ 11Z1 B112 + Z2 B122 + E1E22 1Zœ 1 Z12-1 Zœ 1 Y. Bn 112B1121Y - Z1 B1122¿ 1Y - Z1 B1122. Var1E2 = s2 I.E1E2 = 0 = Z1 B112 + Z2 B122 + E Y1n*12 = S Z 1 1n*1q+122 Z 2 1n*1r-q22 T D B 112 11q+12 *12 B 122 11r-q2 *12 T + E1n*12 r + 1Z = 7Z1 /H21752Z28 387
5,unknown,"2 1n*1r-q22 T D B 112 11q+12 *12 B 122 11r-q2 *12 T + E1n*12 r + 1Z = 7Z1 /H21752Z28 387 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:20. Copyright © 2013. Pearson Education, Limited. All rights reserved. AppliedMutivariateStatisticalAnalysis: PearsonNew Internat"
6,unknown,PearsonNew InternationalEdition Chapter 7 Multivariate Linear Regression Models is the same as that for the single-response regression model. [See (7-3).] The other matrix quantities have multivariate counterparts. Set = D Eœ 1 Eœ 2 o Eœ n T E1n*m2 = D e1 1 e1 2 Á e1 m e2 1 e2 2 Á e2 m oo ∞ o en 1 en 2 Á en m T = 7E112 /H21752E122 /H21752Á /H21752E1m28 B11r+12 *m2 = D b0 1 b0 2 Á b0 m b1 1 b1 2 Á 
7,unknown,b1 1 b1 2 Á b1 m oo ∞ o br 1 br 2 Á br m T = 7B112 /H21752B122 /H21752Á /H21752B1m28 Y1n*m2 = D Y1 1 Y1 2 Á Y1 m Y2 1 Y2 2 Á Y2 m oo ∞ o Yn 1 Yn 2 Á Yn m T = 7Y112 /H21752Y122 /H21752Á /H21752Y1m28 The multivariate linear regression model is with (7-23) The m observations on the jth trial have covariance matrix but ob- servations from different trials are uncorrelated. Here and are unknown paramet
8,unknown,"parameters; the design matrix Z has jth row 7zj 0 , zj 1 , Á , zj r8. si kB /H9018= 5si k6, E1E1i22 = 0 and Cov 1E1i2 , E1k22 = si k I i, k = 1, 2, Á , m Y1n*m2 = Z1n*1r+122 B11r+12 *m2 + E1n*m2 Simply stated, the ith response follows the linear regression model (7-24) with = However, the errors for different responses on the same trial can be correlated. Given the outcomes and the values of the p"
9,unknown,"can be correlated. Given the outcomes and the values of the predictor variables Z with full column rank, we determine the least squares estimates exclusively from the observations on the ith response. In conformity with the single-response solution, we take (7-25)Bn 1i2 = 1Z¿ Z2-1 Z¿ Y1i2 Y1i2 Bn 1i2 Y si i I.Cov 1E1i22 Y1i2 = ZB1i2 + E1i2 , i = 1, 2, Á , m Y1i2 388"
10,unknown,"Y1i2 Bn 1i2 Y si i I.Cov 1E1i22 Y1i2 = ZB1i2 + E1i2 , i = 1, 2, Á , m Y1i2 388 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:20. Copyright © 2013. Pearson Education, Limited. All rights reserved. Sury importa Multivariate Multiple Regression Collecting these univa"
11,unknown,"Collecting these univariate least squares estimates, we obtain or (7-26) For any choice of parameters = the matrix of errors is The error sum of squares and cross products matrix is (7-27) The selection minimizes the ith diagonal sum of squares Consequently, is minimized by the choice Also, the generalized variance is min- imized by the least squares estimates (See Exercise 7.11 for an additional "
12,unknown,"ized sum of squares property.) Using the least squares estimates we can form the matrices of Predicted values: Residuals: (7-28) The orthogonality conditions among the residuals, predicted values, and columns of Z, which hold in classical linear regression, hold in multivariate multiple regression. They follow from = Specifically, (7-29) so the residuals are perpendicular to the columns of Z.A l s"
13,unknown,"(7-30) confirming that the predicted values are perpendicular to all residual vectors Because or = ± = ± (7-31) P residual 1error2 sum of squares and cross products Qa predicted sum of squares and cross products ba total sum of squares and cross products b En ¿ EnYN ¿ YNY¿ Y Y¿ Y = 1YN + En 2œ1YN + En 2 = YN ¿ YN + En ¿ En + 0 + 0¿ Y = YN + En ,En1k2 . YN 1i2 YN ¿ En = Bn ¿ Z¿ 7I - Z 1Z¿ Z2-1 Z¿ 8"
14,unknown,"En1i2 Z¿ En = Z¿ 7I - Z 1Z¿ Z2-1 Z¿ 8 Y = 0 Z¿- Z¿= 0.Z¿ 7I - Z 1Z¿ Z2-1 Z¿ 8 En = Y - YN = 7I - Z 1Z¿ Z2-1 Z¿ 8 Y YN = ZBn = Z 1Z¿ Z2-1 Z¿ Y Bn , Bn . ƒ1Y - ZB2¿ 1Y - ZB2 ƒB = Bn . tr 71Y - ZB2¿ 1Y - ZB281Y1i2 - Zb1i22¿ 1Y1i2 - Zb1i22. b1i2 = Bn 1i2 = C 1Y112 - Zb1122¿ 1Y112 - Zb1122 Á 1Y112 - Zb1122¿ 1Y1m2 - Zb1m22 o o 1Y1m2 - Zb1m22¿ 1Y112 - Zb1122 Á 1Y1m2 - Zb1m22¿ 1Y1m2 - Zb1m22 S 1Y - ZB2¿ 1"
15,unknown,"S 1Y - ZB2¿ 1Y - ZB2 Y - ZB. 7b112 /H21752b122 /H21752Á /H21752b1m28,B Bn = 1Z¿ Z2-1 Z¿ Y Bn = 7Bn 112 /H21752Bn 122 /H21752Á /H21752Bn 1m28 = 1Z¿ Z2-1 Z¿ 7Y112 /H21752Y122 /H21752Á /H21752Y1m28 389 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:20. Copyright © 201"
16,unknown,"Copyright © 2013. Pearson Education, Limited. All rights reserved. Chapter 7 Multivariate Linear Regression Models The residual sum of squares and cross products can also be written as (7-32) Example 7.8 (F itting a multivar iate straight-line regression model) To illustrate the calculations of and we fit a straight-line regression model (see Panel 7.2), to two responses and using the data in Exam"
17,unknown,"observations on an additional response, are as follows: Y2Y1 Yj 2 = b0 2 + b1 2 zj 1 + ej 2 , j = 1, 2, Á , 5 Yj 1 = b0 1 + b1 1 zj 1 + ej 1 En ,Bn , YN , En ¿ En = Y¿ Y - YN ¿ YN = Y¿ Y - Bn ¿ Z¿ ZBn title ‘Multivariate Regression Analysis’; data mra; infile ‘Example 7-8 data; input y1 y2 z1; PROGRAM COMMANDS proc glm data = mra; model y1 y2 = z1/ss3; manova h = z1/printe; (continues on next page"
18,unknown,(continues on next page) PANEL 7.2 SAS ANALYSIS FOR EXAMPLE 7.8 USING PROC. GLM. ¯˚˚˚˘˚˚˚˙ General Linear Models Procedure Dependent Variable: Y1 OUTPUT Source DF Sum of Squares Mean Square F Value Pr > F Model 14 0 . 0 0 0 0 0 0 0 0 4 0 . 0 0 0 0 0 0 0 02 0 . 0 0 0 . 0 2 0 8 Error 3 6.00000000 2.00000000 Corrected Total 44 6 . 0 0 0 0 0 0 0 0 R-Square C.V. Root MSE Y1 Mean 0.869565 28.28427 1.414
19,unknown,"R-Square C.V. Root MSE Y1 Mean 0.869565 28.28427 1.414214 5.00000000 01 234 14 389 232-1-1y2 y1 z1 The design matrix Z remains unchanged from the single-response problem.We find that Z¿= B11111 01234 R 1Z¿ Z2-1 = B .6 -.2 -.2 .1 R 390 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on "
20,unknown,"Copyright © 2013. Pearson Education, Limited. All rights reserved. Bojcoefficients -2x5 - Bijcoefficients area of - interest C Multivariate Multiple Regression Source DF Type III SS Mean Square F Value Pr > F Z1 1 40.00000000 40.00000000 20.00 0.0208 T for H0: Std Error of Parameter Estimate Parameter = 0 Pr > ITI Estimate INTERCEPT 1.000000000 0.91 0.4286 1.09544512 Z1 2.000000000 4.47 0.0208 0.4"
21,unknown,Dependent Variable: Y2 Source DF Sum of Squares Mean Square F Value Pr > F Model 1 10.00000000 10.00000000 7.50 0.0714 Error 3 4.00000000 1.33333333 Corrected Total 4 14.00000000 R-Square C.V. Root MSE Y2 Mean 0.714286 115.4701 1.154701 1.00000000 Source DF Type III SS Mean Square F Value Pr > F Z1 1 10.00000000 10.00000000 7.50 0.0714 T for H0: Std Error of Parameter Estimate Parameter = 0 Pr > I
22,unknown,INTERCEPT –1.000000000 –1.12 0.3450 0.89442719 Z1 1.000000000 2.74 0.0714 0.36514837 E = Error SS & CP Matrix Y1 Y2 Y1 6 –2 Y2 –2 4 Manova Test Criteria and Exact F Statistics for the Hypothesis of no Overall Z1 Effect H = Type III SS&CP Matrix for Z1 E = Error SS&CP Matrix S = 1 M = 0 N = 0 Statistic Value FN u m D F D e n D F P r > F Wilks’ Lambda 0.06250000 15.0000 2 20 . 0 6 2 5 Pillai’s Trace
23,unknown,"Hotelling-Lawley Trace 15.00000000 15.0000 2 20 . 0 6 2 5 Roy’s Greatest Root 15.00000000 15.0000 2 20 . 0 6 2 5 PANEL 7.2 (continued) 391 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:20. Copyright © 2013. Pearson Education, Limited. All rights reserved. Chapter "
24,unknown,"Chapter 7 Multivariate Linear Regression Models and so From Example 7.3, Hence, The fitted values are generated from and Collectively, and Note that Since YN ¿ YN = B165 45 45 15 R and En ¿ En = B 6 -2 -24 R Y¿ Y = B 14 3 8 9 -1 -1232 R E 1 -1 4 -1 32 83 92 U = B171 43 43 19 R En ¿ YN = B01 -21 0 0 -11 1 -1R E 1 -1 30 51 72 93 U = B00 00 R En = Y - YN = B01 -21 0 0 -11 1 -1R œ YN = ZBn = E 10 11 1"
25,unknown,"œ YN = ZBn = E 10 11 12 13 14 U B1 -1 21 R = E 1 -1 30 51 72 93 U yn2 =- 1 + z2 .yn1 = 1 + 2z1 Bn = 7Bn 112 /H21752Bn 1228 = B1 -1 21 R = 1Z¿ Z2-1 Z¿ 7y112 /H21752y1228 Bn 112 = 1Z¿ Z2-1 Z¿ y112 = B1 2R Bn 122 = 1Z¿ Z2-1 Z¿ y122 = B .6 -.2 -.2 .1 R B 5 20R = B -1 1R Z¿ y122 = B11111 01234 R E -1 -1 2 3 2 U = B 5 20R 392 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis :"
26,unknown,"Created from durham on 2024-10-12 16:08:20. Copyright © 2013. Pearson Education, Limited. All rights reserved. Multivariate Multiple Regression the sum of squares and cross-products decomposition is easily verified. /H17039 Result 7.9. For the least squares estimator = deter- mined under the multivariate multiple regression model (7-23) with full rank = and The residuals = satisfy and = so Also, a"
27,unknown,"= so Also, and are uncorrelated. Proof. The ith response follows the multiple regression model Also, from (7-24), (7-33) and so = and = Next, Using Result 4.9, with U any random vector and A a fixed matrix, we have that == Consequently, from the proof of Result 7.1 and using Result 2A.12 = si k tr 71I - Z 1Z¿ Z2-1 Z¿ 28 = si k1n - r - 12 E1Enœ 1i2 En1k22 = E1Eœ 1i21I - Z 1Z¿ Z2-1 Z¿ 2 E1k22 = tr 7"
28,unknown,"tr 7AE1UU¿ 28.E7tr 1AUU¿ 28E7U¿ AU8 = 1Z¿ Z2-1 Z¿ E1E1i2 Eœ 1k22 Z 1Z¿ Z2-1 = si k1Z¿ Z2-1 Cov 1Bn 1i2 , Bn 1k22 = E1Bn 1i2 - B1i22 1Bn 1k2 - B1k22œ 0.E1En1i22B1i2E1Bn 1i22 En1i2 = Y1i2 - YN 1i2 = 7I - Z1Z¿ Z2-1 Z¿ 8 Y1i2 = 7I - Z1Z¿ Z2-1 Z¿ 8 E1i2 Bn 1i2 - B1i2 = 1Z¿ Z2-1 Z¿ Y1i2 - B1i2 = 1Z¿ Z2-1 Z¿ E1i2 Y1i2 = ZB1i2 + E1i2 , E1E1i22 = 0, and E1E1i2 Eœ 1i22 = si i I BnEn E1En 2 = 0 and E a 1 n -"
29,unknown,"BnEn E1En 2 = 0 and E a 1 n - r - 1 En ¿ En b = /H9018 1n - r - 12 si k,E1Enœ 1i2 En1k22 E1En1i22 = 0Y - ZBnEn = 7En112 /H21752En122 /H21752Á /H21752En1m28 Cov 1Bn 1i2 , Bn 1k22 = si k 1Z¿ Z2-1, i, k = 1, 2, Á , m E1Bn 1i22 = B1i2 or E1Bn 2 = B r + 1 6 n,1Z2 7Bn 112 /H21752Bn 122 /H21752Á /H21752Bn 1m28Bn Y¿ Y = YN ¿ YN + En ¿ En 393 Johnson, R., & Wichern, D. (2013). Applied multivariate statisti"
30,unknown,"Created from durham on 2024-10-12 16:08:20. Copyright © 2013. Pearson Education, Limited. All rights reserved. Chapter 7 Multivariate Linear Regression Models Dividing each entry of by we obtain the unbiased estimator of Finally, so each element of is uncorrelated with each element of /H17039 The mean vectors and covariance matrices determined in Result 7.9 enable us to obtain the sampling propert"
31,unknown,"to obtain the sampling properties of the least squares predictors. We first consider the problem of estimating the mean vector when the predictor variables have the values = The mean of the ith response variable is and this is estimated by the ith component of the fitted regression relationship. Collectively, (7-34) is an unbiased estimator since == for each compo- nent. From the covariance matrix"
32,unknown,"nent. From the covariance matrix for and the estimation errors – have covariances (7-35) The related problem is that of forecasting a new observation vector = at According to the regression model, = where the “new” error = is independent of the errors and satisfies and = The forecast errorfor the ith component of is so = indicating that is an unbiased predictor of The forecast errors have covarian"
33,unknown,(7-36) Note that since = is independent of A similar result holds for Maximum likelihood estimators and their distributions can be obtained when the errors have a normal distribution.E E1e0 i1Bn 1k2 - B1k22¿ 2.E0 . 1Z¿ Z2-1 Z¿ E1i2 + B1i2Bn 1i2E11Bn 1i2 - B1i22 e0 k2 = 0 = si k11 + zœ 01Z¿ Z2-1 z02 - zœ 0 E11Bn 1i2 - B1i22 e0 k2 - E1e0 i1Bn 1k2 - B1k22œ2z0 = E1e0 i e0 k2 + zœ 0 E1Bn 1i2 - B1i22 1B
34,unknown,"= E1e0 i - zœ 01Bn 1i2 - B1i222 1e0 k - zœ 01Bn 1k2 - B1k222 E1Y0 i - zœ 0 Bn 1i22 1Y0 k - zœ 0 Bn 1k22 Y0 i. zœ 0 Bn 1i2E1e0 i2 - zœ 0 E1Bn 1i2 - B1i22 = 0,E1Y0 i - zœ 0 Bn 1i22 = e0 i - zœ 01Bn 1i2 - B1i22 Y0 i - zœ 0 Bn 1i2 = Y0 i - zœ 0 B1i2 + zœ 0 B1i2 - zœ 0 Bn 1i2 Y0si k.E1e0 i e0 k2E1e0 i2 = 0 E7e0 1 , e0 2 , Á , e0 m8E0 œ zœ 0 B1i2 + e0 iY0 iz0 .7Y0 1 , Y0 2 , Á , Y0 m8 Y0 œ = si k zœ 01Z"
35,unknown,"Y0 œ = si k zœ 01Z¿ Z2-1 z0 E7zœ 01B1i2 - Bn 1i22 1B1k2 - Bn 1k22œ z08 = zœ 01E1B1i2 - Bn 1i22 1B1k2 - Bn 1k22œ2z0 zœ 0 Bn 1i2zœ 0 B1i2Bn 1k2 ,Bn 1i2 zœ 0 B1i2zœ 0 E1Bn 1i22E1zœ 0 Bn 1i22zœ 0 B zœ 0 Bn = 7zœ 0 Bn 112 /H21752zœ 0 Bn 122 /H21752Á /H21752zœ 0 Bn 1m28 zœ 0 Bn 1i2 ,zœ 0 B1i2 , 71, z0 1 , Á , z0 r8.z0 œ En .Bn = si k11Z¿ Z2-1 Z¿- 1Z¿ Z2-1 Z¿ 2 = 0 = 1Z¿ Z2-1 Z¿ si k I 1I - Z 1Z¿ Z2-1 Z¿"
36,unknown,"= 1Z¿ Z2-1 Z¿ si k I 1I - Z 1Z¿ Z2-1 Z¿ 2 = 1Z¿ Z2-1 Z¿ E1E1i2 Eœ 1k22 1I - Z 1Z¿ Z2-1 Z¿ 2 C o v 1Bn 1i2 , En1k22 = E71Z¿ Z2-1 Z¿ E1i2 Eœ 1k21I - Z 1Z¿ Z2-1 Z¿ 28 /H9018. n - r - 1,En ¿ EnEnœ 1i2 En1k2 394 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:20. Copyrig"
37,unknown,"Copyright © 2013. Pearson Education, Limited. All rights reserved. Multivariate Multiple Regression Result 7.10. Let the multivariate multiple regression model in (7-23) hold with full rank and let the errors have a normal distribu- tion. Then is the maximum likelihood estimator of and has a normal distribution with and = Also, is independent of the max- imum likelihood estimator of the positive d"
38,unknown,"imum likelihood estimator of the positive definite given by and The maximized likelihood . Proof. (See website: www.prenhall.com/statistics) /H17039 Result 7.10 provides additional support for using least squares estimates. When the errors are normally distributed, and are the maximum likeli- hood estimators of and respectively. Therefore, for large samples, they have nearly the smallest possible "
39,unknown,"Comment. The multivariate multiple regression model poses no new computa- tional problems. Least squares (maximum likelihood) estimates, = are computed individually for each response variable. Note, however, that the model requires that the same predictor variables be used for all responses. Once a multivariate multiple regression model has been fit to the data, it should be subjected to the diagn"
40,unknown,be subjected to the diagnostic checks described in Section 7.6 for the single-response model. The residual vectors can be examined for normality or outliers using the techniques in Section 4.6. The remainder of this section is devoted to brief discussions of inference for the normal theory multivariate multiple regression model. Extended accounts of these procedures appear in [2] and [18]. Likelih
41,unknown,"procedures appear in [2] and [18]. Likelihood Ratio Tests for Regression Parameters The multiresponse analog of (7-12), the hypothesis that the responses do not depend on becomes (7-37) Setting we can write the general model as E1Y2 = ZB = 7Z1 /H21752Z28 C B112 B122 S = Z1 B112 + Z2 B122 Z = S Z 1 1n*1q+122 Z 2 1n*1r-q22 T , H0 : B122 = 0 where B = D B 112 11q+12 *m2 B 122 11r-q2 *m2 T zq+1 , zq+2"
42,unknown,"11r-q2 *m2 T zq+1 , zq+2 , Á , zr, 7enj 1 , enj 2 , Á , enj m8 1Z¿ Z2-1 Z¿ y1i2 ,Bn 1i2 /H9018,B n-1 En ¿ EnBn L (Mn , /H9018N ) = (2p)-mn>2 ƒ /H9018N ƒ-n>2 e-mn>2 n/H9018N is distributed as Wp, n-r-11/H90182 /H9018N = 1 n En ¿ En = 1 n 1Y - ZBn 2œ 1Y - ZBn 2 /H9018 Bnsi k1Z¿ Z2-1 .Cov 1Bn 1i2 , Bn 1k22E1Bn 2 = B BnB Bn = 1Z¿ Z2-1 Z¿ Y En Ú 1r + 12 + m,1Z2 = r + 1, 395"
43,unknown,"BnB Bn = 1Z¿ Z2-1 Z¿ Y En Ú 1r + 12 + m,1Z2 = r + 1, 395 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:20. Copyright © 2013. Pearson Education, Limited. All rights reserved. - givenZo, ..., za are false; I Cambewe no a - lowermatrix Chapter 7 Multivariate Linear R"
44,unknown,"Chapter 7 Multivariate Linear Regression Models Under and the likelihood ratio test of is based on the quantities involved in the where and From Result 7.10, the likelihood ratio, can be expressed in terms of generalized variances: (7-38) Equivalently,Wilks’ lambda statistic can be used. Result 7.11. Let the multivariate multiple regression model of (7-23) hold with Z of full rank and Let the erro"
45,unknown,"of full rank and Let the errors be normally distributed. Under is distributed as independently of which, in turn, is distributed as The likelihood ratio test of is equivalent to rejecting for large values of For n large,5 the modified statistic has, to a close approximation, a chi-square distribution with d.f. Proof. (See Supplement 7A.) /H17039 If Z is not of full rank, but has rank then where is"
46,unknown,"is the generalized inverse discussed in [22]. (See also Exercise 7.6.) The distributional conclusions stated in Result 7.11 remain the same, provided that r is replaced by and by rank However, not all hypotheses concerning can be tested due to the lack of uniqueness in the identification of caused by the linear dependencies among the columns of Z.N e v e r t h e l e s s ,t h e g e n e r a l i z e "
47,unknown,"allows all of the important MANOVA models to be analyzed as special cases of the multivariate multiple regression model. B B1Z12.q + 1r1 1Z¿ Z2- Bn = 1Z¿ Z2- Z¿ Y,r1 + 1, m1r - q2 - Bn - r - 1 - 1 2 1m - r + q + 12 R ln ¢ ƒ/H9018N ƒ ƒ/H9018N 1 ƒ ≤ -2 ln ¶=- n ln ¢ ƒ/H9018N ƒ ƒ/H9018N 1 ƒ ≤ =- n ln ƒn/H9018N ƒ ƒn/H9018N + n1/H9018N 1 - /H9018N 2 ƒ H0 H0Wp, r-q1/H90182. n1/H9018N 1 - /H9018N 2Wp, n-"
48,unknown,"E1r + 12 + m … n.r + 1 ¶ 2>n = ƒ/H9018N ƒ ƒ/H9018N 1 ƒ ¶= maxB112, /H9018 L1B112 , /H90182 maxB, /H9018 L1B, /H90182 = L1Bn 112 , /H9018N 12 L1Bn , /H9018N 2 = ¢ ƒ/H9018N ƒ ƒ/H9018N 1 ƒ ≤ n>2 ¶ , /H9018N 1 = n-11Y - Z1 Bn 1122œ 1Y - Z1 Bn 1122.Bn 112 = 1Zœ 1 Z12-1 Zœ 1 Y = n1/H9018N 1 - /H9018N 2 = 1Y - Z1 Bn 1122œ 1Y - Z1 Bn 1122 - 1Y - ZBn 2œ 1Y - ZBn 2 extra sum of squares and cross products H0"
49,unknown,"extra sum of squares and cross products H0Y = Z1 B112 + EH0 : B122 = 0, 5Technically, both and should also be large to obtain a good chi-square approximation.n - mn - r 396 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:20. Copyright © 2013. Pearson Education, Limi"
50,unknown,"Copyright © 2013. Pearson Education, Limited. All rights reserved. Multivariate Multiple Regression Example 7.9 (Testing the importance of additional predictors with a multivariate response) The service in three locations of a large restaurant chain was rated according to two measures of quality by male and female patrons. The first service- quality index was introduced in Example 7.5. Suppose we "
51,unknown,"quality index was introduced in Example 7.5. Suppose we consider a regression model that allows for the effects of location, gender, and the location–gender interaction on both service-quality indices. The design matrix (see Example 7.5) remains the same for the two-response situation. We shall illustrate the test of no location-gender inter- action in either response using Result 7.11. A computer"
52,unknown,"Let be the matrix of interaction parameters for the two responses. Although the sample size is not large, we shall illustrate the calculations involved in the test of given in Result 7.11. Setting we test by referring to a chi-square percentage point with d.f. Since we do not reject at the 5% level. The interaction terms are not needed. /H17039 Information criterion are also available to aid in th"
53,unknown,"Information criterion are also available to aid in the selection of a simple but adequate multivariate multiple regresson model. For a model that includes d predictor variables counting the intercept, let Then, the multivariate multiple regression version of the Akaike’s information criterion is This criterion attempts to balance the generalized variance with the number of parameters. Models with "
54,unknown,"parameters. Models with smaller AIC’s are preferable. In the context of Example 7.9, under the null hypothesis of no interaction terms, we have , response variables, and terms, so More generally, we could consider a null hypothesis of the form = where C is and is of full rank For the choices1r - q2.1r - q2 * 1r + 12 Ω0 ,H0 : CB = 18 * ln(20545.7) - 16 = 162.75 A I C= n ln(ƒ/H9018ƒ) - 2p * d = 18 l"
55,unknown,"1 18 c 3419.15 1267.88 1267.88 2417.07 d ` b - 2 * 2 * 4 d = 4p = 2n = 18 AIC = n ln(ƒ /H9018N d ƒ) - 2p * d /H9018N d = 1 n (residual sum of squares and cross products matrix) H09.49, =3.28 6 x4 21.052m1r1 - q12 = 2122 = 4 =- B18 - 5 - 1 - 1 2 12 - 5 + 3 + 12 R ln 1.76052 = 3.28 - Bn - r1 - 1 - 1 2 1m - r1 + q1 + 12 R ln ¢ ƒn/H9018N ƒ ƒn/H9018N + n1/H9018N 1 - /H9018N 2 ƒ ≤ H0a = .05,H0 : B122 = "
56,unknown,"≤ H0a = .05,H0 : B122 = 0 n = 18 B122 a extra sum of squares and cross products b = n1/H9018N 1 - /H9018N 2 = B441.76 246.16 246.16 366.12 R a residual sum of squares and cross products b = n/H9018N = B2977.39 1021.72 1021.72 2050.95 R 397 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durha"
57,unknown,"Copyright © 2013. Pearson Education, Limited. All rights reserved. figureout how r & a , interact, ⊥ predictors -extra variables no . ofresponsevariables ↓ no·ofredicts -what are r& 9? 3 + 2 = 5 Chapter 7 Multivariate Linear Regression Models and this null hypothesis becomes the case considered earlier. It can be shown that the extra sum of squares and cross products generated by the hypothesis is"
58,unknown,"Under the null hypothesis, the statistic is distributed as inde- pendently of This distribution theory can be employed to develop a test of similar to the test discussed in Result 7.11. (See, for example, [18].) Other Multivariate Test Statistics Tests other than the likelihood ratio test have been proposed for testing in the multivariate multiple regression model. Popular computer-package program"
59,unknown,"Popular computer-package programs routinely calculate four multivariate test statistics. To connect with their output, we introduce some alternative notation. Let E be the error,o r r e s i d u a l ,s u m o f s q u a r e s a n d c r o s s p r o d u c t s m a t r i x that results from fitting the full model. The hypothesis,o r e x t r a ,s u m o f squares and cross-products matrix The statistics ca"
60,unknown,"The statistics can be defined in terms of E and H directly, or in terms of the nonzero eigenvalues of where Equivalently, they are the roots of The definitions are Roy’s test selects the coefficient vector a so that the univariate F-statistic based on a has its maximum possible value. When several of the eigenvalues are moder- ately large, Roy’s test will perform poorly relative to the other three"
61,unknown,"studies suggest that its power will be best when there is only one large eigenvalue. Charts and tables of critical values are available for Roy’s test. (See [21] and [17].) Wilks’ lambda, Roy’s greatest root, and the Hotelling–Lawley trace test are nearly equivalent for large sample sizes. If there is a large discrepancy in the reported P-values for the four tests, the eigenvalues and vectors may "
62,unknown,"eigenvalues and vectors may lead to an interpretation. In this text, we report Wilks’ lambda, which is the likelihood ratio test. hia¿ Yj Roy’s greatest root = h1 1 + h1 Hotelling–Lawley trace = a s i=1 hi = tr 3HE-14 P i l l a i ’ s trace = a s i=1 hi 1 + hi = tr 3H1H + E2-14 W i l k s ’ lambda = q s i=1 1 1 + hi = ƒE ƒ ƒE + H ƒ ƒ1/H9018N 1 - /H9018N 2 - h/H9018N ƒ= 0. s = min 1p, r - q2.HE-1 ,h1"
63,unknown,"H = n1/H9018N 1 - /H9018N 2 p * p E = n/H9018N p * p H0 : B122 = 0 H0 : CB =Ω 0 /H9018N . Wr-q1/H90182n1/H9018N 1 - /H9018N 2 n1/H9018N 1 - /H9018N 2 = 1CBn -Ω 02œ1C 1Z¿ Z2-1 C¿ 2-11CBn -Ω 02 H0 H0 : CB = B122 = 0,Ω0 = 0,C = S 0 /H21752I1r-q2 *1r-q2 T 398 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Cr"
64,unknown,"Created from durham on 2024-10-12 16:08:20. Copyright © 2013. Pearson Education, Limited. All rights reserved. Multivariate Multiple Regression Predictions from Multivariate Multiple Regressions Suppose the model with normal errors has been fit and checked for any inadequacies. If the model is adequate, it can be employed for predictive purposes. One problem is to predict the mean responses corres"
65,unknown,"One problem is to predict the mean responses corresponding to fixed values of the predictor variables. Inferences about the mean responses can be made using the distribution theory in Result 7.10. From this result, we determine that and The unknown value of the regression function at is So, from the discussion of the -statistic in Section 5.2, we can write (7-39) and the confidence ellipsoid for i"
66,unknown,"(7-39) and the confidence ellipsoid for is provided by the inequality (7-40) where is the upper th percentile of an F-distribution with m and d.f. The simultaneous confidence intervals for = are (7-41) where is the ith column of and is the ith diagonal element of The second prediction problem is concerned with forecasting new responses = at Here is independent of Now, independently of so the predi"
67,unknown,"= at Here is independent of Now, independently of so the prediction ellipsoid for becomes (7-42) The simultaneous prediction intervalsfor the individual responses are (7-43)i = 1, 2, Á , m zœ 0 Bn 1i2 ; Aa m1n - r - 12 n - r - m b Fm, n-r-m1a2 A11 + zœ 01Z¿ Z2-1 z02 a n n - r - 1 sn i ib , Y0 i10011 - a2% … 11 + zœ 01Z¿ Z2-1 z02 ca m1n - r - 12 n - r - m b Fm, n-r-m1a2d 1Y0 - Bn ¿ z02¿ a n n - r -"
68,unknown,"1Y0 - Bn ¿ z02¿ a n n - r - 1 /H9018N b -1 1Y0 - Bn ¿ z02 Y010011 - a2%n/H9018N , Y0 - Bn ¿ z0 = 1B - Bn 2¿ z0 + E0 is distributed as Nm10, 11 + zœ 01Z¿ Z2-1z02/H90182 E.E0z0 .B¿ z0 + E0Y0 /H9018N .sn i iBnBn 1i2 i = 1, 2, Á , m zœ 0 Bn 1i2 ; Aa m1n - r - 12 n - r - m b Fm, n-r-m1a2 Azœ 01Z¿ Z2-1 z0 a n n - r - 1 sn i ib , zœ 0 B1i2E1Yi210011 - a2% n - r - m 1100a2Fm, n-r-m1a2 … zœ 01Z¿ Z2-1 z0 ca"
69,unknown,"… zœ 01Z¿ Z2-1 z0 ca m1n - r - 12 n - r - m b Fm, n-r-m1a2d 1B¿ z0 - Bn ¿ z0 2¿ a n n - r - 1 /H9018N b -1 1B¿ z0 - Bn ¿ z0 2 B¿ z010011 - a2% T2 = ¢ Bn ¿ z0 - B¿ z0 2zœ 01Z¿ Z2-1 z0 ≤ ¿ ¢ n n - r - 1 /H9018N ≤ -1 ¢ Bn ¿ z0 - B¿ z0 2zœ 01Z¿ Z2-1 z0 ≤ T2 B¿ z0 .z0 n/H9018N is independently distributed as Wn-r-11/H90182 Bn ¿ z0 is distributed as Nm1B¿ z0 , zœ 01Z¿ Z2-1 z0 /H90182 z0 E,Y = ZB + E, 39"
70,unknown,"Bn ¿ z0 is distributed as Nm1B¿ z0 , zœ 01Z¿ Z2-1 z0 /H90182 z0 E,Y = ZB + E, 399 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:20. Copyright © 2013. Pearson Education, Limited. All rights reserved. & Areaofinterest! ⊥reliabilingoa Chapter 7 Multivariate Linear Re"
71,unknown,"where and are the same quantities appearing in (7-41). Com- paring (7-41) and (7-43), we see that the prediction intervals for the actual values of the response variables are wider than the corresponding intervals for the expected values. The extra width reflects the presence of the random error Example 7.10 (Constructing a confidence ellipse and a prediction ellipse for bivariate responses) A sec"
72,unknown,"responses) A second response variable was measured for the computer-requirement problem discussed in Example 7.6. Measurements on the response disk input/output capacity, corresponding to the and values in that example were Obtain the 95% confidence ellipse for and the 95% prediction ellipse for = for a site with the configuration = Computer calculations provide the fitted equation with Thus, = Fr"
73,unknown,"We find that and Since and a 95% confidence ellipse for = is, from (7-40), the set with This ellipse is centered at Its orientation and the lengths of the major and minor axes can be determined from the eigenvalues and eigenvectors of Comparing (7-40) and (7-42), we see that the only change required for the calculation of the 95% prediction ellipse is to replace = .34725 withzœ 01Z¿ Z2-1 z0 n/H901"
74,unknown,"n/H9018N . 1151.97, 349.172.F2, 31.052 = 9.55. … 1.347252 ca 2142 3 b F2, 31.052d 7zœ 0 B112 - 151.97, zœ 0 B122 - 349.178 142 B5.80 5.30 5.30 13.13 R -1 Bzœ 0 B112 - 151.97 zœ 0 B122 - 349.17R Bzœ 0 B112 zœ 0 B122 RB¿ z0m = 2,r = 2,n = 7, Bn ¿ z0 = C Bn œ 112 Bn œ 122 S z0 = C zœ 0 Bn 112 zœ 0 Bn 122 S = B151.97 349.17R = B5.80 5.30 5.30 13.13 R n/H9018N = B1y112 - ZBn 1122¿ 1y112 - ZBn 11221 y11"
75,unknown,"5.30 13.13 R n/H9018N = B1y112 - ZBn 1122¿ 1y112 - ZBn 11221 y112 - ZBn 1122¿ 1y122 - ZBn 1222 1y122 - ZBn 1222¿ 1y112 - ZBn 11221 y122 - ZBn 1222¿ 1y122 - ZBn 1222 R zœ 0 Bn 122 = 14.14 + 2.2511302 + 5.6717.52 = 349.17 Bn 112 œ = 78.42, 1.08, 428, zœ 0 Bn 112 = 151.97, and zœ 01Z¿ Z2-1 z0 = .34725 714.14, 2.25, 5.678.Bn 122 œs = 1.812. yn2 = 14.14 + 2.25z1 + 5.67z2 71, 130, 7.58.z0 œ7Y0 1 , Y0 28"
76,unknown,"œ7Y0 1 , Y0 28Y0 œ B¿ z0 y2 œ = 7301.8, 396.1, 328.2, 307.4, 362.4, 369.5, 229.18 z2z1 Y2 , e0 i. Fm, n-r-m1a2sn i i,Bn 1i2 , 400 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:20. Copyright © 2013. Pearson Education, Limited. All rights reserved. The Concept of Li"
77,unknown,"The Concept of Linear Regression = 1.34725. Thus, the 95% prediction ellipse for = is also centered at but is larger than the confidence ellipse. Both ellipses are sketched in Figure 7.5. It is the prediction ellipse that is relevant to the determination of computer requirements for a particular site with the given /H17039 7.8 The Concept of Linear Regression The classical linear regression model "
78,unknown,The classical linear regression model is concerned with the association between a single dependent variable Y and a collection of predictor variables The regression model that we have considered treats Y as a random variable whose mean depends upon fixed values of the ’s.This mean is assumed to be a linear func- tion of the regression coefficients The linear regression model also arises in a diffe
79,unknown,"The linear regression model also arises in a different setting. Suppose all the variables Y, are random and have a joint distribution, not necessarily normal, with mean vector and covariance matrix Partitioning and in an obvious fashion, we write with (7-44) can be taken to have full rank.6 Consider the problem of predicting Y using the (7-45)linear predictor = b0 + b1 Z1 + Á + br Zr = b0 + b¿ Z /"
80,unknown,"/H9018Z Z SZ Y œ = 7sY Z1 , sY Z2 , Á , sY Zr8 and /H9018= D s Y Y 11 * 12 S Z Y 1r* 12 S Z œ Y 11 * r2 /H9018 Z Z 1r* r2 TM = D m Y 11 * 12 M Z 1r* 12 T /H9018 M/H90181r+ 12 * 1r+ 12 .M1r+ 12 * 1 Z2 , Á , ZrZ1 , b1 , Á , br.b0 , zi z2 , Á , zr.z1 , z0 . 1151.97, 349.172, 7Y0 1 , Y0 28Y0 œ1 + zœ 01Z¿ Z2-1 z0 Prediction ellipse Confidence ellipse Response 1120 340 360 380 Response 2 0 140 160 180 F"
81,unknown,"340 360 380 Response 2 0 140 160 180 Figure 7.5 95% confidence and prediction ellipses for the computer data with two responses. 6 If is not of full rank, one variable—for example, —can be written as a linear combination of the other ’s and thus is redundant in forming the linear regression function That is, Z may be replaced by any subset of components whose nonsingular covariance matrix has the "
82,unknown,"Z¿ B.Zi Zk/H9018Z Z 401 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:45. Copyright © 2013. Pearson Education, Limited. All rights reserved. Chapter 7 Multivariate Linear Regression Models For a given predictor of the form of (7-45), the error in the prediction of"
83,unknown,"(7-46) Because this error is random, it is customary to select and b to minimize the (7-47) Now the mean square error depends on the joint distribution of Y and Z only through the parameters and It is possible to express the “optimal” linear pre- dictor in terms of these latter quantities. Result 7.12. The linear predictor ± with coefficients has minimum mean square among all linear predictors of "
84,unknown,"square error is Also, = is the linear predictor having maxi- mum correlation with Y;t h a t i s , Proof. Writing = ±– we get Adding and subtracting we obtain The mean square error is minimized by taking = making the last term zero, and then choosing == to make the third term zero. The minimum mean square error is thus – Next, we note that == so Employing the extended Cauchy–Schwartz inequality of "
85,unknown,"obtain 1b¿ SZ Y22 … b¿ /H9018Z Z b Sœ Z Y /H9018Z Z -1 SZ Y B = /H9018Z Z , 7Corr 1b0 + b¿ Z, Y282 = 7b¿ SZ Y82 sY Y1b¿ /H9018Z Z b2 , for all b0 , b b¿ SZ YCov 1b¿ Z, Y2Cov 1b0 + b¿ Z, Y2 Sœ Z Y /H9018Z Z -1 SZ Y.sY Y b0mY - 1/H9018Z Z -1 SZ Y2œ MZb0 B,b = /H9018Z Z -1 SZ Y + 1b - /H9018Z Z -1 SZ Y2œ /H9018Z Z1b - /H9018Z Z -1 SZ Y2 E1Y - b0 - b¿ Z22 = sY Y - Sœ Z Y /H9018Z Z -1 SZ Y + 1mY - b0 -"
86,unknown,"Z Y /H9018Z Z -1 SZ Y + 1mY - b0 - b¿ MZ22 Sœ Z Y /H9018Z Z -1 SZ Y, = sY Y + b¿ /H9018Z Z b + 1mY - b0 - b¿ MZ22 - 2b¿ SZ Y - 2E7b¿ 1Z - MZ21Y - mY28 = E1Y - mY22 + E1b¿ 1Z - MZ222 + 1mY - b0 - b¿ MZ22 E1Y - b0 - b¿ Z22 = E7Y - mY - 1b¿ Z - b¿ MZ2 + 1mY - b0 - b¿ MZ282 1mY - b¿ MZ2,1mY - b¿ MZ2b0 + b¿ Zb0 + b¿ Z = A B¿ /H9018Z Z B sY Y = A Sœ Z Y /H9018Z Z -1 SZ Y sY Y Corr 1Y, b0 + B¿ Z2 = maxb0"
87,unknown,"Corr 1Y, b0 + B¿ Z2 = maxb0, b Corr 1Y, b0 + b¿ Z2 1Z - MZ2mY + Sœ Z Y /H9018Z Z -1b0 + B¿ Z E1Y - b0 - B¿ Z22 = E1Y - mY - Sœ Z Y /H9018Z Z -1 1Z - MZ222 = sY Y - Sœ Z Y /H9018Z Z -1 SZ Y B = /H9018Z Z -1 SZ Y, b0 = mY - B¿ MZ B¿ Zb0 /H9018.M mean square error = E1Y - b0 - b¿ Z22 b0 prediction error = Y - b0 - b1 Z1 - Á - br Zr = Y - b0 - b¿ Z 402"
88,unknown,"b0 prediction error = Y - b0 - b1 Z1 - Á - br Zr = Y - b0 - b¿ Z 402 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:45. Copyright © 2013. Pearson Education, Limited. All rights reserved. The Concept of Linear Regression or with equality for b = The alternative expr"
89,unknown,"correlation follows from the equation == = /H17039 The correlation between Y and its best linear predictor is called the population multiple correlation coefficient (7-48) The square of the population multiple correlation coefficient, is called the population coefficient of determination .N o t e t h a t ,u n l i k e o t h e r c o r r e l a t i o n c o e f f i - cients, the multiple correlation co"
90,unknown,"cients, the multiple correlation coefficient is a positive square root, so The population coefficient of determination has an important interpretation. From Result 7.12, the mean square error in using to forecast Y is (7-49) If there is no predictive power in Z.A t t h e o t h e r e x t r e m e , i m - plies that Y can be predicted with no error. Example 7.11 (Determining the best linear predictor"
91,unknown,"Example 7.11 (Determining the best linear predictor, its mean square error, and the multiple correlation coefficient) Given the mean vector and covariance matrix of Y, determine (a) the best linear predictor ± (b) its mean square error, and (c) the multiple correlation coefficient. Also, verify that the mean square error equals First, so the best linear predictor is = The mean square error is sY Y"
92,unknown,"sY Y - Sœ Z Y /H9018Z Z -1 SZ Y = 10 - 71, -18 B .4 -.6 -.6 1.4 R B 1 -1R = 10 - 3 = 7 3 + Z1 - 2Z2 .b0 + B¿ Z b0 = mY - B¿ MZ = 5 - 71, -28 B2 0R = 3 B = /H9018Z Z -1 SZ Y = B73 32 R -1 B 1 -1R = B .4 -.6 -.6 1.4 R B 1 -1R = B 1 -2R sY Y11 - rY1Z2 2 2. b2 Z2 ,b0 + b1 Z1 /H9018= C sY Y Sœ Z Y SZ Y /H9018Z Z S = C 10 1 -1 17 3 -13 2 SM = C mY MZ S = C 5 2 0 S and Z1 , Z2 , rY1Z2 2 = 1rY1Z2 2 = 0, s"
93,unknown,"2 = 1rY1Z2 2 = 0, sY Y - Sœ Z Y /H9018Z Z -1 SZ Y = sY Y - sY Ya Sœ Z Y /H9018Z Z -1 SZ Y sY Y b = sY Y11 - rY1Z2 2 2 b0 + B¿ Z 0 … rY1Z2 … 1. rY1Z2 2 , rY1Z2 =+ A Sœ Z Y /H9018Z Z -1 SZ Y sY Y B¿ /H9018Z Z B. Sœ Z Y /H9018Z Z -1 /H9018Z Z BSœ Z Y BSœ Z Y /H9018Z Z -1 SZ Y /H9018Z Z -1 SZ Y = B. 7Corr 1b0 + b¿ Z, Y282 … Sœ Z Y /H9018Z Z -1 SZ Y sY Y 403"
94,unknown,"-1 SZ Y /H9018Z Z -1 SZ Y = B. 7Corr 1b0 + b¿ Z, Y282 … Sœ Z Y /H9018Z Z -1 SZ Y sY Y 403 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:45. Copyright © 2013. Pearson Education, Limited. All rights reserved. Chapter 7 Multivariate Linear Regression Models and the m"
95,unknown,"and the multiple correlation coefficient is Note that = is the mean square error. /H17039 It is possible to show (see Exercise 7.5) that (7-50) where is the upper-left-hand corner of the inverse of the correlation matrix determined from The restriction to linear predictors is closely connected to the assumption of normality. Specifically, if we take then the conditional distribution of Y with fixe"
96,unknown,"then the conditional distribution of Y with fixed (see Result 4.6) is The mean of this conditional distribution is the linear predictor in Result 7.12. That is, (7-51) and we conclude that is the best linear predictor of Y when the population is The conditional expectation of Y in (7-51) is called the regression function.F o r n o r m a l p o p u l a t i o n s ,i t i s l i n e a r . When the popul"
97,unknown,"When the population is not normal, the regression function need not be of the form Nevertheless, it can be shown (see [22]) that whatever its form, predicts Y with the smallest mean square error. Fortunately, this wider optimality among all estimators is possessed by the linear predictor when the population is normal. Result 7.13. Suppose the joint distribution of Y and Z is Let be the sample mean"
98,unknown,"be the sample mean vector and sample covariance matrix, respectively, for a random sample of size n from this population. Then the maximum likelihood estimators of the coefficients in the linear predictor are Bn = SZ Z -1 sZ Y , Bn 0 = Y - sœ Z Y SZ Z -1 Z = Y - Bn ¿ Z Mn = BY ZR and S = BsY Y sœ Z Y sZ Y SZ Z R Nr+ 11M, /H90182. E1Y ƒ z1 , z2 , Á , zr2, b0 + B¿ z. E1Y ƒ z1 , z2 , Á , zr2 Nr+ 11M,"
99,unknown,"E1Y ƒz1 , z2 , Á , zr2 = b0 + B¿ z E1Y ƒz1 , z2 , Á , zr2 = mY + Sœ Z Y /H9018Z Z -1 1z - MZ2 N1mY + Sœ Z Y /H9018Z Z -1 1Z - MZ2, sY Y - Sœ Z Y /H9018Z Z -1 SZ Y2 z2 , Á , zrz1 , E Y Z1 Z2 o Zr U to be distributed as Nr+ 11M, /H90182 /H9018. rY Y 1 - rY1Z2 2 = 1 rY Y 10A 1 - 3 10 B = 7sY Y11 - rY1Z2 2 2 rY1Z2 = A Sœ Z Y /H9018Z Z -1 SZ Y sY Y = A 3 10 = .548 404"
100,unknown,"10A 1 - 3 10 B = 7sY Y11 - rY1Z2 2 2 rY1Z2 = A Sœ Z Y /H9018Z Z -1 SZ Y sY Y = A 3 10 = .548 404 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:45. Copyright © 2013. Pearson Education, Limited. All rights reserved. The Concept of Linear Regression Consequently, the"
101,unknown,"Consequently, the maximum likelihood estimator of the linear regression function is and the maximum likelihood estimator of the mean square error is Proof. We use Result 4.11 and the invariance property of maximum likelihood esti- mators. [See (4-20).] Since, from Result 7.12, and the conclusions follow upon substitution of the maximum likelihood estimators for /H17039 It is customary to change th"
102,unknown,"mean square error, = in order to obtain the unbiased estimator (7-52) Example 7.12 (Maximum likelihood estimate of the regression function—single response) For the computer data of Example 7.6, the observations on Y (CPU time), (orders), and (add–delete items) give the sample mean vector and sample covariance matrix: S = C sY Y sœ Z Y sZ Y SZ Z S = C 467.913 418.763 35.983 418.763 377.200 28.034 3"
103,unknown,"35.983 28.034 13.657 S Mn = C y– Z S = C 150.44 130.24 3.547 S Z2Z1 n = 7 a n - 1 n - r - 1 b 1sY Y - sœ Z Y SZ Z -1 sZ Y2 = a n j= 1 1Yj - bn 0 - Bn ¿ Zj22 n - r - 1 E1Y - b0 - B¿ Z22,sY Y#Z n - 1r + 12 M = B mY MZ R and /H9018= BsY Y Sœ Z Y SZ Y /H9018Z Z R Mn = BY ZR and /H9018N = Bsn Y Y Sn œ Z Y Sn Z Y /H9018N Z Z R = a n - 1 n b S mean square error = sY Y#Z = sY Y - Sœ Z Y /H9018Z Z -1 SZ Y "
104,unknown,"Z Y /H9018Z Z -1 SZ Y B = /H9018Z Z -1 SZ Y , b0 + B¿ z = mY + Sœ Z Y /H9018Z Z -1 1z - MZ2 b0 = mY - 1/H9018Z Z -1 SZ Y2¿ MZ , sn Y Y#Z = n - 1 n 1sY Y - sœ Z Y SZ Z -1 sZ Y2 B¿ Z42E3Y - b0 - bn 0 + Bn ¿ z = Y + sœ Z Y SZ Z -1 1z - Z2 405 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durha"
105,unknown,"Created from durham on 2024-10-12 16:08:45. Copyright © 2013. Pearson Education, Limited. All rights reserved. Chapter 7 Multivariate Linear Regression Models Assuming that Y,a n d a r e j o i n t l y n o r m a l , o b t a i n t h e e s t i m a t e d r e g r e s s i o n function and the estimated mean square error. Result 7.13 gives the maximum likelihood estimates and the estimated regression fun"
106,unknown,"The maximum likelihood estimate of the mean square error arising from the prediction of Y with this regression function is /H17039 Prediction of Several Variables The extension of the previous results to the prediction of several responses is almost immediate. We present this extension for normal populations. Suppose with By Result 4.6, the conditional expectation of given the fixed values of the "
107,unknown,"of the predictor variables, is (7-53) This conditional expected value, considered as a function of is called the multivariate regression of the vector Y on Z.I t i s c o m p o s e d o f m univariate regressions. For instance, the first component of the conditional mean vector is ± = which minimizes the mean square error for the prediction of The matrix = is called the matrix of regression coeffici"
108,unknown,"/H9018Y Z /H9018Z Z -1Bm * rY1 . E1Y1 ƒ z1 , z2 , Á , zr2,/H9018Y1 Z /H9018Z Z -1 1z - MZ2mY1 z2 , Á , zr,z1 , E7Y ƒ z1 , z2 , Á , zr8 = MY + /H9018Y Z/H9018Z Z -1 1z - MZ2 z2 , Á , zrz1 , Y2 , Á , Ym8¿ ,7Y1 , /H9018= D /H9018 Y Y 1m* m2 /H9018 Y Z 1m* r2 /H9018 Z Y 1r* m2 /H9018 Z Z 1r* r2 TM = D M Y 1m* 12 M Z 1r* 12 T and D Y1m* 12 Z1r* 12 T is distributed as Nm+ r1M, /H90182 Y2 , Á , Ym Y1 , ="
109,unknown,"T and D Y1m* 12 Z1r* 12 T is distributed as Nm+ r1M, /H90182 Y2 , Á , Ym Y1 , = .894 = a 6 7 b ¢467.913 - 3418.763, 35.9834 B .003128 -.006422 -.006422 .086404 R B418.763 35.983R≤ a n - 1 n b 1sY Y - sZ Y œ SZ Z -1 sZ Y2 bn 0 + Bn ¿ z = 8.42 - 1.08z1 + .42z2 = 8.421 bn 0 = y– - Bn ¿ z– = 150.44 - 71.079, .4208 B130.24 3.547R = 150.44 - 142.019 Bn = SZ Z -1 sZ Y = B .003128 -.006422 -.006422 .08640"
110,unknown,"-1 sZ Y = B .003128 -.006422 -.006422 .086404 R B418.763 35.983R = B1.079 .420R Z2Z1 , 406 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:45. Copyright © 2013. Pearson Education, Limited. All rights reserved. The Concept of Linear Regression The error of prediction"
111,unknown,"The error of prediction vector has the expected squares and cross-products matrix (7-54) Because and are typically unknown, they must be estimated from a random sample in order to construct the multivariate linear predictor and determine expect- ed prediction errors. Result 7.14. Suppose Y and Z are jointly distributed as Then the re- gression of the vector Y on Z is The expected squares and cross"
112,unknown,"gression of the vector Y on Z is The expected squares and cross-products matrix for the errors is Based on a random sample of size n,t h e m a x i m u m l i k e l i h o o d e s t i m a t o r o f t h e regression function is and the maximum likelihood estimator of is Proof. The regression function and the covariance matrix for the prediction errors follow from Result 4.6. Using the relationships we"
113,unknown,we deduce the maximum likelihood statements from the invariance property [see (4-20)] of maximum likelihood estimators upon substitution of /H17039 It can be shown that an unbiased estimator of is (7-55)= 1 n - r - 1 a n j= 1 1Yj - Bn 0 - Bn Zj2 1Yj - Bn 0 - Bn Zj2¿ a n - 1 n - r - 1 b 1SY Y - SY Z SZ Z -1 SZ Y2 /H9018Y Y#Z = a n - 1 n b C SY Y SY Z SZ Y SZ Z S= C /H9018N Y Y /H9018N Y Z /H9018N Z
114,unknown,"S= C /H9018N Y Y /H9018N Y Z /H9018N Z Y /H9018N Z Z S = a n - 1 n b SMn = C Y Z S ; /H9018N /H9018Y Y#Z = /H9018Y Y - /H9018Y Z /H9018Z Z -1 /H9018Z Y = /H9018Y Y - B /H9018Z Z B¿ B0 + B z = MY + /H9018Y Z /H9018Z Z -1 1z - MZ2 B0 = MY - /H9018Y Z /H9018Z Z -1 MZ , B = /H9018Y Z /H9018Z Z -1 /H9018N Y Y#Z = a n - 1 n b 1SY Y - SY Z SZ Z -1 SZ Y2 /H9018Y Y#Z Bn 0 + Bn z = Y + SY Z SZ Z -1 1z - Z2 "
115,unknown,"-1 1z - Z2 E1Y - B0 - BZ2 1Y - B0 - BZ2¿= /H9018Y Y#Z = /H9018Y Y - /H9018Y Z /H9018Z Z -1 /H9018Z Y B0 + Bz = MY - /H9018Y Z /H9018Z Z -1 MZ + /H9018Y Z /H9018Z Z -1 z = MY + /H9018Y Z /H9018Z Z -1 1z - MZ2 Nm+ r1M, /H90182. /H9018M = /H9018Y Y - /H9018Y Z /H9018Z Z -1 /H9018Z Y = /H9018Y Y - /H9018Y Z /H9018Z Z -1 1/H9018Y Z2¿- /H9018Y Z /H9018Z Z -1 /H9018Z Y + /H9018Y Z /H9018Z Z -1 /H9018Z Z "
116,unknown,"-1 /H9018Z Z /H9018Z Z -1 1/H9018Y Z2¿ /H9018Y Y#Z = E7Y - MY - /H9018Y Z /H9018Z Z -1 1Z - MZ28 7Y - MY - /H9018Y Z /H9018Z Z -1 1Z - MZ28¿ Y - MY - /H9018Y Z /H9018Z Z -1 1Z - MZ2 407 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:45. Copyright © 2013. Pearson Ed"
117,unknown,"Chapter 7 Multivariate Linear Regression Models Example 7.13 (Maximum likelihood estimates of the regression functions—two responses) We return to the computer data given in Examples 7.6 and 7.10. For time, I/O capacity, and add–delete items, we have and Assuming normality, we find that the estimated regression function is Thus, the minimum mean square error predictor of is Similarly, the best pre"
118,unknown,The maximum likelihood estimate of the expected squared errors and cross- products matrix is given by = a 6 7 b B1.043 1.042 1.042 2.572 R = B.894 .893 .893 2.205 R - B 418.763 35.983 1008.976 140.558 R B .003128 -.006422 -.006422 .086404 R B418.763 1008.976 35.983 140.558 R≤ = a 6 7 b ¢B 467.913 1148.536 1148.536 3072.491 R a n - 1 n b 1SY Y - SY Z SZ Z -1 SZ Y2 /H9018Y Y#Z 14.14 + 2.25z1 + 5.67z
119,unknown,Y2 150.44 + 1.0791z1 - 130.242 + .4201z2 - 3.5472 = 8.42 + 1.08z1 + .42z2 Y1 = B150.44 327.79R + B1.079 1z1 - 130.242 + .420 1z2 - 3.5472 2.254 1z1 - 130.242 + 5.665 1z2 - 3.5472 R * B .003128 -.006422 -.006422 .086404 R Bz1 - 130.24 z2 - 3.547 R = B150.44 327.79R + B 418.763 35.983 1008.976 140.558 R Bn 0 + Bn z = y– + SY Z SZ Z -1 1z - z–2 S = BSY Y SY Z SZ Y SZ Z R = D 467.913 1148.556 418.763 
120,unknown,"467.913 1148.556 418.763 35.983 1148.556 3072.491 1008.976 140.558 418.763 1008.976 377.200 28.034 35.983 140.558 28.034 13.657 T Mn = By z–R = D 150.44 327.79 130.24 3.547 T Z2 =Z1 = orders,Y2 = diskY1 = CPU 408 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:45. C"
121,unknown,"Copyright © 2013. Pearson Education, Limited. All rights reserved. The Concept of Linear Regression The first estimated regression function, ± and the associated mean square error, .894, are the same as those in Example 7.12 for the single-response case. Similarly, the second estimated regression function, ± is the same as that given in Example 7.10. We see that the data enable us to predict the f"
122,unknown,"We see that the data enable us to predict the first response, with smaller error than the second response, The positive covariance .893 indicates that over- prediction (underprediction) of CPU time tends to be accompanied by overpredic- tion (underprediction) of disk capacity. /H17039 Comment. Result 7.14 states that the assumption of a joint normal distribu- tion for the whole collection leads to"
123,unknown,"equations We note the following: 1. The same values, are used to predict each 2. The are estimates of the th entry of the regression coefficient matrix = for We conclude this discussion of the regression problem by introducing one further correlation coefficient. Partial Correlation Coefficient Consider the pair of errors obtained from using the best linear predictors to predict and Their correla-"
124,unknown,"tion, determined from the error covariance matrix = measures the association between and after eliminating the effects of We define the partial correlation coefficient between and eliminating by (7-56) where is the th entry in the matrix = The corresponding sample partial correlation coefficient is (7-57)rY1 Y2#Z = sY1 Y2#Z 1sY1 Y1#Z 1sY2 Y2#Z /H9018Y Y - /H9018Y Z /H9018Z Z -1 /H9018Z Y ./H9018Y "
125,unknown,"rY1 Y2#Z = sY1 Y2#Z 1sY1 Y1#Z 1sY2 Y2#Z Z2 , Á , Zr, Z1 ,Y2 ,Y1 Z2 , Á , Zr. Z1 ,Y2Y1 /H9018Y Y - /H9018Y Z /H9018Z Z -1 /H9018Z Y ,/H9018Y Y#Z Y2 .Y1 Y2 - mY2 - /H9018Y2 Z /H9018Z Z -1 1Z - MZ2 Y1 - mY1 - /H9018Y1 Z /H9018Z Z -1 1Z - MZ2 i, k Ú 1./H9018Y Z /H9018Z Z -1B 1i, k2bn i k Yi.z2 , Á , zrz1 , ynm = bn 0 m + bn 1 m z1 + Á + bn r m zr oo yn2 = bn 0 2 + bn 1 2 z1 + Á + bn r 2 zr yn1 = bn 0 "
126,unknown,"yn1 = bn 0 1 + bn 1 1 z1 + Á + bn r 1 zr Z2 , Á , ZrZ1 ,Y2 , Á , Ym,Y1 , Y2 . Y1 , 5.67z2 ,14.14 + 2.25z1 .42z2 ,8.42 + 1.08z1 409 Johnson, R., & Wichern, D. (2013). Applied multivariate statistical analysis : Pearson new international edition. Pearson Education, Limited. Created from durham on 2024-10-12 16:08:45. Copyright © 2013. Pearson Education, Limited. All rights reserved. Cholesky-based m"
127,unknown,"Cholesky-based multivariate Gaussian regression Thomas Muschinskia,b,∗, Georg J. Mayrb, Thorsten Simona,b, Nikolaus Umlaufa, Achim Zeileisa aFaculty of Economics and Statistics, Universit¨ at Innsbruck, Austria bDepartment of Atmospheric and Cryospheric Sciences, Universit¨ at Innsbruck, Austria Abstract Distributional regression is extended to Gaussian response vectors of dimension greater than t"
128,unknown,Distributional regression is extended to Gaussian response vectors of dimension greater than two by parameterizing the covariance matrix Σ of the response distribution using the entries of its Cholesky de- composition. The more common variance-correlation parameterization limits such regressions to bivariate responses – higher dimensions require complicated constraints among the correlations to en
129,unknown,"nite Σ and a well-deﬁned probability density function. In contrast, Cholesky-based parameterizations ensure positive deﬁniteness for all distributional dimensions no matter what values the parameters take, enabling estimation and regularization as for other distributional regression models. In cases where components of the response vector are assumed to be conditionally independent beyond a certai"
130,unknown,"can be further reduced by setting Cholesky parameters beyond this lag to zero a priori. Cholesky-based mul- tivariate Gaussian regression is ﬁrst illustrated and assessed on artiﬁcial data and subsequently applied to a real-world 10-dimensional weather forecasting problem. There the regression is used to obtain reliable joint probabilities of temperature across ten future times, leveraging tempora"
131,unknown,"period to obtain more precise and meteorologically consistent probabilistic forecasts. 1. Introduction Distributional regression models (Stasinopoulos et al., 2018) – also called generalized additive models for location, scale and shape (GAMLSS, Rigby and Stasinopoulos, 2005) – extend generalized additive models (GAM, Hastie and Tibshirani, 1990) to allow any parametric distribution for the respon"
132,unknown,"of the response distribution – not just the mean – can therefore be linked to an additive predictor. Many diﬀerent univariate response distributions have been employed in such additive distributional regressions, ranging from zero-inﬂated and overdispersed count data (Klein et al., 2015b; Simon et al., 2019) to survival analysis (K¨ ohler et al., 2017; Burke et al., 2019) or geoadditive hazards re"
133,unknown,"2007). Much fewer applications exist for multivariate response distributions. A notable exception is (Klein et al., 2015a) where a bivariate response for childhood undernutrition in India is modeled with a bivariate Gaussian distribution based on the two means, variances and the correlation, all with suitable link functions. However, an extension to higher dimensions is not straightforward because"
134,unknown,"would not assure that the corresponding prediction of the covariance matrix Σ is positive deﬁnite – which in turn is necessary for a well-deﬁned probability density function. Moreover, the number of parameters for Σ increases quadratically with the dimension of the response, thus necessitating some form of regularization for the high model complexity. We embed multivariate Gaussian regression into"
135,unknown,"work by parameterizing Σ through the entries of its basic or modiﬁed Cholesky decomposition (Pourahmadi, ∗Corresponding author Email addresses: Thomas.Muschinski@uibk.ac.at (Thomas Muschinski), Georg.Mayr@uibk.ac.at (Georg J. Mayr), Thorsten.Simon@uibk.ac.at (Thorsten Simon), Nikolaus.Umlauf@uibk.ac.at (Nikolaus Umlauf), Achim.Zeileis@R-project.org (Achim Zeileis) arXiv:2102.13518v2 [stat.ME] 27 O"
136,unknown,"Achim.Zeileis@R-project.org (Achim Zeileis) arXiv:2102.13518v2 [stat.ME] 27 Oct 2021 1999), respectively. The resulting parameterizations are unconstrained, meaning that regardless of the val- ues the additive predictors take, the corresponding covariance matrix Σ is guaranteed to be positive deﬁnite. This facilitates regularization through penalized maximum likelihood or Bayesian estimation of th"
137,unknown,"sion coeﬃcients because the additive predictors can be regularized separately. Furthermore, the Cholesky parameterizations allow model complexity to be restricted a priori in cases where the response variables are ordered (for example with respect to time or one dimension in space). Namely, a covariance with an r-order antedependence structure (AD-r, Gabriel, 1962; Zimmerman et al., 1998) can be a"
138,unknown,lag may be assumed for the autocorrelations. The remainder of this paper is structured as follows: A brief overview of methods for covariance ma- trix estimation (without dependence on regressors) in Sec. 2 motivates leveraging the basic and modiﬁed Cholesky decompositions of Σ for a distributional regression (see Sec. 3) with a multivariate Gaussian re- sponse (Sec. 4). Multivariate Gaussian regr
139,unknown,"Subsequently, in Sec. 6 the model is applied to a ten-dimensional weather forecasting application and diﬀer- ent parameterizations are compared. A discussion of strengths and limitations of the new Cholesky-based multivariate Gaussian regression framework is found in Sec. 7. Summarizing remarks conclude the paper in Sec. 8. 2. Parameterizations of the covariance matrix For addressing the challenge"
140,unknown,"tant ﬁrst step is to adopt an unconstrained parameterization of the covariance matrix Σ. This not only facilitates estimation of the parameters with standard optimizers and without complicated constraints, it also enables diﬀerent forms of regularizations or restrictions of the model complexity. Hence, we review diﬀerent parameterizations of the covariance matrix proposed in the literature, especi"
141,unknown,"their suitability in multivariate Gaussian regression. An overview is provided in Table 1. 2.1. Positive deﬁniteness of the covariance matrix The covariance Σ of a k-dimensional random variable y from a multivariate Gaussian distribution is a symmetric k×kmatrix, containing k·(k+1)/2 unique variances and covariances. However, these parameters cannot be chosen freely when deﬁning Σ, but must satisf"
142,unknown,"cannot be chosen freely when deﬁning Σ, but must satisfy z⊤Σ z >0 for all z̸= 0. (1) to ensure Σ is positive deﬁnite. Only in this case does the corresponding probability density function f(y|µ,Σ) exist: f(y|µ,Σ) = 1√ (2π)k |Σ | exp { −1 2(y−µ)⊤Σ−1(y−µ) } , (2) where µ= E(y) is the expectation of y. To ensure positive deﬁniteness, joint restrictions for the elements of Σ are necessary. The same is"
143,unknown,"for two other natural parameterizations, namely the precision matrix Σ −1 and the variance-correlation decomposition of Σ. Similarly, a parameterization using the spectral decomposition is interpretable with respect to the eigenstructure of Σ, but constraints enter through the orthogonality of the corresponding eigenvectors (Pourahmadi, 2013). When estimating a ﬁxed covariance matrix from empirica"
144,unknown,"some techniques ensure positive deﬁniteness – e.g., glasso (Friedman et al., 2008) and tapering (Furrer et al., 2006) – while others do not – e.g., hard thresholding (Bickel and Levina, 2008). Ensuring a positive deﬁnite Σ becomes even more diﬃcult in the context of a distributional regression – where parameters underlying the covariance matrix should be linked to regressor variables. Here, it is"
145,unknown,"particularly beneﬁcial to employ a parameterization which ensures positive deﬁniteness without requiring joint constraints and then to combine this with link functions mapping the parameters to the real line. The simplest illustration for this is the case of a univariate Gaussian distribution (i.e., k= 1) with variance σ2. To assure positivity, a log link is typically used, mapping the set of posi"
146,unknown,"2 Parameterization No constraints required Natural interpretation for positive deﬁnite Σ of parameters Covariance ✓ Precision ✓ Variance-correlation (k> 2) ✓ Spectral decomposition Matrix logarithm ✓ Cholesky ✓ Modiﬁed Cholesky ✓ ✓ Table 1: Possible parameterizations of the covariance matrix in multivariate Gaussian distributions, along with properties that are crucial for linking the covariance s"
147,unknown,"Cholesky decomposition is particularly appealing as its derivation is less burdensome than the matrix logarithm. predictor (Stasinopoulos et al., 2018). Another notable case is the bivariate Gaussian distribution (i.e., k = 2) where the variance-correlation decomposition can be adopted with log links for the two variances and a suitable link for the correlation parameter restricted to the interval"
148,unknown,"It is also possible to extend the log-link approach to k > 2 dimensions by using the matrix logarithm, which maps positive deﬁnite symmetric matrices Σ to symmetric matrices A = log Σ with unconstrained entries (Pourahmadi, 2013). However, the disadvantages are that (i) the parameters in A have no natural interpretation and (ii) the matrix logarithm involves a Taylor series expansion that is rathe"
149,unknown,"compute. 2.2. Cholesky-based parameterizations A mathematically and computationally more appealing approach that also yields an unconstrained pa- rameterization is based on the Cholesky decomposition of Σ. Any Σ can be uniquely decomposed as the product of a positive-diagonal lower triangular matrix L with its transpose L⊤: Σ = LL⊤ , Σ−1 = (L−1)⊤L−1. (3) Subsequently the precision matrix Σ −1 resu"
150,unknown,"Both L and L−1 oﬀer unconstrained parameterizations of Σ. Although neither the individual parameters in Lnor those in L−1 are easily interpretable, the latter matrix as a whole has an elegant interpretation. If y∼N(µ,Σ), multiplication with L−1 can be used to uncorrelate y: L−1(y−µ) ∼N(0,I). To obtain parameters that are not only unconstrained but individually interpretable, Pourahmadi (1999) sugg"
151,unknown,"suggests a modiﬁed Cholesky decomposition that diagonalizes L−1: Σ−1 = T⊤D−1T. (4) In setups where the k components of y have a natural order (e.g., longitudinal data), the entries of the matrices T and D are related to the autoregressive structure of y ∼N (µ,Σ). The elements of the lower triangular matrix T are denoted −φij (i<j ) – where φij are the coeﬃcients of an autoregression on y– and"
152,unknown,"the elements of the diagonal matrix D are denoted as ψi (i = 1,...,k ) – corresponding to the innovation variances: ˆyj = µj + j−1∑ i=1 φij ·(yi −µi) for j = 2,...,k, (5) ψi = var( yi −ˆyi) for i= 1,...,k. (6) These intuitive interpretations of the parameters φij and ψi facilitate regularization of the high model complexity, particularly when k is large. Suggestions from the literature include: us"
153,unknown,"φij (Levina et al., 2008); approximating the elements of T and D by low-order polynomials (Pourahmadi, 3 1999, 2000; Pan and Pan, 2017); or cutting oﬀ the autocorrelation coeﬃcients at a maximum lag of r (Wu and Pourahmadi, 2003), i.e., setting φij = 0 for higher lags j−i > r. The latter approach thus yields a banded T matrix, corresponding to a so-called order- r antedependence (AD-r, Gabriel, 19"
154,unknown,"et al., 1998). Note that since T and L−1 (Eq. 3) share the same pattern of zeros, AD- r covariances can be modeled using both the modiﬁed and basic Cholesky parameterizations, although the individual elements of L−1 are not directly interpretable as autocorrelation coeﬃcients. In summary, both Cholesky-based parameterizations are appealing candidates for a distributional multi- variate Gaussian re"
155,unknown,"eterization that still ensures positive deﬁnite covariances, can be regularized using frequentist or Bayesian techniques, and can additionally be restricted to an AD- r antedependence, if the k components are auto- correlated with plausible maximum lag of r. The modiﬁed Cholesky decomposition has the advantage that individual parameters are interpretable while the basic Cholesky is slightly easier"
156,unknown,"3. Distributional regression In this section we brieﬂy introduce the general distributional regression framework into which we embed Cholesky-based multivariate Gaussian regression in the next Sec. 4. Speciﬁcally, the model speciﬁcation will be a special case of the general setup from Sec. 3.1 so that the corresponding estimation techniques – both frequentist and Bayesian – from Sec. 3.2 can be le"
157,unknown,"models is presented in Sec. 3.3. 3.1. Model speciﬁcation The idea in distributional regression (e.g., Rigby and Stasinopoulos, 2005; Klein et al., 2015b; Umlauf et al., 2018) is to adopt some K-parametric distribution Dfor the response variable y, linking each of the distributional parameters θk, k= 1,...,K , to separate ﬂexible additive predictors ηk y∼D(h1(θ1) = η1, h2(θ2) = η2,..., h K(θK) = ηK"
158,unknown,"y∼D(h1(θ1) = η1, h2(θ2) = η2,..., h K(θK) = ηK) , (7) typically using known monotonic and twice diﬀerentiable link functions hk(·), mapping the support of each parameter to the unrestricted real values of the predictors. The predictors combine additively eﬀects of regressor variable(s) xjk, j = 1,...,J k, with ηk = f1k(x1k) + ... + fJkk(xJkk), (8) where functions fjk(·) can be, e.g., linear terms,"
159,unknown,"intercepts, or spatial eﬀects. Rather than explicitly listing all common types of model terms here, we refer to the literature on GAM (Hastie and Tibshirani, 1990; Wood, 2017), GAMLSS (Rigby and Stasinopoulos, 2005), or Bayesian versions thereof (Umlauf et al., 2018). In this framework, although functions fjk(·) may be nonlinear, they can be represented by a linear combination of so-called basis f"
160,unknown,"coeﬃcients fjk(xjk) = ∑djk l=1 βljkBljk(xjk). For example, functions fjk(·) could be represented by P-splines (Eilers and Marx, 1996) or thin-plate regression splines (Wood, 2003). Hence, this representation of functions makes this model class very ﬂexible and well suited for modeling complex relationships. 3.2. Model estimation In a frequentist setting, distributional regression models are common"
161,unknown,"type algorithms maximizing the (penalized) log-likelihood, where parameter updates are usually obtained by zig-zag iterations over distribution parameters θk and model terms fjk(·) (see, e.g., Rigby and Stasinopou- los, 2005). Moreover, to avoid overﬁtting, nonlinear terms are estimated using penalization techniques as developed for GAMs (Wood, 2017), i.e., the wiggliness of each model term is con"
162,unknown,"ing parameters, which can be selected by techniques such as the Akaike information criterion (AIC). The resulting updating equations are known as penalized iteratively weighted least squares (IWLS, Gamerman, 1997). The great beneﬁt of the generic IWLS representation using a basis function approach is that in most 4 cases only ﬁrst and second order derivatives of the log-likelihood with respect to "
163,unknown,"to implement a new distribution. This is taken advantage of in Sec. 4 for setting up the estimating equations for the new Cholesky-based multivariate Gaussian regression model. In addition to this classical GAM-style penalized estimation, the problem of overﬁtting can also be addressed by boosting algorithms developed for distributional regression (Mayr et al., 2012) or by Lasso- type penalization"
164,unknown,"type penalization including factor fusion (Groll et al., 2019). However, in the frequentist framework smoothing parameter optimization for complex distributional re- gression models can be problematic and computing valid inferential statistics is sometimes diﬃcult or even impossible. The fully Bayesian approach using Markov chain Monte Carlo (MCMC) simulation techniques is particularly attractive "
165,unknown,"as ﬁxed, meaning that the parameters βljk in a Bayesian model each follow a prior distribution and the estimates are computed using the joint posterior distribution, which is proportional to the product of like- lihood and prior. A common choice is to use multivariate normal priors for the regression coeﬃcients and inverse Gamma (usually the default for spline based models) or half-Cauchy priors f"
166,unknown,"(can be advantageous with random eﬀects) that enforce regularization (inverse smoothing parameter in the frequentist approach). For details see, e.g., Umlauf et al. (2018). For eﬃciency, MCMC algorithms usually draw parameters from the posterior in blocks from full condi- tional distributions, i.e., for each model term fjk(·). The full conditionals are available in closed form only in rare cases, "
167,unknown,"expansion of the log-posterior centered at the last parameter state (Gamerman, 1997), which leads to an IWLS-based Metropolis-Hastings algorithm with an acceptance step. Thus, also for full Bayesian inference of distributional regression models only ﬁrst and second order derivatives are needed. For a more detailed introduction to Bayesian estimation of distributional regression models see Umlauf a"
168,unknown,"3.3. Software implementation A general ﬂexible implementation of distributional regression with particular emphasis on Bayesian es- timation is provided in the R package bamlss (Umlauf et al., 2021). The multivariate Gaussian regression models with basic and modiﬁed Cholesky parameterizations, as introduced in the next section, are imple- mented as families for bamlss. For now, these families are "
169,unknown,"available from the Gitlab server of Universit¨ at Innsbruck athttps://git.uibk.ac.at/c4031039/mvnchol. In the future, we plan to integrate the families into bamlss. 4. Cholesky-based multivariate Gaussian regression This section introduces the novel multivariate Gaussian regression approach we have developed by blend- ing powerful results from the literature on Cholesky-based parameterizations wit"
170,unknown,"butional regression, brieﬂy reviewed in the previous Sec. 2 and 3, respectively. The multivariate Gaussian regression setup is introduced in Sec. 4.1 and subsequently combined with either the basic (Sec. 4.2) or the modiﬁed (Sec. 4.3) Cholesky parameterization to guarantee a positive deﬁnite covariance matrix Σ. In order to leverage the typical strategies for estimation and regularization of distr"
171,unknown,"log-likelihood of the multivariate Gaussian regression model is provided in Sec. 4.4 along with the ﬁrst and second derivatives with respect to the predictors. 4.1. Multivariate Gaussian regression In multivariate Gaussian regression the response yis a length-kvector assumed to follow a k-dimensional Gaussian distribution y∼N(µ,Σ), (9) with probability density function provided in Eq. 2. All param"
172,unknown,"All parameters of N – the k components of µ and the k·(k+ 1)/2 parameters specifying Σ – may be linked to predictors. For the kmeans in µthis is straightforward as these parameters are unconstrained and 5 may take any real value. Therefore, we simply link them to the corresponding additive ﬂexible predictors using the identity function. µi = ηµ,i, i = 1,...,k. (10) In contrast, as already argued i"
173,unknown,"elements of Σ to respective additive predictors. This would not ensure that Σ is positive deﬁnite. Instead we propose to link either the elements of the basic or modiﬁed Cholesky parameterization of Σ to additive predictors. While parameterizations based on the Cholesky decomposition have been widely used to estimate ﬁxed covariances from sparse observations (Pourahmadi, 1999, 2013), we exploit th"
174,unknown,"for estimating covariances that depend on further covariates. 4.2. Basic Cholesky parameterization In the basic Cholesky parameterization, Σ is deﬁned through the k diagonal elements λii > 0 and k·(k−1)/2 oﬀ-diagonal elements λij of the inverse Cholesky factor: (L−1)⊤=   λ11 λ12 λ13 ··· λ1k 0 λ22 λ23 ··· λ2k 0 0 λ33 ··· λ3k ... ... ... ... ... 0 0 0 ··· λkk   , where Σ = LL⊤. (11)"
175,unknown,"0 λ22 λ23 ··· λ2k 0 0 λ33 ··· λ3k ... ... ... ... ... 0 0 0 ··· λkk   , where Σ = LL⊤. (11) Restricting the diagonal elements to be positive ensures a unique decomposition, motivating the use of a log link on these parameters while the oﬀ-diagonal elements may take any real value so that an identity link can be used: log(λii) = ηλ,ii, where i= 1,...,k, (12) λij = ηλ,ij, where i= 1,...,k −1,"
176,unknown,"log(λii) = ηλ,ii, where i= 1,...,k, (12) λij = ηλ,ij, where i= 1,...,k −1, and j = i+ 1,...,k. (13) Modeling the elements of the inverse Cholesky factor L−1 is motivated by the following considerations: (i) Unlike for the parameterization based on L, no computationally intensive matrix inversions are required during model estimation. (ii) There is an autoregressive interpretation for parameter val"
177,unknown,"Hence, in some situations, where it is not be necessary to model all k·(k−1)/2 oﬀ-diagonal elements, some elements may be restricted to zero. Namely, when the components of y have a natural order (e.g., longitudinal data) and a maximum lag in the autocorrelations is reasonable, then an order-rantedependence (AD-r, Gabriel, 1962; Zimmerman et al., 1998) model can be employed. This sets all λij = 0 "
178,unknown,"For large k and small r this yields a signiﬁcant reduction in model complexity. 4.3. Modiﬁed Cholesky parameterization Alternatively, the modiﬁed Cholesky decomposition of Pourahmadi (1999) diagonalizes the inverse Cholesky factor (L−1)⊤from Eq. 11, yielding Σ −1 = T⊤D−1T. The new parameters are those contained in the diag- onal matrix D and the upper unitriangular T⊤. D=   ψ1 0 0 ··· 0 0 ψ"
179,unknown,"ψ1 0 0 ··· 0 0 ψ2 0 ··· 0 0 0 ψ3 ··· 0 ... ... ... ... ... 0 0 0 ··· ψk   , T ⊤=   1 −φ12 −φ13 ··· −φ1k 0 1 −φ23 ··· −φ2k 0 0 1 ··· −φ3k ... ... ... ... ... 0 0 0 ··· 1   . (14) The ψi in Dand the φij in T⊤are called the innovation variances and generalized autoregressive parameters of y, respectively. They have meaningful interpretations when the components of y have a natura"
180,unknown,"6 Analogously to the basic Cholesky parameterization, a log link is used for the innovation variances to ensure positive deﬁniteness while the generalized autoregressive parameters may take any real values: log(ψi) = ηψ,i, where i= 1,...,k, (15) φij = ηφ,ij, where i= 1,...,k −1, and j = i+ 1,...,k. (16) Again, it is possible to reduce model complexity when an AD- r model can be assumed. Similar to"
181,unknown,"Cholesky parameterization, this sets all φij = 0 with j−i>r . 4.4. The log-likelihood and its derivatives By rearranging the probability density function of the multivariate Gaussian distribution (Eq. 2) we obtain the likelihood of distributional parameters for an observation vector y. For mathematical ease, we work with the log-transformed likelihood. ℓ(µ,L−1|y) = −k 2 log(2π) + log(|L−1|) −1 2(y"
182,unknown,"ℓ(µ,L−1|y) = −k 2 log(2π) + log(|L−1|) −1 2(y−µ)⊤(L−1)⊤L−1(y−µ). (17) Likelihood-based model estimation maximizes the sum of the individual log-likelihoods (Eq. 17) over all n observation vectors contained in the dataset. For computationally eﬃcient estimation, be it frequentist or Bayesian, this requires derivatives of the log-likelihood with respect to the additive predictors. We derive analytic"
183,unknown,"respect to all η∗. The ﬁrst derivatives in the basic parameterization are found to be ∂ℓ ∂ηµ,i = k∑ j=1 ςij˜yj ∂ℓ ∂ηλ,ii = 1 −λii˜yi i∑ m=1 (˜ymλmi) ∂ℓ ∂ηλ,ij = −˜yi j∑ m=1 (˜ymλmj) , (18) where ˜y= y−µ and ςij = (Σ−1)ij. The corresponding second derivatives are ∂2ℓ ∂η2 µ,i = −ςii = − k∑ j=i λ2 ij ∂2ℓ ∂η2 λ,ii = −2λ2 ii˜y2 i −λii˜yi · i−1∑ m=1 (˜ymλmi) ∂2ℓ ∂η2 λ,ij = −˜y2 i. (19) These are always "
184,unknown,"convex optimization problem. The same is true for the modiﬁed Cholesky parameterization (Appendix B). 5. Simulation study To investigate the ﬁnite-sample empirical performance of the novel multivariate Gaussian regression proposed in Sec. 4, this section conducts a systematic simulation study with more supplementary results provided in Appendix C. Speciﬁcally, we consider a setup where all distrib"
185,unknown,"φ from a modiﬁed Cholesky parameterization) of the response variable y depend on a covariate x, either 7 Parameters (true vs. estimated) −0.5 0.5 1.0 1.5 2.0 2.5 −1.0 −0.5 0.0 0.5 1.0 µ1 µ2 µ3 n = 500 Means Parameters (true vs. estimated) −3.5 −2.5 −1.5 −0.5 −1.0 −0.5 0.0 0.5 1.0 logψ 1 logψ 2 logψ 3 n = 500 Innovation vars. −1.0 −0.5 0.0 0.5 1.0 1.5 −1.0 −0.5 0.0 0.5 1.0 φ12 φ13 φ23 n = 500 Autor"
186,unknown,Parameters (true vs. estimated) −0.5 0.5 1.0 1.5 2.0 2.5 −1.0 −0.5 0.0 0.5 1.0 µ1 µ2 µ3 n = 5000 x Parameters (true vs. estimated) −3.5 −2.5 −1.5 −0.5 −1.0 −0.5 0.0 0.5 1.0 logψ 1 logψ 2 logψ 3 n = 5000 x −1.0 −0.5 0.0 0.5 1.0 1.5 −1.0 −0.5 0.0 0.5 1.0 φ12 φ13 φ23 n = 5000 x Figure 1: Eﬀects for the three means (left) and modiﬁed Cholesky parameters (center and right) estimated from datasets of
187,unknown,"size n = 500 (top) and 5000 (bottom). Credible intervals obtained through MCMC sampling (Fig. 3) are indicated by color shading. The true dependencies (Eq. 20) are depicted by solid black lines. in a linear or nonlinear way (Sec. 5.1). Using a ﬂexible regression model (Sec. 5.2) with additive spline- based predictors (i.e., capable of capturing the true eﬀects) it is investigated how quickly recov"
188,unknown,"distributional parameters improves as the sample size increases (5.3). These results are supplemented in Ap- pendix C by investigating model misspeciﬁcations and eﬀects of increasing the dimension of the multivariate response variable. 5.1. Data generation Data sets are constructed by simulating nvalues of xfrom a uniform distribution on the interval (−1,1). Then for each value of x a 3-dimensiona"
189,unknown,"distribution whose parameters depend on x. For each type of parameter a mixture of constant, linear and quadratic dependencies is used. The exact equations are given below and visualized by solid black lines in Fig. 1 along with corresponding estimated dependencies for two simulated data sets of n = 500 and n= 5000, respectively. µ1 = 1 log( ψ1) = −2 φ12 = (1 + x2)/4 µ2 = 1 + x log(ψ2) = −2 + x φ "
190,unknown,"µ2 = 1 + x log(ψ2) = −2 + x φ 13 = 0 µ3 = 1 + x2 log(ψ3) = −2 + x2 φ23 = (3 + x)/4. (20) While Fig. 1 emphasizes the dependency of the distributional parameters (means, innovation variances, and autoregressive parameters) on the covariatex, Fig. 2 brings out how the corresponding means, variances, and correlations (see Eq. 14) relate across the components of the responsey. Three setups are shown, "
191,unknown,"when computing the parameters for x= −1, 0, and 1, respectively. 8 µ1 µ2 µ3 0 1 2 x = −1 Means µ1 µ2 µ3 x = 0 µ1 µ2 µ3 x = 1 σ1 2 σ2 2 σ3 2 0 0.5 1 Variances σ1 2 σ2 2 σ3 2 σ1 2 σ2 2 σ3 2 0.64 ρ12 0.15 ρ13 0.23 ρ23 Correlations 1 2 3 3 2 1 0.24 ρ12 0.15 ρ13 0.61 ρ23 1 2 3 3 2 1 0.29 ρ12 0.21 ρ13 0.72 ρ23 1 2 3 3 2 1 Figure 2: The means, variances and correlations of the simulated trivariate Gaussi"
192,unknown,"(center), and 1 (right). The particular choices for the model speciﬁcation in Eq. 20 are made so that the corresponding covariance matrix is of ﬁrst-order antedependence (AD-1) type. Speciﬁcally, the ﬁrst variance – that is always equal to the ﬁrst innovation variance – is kept constant (independent of x) at σ2 1 = ψ1 = exp(−2) ≈0.14. Similarly, a constant φ13 = 0 is used so that the ﬁrst and thir"
193,unknown,"AD-1). As shown in Fig. 2, this does not result in a zero correlation ρ13, but rather one determined by the remaining correlations, i.e., ρ13 = ρ12 ·ρ23. 5.2. Regression model speciﬁcation Multivariate Gaussian regression models employing the modiﬁed Cholesky parameterization are used to estimate the distribution of y conditionally on x. The three means and six modiﬁed Cholesky parameters are all "
194,unknown,are all modeled by thin-plate splines sj(x) each composed of 10 basis functions: µ1 = s1(x) log( ψ1) = s4(x) φ12 = s7(x) µ2 = s2(x) log( ψ2) = s5(x) φ13 = s8(x) µ3 = s3(x) log( ψ3) = s6(x) φ23 = s9(x). (21) Bayesian MCMC estimation is employed via the IWLS-based Metropolis-Hastings algorithm (see Sec. 3). Convergence can be checked using trace and autocorrelation plots for the regression coeﬃcient
195,unknown,the spline basis functions (see Sec. 3). Fig. 3 shows these diagnostics for the same simulated data set with n= 5000 also employed in Fig. 1. Credible intervals for distributional parameters derived from the MCMC samples are used to test whether or not a predicted eﬀect is signiﬁcant. Overﬁtting of the complex model is avoided by choosing prior distributions for the smoothing variances that enforc
196,unknown,9 0 200 400 600 800 1000 0.98 0.99 1.00 1.01 Iterations Intercept samples 0 5 10 15 20 25 30 0.0 0.2 0.4 0.6 0.8 1.0 Lag ACF ACF of intercept samples Figure 3: Trace plots of the MCMC samples (left) and autocorrelation (right) for the intercept of splines1 modeling µ1 (Eq. 21). 0.00 0.04 0.08 0.12 100 1000 10000 µ1 (constant) µ2 (linear) µ3 (quadratic) RMSE of parameter estimates n Means 0.00 0.02
197,unknown,"rmse[, ""innov1""] 100 1000 10000 ψ 1 (constant) ψ 2 (linear) ψ 3 (quadratic) n Innovation vars. 0.00 0.05 0.10 0.15 0.20 rmse[, ""phi12""] 100 1000 10000 φ12 (quadratic) φ13 (constant) φ23 (linear) n Autoregressive pars. Figure 4: Root-mean-square error (RMSE) of estimates for mean components (left), innovation variances (center), and gener- alized autoregressive parameters (right) for increasing sam"
198,unknown,linear or quadratic) are included in parenthesis. 5.3. Results Fig. 1 already conveys that the true distributional parameters from Eq. 20 are recovered well by the regression model from Eq. 21. The 95% credible intervals from the MCMC simulations almost always contain the true values and become much more narrow as the sample size is increased from n= 500 to 5000. Estimates of the mean parameters a
199,unknown,"estimates of the covariance parameters. However, the results in Fig. 1 are based on only a single draw for each of the considered sample sizes. To investigate the increasing predictive skill more thoroughly, we consider 100 replications for each n = 100,500,1000,5000,10000 and assess the root-mean-squared errors (RMSE) between the true distributional parameters and their corresponding estimates (s"
200,unknown,"at 10000 randomly sampled x from the interval (−1,1). For all distributional parameters the RMSE decreases with increasing sample size n. Also, RMSE increases with increasing complexity of the dependency on x. For a given parameter type (i.e., µ, ψ, or φ) constant parameters are associated with the lowest RMSE, followed by those with linear and quadratic dependencies. 10 In addition to these intui"
201,unknown,"also hold for higher-dimensional responses. Moreover, it is shown there that the ﬂexible spline speciﬁcations are not very costly in terms of predictive performance. More parsimonious linear speciﬁcations perform only slightly better even when the true eﬀects are linear, but considerably worse when misspeciﬁed. Finally, to show that the multivariate Gaussian regression model can not only deal with"
202,unknown,"Sec. 6 presents a 10-dimensional weather forecasting problem with 21 covariates. 6. Application to probabilistic weather forecasting To illustrate multivariate Gaussian regression in practice, the following multivariate weather forecasting problem is considered: predicting the temperature for ten future time points, so-called lead times, simulta- neously. For reliable and meteorologically consiste"
203,unknown,"the marginal distribution of temperature for each individual time point, but also the joint distribution of temperature across all times of interest. This section provides information on numerical weather predic- tions (Sec. 6.1), the data construction (Sec. 6.2), multivariate Gaussian regression speciﬁcation (Sec. 6.3), estimated eﬀects and covariance predictions (Sec. 6.4), and evaluation of the"
204,unknown,More general discussions and comparisons follow in Sec. 7. 6.1. Background Numerical weather prediction (NWP) models predict the future state of the atmosphere at multiple lead times by numerically integrating the governing physical diﬀerential equations. The numerical integration begins with a best guess of the current state of the atmosphere obtained from in-situ and remote observations around t
205,unknown,"that is several kilometers wide horizontally and several hundred meters thick vertically (Bauer et al., 2015). To account for errors from the initialization and unresolved processes due to the discretization, typically an ensemble of NWP forecasts is generated using slightly diﬀerent approximations (Bauer et al., 2015; Leutbecher and Palmer, 2008). In a ﬁnal postprocessing step, statistical regres"
206,unknown,"linking actual weather observations to output from the NWP ensemble outputs in order to improve forecast accuracy and better calibrate the uncertainty of the predictions (Gneiting et al., 2007). Distributional regression has become a popular method to postprocess NWP ensembles, but its state of the art is limited to univariate (following the seminal work of Gneiting et al., 2005) or bivariate resp"
207,unknown,"(Pinson, 2012; Schuhen et al., 2012; Lang et al., 2019). However, some meteorological applications require higher-dimensional joint probability forecasts across several quantities, locations, or lead times (Feldmann et al., 2015; Worsnop et al., 2018; Schoenach et al., 2020). Typically, the prediction errors in such multi- variate forecasting problems are correlated, but rather than estimating thi"
208,unknown,"the distributional regression model, it is usually reconstructed from the empirical NWP ensemble or from historical observations (e.g., Schefzik et al., 2013). Here, we leverage the novel Cholesky-based multivariate Gaussian regression model to predict a full ten-dimensional temperature distribution based on covariates from an NWP ensemble. 6.2. Data Two-meter temperature forecasts from the Global"
209,unknown,"for Innsbruck, Austria, are postprocessed simultaneously for ten lead times between 186 hours (+7.75 days) and 240 hours (+10 days). The 11 NWP ensemble members of the GEFS have spatial resolutions of approximately 70 km and temporal resolutions of 6 hours. The forecasts are initialized at 00 UTC of 1798 distinct dates over 5 years and bilinearly interpolated to the spatial coordinates of Innsbruc"
210,unknown,"distinct dates over 5 years and bilinearly interpolated to the spatial coordinates of Innsbruck. Following Gneiting et al. (2005) and Gebetsberger et al. (2019) the means meani and log-transformed standard deviations logsdi of the GEFS ensemble members for each leadtime i are used as covariates for the statistical postprocessing model. Additionally, the day of the year yday of the GEFS run initial"
211,unknown,is included to account for seasonal variations in the postprocessing. See Tab. 2 for an overview. 11 Variable Description meani Ensemble mean temperature forecast for lead time i logsdi Logarithm of ensemble standard deviation for lead time i yday Day of year (to capture seasonal variations) Table 2: Covariates used as predictor variables to model multivariate Gaussian parameters for postprocessin
212,unknown,"placeholder i stands for one of ten lead times (+7.75d, +8d, ... , +10d). Model name Section No. of covariance parameters Flexible Intercept Zero Basic Cholesky 6.3.1 55 0 0 Modiﬁed Cholesky 6.3.1 55 0 0 Basic Cholesky AD5 6.3.2 45 0 10 Modiﬁed Cholesky AD5 6.3.2 45 0 10 AR1 6.3.3 11 0 44 Constant correlation 6.3.3 10 45 0 Table 3: A 10-dimensional error covariance matrix Σ has 55 degrees of freed"
213,unknown,"ferent parameterizations of Σ. Depending on the parameterization, parameters may either be modeled on predictors, estimated as intercepts or are restricted to zero a priori. Each row of the resulting dataset is associated with a single initialization of the GEFS on one speciﬁc date. There are 10 lead times of interest, so it contains 10 ensemble means and 10 log-transformed standard deviations of "
214,unknown,"sponding observed temperatures obsi from the weather station at Innsbruck Airport. Since the lead times are spaced 6 hours apart and there is a model initialization for every day, some observations appear in the response vector for multiple initializations but as diﬀerent components. This is not the case for predictors because NWP forecasts always change from one model initialization to the next. "
215,unknown,"6.3. Model speciﬁcations The observed temperature obs at Innsbruck is modeled for ten sequential lead times – every 6 hours between 7.75 and 10 days in the future – by a 10-dimensional Gaussian distribution y= (obs+7.75d,..., obs+10d)⊤∼N(µ,Σ). (22) Distributional parameters of Nare linked to ﬂexible additive predictors containing meani, logsdi and yday. In all regressions, the ten mean parameters "
216,unknown,"µi = s0,i(yday) + s1,i(yday) ·meani. (23) These are linear models of the ensemble mean forecasts, but with seasonally varying intercepts and slopes estimated by nonlinear cyclical splines s0,i and s1,i, respectively. The regressions diﬀer in how the covariance matrix is parameterized – based on its Cholesky or variance-correlation decomposition – and subsequently how ﬂexibly it may be modeled (Tab"
217,unknown,"how ﬂexibly it may be modeled (Tab. 3). 6.3.1. Cholesky parameterizations with fully ﬂexible Σ Both the basic and modiﬁed Cholesky parameterizations permit all 55 covariance-specifying parameters to be linked to covariates. The modiﬁed Cholesky parameterization employs the following setup: log(ψi) = g0,i(yday) + g1,i(yday) ·logsdi φij = hij(yday). (24) Again, g0,i and g1,i are nonlinear cyclical f"
218,unknown,"the log-transformed ensemble standard deviations with the innovation variances ψi. Seasonal variations are permitted for the generalized autoregressive parameters φij and approximated by cyclical splines hij. 12 The corresponding basic Cholesky parameterization employs the analogous setup, simply replacing the innovation variances ψi with λii and the generalized autoregressive parameters φij with "
219,unknown,"6.3.2. Cholesky parameterizations with assumed structure for Σ Motivated by a seasonal autoregressive model, an antedependence model of order 5 (AD-5) may be adopted for the covariance structure. Namely, combining autocorrelations for lag 1 (previous lead time, 6 hours ago) and lag 4 (previous day, 24 hours ago) in a multiplicative way would lead to autocorrelations up to lag 5 and these are captu"
220,unknown,"autoregressive parameters with lags at most 5 are modeled for the covariance as in Eq. 24 and higher lag parameters are ﬁxed at zero. This model is referred to as modiﬁed Cholesky AD5 : φij = { hij(yday), if j−i≤5 0, if j−i> 5 (25) Assuming a covariance of type AD-r has the advantage that the number of covariance parameters increases linearly with the dimension k rather than quadratically, as with"
221,unknown,"linearly with the dimension k rather than quadratically, as with unstructured covariances. For the corresponding basic Cholesky AD5 parameterization, the ψi are again replaced by λii and the φij with λij. 6.3.3. Reference methods using a variance-correlation parameterization The Cholesky-based multivariate Gaussian regression models are compared to two reference methods which parameterize Σ throug"
222,unknown,"standard deviations are linked to the same additive predictors as were the diagonal elements of the basic and modiﬁed Cholesky decompositions. Again a log-link is required to ensure the estimated parameters are positive: log(σi) = g0,i(yday) + g1,i(yday) ·logsdi. (26) The problem with a variance-correlation parameterization is that positive deﬁniteness is not generally guaranteed when linking all "
223,unknown,"where the correlation matrix P is assumed to conditionally follow a ﬁrst order autoregressive structure. P =   1 ρ ρ 2 ··· ρ9 ρ 1 ρ ··· ρ8 ρ2 ρ 1 ··· ρ7 ... ... ... ... ... ρ9 ρ8 ρ7 ··· 1   . (27) As a result, P is determined by one single parameter ρ instead of k·(k−1)/2 correlations. Σ is positive deﬁnite if |ρ|< 1, which allows us to model seasonally varying P, but comes at the co"
224,unknown,"assumptions about the covariance across the k = 10 lead times. This model is referred to as AR1 and can be denoted by r(ρ) = h(yday), (28) where r = ρ/ √ 1 −ρ2 is the link function mapping the range of the parameter ( −1,1) to the unrestricted predictor as for the correlation in the bivariate Gaussian regression of Klein et al. (2015a). Finally, we compare the Cholesky-based parameterizations agai"
225,unknown,"stant correlation for each element i, j. In terms of the distributional regression model this means that every correlation parameter is modeled as an intercept only. r(ρij) = interceptij. (29) Thus, unlike all previous speciﬁcations considered above, the correlation structure remains ﬁxed and does not change across the days of the year. 13 +8d +8.5d 60 150 240 330 0 10 20 s0,iMeans (intercept) +8d"
226,unknown,"+8d +8.5d 60 150 240 330 0 1 2 3 g0,iInnovation vars. (intercept) +8d, +8.25d +8.5d, +8.75d 60 150 240 330 0.5 1.0 hi,jAutoregressive pars. (6h lag) yday 60 150 240 330 0.0 0.5 +8d +8.5d s1,iMeans (slope) yday +8d +8.5d 60 150 240 330 −0.5 0.0 0.5 1.0 g1,iInnovation vars. (slope) yday +8d, +9d +8.5d, +9.5d 60 150 240 330 0.0 0.5 hi,jAutoregressive pars. (24h lag) Figure 5: Selected nonlinear eﬀect"
227,unknown,"obtained from MCMC sampling. Upper row: Seasonally-varying intercepts for the means (left) and log-variances (center) and autoregressive parameters for lag 1 (6h, right). Lower row: Seasonally-varying slopes for the means (left) and log-variances (center) and autoregressive parameters for lag 4 (24h, right). Red colors indicate forecasts valid for daytime, blue ones for nighttime. 6.4. Estimated e"
228,unknown,"nighttime. 6.4. Estimated eﬀects and predictions To highlight the ﬂexibility of the Cholesky-based regression models, a selection of the nonlinear eﬀects estimated by the modiﬁed Cholesky model are presented in Fig. 5. The functions s0,i and s1,i in Eqs. 23– 24 can be thought of as seasonally varying intercepts and slopes in linear models relating ensemble and distributional means. Slopes s1,i are"
229,unknown,"(where the ensemble means/variances would directly correspond to the observed means/variances). This means that the GEFS forecast meani contains limited additional information about the true temperature compared to that inherent inyday. Therefore intercepts s0,i begin to approximate a temperature climatology, with summer maxima approximately 15 degrees higher than winter minima (Fig. 5). For the i"
230,unknown,"and slopes in linear models. This time though, they relate the log-transformed standard deviations of the ensemble logsdi to the log-transformed innovation variances ψi. Since the GEFS means did not contain much information about the distributional means, it comes as no surprise that logsdi are even less valuable predictors. The slopes g1,i average close to zero throughout the year. Intercepts g0,"
231,unknown,"variations for nighttimes (blue) but not for daytime (red). The eﬀects hij = φij can be directly interpreted as seasonal variations of the generalized autoregressive parameters and paint a complex picture. For some combinations of iand j the estimated seasonal variations are signiﬁcant and for others they are not, with no simple dependency on lag j−i or index i. Once all s⋆, g⋆ and h⋆ have been es"
232,unknown,"covariance ˆΣ can be computed from the NWP-derived variables yday, mean⋆ and logsd⋆. Fig. 6 visualizes forecasts for two days in 2015: one in winter (top) and one in fall (bottom). In the left panels, simulated temperature vectors across all ten lead times are shown in gray along with the actual observations in black. The mean pattern is approximated reasonably well, albeit with relatively large v"
233,unknown,14 Model name Runtime for cross validation 1 2 3 4 5 Basic Cholesky 118 100 90 75 89 Modiﬁed Cholesky 118 95 166 96 118 Basic Cholesky AD5 75 79 72 61 66 Modiﬁed Cholesky AD5 102 82 74 73 83 AR1 23 23 25 25 20 Constant correlation 149 152 151 147 151 Table 4: Runtime (in minutes) of 10-dimensional Gaussian regression models for temperature forecasting. 0 5 10 +8d +9d +10d Simulated vectors 2015−01
234,unknown,"Temperature (°C) 0.0 0.2 0.4 0.6 0.8 1.0 Correlation P^ 2015−01−03 +8d +9d +10d +10d +9d +8d 5 10 15 Lead time (days) obs +8d +9d +10d Simulated vectors 2015−10−10 Temperature (°C) Lead time (days) 0.0 0.2 0.4 0.6 0.8 1.0 Correlation P^ 2015−10−10 +8d +9d +10d +10d +9d +8d Figure 6: Predictions from the modiﬁed Cholesky model for µ and Σ given values for yday, mean, and logsd for two speciﬁc"
235,unknown,"days: 2015-01-03 (in winter, top) and 2015-10-10 (in fall, bottom). Left: Vectors containing forecast scenarios for the ten lead times are depicted by thin grey lines. These are simulated from the predicted 10-dimensional Gaussian distributions. The means of these distributions are shown as dashed black lines. The true observations are thick black circles connected by lines. Right: Heat maps depic"
236,unknown,"Right: Heat maps depicting the corresponding estimated correlation matrices ˆP. times. The estimated correlation matrices ˆP for the two days are included in the right panels. Clearly the correlation is not constant throughout the year – as assumed in theconstant correlation model – but diﬀers substantially between winter and fall. For one, correlations are generally higher in winter, but"
237,unknown,"the pattern of correlations is also much more complex in fall. In winter a ﬁrst-order autoregressive process – as assumed in the AR1 model – might ﬁt reasonably well. However, in the fall, this is not the case and instead there are large diurnal variations in correlations for a given lag. For example, forecast errors at 6 UTC in the morning (e.g., +8.25d, +9.25d) have little inﬂuence on the subseq"
238,unknown,"This is not the case in wintertime, where correlations are less variable for a given lag. 15 AR1 Modified Cholesky AD5 Basic Cholesky AD5 Modified Cholesky Basic Cholesky −2 −1 0 1 DSS compared to constant correlation model Worse Better Figure 7: Diﬀerences in Dawid-Sebastiani Score (DSS) to the reference constant correlation model, aggregated by year and month. Positive values mean the given mode"
239,unknown,"not the entire boxplot for AR1 is shown (minimum value of −4.6). 6.5. Model performance It is evident that Cholesky-based regressions allow Σ to be modeled ﬂexibly based on the additive predictors. Another question is whether this increased ﬂexibility improves the quality of the postprocessed joint probability forecasts. As the true distributions are unknown, the quality of the predicted distribut"
240,unknown,"is evaluated using the Dawid-Sebastiani score (DSS, Dawid and Sebastiani, 1999; Gneiting and Raftery, 2007). The DSS is a popular multivariate score in postprocessing and linearly related to the log-likelihood of the predicted distributional parameters for a given observation vector. Scores are evaluated out of sample using ﬁve-fold cross-validation. Scores for each method are aggregated by year a"
241,unknown,ence constant correlation model (Fig. 7). All Cholesky models perform better than the constant correlation model (vertical line at zero) and much better than the AR1 model. The models employing the basic pa- rameterization (basic Cholesky and basic Cholesky AD5 ) are better than the constant correlation model in 75% of months. The modiﬁed Cholesky models are comparable to the corresponding basic C
242,unknown,"only very slightly worse. 7. Discussion The results from the Cholesky-based multivariate Gaussian regression are discussed further here, in par- ticular regarding the suitability of the novel method for postprocessing multivariate NWP forecasts (Sec. 7.1), its sensitivity to ordering of the response vector (Sec. 7.2), and practical limitations to its application along with potential remedies (Sec."
243,unknown,"with potential remedies (Sec. 7.3). 7.1. Perspectives for multivariate NWP postprocessing In state of the art NWP postprocessing joint probability forecasts typically do not take the form of joint probability density functions, but are rather ensembles obtained through ensemble copula coupling (ECC, Schefzik et al., 2013). In ECC the margins of an NWP ensemble are calibrated through univariate pos"
244,unknown,"postprocessing while retaining the ensemble’s order statistics. In our application, ECC performed much worse than all other models according to the DSS (Fig. 7, ECC median diﬀerence of 6 compared to the constant correlation model and not shown). According to another popular multivariate score – the variogram score – ECC again performed worse than all other postprocessing methods (Fig. 8, ECC not s"
245,unknown,"methods according to the DSS, the variogram scores show no signiﬁcant improvement compared to the constant correlation model. This may be due to the variogram score being much more sensitive to the mean and variance than to potential misspeciﬁcations of the correlations (Lang et al., 2019). The poor performance of ECC is likely due to the overall poor predictive skill for GEFS forecasts more"
246,unknown,"than a week in advance. ECC may perform more favorably at shorter lead times, but even here there is a limit to how well tens of ensemble members can possibly capture multivariate dependencies with dimensions of the same order. Additionally it is quite a strong assumption that the ensemble order statistics should reﬂect error dependencies across the postprocessed univariate forecasts. Cholesky-bas"
247,unknown,"16 AR1 Modified Cholesky AD5 Basic Cholesky AD5 Modified Cholesky Basic Cholesky −5 0 5 Variogram skill score (%) compared to constant correlation model Worse Better Figure 8: Variogram skill score (%) relative to the reference constant correlation model, based on scores aggregated by year and month. Positive values mean the given model outperforms the reference. Reordering 5 Reordering 4 Reorderi"
248,unknown,"Reordering 3 Reordering 2 Reordering 1 Basic Cholesky DSS compared to constant correlation model −1 0 1Worse Better Figure 9: Diﬀerences in Dawid-Sebastiani Score (DSS) to constant correlation as in Fig. 7. The ﬁve reorderings have the same model setup as the basic Cholesky, but are estimated after random permutations of the variable order. regression do not rely on these assumptions and can also "
249,unknown,"ensemble) is available. 7.2. Sensitivity to ordering of the response A known limitation of the modiﬁed Cholesky decomposition for ﬁxed covariance estimation is that an ordering of the response components needs to be available or be assumed (Pourahmadi, 2013). Many regular- ization techniques impose an assumed structure on the parameters which would be changed by rearranging the components. Somewha"
250,unknown,"are quite insensitive to random permutations of the variables (Fig. 9). One probable explanation for this is that the individual regression equations for all distributional parameters are regularized separately, while in the ﬁxed covariance estimation of Pourahmadi (2013) the ordering is explicitly exploited for imposing restrictions on the parameters. 7.3. Future work Model complexity is still ma"
251,unknown,"tributional parameters and more than 500 model parameters to estimate from data with a sample size of n= 1798 with runtimes on the order of an hour or two (Tab. 4). A fully ﬂexible parameterization becomes computationally demanding long before k = 100, where 5150 distributional parameters would need to be modeled. For very large k it is also not suﬃcient to reduce complexity just by assuming Σ is "
252,unknown,"modeled. For very large k it is also not suﬃcient to reduce complexity just by assuming Σ is AD- r. When there is a natural order to the variables, very parsimonious covariance parameterizations can be obtained by enforcing structure among the Cholesky parameters. Pourahmadi (1999) for example assumes polynomial dependencies among the innovation variances and autoregressive parameters. Σ is then s"
253,unknown,"quently deﬁned through the coeﬃcients of these polynomials. The polynomial coeﬃcents could be modeled on predictors in place of the Cholesky parameters. Alternatively, smooth nonlinear functions may be used to approximate the parameter structure. Reparameterizations of this sort would extend the applicability of multivariate Gaussian regression to much higher dimensions. 17 8. Conclusions We have "
254,unknown,"We have developed regression models for a multivariate Gaussian response, where all distributional parameters may be linked to ﬂexible additive predictors. Modeling the mean components of the multivariate dependent variable in such cases is no diﬀerent from the univariate case, but it becomes diﬃcult to ensure the covariance matrix is positive deﬁnite for dimensions greater than two. Common parame"
255,unknown,"as variances and a correlation matrix require joint constraints among parameters to guarantee this property. Such constraints are diﬃcult to ensure in the context of a regression. These challenges are addressed by adopting a parameterization of Σ based on its basic and modiﬁed Cholesky decomposition, respectively. These parameterizations are unconstrained, ensuring positive deﬁnite Σ for any predi"
256,unknown,"covariance – may be ﬂexibly modeled. The ability to model k ·(k + 3)/2 distributional parameters comes at the cost of high complexity. Regression models can be regularized through penalized likelihood maximization (frequentist approach) or by choosing appropriate prior distributions (Bayesian). Furthermore, when the dependent response variable has a natural order, the degrees of freedom of the cov"
257,unknown,"maximum lag for dependencies among the response components. The triangular matrices in the basic and modiﬁed Cholesky parameterizations of such a covariance are banded. Subsequently, a large class of parsimonious covariance matrices may be modeled through a priori restrictions on the parameter space– setting parameters to zero a priori. This limits model complexity by decreasing the number of dist"
258,unknown,"parameters that are linked to predictors. Appendix A. Basic Cholesky parameterization The log-likelihood of the distributional parameters for an observation vector y is given by ℓ(µ,L−1|y) = −k 2 log(2π) + log(|L−1|) −1 2(y−µ)⊤(L−1)⊤L−1(y−µ). (A.1) In terms of the individual matrix entries Eq. A.1 can be expressed as ℓ(µ,L−1|y) = −k 2 log(2π) + k∑ i=1 log λii −1/2z⊤z, (A.2) where z is the vector z"
259,unknown,"z= L−1 ˜y=   λ11 0 0 ··· 0 λ12 λ22 0 ··· 0 λ13 λ23 λ33 ··· 0 ... ... ... ... ... λ1k λ2k λ3k ··· λkk     ˜y1 ˜y2 ˜y3 ... ˜yk   (A.3) and ˜y= y−µ. The mean parameters and oﬀ-diagonal Cholesky entries only inﬂuence the log-likelihood through this third term containing z. Partial derivatives with respect to the means are given by ∂ℓ ∂µi = ∂ℓ ∂ηµ,i = k∑ j=1 ςij˜yj, (A.4) wh"
260,unknown,"∂ηµ,i = k∑ j=1 ςij˜yj, (A.4) where ςij refers to the corresponding element of Σ −1 = (L−1)⊤L−1. For the oﬀ-diagonal Cholesky entries, ∂ℓ ∂ηλ,ij = ∂ℓ ∂λij = −1 2 k∑ n=1 [ 2 ( n∑ m=1 (˜ymλmn) ) ˜yi1 j(n) ] = −˜yi j∑ m=1 (˜ymλmj) . (A.5) 18 Derivatives with respect to the diagonal entries of L−1 also involve the second likelihood term and are given by ∂ℓ ∂λii = 1 λii −˜yi i∑ m=1 (˜ymλmi) . (A.6) The "
261,unknown,"The log-link on λii means ∂λii/∂ηλ,ii = λii, and so ∂ℓ ∂ηλ,ii = 1 −λii˜yi i∑ m=1 (˜ymλmi). (A.7) Second derivatives for parameters with identity link are found to be ∂2ℓ ∂η2 µ,i = −ςii = − k∑ j=i λ2 ij, (A.8) ∂2ℓ ∂η2 λ,ij = −˜y2 i. (A.9) The log-link on diagonal entries results in ∂2ℓ ∂η2 λ,ii = ∂ ∂ηλ,ii [1 −λii˜yi i∑ m=1 (˜ymλmi)] = ∂ ∂ηλ,ii [1 −λ2 ii˜y2 i] − ∂ ∂ηλ,ii [λii˜yi i−1∑ m=1 (˜ymλmi)] ="
262,unknown,"= −2λii · ∂λii ∂ηλ,ii ·˜y2 i − ∂λii ∂ηλ,ii ·˜yi i−1∑ m=1 (˜ymλmi) = −2λ2 ii˜y2 i −λii˜yi · i−1∑ m=1 (˜ymλmi). (A.10) Appendix B. Modiﬁed Cholesky parameterization The modiﬁed Cholesky parameters are related to the basic parameters by L−1 = D−1/2T which implies λii = ψ−1/2 i , λij = −φij ·ψ−1/2 j . (B.1) The log-likelihood in Eq. A.2 can be rewritten with respect to the new parameters: ℓ(µ,ψ,φ |y) "
263,unknown,"ℓ(µ,ψ,φ |y) = −k 2 log(2π) −1 2 k∑ i=1 log ψi −1 2 k∑ j=1 ( j∑ i=1 ( ˜yiφijψ−1/2 j ))2 . (B.2) For notational simplicity we deﬁne φii = −1. The partial derivatives of the log-likelihood with respect to µ, λij and λii (Eqs. A.4, A.5, A.6) can be related to derivatives with respect to the modiﬁed Cholesky parameters using Eq. B.1: ∂ℓ ∂µ = T⊤D−1T˜y, (B.3) ∂ℓ ∂φij = ∂ℓ ∂λij ·∂λij ∂φij (B.4) 19 and ∂ℓ "
264,unknown,"∂ℓ ∂ψi = ∂ℓ ∂λii ·∂λii ∂ψi + i−1∑ m=1 ( ∂ℓ ∂λmi ·∂λmi ∂ψi ) . (B.5) Subsequently ∂λij ∂φij = −ψ−1/2 j (B.6) and ∂λii ∂ψi = −1 2ψ−3/2 i , ∂λij ∂ψj = 1 2φijψ−3/2 j . (B.7) Substituting the partial derivatives of ℓ with respect to λii and λij in the basic Cholesky parameterization, one obtains ∂ℓ ∂φij = −˜yiψ−1 j j∑ i=1 ˜yiφij (B.8) and ∂ℓ ∂ψi = −1 2 [ 1 ψi + ˜yi i∑ m=1 ( ˜ymφmiψ−2 i ) ] + 1 2ψ−2 i i"
265,unknown,"i−1∑ m=1 [ ˜ymφmi ( i∑ n=1 ˜ynφni )] (B.9) which simpliﬁes to ∂ℓ ∂ψi = 1 2ψ−2 i   ( i∑ m=1 ˜ymφmi )2 −ψi  . (B.10) Since ψi are estimated using a log-link (log( ψi) = ηψ,i), derivatives with respect to predictors become ∂ℓ ∂ηψ,i = ∂ℓ ∂ψi ψi = 1 2  1 ψi ( i∑ m=1 ˜ymφmi )2 −1  . (B.11) The remaining parameters use an identity link so ∂ℓ/∂ηφ,ij = ∂ℓ/∂φij and ∂ℓ/∂ηµ,i = ∂ℓ/∂µi. Continuing with"
266,unknown,"∂2ℓ ∂η2 φ,ij = −˜y2 i/ψj , ∂2ℓ ∂η2 ψ,i = − 1 2ψi ( i∑ m=1 ˜ymφmi )2 and ∂2ℓ ∂η2 µ,i = −ςii, (B.12) where ςii is the i-th diagonal entry of Σ −1. Appendix C. Additional simulations Here, the simulation study of Sec. 5 is extended to test the inﬂuence of nonlinear eﬀects on misspeciﬁed linear models (Appendix C.1) as well as model peformance in higher dimensions (Appendix C.2). The sample size remai"
267,unknown,sample size remains ﬁxed at n= 5000. Appendix C.1. Model misspeciﬁcations The data generating process of Sec. 5 is modiﬁed to obtain datasets with varying degrees of nonlinearity in the parameters of the response distribution. This is done by multiplying the quadratic terms of x in Eq. 20 with a nonlinearity parameter α: µ1 = 1 log( ψ1) = −2 φ12 = (1 + α·x2)/4 µ2 = 1 + x log(ψ2) = −2 + x φ 13 = 0 
268,unknown,"µ3 = 1 + α·x2 log(ψ3) = −2 + α·x2 φ23 = (3 + x)/4. (C.1) 20 0.000 0.005 0.010 0.015 0.020 0 0.1 0.2 0.3 0.4 0.5 0.6 µ1: spline µ2: spline µ3: spline µ1: linear µ2: linear µ3: linear RMSE of parameter estimates Nonlinearity parameter α Means 0.000 0.004 0.008 rmse[, ""innov1""] 0 0.1 0.2 0.3 0.4 0.5 0.6 ψ 1: spline ψ 2: spline ψ 3: spline ψ 1: linear ψ 2: linear ψ 3: linear Nonlinearity parameter α I"
269,unknown,"0.00 0.01 0.02 0.03 0.04 0.05 rmse[, ""phi12""] 0 0.1 0.2 0.3 0.4 0.5 0.6 φ12: spline φ13: spline φ23: spline φ12: linear φ13: linear φ23: linear Nonlinearity parameter α Autoregressive pars. Figure C.10: The RMSE of distributional parameter estimates obtained using multivariate Gaussian regression with splines (circles, solid lines) or linear models (squares, dashed lines) as a function of the degr"
270,unknown,"Multivariate Gaussian regression is performed using (i) splines for all distributional parameters as in Sec. 5 and (ii) linear models for all distributional parameters. Whenα= 0, all of the true dependencies are constant or linear, which means linear models for the distributional parameters are correctly speciﬁed. When α is increased, though, the linear models for the parameters with quadratic dep"
271,unknown,"φ12) are misspeciﬁed. The dependencies used in the simulation study of Sec. 5 correspond to α= 1. For the case of true linear dependencies (i.e., α= 0), linear predictors for the distributional parameters perform slightly better than using splines (Fig. C.10). However, for larger α linear predictors perform much worse. The RMSE of the misspeciﬁed mean parameter µ3 triples just by increasing α to 0"
272,unknown,"much worse. The RMSE of the misspeciﬁed mean parameter µ3 triples just by increasing α to 0.1. The increase is more gradual for the misspeciﬁed innovation variance ψ3 and even more so for the autoregressive parameter φ12. For small αonly these terms perform poorly in the linear speciﬁcation, but for larger αother terms deteriorate as well. Splines in comparison are much more robust to nonlinearity"
273,unknown,"parameter dependencies. Appendix C.2. Distributional dimension For a trivariate response, multivariate Gaussian regression is already quite complex. It involves modeling nine distributional parameters by one or more predictor variables. This complexity increases quadratically with the dimension. To investigate how an increase in model complexity inﬂuences the predictive skill of multivariate Gauss"
274,unknown,"k= 5, 10 and 15, where there are 20, 65 and 135 distributional parameters, respectively. The constant, linear, and quadratic dependencies for the means in Eq. 20 are repeated so that µi+3 = µi (i.e., µ11 = µ7 = µ4 = µ1). Similarly, for the log-transformed innovation variances log ψi+3 = log ψi. The autoregressive parameters are slightly diﬀerent since they have two indices i and j. In the simulati"
275,unknown,"Sec. 5, only the lag-1 autoregressive parameters φ12 and φ23 depend on x; the higher-lag parameter φ13 is set to 0. This pattern is retained for the higher dimensional simulations so that φij = 0 for all j−i >1. The eﬀects for the lag-1 parameters φ12 and φ23 are repeated, which results in φ(i+2)(i+3) = φi(i+1). The means, variances and correlation matrices of the higher dimensional distributions "
276,unknown,"0 and -1 in Fig. C.11 analagously to Fig. 2. The predictive skill of multivariate Gaussian regression models for individual distributional parameters does not suﬀer when the dimension is increased (Fig. C.12. The RMSEs for the nine parameters of the trivariate distribution (µ1,µ2,µ3,ψ1,ψ2,ψ3,φ12,φ23,φ13) are nearly identical for k = 3, 5, 10 and 15. The same is true for higher dimensional means an"
277,unknown,"generalized autoregressive parameters, the average RMSE is even lower at higher kbecause a larger fraction of the true paramters are constants ( φij = 0 for j−i> 1). 21 µ1 µ3 µ5 µ10 µ15 0 1 2 x = −1 Means µ1 µ3 µ5 µ10 µ15 x = 0 µ1 µ3 µ5 µ10 µ15 x = 1 σ1 2 σ3 2 σ5 2 σ10 2 σ15 2 0 0.5 1 Variances σ1 2 σ3 2 σ5 2 σ10 2 σ15 2 σ1 2 σ3 2 σ5 2 σ10 2 σ15 2 Correlations 1 3 5 10 15 15 10 5 3 1 1 3 5 10 15 1"
278,unknown,"2 σ10 2 σ15 2 Correlations 1 3 5 10 15 15 10 5 3 1 1 3 5 10 15 15 10 5 3 1 1 3 5 10 15 15 10 5 3 1 Figure C.11: As in Fig. 2, but extended to include means, variances and correlation matrices for the simulating distributions at higher dimensions k= 5, 10 and 15. The ﬁrst three means and variances are colored to match Fig. 2. 0.00 0.02 0.04 0.06 3 5 10 15 µ1 (constant) µ2 (linear) µ3 (quadratic) RM"
279,unknown,"RMSE of parameter estimates k Means 0.00 0.02 0.04 0.06 rmse[, ""innov1""] 3 5 10 15 ψ 1 (constant) ψ 2 (linear) ψ 3 (quadratic) k Innovation vars. 0.00 0.02 0.04 0.06 rmse[, ""phi12""] 3 5 10 15 φ12 (quadratic) φ13 (constant) φ23 (linear) k Autoregressive pars. Figure C.12: RMSE of estimates for mean components (left column), innovation variances (center) and generalized autore-"
280,unknown,"gressive parameters (right) for diﬀerent dimensions k. Parameters of the trivariate distribution are colored and their true dependency on x included in parenthesis (i.e., constant, linear or quadratic). Higher dimensional parameters are indicated by grey. 22 Acknowledgements This project was funded by the Austrian Science Fund (FWF, grant no. P 31836). The authors thank the Zentralanstalt f¨ ur Me"
281,unknown,"tional results presented here have been achieved (in part) using the LEO HPC infrastructure of Universit¨ at Innsbruck. References Bauer, P., Thorpe, A., Brunet, G., 2015. The quiet revolution of numerical weather prediction. Nature 525, 47. doi: 10.1038/ nature14956. Bickel, P.J., Levina, E., 2008. Covariance regularization by thresholding. The Annals of Statistics 36, 2577–2604. doi: 10.1214/ 08"
282,unknown,"08-aos600. Burke, K., Jones, M.C., Noufaily, A., 2019. A Flexible Parametric Modelling Framework for Survival Analysis. arXiv 1901.03212. arXiv.org E-Print Archive. URL: https://arxiv.org/abs/1901.03212. Dawid, A.P., Sebastiani, P., 1999. Coherent dispersion criteria for optimal experimental design. The Annals of Statistics 27, 65–81. doi: 10.1214/aos/1018031101. Eilers, P.H.C., Marx, B.D., 1996. "
283,unknown,"doi:10.1214/ss/1038425655. Feldmann, K., Scheuerer, M., Thorarinsdottir, T.L., 2015. Spatial postprocessing of ensemble forecasts for temperature using nonhomogeneous Gaussian regression. Monthly Weather Review 143, 955–971. doi: 10.1175/mwr-d-14-00210.1 . Friedman, J., Hastie, T., Tibshirani, R., 2008. Sparse inverse covariance estimation with the graphical lasso. Biostatistics 9, 432–441. doi: 1"
284,unknown,"432–441. doi: 10.1093/biostatistics/kxm045. Furrer, R., Genton, M.G., Nychka, D., 2006. Covariance tapering for interpolation of large spatial datasets. Journal of Computational and Graphical Statistics 15, 502–523. doi: 10.1198/106186006x132178. Gabriel, K.R., 1962. Ante-dependence analysis of an ordered set of variables. The Annals of Mathematical Statistics 33, 201–212. doi: 10.1214/aoms/117770"
285,unknown,"201–212. doi: 10.1214/aoms/1177704724. Gamerman, D., 1997. Sampling from the posterior distribution in generalized linear mixed models. Statistics and Computing 7, 57–68. doi: 10.1023/a:1018509429360. Gebetsberger, M., Stauﬀer, R., Mayr, G.J., Zeileis, A., 2019. Skewed logistic distribution for statistical temperature post- processing in mountainous areas. Advances in Statistical Climatology, Mete"
286,unknown,"https://ascmo.copernicus.org/articles/5/87/2019/, doi:10.5194/ascmo-5-87-2019 . Gneiting, T., Balabdaoui, F., Raftery, A.E., 2007. Probabilistic forecasts, calibration and sharpness. Journal of the Royal Statistical Society B 69, 243–268. doi: 10.21236/ada454827. Gneiting, T., Raftery, A.E., 2007. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Associ"
287,unknown,"Association 102, 359–378. doi: 10.1198/016214506000001437. Gneiting, T., Raftery, A.E., Westveld III, A.H., Goldman, T., 2005. Calibrated probabilistic forecasting using ensemble model output statistics and minimum CRPS estimation. Monthly Weather Review 133, 1098–1118. doi: 10.1175/mwr2904.1. Groll, A., Hambuckers, J., Kneib, T., Umlauf, N., 2019. Lasso-type penalization in the framework of gener"
288,unknown,"for location, scale and shape. Computational Statistics & Data Analysis 140, 59–74. doi: 10.1016/j.csda.2019.06.005. Hamill, T.M., Bates, G.T., Whitaker, J.S., Murray, D.R., Fiorino, M., Galarneau Jr, T.J., Zhu, Y., Lapenta, W., 2013. NOAA’s second-generation global medium-range ensemble reforecast dataset. Bulletin of the American Meteorological Society 94, 1553–1565. doi: 10.1175/bams-d-12-00014"
289,unknown,"1553–1565. doi: 10.1175/bams-d-12-00014.1 . Hastie, T.J., Tibshirani, R.J., 1990. Generalized Additive Models. volume 43. Chapman & Hall/CRC. Klein, N., Kneib, T., Klasen, S., Lang, S., 2015a. Bayesian structured additive distributional regression for multivariate responses. Journal of the Royal Statistical Society C 64, 569–591. doi: 10.1111/rssc.12090. Klein, N., Kneib, T., Lang, S., 2015b. Baye"
290,unknown,"overdispersed count data. Journal of the American Statistical Association 110, 405–419. doi:10.1080/01621459.2014.912955. Kneib, T., Fahrmeir, L., 2007. A mixed model approach for geoadditive hazard regression. Scandinavian Journal of Statistics 34, 207–228. doi: 10.1111/j.1467-9469.2006.00524.x. K¨ ohler, M., Umlauf, N., Beyerlein, A., Winkler, C., Ziegler, A.G., Greven, S., 2017. Flexible Bayesi"
291,unknown,"with an application to type 1 diabetes research. Biometrical Journal 59, 1144–1165. doi: 10.1002/bimj.201600224. Lang, M.N., Mayr, G.J., Stauﬀer, R., Zeileis, A., 2019. Bivariate Gaussian models for wind vectors in a distributional regression framework. Advances in Statistical Climatology, Meteorology and Oceanography 5, 115–132. doi: 10.5194/ ascmo-5-115-2019 . Leutbecher, M., Palmer, T.N., 2008."
292,unknown,"jcp.2007.02.014. Levina, E., Rothman, A., Zhu, J., 2008. Sparse estimation of large covariance matrices via a nested lasso penalty. The Annals of Applied Statistics 2, 245–263. doi: 10.1214/07-aoas139. Mayr, A., Fenske, N., Hofner, B., Kneib, T., Schmid, M., 2012. Generalized additive models for location, scale and shape for high dimensional data—a ﬂexible approach based on boosting. Journal of th"
293,unknown,"Statistics) 61, 403–427. doi: https://doi.org/10.1111/j.1467-9876.2011.01033.x. 23 Pan, J., Pan, Y., 2017. jmcm: An R package for joint mean-covariance modeling of longitudinal data. Journal of Statistical Software 82, 1–29. doi: 10.18637/jss.v082.i09. Pinson, P., 2012. Adaptive calibration of (u,v)-wind ensemble forecasts. Quarterly Journal of the Royal Meteorological Society 138, 1273–1284. doi:"
294,unknown,"138, 1273–1284. doi: 10.1002/qj.1873. Pourahmadi, M., 1999. Joint mean-covariance models with applications to longitudinal data: Unconstrained parameterisation. Biometrika 86, 677–690. doi: 10.1093/biomet/86.3.677. Pourahmadi, M., 2000. Maximum likelihood estimation of generalised linear models for multivariate normal covariance matrix. Biometrika 87, 425–435. doi: 10.1093/biomet/87.2.425."
295,unknown,"Biometrika 87, 425–435. doi: 10.1093/biomet/87.2.425. Pourahmadi, M., 2013. High-Dimensional Covariance Estimation: With High-Dimensional Data. volume 882. John Wiley & Sons. doi: 10.1002/9781118573617. Rigby, R.A., Stasinopoulos, D.M., 2005. Generalized additive models for location, scale and shape. Journal of the Royal Statistical Society C 54, 507–554. doi: 10.1111/j.1467-9876.2005.00510.x."
296,unknown,"Statistical Society C 54, 507–554. doi: 10.1111/j.1467-9876.2005.00510.x. Schefzik, R., Thorarinsdottir, T.L., Gneiting, T., 2013. Uncertainty quantiﬁcation in complex simulation models using ensemble copula coupling. Statistical Science 28, 616–640. doi: 10.1214/13-sts443. Schoenach, D., Simon, T., Mayr, G.J., 2020. Postprocessing ensemble forecasts of vertical temperature proﬁles. Advances in St"
297,unknown,"Statistical Climatology, Meteorology and Oceanography 6, 45–60. doi: 10.5194/ascmo-6-45-2020 . Schuhen, N., Thorarinsdottir, T.L., Gneiting, T., 2012. Ensemble model output statistics for wind vectors. Monthly Weather Review 140, 3204–3219. doi: 10.1175/mwr-d-12-00028.1 . Simon, T., Mayr, G.J., Umlauf, N., Zeileis, A., 2019. NWP-based lightning prediction using ﬂexible count data regression. Advan"
298,unknown,"Stasinopoulos, M.D., Rigby, R.A., De Bastiani, F., 2018. GAMLSS: A distributional regression approach. Statistical Modelling 18, 248–273. doi: 10.1177/1471082x18759144. Umlauf, N., Klein, N., Simon, T., Zeileis, A., 2021. bamlss: A Lego toolbox for ﬂexible Bayesian regression (and beyond). Journal of Statistical Software 100, 1–55. Forthcoming. Umlauf, N., Klein, N., Zeileis, A., 2018. BAMLSS: Bay"
299,unknown,"Journal of Computational and Graphical Statistics 27, 612–627. doi: 10.1080/10618600.2017.1407325. Umlauf, N., Kneib, T., 2018. A primer on Bayesian distributional regression. Statistical Modelling 18, 219–247. doi: 10.1177/ 1471082X18759140. Wood, S.N., 2003. Thin plate regression splines. Journal of the Royal Statistical Society B 65, 95–114. doi: https://doi.org/ 10.1111/1467-9868.00374."
300,unknown,"10.1111/1467-9868.00374. Wood, S.N., 2017. Generalized Additive Models: An Introduction with R. 2nd ed., Chapman & Hall/CRC, Boca Raton. doi:10.1201/9781315370279. Worsnop, R.P., Scheuerer, M., Hamill, T.M., Lundquist, J.K., 2018. Generating wind power scenarios for probabilistic ramp event prediction using multivariate statistical post-processing. Wind Energy Science 3, 371–393. doi: 10.5194/ wes"
301,unknown,"wes-3-371-2018 . Wu, W.B., Pourahmadi, M., 2003. Nonparametric estimation of large covariance matrices of longitudinal data. Biometrika 90, 831–844. doi: 10.1093/biomet/90.4.831. Zimmerman, D.L., N´ u˜ nez-Ant´ on, V., El-Barmi, H., 1998. Computational aspects of likelihood-based estimation of ﬁrst-order antedependence models. Journal of Statistical Computation and Simulation 60, 67–84. doi: 10.10"
302,unknown,"24 The Cholesky Decomposition of a Toeplitz Matrix and a Wiener-Kolmogorov Filter for Seasonal Adjustment Emi Mise, University of Leicester D.S.G. Pollock, University of Leicester Working Paper No. 20 01 January 2020"
303,unknown,THE CHOLESKY DECOMPOSITION OF A TOEPLITZ MATRIX AND A WIENER–KOLMOGOROV FILTER FOR SEASONAL ADJUSTMENT By Emi MISE and D.S.G. POLLOCK University of Leicester Email: stephen pollock@sigmapi.u-net.com Website: http://www.le.ac.uk/users/dsgp1/ This note describes the use of the Cholesky decomposition in solving the equation Ab = y when A = A′ is a symmetric matrix of full rank. A specialised version 
304,unknown,"A = A′ is a symmetric matrix of full rank. A specialised version of the algorithm is provided for the case where A is a banded Toeplitz matrix, in which each band contains a unique repeated element and where the number of bands is considerably less than the order of the matrix, which is assumed to be large. This circumstance demands that steps should be taken to minimise the use of the computer’s "
305,unknown,"of the computer’s memory. An example is provided of the use of the algorithm in implementing a ﬁnite-sample Wiener–Kolmogorov ﬁlter aimed at removing the seasonal ﬂuctuations from economic data. Introduction The Cholesky decomposition of a symmetric matrix A = A′ of full-rank has numerous applications. Amongst these is the solution of linear equations. If the matrix is also positive deﬁnite, then "
306,unknown,"exists a decomposition in the form of A = LL′, where L is a lower triangular matrix. Such a decomposition usually requires square roots to be taken in calculating the diagonal ele- ments of the matrix L. Since this is a time-consuming operation, it is more eﬃcient to compute a decomposition of the form A = LDL ′, where L is a lower triangular matrix with units on the diagonal and D is a diagonal m"
307,unknown,"This decomposition is more general than the LL′ decomposition, since there is no requirement that the matrix should be positive deﬁnite. If it is not, then one will ﬁnd negative elements amongst the diagonal elements of D. An important use of the Cholesky decomposition is in solving the equation Ab = y when A is symmetric. A further specialisation arises when A is a banded Toeplitz matrix, which h"
308,unknown,"repeated element along each of its diagonals. However, in the process of calculating the decomposition, the Toeplitz structure may be over- looked, sine it cannot be exploited to obtain any extra computational eﬃciency. Extra eﬃciency and the conservation of computer memory are available whenever the matrix has a limited number of nonzero bands, whether or not they contain repeated elements. In so"
309,unknown,"In solving the equation Ab = LDL ′b = y, we deﬁne p = DL ′b. First, the equation Lp = y is solved for p by a process of forward-substitution. Then, the equation L′b = q is solved for b, where q = D−1 p is formed by dividing each element of the vector p by the corresponding element of the diagonal matrix D. This equation is solved by back-substitution. In this note, two versions of the Cholesky pro"
310,unknown,"In this note, two versions of the Cholesky procedure for solving the equation Ab = y will be presented. The ﬁrst version makes no assumption regarding the matrix A other than its symmetry and its full rank. In the second version, A is assumed to be a narrow-band Toeplitz matrix, which may be of a considerable order. In that case, it is important to take care not to waste the storage space of the c"
311,unknown,space of the computer’s memory. This requires mapping the nonzero elements of the matrix into a restricted space by altering their indices. The latter half of the paper concerns an example of the use of this version of the algorithm in eﬀecting the seasonal adjustment of economic data. 1 MISE and POLLOCK: TIME-SERIES ANALYSIS The Standard Cholesky Decomposition The standard LDL ′ decomposition can
312,unknown,symmetric matrix A of order 4: (1)   a00 a10 a20 a30 a10 a11 a21 a31 a20 a21 a22 a32 a30 a31 a32 a33   =   1 0 0 0 l10 1 0 0 l20 l21 1 0 l30 l31 l32 1     d0 0 0 0 0 d1 0 0 0 0 d2 0 0 0 0 d3     1 l10 l20 l30 0 1 l21 l31 0 0 1 l32 0 0 0 1   =   d0 0 0 0 d0 l10 d1 0 0 d0 l20 d1 l21 d2 0 d0 l30 d1 l31 d2 l32 d3     1 l10 l20 l30 0 1 l21 l31 0 0 1 l32 0 0 0 1  . Th
313,unknown,"The objective is to factorise the matrix and, in the process, to create the product (2) P =   d0 0 0 0 l10 d1 0 0 l20 l21 d2 0 0 l31 l32 d3   :   1 ∗ ∗ ∗ 2 3 ∗ ∗ 4 5 6 ∗ 7 8 9 10  , which contains all of the elements of the factorisation. Beside the matrix P is a matrix of numbers and asterisks that indicates a sequence in which the elements can be computed. The computations are indica"
314,unknown,are indicated by the following list: a00 = d0 =⇒ d0 = a00 a10 = d0 l10 =⇒ l10 = a10 /d0 a11 = d0 l2 10 + d1 =⇒ d1 = a11 −d0 l2 10 a20 = d0 l20 =⇒ l20 = a20 /d0 a21 = d0 l20 l10 + d1 l21 =⇒ l21 = ( a21 −d0 l20 l10 )/d1 a22 = d0 l2 20 + d1 l2 21 + d2 =⇒ d2 = a22 −d0 l2 20 −d1 l2 21 a30 = d0 l30 =⇒ l30 = a30 /d0 a31 = d0 l30 l10 + d1 l31 =⇒ l31 = ( a31 −d0 l30 l10 )/d1 a32 = d0 l30 l20 + d1 l31 l21 +
315,unknown,"a33 = d0 l2 30 + d1 l2 31 + d2 l2 32 + d3 =⇒ d3 = a33 −d0 l2 30 −d1 l2 31 −d2 l2 32 {1} {2} {3} {4} {5} {6} {7} {8} {9} {10 } These results can be generalised to the case of a matrix of an arbitrary order n. Consider a generic element of A = [ aij], which is on or below the diagonal of the matrix such that i ≥ j. It is readily conﬁrmed that (4) aij = j∑ k=0 dklikljk , where dk is the kth element o"
316,unknown,"expression for the subdiagonal elements of the ith row of L, and to an expression for the ith element 2 CHOLESKY DECOMPOSITION AND SEASONAL ADJUSTMENT of the diagonal matrix D: (5) lij = 1 dj { aij − i−1∑ k=0 dklikljk } , di = aii − j−1∑ k=0 dkl2 ik. The solution to the equation Ab = LDL ′b = y is by solving the equation Lp = y for p by forward substitution and, thereafter, by solving the equation"
317,unknown,"of the example, these equations are (6)   1 0 0 0 l10 1 0 0 l20 l21 1 0 l30 l31 l32 1     p0 p1 p2 p3   =   y0 y1 y2 y3   and   1 l10 l20 l30 0 1 l21 l31 0 0 1 l32 0 0 0 1     b0 b1 b2 b3   =   q0 = p0 /d0 q1 = p1 /d1 q2 = p2 /d2 q3 = p3 /d3  . Their solutions are (7) p0 = y0 p1 = y1 −l10 p0 p2 = y2 −l20 p0 −l21 p1 p3 = y3 −l30 p0 −l31 p1 −l32 p2 and b3 = q3 b2 "
318,unknown,"and b3 = q3 b2 = q2 −l32 b3 b1 = q1 −l31 b3 −l21 b2 b0 = q0 −l30 b3 −l20 b1 −l10 b1 . The following Pascal procedure uses the above equations in the process of factorising A = LDL ′ and of solving the equation Ab = LDL ′b = y. The complete matrix A is passed to the procedure. It is returned with the subdiagonal elements of L replacing its own subdiagonal elements, and with the elements of D along "
319,unknown,"determinant of the original matrix A by forming the product of the elements of D. The vector y, which is passed to the procedure, becomes, in succession, the vectors p, q and b, all of which occupy the space of y. (8) procedure Cholesky (n : integer ; var a : matrix ; var y : vector ); var i,j,k : integer ; begin for i := 0 to n do for j := 0 to i do begin {i,j } for k := 0 to j −1 do a[i,j ] := a"
320,unknown,"if j <i then begin 3 MISE and POLLOCK: TIME-SERIES ANALYSIS a[i,j ] := a[i,j ]/a[j,j ]; a[j,i ] := 0 .0; end ; end ; {i,j } {Forward Solve } for i := 0 to n do for j := 0 to i −1 do y[i] := y[i] −a[i,j ] ∗y[j]; {Divide by the Diagonal } for i := 0 to n do y[i] := y[i] −a[i,j ]/a[i,i ] {Back Solve } for i := n downto 0 do for j := n downto i + 1 do y[i] := y[i] −a[j,i ] ∗y[j]; end ; {Cholesky } The"
321,unknown,"y[i] := y[i] −a[j,i ] ∗y[j]; end ; {Cholesky } The Decomposition of a Banded Toeplitz Matrix The objective is to create an algorithm for decomposing a symmetric narrow banded Toeplitz matrix. Only the lower triangular nature of the factor L and the limitation in the number of bands need to be taken into account. It helps if, in the ﬁrst instance, the fact is ignored that each of the bands contains"
322,unknown,storage space and its avoidance of calculations that produce zero-valued elements. The potential for conserving memory space can be demonstrated by the case of a matrix factor that has n = 5 rows and only q = 2 subdiagonal bands. The reformatted matrix that eliminates the majority of the zero-valued elements of L will be denoted by M. The rearrangement the elements in order to conserve storage is 
323,unknown,(9) L =   l00 0 0 0 0 l10 l11 0 0 0 l20 l21 l22 0 0 0 l31 l32 l33 0 0 0 l42 l43 l44   −→M =   l00 0 0 l11 l10 0 l22 l21 l20 l33 l32 l31 l44 l43 l42   =   m00 0 0 m10 m11 0 m20 m21 m22 m30 m31 m32 m40 m41 m42   It will be observed that the ordering of the nonzero elements in each of the rows is reversed in passing from L to M and that most of the zero-valued elements a
324,unknown,"from L to M and that most of the zero-valued elements are eliminated. The nonzero element lij within L becomes the element mi,i−j = li,j within M. Conversely, mi,j = li,i−j. The decomposition of the symmetric matrix A = [ αi,i−j] can be represented as follows:   α00 α11 α22 {α33 } α11 α10 α21 α32 α22 α21 α20 α31 {α33 } α32 α31 α30   =   1 0 0 0 m11 1 0 0 m22 m21 1 0 {m33 } m32 m31 1   "
325,unknown,"  d0 0 0 0 0 d1 0 0 0 0 d2 0 0 0 0 d3     1 m11 m22 {m33 } 0 1 m21 m32 0 0 1 m31 0 0 0 1   4 CHOLESKY DECOMPOSITION AND SEASONAL ADJUSTMENT =   d0 0 0 0 d0 m11 d1 0 0 d0 m22 d1 m21 d2 0 {d0 m33 } d1 m32 d2 m31 d3     1 m11 m22 {m33 } 0 1 m21 m32 0 0 1 m31 0 0 0 1  (10) The change of notation whereby aij = αi,i−j has no eﬀect within the context of the matrix A, since it does"
326,unknown,"it does not change the values of the elements. However, the mappping ai,i−j = αij −→ mij, which occurs at the start of the computations, is crucial for the eﬃcient storage of the nonzero elements of the lower triangle of the symmetric matrix. Thus, at the outset, the nonzero elements of the lower triangle of A are stored in the matrix M which will eventually contain all the elements of the decompo"
327,unknown,"decomposition. There are braces surrounding the elements α33 and m33 as a reminder that these may be zero-valued, in which case, there will be q = 2 supradiagonal and subdiagonal bands in the matrix A and q subdiagonal bands in its factor L = [ mi,i−j] . The computations are indicated by the following list: (11) α00 = d0 =⇒ d0 = α00 α11 = d0 m11 =⇒ m11 = α11 /d0 α10 = d0 m2 11 + d1 =⇒ d1 = α10 −d0"
328,unknown,11 α22 = d0 m22 =⇒ m22 = α22 /d0 α21 = d0 m22 m11 + d1 m21 =⇒ m21 = ( α21 −d0 m22 m11 )/d1 α20 = d0 m2 22 + d1 m2 21 + d2 =⇒ d2 = α20 −d0 m2 22 −d1 m2 21 {α33 = d0 m33 =⇒ m33 = α33 /d0 } α32 = {d0 m33 m11 }+ d1 m32 =⇒ m32 = ( α32 −{ d0 m33 m11 )/d1 } α31 = {d0 m33 m22 }+ d1 m32 m21 + d2 m31 =⇒ m31 = ( α31 −{ d0 m33 m22 }− d1 m32 m21 )/d2 α30 = {d0 m2 33 }+ d1 m2 32 + d2 m2 31 + d3 =⇒ d3 = α30 −{ d
329,unknown,"α30 = {d0 m2 33 }+ d1 m2 32 + d2 m2 31 + d3 =⇒ d3 = α30 −{ d0 m2 33 }− d1 m2 32 −d2 m2 31 These results can be generalised to the case of a matrix of an arbitrary order n. Consider a generic element of A = [ αij] which is on or below the diagonal of the matrix such that i ≥ j. It is readily conﬁrmed that (12) αij = p∑ k= r dkmi,i−kmp,p−k, p = i −j, r = Max(0 ,i −q), where mik is an element from M "
330,unknown,"where mik is an element from M and dk = mk0 is the kth element of D. This equation gives rise to a generic expression for the subdiagonal elements of the jth column of M, and to an expression for the ith element of the diagonal matrix D: (13) mij = 1 dj { αij − p−1∑ k= r dkmi,i−kmp,p−k } , di = αi0 − i−1∑ k= r dkm2 i,i−k. In comparing these expressions with those under (4), one should note that th"
331,unknown,"with a negative sign, which is in consequence of the reversal of the order of the elements within the matrix M. The initial value of the index is r ≥0, which diﬀers from 0 when the number of subdiagonal 5 MISE and POLLOCK: TIME-SERIES ANALYSIS bands is q <n , where n is the limiting index. The case were A is a Toeplitz matrix of order n+ 1 and with q supradiagonal and q subdiagonal bands is of pri"
332,unknown,"n = 3 and and q = 2, the matrix in question is (14) A =   γ0 γ1 γ2 0 γ1 γ0 γ1 γ2 γ0 γ1 γ0 γ1 0 γ2 γ1 γ0  , which can be stored as the vector [ γ0 ,γ 1 ,γ 2 ]. These elements are transferred to the matrix M as the calculations proceed by setting mij = γj. Then, the elements mij will serve in place of the elements αij in the computations that are represented by the display under (11) and, even"
333,unknown,become the ﬁnal products that are to be found in the matrix M The solution to the equation Ab = LDL ′b = y is via the following equations (15)  1 0 0 0 m11 1 0 0 m22 m21 1 0 {m33 } m32 m31 1     p0 p1 p2 p3   =   y0 y1 y2 y3   and   1 m11 m22 {m33 } 0 1 m21 m32 0 0 1 m31 0 0 0 1     b0 b1 b2 b3   =   q0 = p0 /d0 q1 = p1 /d1 q2 = p2 /d2 q3 = p3 /d3  . Their solut
334,unknown,(16) p0 = y0 p1 = y1 −m11 p0 p2 = y2 −m22 p0 −m21 p1 p3 = y3 −{ m33 p0 }− m32 p1 −m31 p2 and b3 = q3 b2 = q2 −m31 b3 b1 = q1 −m32 b3 −m21 b2 b0 = q0 −{ m33 b3 }− m22 b2 −m11 b1 . The braces surrounding the element α33 continue to serve as a reminder that this may be zero- valued. The following procedure implements the algorithm that solves the equation Ab = y in the case where A is a full-rank sym
335,unknown,"where A is a full-rank symmetric Toeplitz matrix with q subdiagonal and supradiagdonal bands. (17) procedure Toeplitz (var n,q : integer ; var gamma : vector ; var y : longVector ); var i,j,k,p,r,s : integer ; m : array [0 ..maxArray, 0..maxOrder ] of real ; {longMatrix } begin {Factorise } for i := 0 to n do begin {i} r := Max (0 ,i −q); s := Min (i,q ); for j := s downto 0 do begin {j} 6 CHOLESK"
336,unknown,"p := i −j; m[i,j ] := gamma [j]; for k := r to p −1 do m[i,j ] := m[i,j ] −m[k, 0] ∗m[i,i −k] ∗m[p,p −k]; if j >0 then m[i,j ] := m[i,j ]/m[p, 0]; end ;{j} end ;{i} {Forward Solve } for i := 0 to n do for j := Max (0 ,i −q) to i −1 do y[i] := y[i] −m[i,i −j] ∗y[j]; {Divide by the diagonal } for i := 0 to n do y[i] := y[i]/m[i,0]; {Back Solve } for i := n downto 0 do for j := Min (i + q,n ) downto "
337,unknown,"for j := Min (i + q,n ) downto i + 1 do y[i] := y[i] −m[j,j −i] ∗y[j]; end ;{Toeplitz } In this procedure, various restrictions are imposed on the range of j in consequence of the lim- itation of the number q of supradiagonsl and subdiagonal bands in the matrix A. The ﬁrst of these restrictions applies directly to the matrix M. The number of nonzero elements in its ith row is re- stricted by s = M"
338,unknown,"only when i ≥q. Next is the restriction r = Max (0 ,i −q) on the initial value of the index k. With the index k running from r = i −q to p = i −j, this corresponds to the fact that αij of (10) comprises a full set of q + 1 elements only when j = 0, which is when it is a diagonal element of A. The remaining restrictions aﬀect the processes of forward substitution and back substitution. First, there"
339,unknown,"This is entails the restriction on m[i,i −j] that i−j ≤q; and it accommodates the case where i−q ≤0. Then, within the backwards process, there is the restriction that the initial value of the index of j is Min (i + q,n ). This is entails the restriction on m[j,j −i] that j −i ≤ q; and it accommodates the case where the index i has declined by less than q. A Wiener–Kolmogorov Filter for Eliminating"
340,unknown,"A Wiener–Kolmogorov Filter for Eliminating Seasonal Fluctuations The procedure Toeplitz can be illustrated by considering a Wiener–Kolmogorov ﬁlter for elimi- nating the seasonal ﬂuctuations from an economic data sequence. The business of seasonal adjustment is commonly subsumed under the analysis of unobserved components, whereby a economic data se- quence is expressed as a sum of separate compon"
341,unknown,"A taxonomy of unobserved components has been described in relation to the STAMP computer program by Koopmans et al. (1995). A taxonomy that is appropriate to the purposes of this paper 7 MISE and POLLOCK: TIME-SERIES ANALYSIS expresses the logarithmic data sequence d(t) = {dt = ln( Dt); t = 0 ,±1,±2,... }as (18) d(t) = π(t) + ζ(t) + ξ(t) + η(t). Here, π(t) is a trend function that is liable to be "
342,unknown,"the trend might be a more complicated function that is designed to absorb major structural breaks in the data. (See, for example, Pollock 2016.) The remaining components, which are sequences of zero mean, are ζ(t), which represents a secular cycle or business cycle, ξ(t), which represents the seasonal ﬂuctuations, and η(t), which represents an irregular component. A common recourse it to merge ﬁrs"
343,unknown,"A common recourse it to merge ﬁrst two components of (18) to form a trend-cycle function τ(t) = π(t) + ζ(t), which is liable to be described by an integrated moving average with an autoregressive component consisting of a twofold diﬀerence operator. An alternative recourse is to merge the secular cycle ζ(t) with the irregular component η(t) to create a component υ(t) = ζ(t) + η(t) that could be mo"
344,unknown,"modelled via a stationary ARMA process. This is in line with the procedures of this paper. The immediate purpose is to derive a ﬁlter that can be relied on to remove the seasonal ﬂuc- tuations from the data sequence y(t) = ξ(t) + υ(t) that has been reduced to stationarity by the removal of the trend component π(t). There is no advantage then in representing υ(t) by an elaborate ARMA process; and, "
345,unknown,"υ(t) = η(t) as a white-noise process. The resulting heuristic model of the detrended data may be represented in a z-transform notation by (19) y(z) = ξ(z) + η(z) = P(z) Σ( z) ν(z) + η(z). Here, y(z) = ∑ t ytzt represents the z-transform of a sequence that is commonly assumed to be doubly inﬁnite. However, the convergence of z-transform depends on the assumption that the sequence {yt; t = 0 ,±1,±2,"
346,unknown,"0,±1,±2,... }and {ηt; t = 0 ,±1,±2,... }that are, otherwise, assumed to be generated by white-noise process with the variances V (νt) = σ2 ν and V (ηt) = σ2 η, respectively. The sequence {νt; t = 0 ,±1,±2,... }is the forcing function of the seasonal process. It is subjected to a transfer function represented by the rational function P(z)/Σ( z). The denominator of the function is the polynomial (20"
347,unknown,"(20) Σ( z) = 1 −zs 1 −z = 1 + z + z2 + ··· + zs−1 , wheras the numerator is (21) P(z) = Σ( ρz) = 1 −ρszs 1 −ρz = 1 + ρz + ρ2 z2 + ··· + ρs−1 zs−1 . The poles of the denominator of the transfer function are the complex numbers exp(i2 πj/s ); j = 1,...,s − 1, which lie on the circumference of the unit circle in the complex plane at angles that correspond to the seasonal frequency and its harmonics. "
348,unknown,"ﬂuctuations. The zeros of numerator are the values ρexp(i2 πj/s ); j = 1 ,...,s −1. They serve to conﬁne the ﬂuctuations to narrows bands of frequencies that surround the seasonal frequencies. 8 CHOLESKY DECOMPOSITION AND SEASONAL ADJUSTMENT The presence of complex roots of unit modulus within the polynomial Σ( z) implies that the pro- cess generating {yt; t = 0 ,±1,±2,... }is nonstationary in amp"
349,unknown,"by multiplying throughout by the denominator of the ﬁlter to give (22) Σ( z)y(z) = P(z)ν(z) + Σ( z)η(z) = δ(z) + κ(z) = g(z). The z-transform of the Wiener–Kolmogorov ﬁlter that serves, equally, to extract η(z) from y(z) and Σ( z)η(z) from Σ( z)y(z) is (23) β(z) = σ2 ηΣ( z−1 )Σ( z) σ2 ηΣ( z−1 )Σ( z) + σ2 νP(z−1 )P(z) = Σ( z−1 )Σ( z) Π( z−1 )Π( z) , where (24) Π( z−1 )Π( z) = Σ( z−1 )Σ( z) + λP (z−"
350,unknown,"σ2 ν σ2 η . . The inclusion of the term σ2 ηΣ( z−1 )Σ( z) in the denominator of (23) means that the gain of the Wiener–Kolmogorov ﬁlter cannot exceed unity. However, to ensue that unity is attained at the zero frequency, it is appropriate to normalise the ﬁlter by dividing the coeﬃcients of the numerator and denominator by their respective sums. The eﬀects of a seasonal adjustment ﬁlter are best r"
351,unknown,"The eﬀects of a seasonal adjustment ﬁlter are best represented by its frequency-response function, which shows how the ﬁlter alters the amplitudes of the sinusoidal elements of which a stationary data sequence is composed. Figure 1 shows the frequency response function of two such ﬁlters, wherein the parameter values are ρ = 0 .99 and λ = 0 .5, which give rise to a frequency response function with"
352,unknown,"narrow clefts at the seasonal frequencies, and ρ = 0 .8 and λ = 0 .5, which give rise to one with wide clefts. 0 0.25 0.5 0.75 1 0 π/6 π/3 π/2 2π/3 5π/6 π Figure 1. The frequency response functions of the ordinary seasonal adjustment ﬁlter for monthly data with λ = 0 .5. and ρ = 0 .8 (the solid line) and with λ = 0 .5. and ρ = 0 .99 (the dashed line). The Finite-Sample Wiener–Kolmogorov Filter To "
353,unknown,"To derive the ﬁnite-sample version of the Wiener–Kolmogorov ﬁlter, consider a vector y = [y0 ,y 1 ,...,y T−1 ]′of T values drawn from the process represented by y(z). In accordance with equation (19), the vector may be decomposed as (25) y = ξ + η. 9 MISE and POLLOCK: TIME-SERIES ANALYSIS To create a vector of a stable amplitude, the vector y must be transformed by a matrix Σ s = Σ( LT ) of order "
354,unknown,"by replacing the argument z by the matrix lag operator LT = [ e1 ,...,e T−1 ,0] of order T, which is derived from the identity matrix IT = [ e0 ,e 1 ,...,e T−1 ]′ by deleting the leading column and by adding a column of zeros to the end of the array. When applying the matrix operator to the vector y, the ﬁrst s elements of the product, which are in g∗, are liable to be discarded: (26) Σ( LT )y = ["
355,unknown,"are in g∗, are liable to be discarded: (26) Σ( LT )y = [ S′ ∗ S′ ] y = [ g∗ g ] . Here S′, is a matrix of order ( T −s + 1) ×T, of which the jth row contains the s unit coeﬃcients of S(z) preceeded by j −1 zeros and followed by zeros. The matrix S∗ of order ( s −1) ×T contains a leading lower-triangular matrix ﬁlled with units and a following matrix of order ( s −1) ×(T −s + 1) full of zeros. In c"
356,unknown,"In common with Σ( LT ), the ﬁnite-sample analogue of the operator P(z) has a Toeplitz structure as follows: (27) P(LT ) =   1 0 ··· 0 0 0 ··· 0 0 ρ 1 ··· 0 0 0 ··· 0 0 ... ... . . . ... ... ... . . . ... ρs−2 ρs−3 ··· 1 0 0 ··· 0 0 ρs−1 ρs−2 ··· ρ 1 0 ··· 0 0 0 ρs−1 ··· ρ2 ρ 1 ··· 0 0 ... ... . . . ... ... ... . . . ... ... 0 0 ··· ρs−1 ρs−2 ρs−3 ··· 1 0 0 0 ··· 0 ρs−1 ρs−2 ··· ρ 1"
357,unknown,"  = [R′ ∗ R′ ] . Thus, P(LT ) becomes Σ( LT ) when ρ = 1. Applying S′ to the equation y = ξ + η, representing the seasonally ﬂuctuating data, gives (28) S′y = R′ν + S′η = δ + κ = g. This is just a segment of T −s elements drawn from the process represented by equation (22). The expectations and the dispersion matrices of the component vectors of g are (29) E(δ) = 0 , D (δ) = σ2 νR′"
358,unknown,"(29) E(δ) = 0 , D (δ) = σ2 νR′R, E(κ) = 0 , D (κ) = σ2 ηS′S. The diﬃculty of estimating the vector ξ = y −η of seasonal ﬂuctuations directly is that some starting values or initial conditions are required in order to deﬁne the value at time t = 0. How- ever, since η is from a stationary mean-zero process, it requires only zero-valued initial conditions. Therefore, the starting-value problem can be"
359,unknown,"Therefore, the starting-value problem can be circumvented by concentrating on the estimation of η, wherafter an estimate of ξ = y −η is readily available. The estimates of ξ and η will be denoted by the roman letters x and h respectively. 10 CHOLESKY DECOMPOSITION AND SEASONAL ADJUSTMENT The conditional expectation of η, given the transformed data g = S′y, is provided by the formula (30) h = E(η|g"
360,unknown,"(30) h = E(η|g) = E(η) + C(η,g )D−1 (g){g −E(g)} = C(η,g )D−1 (g)g, where the second equality follows in view of the zero-valued expectations of η and g. Within this expression, there are (31) D(g) = σ2 νR′R + σ2 ηS′S and C(η,g ) = σ2 ηS. Putting these details into (30) gives the following estimate of η: (32) h = σ2 ηS(σ2 νR′R + σ2 ηS′S)−1 S′y = S(S′S + λR′R)−1 S′y, whence (33) x = E(ξ|g) = y −E(η"
361,unknown,"= S(S′S + λR′R)−1 S′y, whence (33) x = E(ξ|g) = y −E(η|g) = y −h = {I −S(S′S + λR′R)−1 S′}y. A simple procedure for calculating h begins by solving the equation (34) ( S′S + λR′R)b = S′y = g for the value of b. Thereafter, one can generate h = Sb. The matrix S′S + λR′R′ is a narrow-band Toeplitz matrix with the structure of the variance- covariance matrix of a moving-average process of order s−1. "
362,unknown,"covariance matrix of a moving-average process of order s−1. The matrix can be encoded in a vector of s elements. The solution to equation (34) may be found via a Cholesky factorisation that sets S′S + λR′R = LDL ′, where L is a lower-triangular matrix with a limited number of nonzero bands and D is a diagonal matrix. The system LDL ′b = g may be cast in the form of Lp = g and solved for p. Then, L"
363,unknown,"for p. Then, L′b = D−1 p can be solved for b whence h = Sb can be derived. It will be observed that the equation (32) may also be written as (35) h = ( SL ′−1 )D−1 (L−1 S′)y, where SL ′−1 is an upper-triangular matrix, and where its transpose L−1 S′is a lower-triangular matrix. This equation corresponds to a method of bi-directional ﬁltering in which p = L−1 S′y represents a real-time ﬁltering and"
364,unknown,"smoothing operation. Additional Procedures The facilities for calculating the seasonally-adjusted data sequence in h must include procedures for calculating S′y = g and h = Sb. The ﬁrst of these procedures is listed as follows: (36) procedure SprimeY (q,n : integer ; var sigma : vector ; var y : longVector ); 11 MISE and POLLOCK: TIME-SERIES ANALYSIS var t,j : integer ; store : real ; begin for t "
365,unknown,"store : real ; begin for t := 0 to n −q do begin {t} store := 0 .0; for j := 0 to q do store := store + sigma [q −j] ∗y[t + j]; y[t] := store ; end ;{t} end ;{SPrimeY } This procedure has a degree of generality that allows the array sigma to contain something other than the s units that are the coeﬃcients of the polynomial Σ( z) of (20), or the s coeﬃcients of P(z). Also, the integer q, which is t"
366,unknown,"Also, the integer q, which is the maximum index of sigma , may diﬀer from s−1, which is the degree of Σ( z) and P(z). The procedure is readily intelligible as an implementation of the formula (37) gt = q∑ j=0 σq−jyt+ j; q = s −1, which corresponds to the multiplications in the lower part of (26), wherein matrix S′ has the same structure as R′ of (27), but with ρ = 1. Because the nonzero elements o"
367,unknown,"structure as R′ of (27), but with ρ = 1. Because the nonzero elements of S′ lie on and above the principal diagonal of the matrix, the elements of g = S′y must be calculated by running from top to bottom of the vector g. The second procedure is marginally more complicated in consequence of the fact that there are end-eﬀects to contend with in the matrices S and R. That is to say, the leading and t"
368,unknown,"the matrices S and R comprise fewer than the full set of s (or q + 1) coeﬃcients: (38) procedure Sy (q,n : integer ; var sigma : vector ; var y : longVector ); var t,j,r,k : integer ; store : real ; begin for t := n downto 0 do begin r := Max (0 ,q −t); k := Min (q,n −t); store := 0 .0; for j := r to k do store := store + sigma [j] ∗y[t −q + j]; y[t] := store ; end ; end ;{Sy } 12 CHOLESKY DECOMPO"
369,unknown,"The procedure implements the formula (37) ht = k∑ j= r σjyt−q+ j; q = s −1, where, in order to accommodate the end-eﬀects, the limits of the summation are r := Max (0 ,q −t) and k := Min (q,n −t). Given that the nonzero elements of S lie on and above the principal diagonal of the matrix, the elements of h = Sd must be calculated by running from bottom to top of the vector h. Seasonal Adjustment in"
370,unknown,"h. Seasonal Adjustment in Practice Central statistical oﬃces use two varieties of methods for seasonal adjustment. In the past, the dominant methods have been the venerable X-11 procedure of Shiskin, Young and Musgrave (1967) and its derivatives. The X-11 program has been fully documented in a monograph of Ladiray and Quenneville (2001). Recently, model-based methods have gained favour. These are "
371,unknown,"Recently, model-based methods have gained favour. These are representes, primarily, by the highly competent TRAMO–SEATS program of Augustin Maravall—see Gomez and Maravall (1997) and Caporello and Maravall (2004). This program follows the prescriptions of Hillmer and Tao (1982) regarding the canonical decomposition of time series aﬀected by seasonal and cyclical variations. Other model-base method"
372,unknown,"Other model-base methods of seasonal adjustment are oﬀered by the Captain Toolbox , described, originally, by Young, Pedregal, and Tych (1999) and, more recently, by Taylor (2017), and by the STAMP program of Koopmans, Harvey, Doornik, and Shephard (1995). A broad perspective on model-based business-cycle analysis and seasonal adjustment has been provided by Kaiser and Mar- avall (2001). It is wit"
373,unknown,"most directly. The model-based methods are the products of a dominant opinion amongst economists that economic investigations should be conducted within the context of well-deﬁned models of economic activities. However, there can be some advantages in exercising direct control over the parameters that determine the characteristics of the seasonal-adjustment ﬁlter. Then, the parameters may be speci"
374,unknown,"in the light of the salient spectral characteristics of the data. Moreover, there may be diﬃculties in estimating a model in consequence of the heterogeneous nature of the data, The methods that are presented in this paper do not require the estimation any model. The appropriate values of parameters λ and ρ, which determine the width of the clefts in the frequency response of the ﬁlter, can be det"
375,unknown,"data sequence. In practice, having set λ to an arbitrary value (0.5 works well), one can rely upon the value of ρ to determine the appropriate ﬁlter. It is notable that the frequency response function of Figure 1 with λ = 0 .5 and ρ = 0 .8 is indistinguishable from the analogous response function estimated by TRAMO–SEATS by applying the airline passenger model of Box and Jenkins (1976) to the loga"
376,unknown,TRAMO–SEATS by applying the airline passenger model of Box and Jenkins (1976) to the logarithms of the monthly international airline passenger totals from January 1949 to December 1960. This is shown in Figure 2. Another problem that aﬀects the time-domain methods of seasonal adjustment is that they nullify completely only the elements at the seasonal seasonal frequency and its harmonics. The seas
377,unknown,"completely only the elements at the seasonal seasonal frequency and its harmonics. The seasonal ﬂuctuations may comprise elements at adjacent frequencies that also need to be removed from the data. A testimony to this problem has been provided by McElroy and Roy (2017), who have provided a means of detecting residual seasonal eﬀects in seasonally-adjusted date. The issue has also been addressed by"
378,unknown,"addressed by Findley et al. (2005). 13 MISE and POLLOCK: TIME-SERIES ANALYSIS 0 0.25 0.5 0.75 1 0 π/4 π/2 3π/4 π Figure 2. The frequency response of the seasonal-adjustment ﬁlter associated with the monthly airline passenger model. The problem can arise in consequence of a variety of data anomalies that aﬀect the regularity of the seasonal ﬂuctuations. These include calendar eﬀects, holidays, stri"
379,unknown,"Methods of dealing with such irregularities by adjusting the data directly have been described, recently, by Attal-Toubert et al. (2018) and by Ladiray (2018). One way of eliminating a wider band of elements in the vicinities of the seasonal frequencies, which can be based on the current procedure, is to create oﬀset ﬁlters that are targeted at the frequencies on either side of the seasonal freque"
380,unknown,"targeted at the seasonal frequencies. The program SEADOS, which is associated with this paper, contains such a facility. 0 0.25 0.5 0.75 1 0 π/4 π/2 3π/4 π Figure 3. The frequency response of the trend extraction ﬁlter associated with the monthly airline passenger model. Estimating a Trend-Cyle Function The procedure of Hillmer and Tao (1982), which is adopted by the TRAMO–SEATS program, decompose"
381,unknown,"decomposes the data into a trend-cycle component, a seasonal component and an irregular component. The seasonally adjusted data is the sum of the trend-cycle component and the irregular component. The trend-cycle component on its own is also of primary interest. Figure 3 shows the frequency response function of the trend-cycle extraction ﬁlter associated with the the airline passenger model of Box"
382,unknown,"of Box and Jenkins (1976) that is employed by TRAMO–SEATS. In the SEADOS program, the fundamental trend is liable to be represented by a polynomial function that is ﬁtted to the data by a least-squares or a weighted least-squares regression. A trend- cycle function, which is equivalent to the function derived from the airline passenger model, may be 14 CHOLESKY DECOMPOSITION AND SEASONAL ADJUSTMEN"
383,unknown,"created by applying a smoothing ﬁlter to residuals from the polynomial regression that have been seasonally adjusted. The smoothed sequence is then added back to the polynomial trend to create the trend-cycle. The unidirectional smoothing ﬁlter M(z) is a simple second-order moving average incorporating a zero at the limiting Nyquist frequency of π, with a value of −1, and a second zero −1/κ ∈(0 ,−"
384,unknown,"(38) M(z) = (1 + z)(1 + κz) 2(1 + κ) = 1 + (1 + κ)z + κz2 2(1 + κ) , with κ ∈[0 ,1] , The bidirectional ﬁlter M(z−1 )M(z) is applied to the seasonally-adjusted data sequence using the procedures SPrimey and Sy of (36) and (38) in sequence, with the array sigma holding the coeﬃcients of M(z). A greater attenuation of the high-frequency elements is achieved using the twofold M2 (z) ﬁlter. 0 0.25 0.5"
385,unknown,ﬁlter. 0 0.25 0.5 0.75 1 0 π/6 π/3 π/2 2π/3 5π/6 π Figure 4. The frequency response of the M2 (z) trend-cycle extraction ﬁlter that mimicks that of the monthly airline passenger model. Figure 4 shows the frequency response function of a ﬁlter that compounds the bidirectionsl M2 (z) smoothing ﬁlter with seasonal-adjustment ﬁlter of which the frequency response is depicted in Figure 1 by the unbroke
386,unknown,"1 by the unbroken line. In this case, the smoothing parameter is κ = 0 .4. The eﬀect of applying this combined ﬁlter to the logarithms of the monthly index of U.S. total sales from January 1953 to December 1964. is shown in Figure 5. 11.75 12 12.25 12.5 0 25 50 75 100 1250 Figure 5. The logarithms of U.S. total retail sales from January 1953 to December 1964 with an interpolated trend-cycle functi"
387,unknown,trend-cycle function. 15 MISE and POLLOCK: TIME-SERIES ANALYSIS The SEADOS Program A program that implements the procedures that have been described in this paper is available at the following address: http://www.le.ac.uk/users/dsgp1/ It is to be found under the legend SEADOS: A Program For Seasonal Adjustment in the Time Domain . The Pascal code of the program is also provided at this address. Th
388,unknown,"using the Free Pascal compiler, which is freely available on the web. Embedded in the program are brief descriptions of its facilities and it its functions. These should provide suﬃcient guidance for operating the program. References Attal-Toubert, K., D. Ladiray, M. Marini and J. Palate, (2018), Moving Trading-Day Eﬀects with X-13 ARIMA-SEATS and TRAMO-SEATS, Chapter 6 in Mazzi, G.L., D. Ladiray,"
389,unknown,"X-13 ARIMA-SEATS and TRAMO-SEATS, Chapter 6 in Mazzi, G.L., D. Ladiray, and D.A. Rieser, (eds.), Handbook on Seasonal Adjustment, 2018 edition, Eurostat: Publications Oﬃce of the European Union, Luxembourg. Box, G.E.P., and G.M. Jenkins, (1976), Time Series Analysis: Forecasting and Control, Revised Edition, Holden Day, San Francisco. Caporello G., and A. Maravall, (2004), Program TSW, Revised Ref"
390,unknown,"dios, Banco de Espa˜ na. Findley, D.F., T.S. McElroy and K.C. Wills, (2005), Modiﬁcations of SEATS’ Diagnostic for Detecting Over- or Underestimation of Seasonal Adjustment Decomposition Components, U.S. Census Bureau. G´ omez, V., and A. Maravall, (1997), TRAMO (Time Series Regression with ARIMA Noise, Missing Observations, and Outliers) and SEATS (Signal Extraction in ARIMA Time Series) Instruct"
391,unknown,"the User, Banco de Espa˜ na, Madrid. G´ omez, V., and A. Maravall, (2001), Seasonal Adjustment and Signal Extraction in Economic Time Series, chapter 8 in D. Pe˜ na, G.C. Tiao, and R.S. Tsay (eds.), A Course in Time Series Analysis, John Wiley and Sons, New York. Hillmer, S.C., and G.C. Tiao, (1982), An ARIMA-Model-Based Approach to Seasonal Adjustment, Journal of the American Statistical Associat"
392,unknown,"Journal of the American Statistical Association, 77, 63–70. Kaiser, R., and A. Maravall, (2001), Measuring Business Cycles in Economic Time Series, Lecture Notes in Statistics 154, Springer-Verlag, New York. Koopmans, S. J., Harvey, A. C., Doornik, J. A. and Shephard, N., (1995), STAMP 5.0: Structural Time Series Analyser, Modeller and Predictor, London: Chapman & Hall, 1995. Ladiray, D., (2018), "
393,unknown,"Ladiray, D., (2018), Calendar Eﬀects, Chapter 5 in in Mazzi, G.L., D. Ladiray, and D.A. Rieser, (eds.), Handbook on Seasonal Adjustment 2018 edition, Eurostat: Publications Oﬃce of the European Union, Luxembourg. Ladiray, D., and B. Quenneville, (2001), Seasonal Adjustment with the X-11 Method, Springer Lecture Notes in Statistics 158, Springer Verlag, Berlin. Mazzi, G.L., D. Ladiray, and D.A. Rie"
394,unknown,"Mazzi, G.L., D. Ladiray, and D.A. Rieser, (2018), Handbook on Seasonal Adjustment, 2018 Edition, Eurostat: Publications Oﬃce of the European Union, Luxembourg. 16 CHOLESKY DECOMPOSITION AND SEASONAL ADJUSTMENT McElroy, T., and A. Roy, (2017), Detection of Seasonality in the Frequency Domain, Center for Statistical Research & Methodology Research and Methodology Directorate U.S. Census Bureau, Wash"
395,unknown,"Washington. Pollock, D.S.G, (2016), Econometric Filters, Computational Economics, 48, 669–691. Shiskin, J., A.H. Young, and J.C. Musgrave, (1967), The X-11 Variant of the Census Method II Seasonal Adjustment, Technical Paper No. 15, Bureau of the Census, U.S. Department of Commerce. Taylor, C.J., (2017), The CAPTAIN Toolbox for System Identiﬁcation, Time Series Analysis, Fore- casting and Control:"
396,unknown,"casting and Control: Getting Started Guide. Lancaster University. Young, P.C., D.J. Pedregal, and W. Tych, (1999), Dynamic Harmonic Regression, Journal of Fore- casting, 18, 369–394. 17 Multiple Output Processes Neil D. Lawrence GPRS 25th–27th February 2015"
397,unknown,Outline Gaussian Processes Multiple Output Processes Approximations Dimensionality Reduction Latent Force Models Outline Gaussian Processes Multiple Output Processes Gauss Markov Process Markov Covariance Function Precision Matrix: Conditional Independence Kronecker Products and Kalman Filters ‘Multitask’ Gaussian Processes Approximations Dimensionality Reduction Latent Force Models Multiple Outpu
398,unknown,"▶ In this section we will study Gaussian processes with multiple outputs. ▶ they have various names, vector valued functions, multiple outputs, multidimensional GPs, multi-task learning. ▶ Key idea, we want to relate several different functions. ▶ Sounds more complex, but actually it’s a special case of a normal GP where one input is discrete. ▶ Question: how to embed covariation between the funct"
399,unknown,"functions. ▶ Start by introducing Kalman ﬁlter/smoother. Simple Markov Chain ▶ Assume 1-d latent state, a vector over time, x = [x1 ... xT]. ▶ Markov property, xi =xi−1 + ϵi, ϵi ∼N(0,α) =⇒xi ∼N(xi−1,α) ▶ Initial state, x0 ∼N (0,α0) ▶ If x0 ∼N (0,α) we have a Markov chain for the latent states. ▶ Markov chain it is speciﬁed by an initial distribution (Gaussian) and a transition distribution (Gaussi"
400,unknown,"Gauss Markov Chain -4 -2 0 2 4 0 1 2 3 4 5 6 7 8 9 x t x0 = 0, ϵi ∼N (0,1) x0 = 0.000, ϵ1 = −2.24 x1 = 0.000 −2.24 = −2.24 Gauss Markov Chain -4 -2 0 2 4 0 1 2 3 4 5 6 7 8 9 x t x0 = 0, ϵi ∼N (0,1) x1 = −2.24, ϵ2 = 0.457 x2 = −2.24 + 0.457 = −1.78 Gauss Markov Chain -4 -2 0 2 4 0 1 2 3 4 5 6 7 8 9 x t x0 = 0, ϵi ∼N (0,1) x2 = −1.78, ϵ3 = 0.178 x3 = −1.78 + 0.178 = −1.6 Gauss Markov Chain -4 -2 0 2"
401,unknown,"0 2 4 0 1 2 3 4 5 6 7 8 9 x t x0 = 0, ϵi ∼N (0,1) x3 = −1.6, ϵ4 = −0.292 x4 = −1.6 −0.292 = −1.89 Gauss Markov Chain -4 -2 0 2 4 0 1 2 3 4 5 6 7 8 9 x t x0 = 0, ϵi ∼N (0,1) x4 = −1.89, ϵ5 = −0.501 x5 = −1.89 −0.501 = −2.39 Gauss Markov Chain -4 -2 0 2 4 0 1 2 3 4 5 6 7 8 9 x t x0 = 0, ϵi ∼N (0,1) x5 = −2.39, ϵ6 = 1.32 x6 = −2.39 + 1.32 = −1.08 Gauss Markov Chain -4 -2 0 2 4 0 1 2 3 4 5 6 7 8 9 x t"
402,unknown,"x0 = 0, ϵi ∼N (0,1) x6 = −1.08, ϵ7 = 0.989 x7 = −1.08 + 0.989 = −0.0881 Gauss Markov Chain -4 -2 0 2 4 0 1 2 3 4 5 6 7 8 9 x t x0 = 0, ϵi ∼N (0,1) x7 = −0.0881, ϵ8 = −0.842 x8 = −0.0881 −0.842 = −0.93 Gauss Markov Chain -4 -2 0 2 4 0 1 2 3 4 5 6 7 8 9 x t x0 = 0, ϵi ∼N (0,1) x8 = −0.93, ϵ9 = −0.41 x9 = −0.93 −0.410 = −1.34 Multivariate Gaussian Properties: Reminder If z ∼N (µ,C) and x = Wz + b the"
403,unknown,"then x ∼N ( Wµ+ b,WCW⊤) Multivariate Gaussian Properties: Reminder Simpliﬁed: If z ∼N ( 0,σ2I ) and x = Wz then x ∼N ( 0,σ2WW⊤) Matrix Representation of Latent Variables x1 x2 x3 x4 x5 ϵ1 ϵ2 ϵ3 ϵ4 ϵ5 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 ×= x1 = ϵ1 Matrix Representation of Latent Variables x1 x2 x3 x4 x5 ϵ1 ϵ2 ϵ3 ϵ4 ϵ5 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 ×= x2 = ϵ1 + ϵ2 M"
404,unknown,Matrix Representation of Latent Variables x1 x2 x3 x4 x5 ϵ1 ϵ2 ϵ3 ϵ4 ϵ5 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 ×= x3 = ϵ1 + ϵ2 + ϵ3 Matrix Representation of Latent Variables x1 x2 x3 x4 x5 ϵ1 ϵ2 ϵ3 ϵ4 ϵ5 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 ×= x4 = ϵ1 + ϵ2 + ϵ3 + ϵ4 Matrix Representation of Latent Variables x1 x2 x3 x4 x5 ϵ1 ϵ2 ϵ3 ϵ4 ϵ5 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 1 1 1
405,unknown,"1 1 1 1 1 ×= x5 = ϵ1 + ϵ2 + ϵ3 + ϵ4 + ϵ5 Matrix Representation of Latent Variables x ϵL1 ×= Multivariate Process ▶ Since x is linearly related to ϵwe know x is a Gaussian process. ▶ Trick: we only need to compute the mean and covariance of x to determine that Gaussian. Latent Process Mean x = L1ϵ Latent Process Mean ⟨x⟩= ⟨L1ϵ⟩ Latent Process Mean ⟨x⟩= L1 ⟨ϵ⟩ Latent Process Mean ⟨x⟩= L1 ⟨ϵ⟩ ϵ∼N (0,"
406,unknown,"Latent Process Mean ⟨x⟩= L10 Latent Process Mean ⟨x⟩= 0 Latent Process Covariance xx⊤= L1ϵϵ⊤L⊤ 1 x⊤= ϵ⊤L⊤ Latent Process Covariance ⟨xx⊤⟩ = ⣨ L1ϵϵ⊤L⊤ 1 ⟩ Latent Process Covariance ⟨xx⊤⟩ = L1 ⟨ϵϵ⊤⟩L⊤ 1 Latent Process Covariance ⟨xx⊤⟩ = L1 ⟨ϵϵ⊤⟩L⊤ 1 ϵ∼N (0,αI) Latent Process Covariance ⟨xx⊤⟩ = αL1L⊤ 1 Latent Process x = L1ϵ Latent Process x = L1ϵ ϵ∼N (0,αI) Latent Process x = L1ϵ ϵ∼N (0,αI) =⇒ Laten"
407,unknown,"=⇒ Latent Process x = L1ϵ ϵ∼N (0,αI) =⇒ x ∼N ( 0,αL1L⊤ 1 ) Covariance for Latent Process II ▶ Make the variance dependent on time interval. ▶ Assume variance grows linearly with time. ▶ Justiﬁcation: sum of two Gaussian distributed random variables is distributed as Gaussian with sum of variances. ▶ If variable’s movement is additive over time (as described) variance scales linearly with time. Cov"
408,unknown,"Covariance for Latent Process II ▶ Given ϵ∼N (0,αI) =⇒ϵ∼N ( 0,αL1L⊤ 1 ) . Then ϵ∼N (0,∆tαI) =⇒ϵ∼N ( 0,∆tαL1L⊤ 1 ) . where ∆t is the time interval between observations. Covariance for Latent Process II ϵ∼N (0,α∆tI) , x ∼N ( 0,α∆tL1L⊤ 1 ) K = α∆tL1L⊤ 1 ki,j = α∆tl⊤ :,il:,j where l:,k is a vector from the kth row of L1: the ﬁrst k elements are one, the next T −k are zero. ki,j = α∆t min(i,j) deﬁne ∆t"
409,unknown,"ki,j = αmin(ti,tj) = k(ti,tj) Covariance for Latent Process II ϵ∼N (0,α∆tI) , x ∼N ( 0,α∆tL1L⊤ 1 ) K = α∆tL1L⊤ 1 ki,j = α∆tl⊤ :,il:,j where l:,k is a vector from the kth row of L1: the ﬁrst k elements are one, the next T −k are zero. ki,j = α∆t min(i,j) deﬁne ∆ti = ti so ki,j = αmin(ti,tj) = k(ti,tj) Covariance for Latent Process II ϵ∼N (0,α∆tI) , x ∼N ( 0,α∆tL1L⊤ 1 ) K = α∆tL1L⊤ 1 ki,j = α∆tl⊤ :,"
410,unknown,"1 ki,j = α∆tl⊤ :,il:,j where l:,k is a vector from the kth row of L1: the ﬁrst k elements are one, the next T −k are zero. ki,j = α∆t min(i,j) deﬁne ∆ti = ti so ki,j = αmin(ti,tj) = k(ti,tj) Covariance for Latent Process II ϵ∼N (0,α∆tI) , x ∼N ( 0,α∆tL1L⊤ 1 ) K = α∆tL1L⊤ 1 ki,j = α∆tl⊤ :,il:,j where l:,k is a vector from the kth row of L1: the ﬁrst k elements are one, the next T −k are zero. ki,j "
411,unknown,"ki,j = α∆t min(i,j) deﬁne ∆ti = ti so ki,j = αmin(ti,tj) = k(ti,tj) Covariance Functions Where did this covariance matrix come from? Markov Process k (t,t′) = αmin(t,t′) ▶ Covariance matrix is built using the inputs to the function t."
412,unknown,"Covariance Functions Where did this covariance matrix come from? Markov Process k (t,t′) = αmin(t,t′) ▶ Covariance matrix is built using the inputs to the function t. -3 -2 -1 0 1 2 3 0 0.5 1 1.5 2 Covariance Functions Where did this covariance matrix come from? Markov Process Visualization of inverse covariance (precision). ▶ Precision matrix is sparse: only neighbours in matrix are non-zero. ▶ T"
413,unknown,"Covariance Functions Where did this covariance matrix come from? Exponentiated Quadratic Kernel Function (RBF, Squared Exponential, Gaussian) k (x,x′) = αexp  −∥x −x′∥2 2 2ℓ2   ▶ Covariance matrix is built using the inputs to the function x. ▶ For the example above it was based on Euclidean distance. ▶ The covariance function is also know as a kernel."
414,unknown,"Covariance Functions Where did this covariance matrix come from? Exponentiated Quadratic Kernel Function (RBF, Squared Exponential, Gaussian) k (x,x′) = αexp  −∥x −x′∥2 2 2ℓ2   ▶ Covariance matrix is built using the inputs to the function x. ▶ For the example above it was based on Euclidean distance. ▶ The covariance function is also know as a kernel. Covariance Functions Where did thi"
415,unknown,Covariance Functions Where did this covariance matrix come from? Exponentiated Quadratic Visualization of inverse covariance (precision). ▶ Precision matrix is not sparse. ▶ Each point is dependent on all the others. ▶ In this case non-Markovian.
416,unknown,Covariance Functions Where did this covariance matrix come from? Markov Process Visualization of inverse covariance (precision). ▶ Precision matrix is sparse: only neighbours in matrix are non-zero. ▶ This reﬂects conditional independencies in data. ▶ In this case Markov structure.
417,unknown,"Simple Kalman Filter I ▶ We have state vector X = [ x1 ... xq ] ∈RT×q and if each state evolves independently we have p(X) = q∏ i=1 p(x:,i) p(x:,i) = N(x:,i|0,K). ▶ We want to obtain outputs through: yi,: = Wxi,: Stacking and Kronecker Products I ▶ Represent with a ‘stacked’ system: p(x) = N(x|0,I ⊗K) where the stacking is placing each column of X one on top of another as x =   x:,1"
418,unknown,"x:,2 ... x:,q   Kronecker Product aK bK cK dKK a b c d ⊗ = Kronecker Product ⊗ = Stacking and Kronecker Products I ▶ Represent with a ‘stacked’ system: p(x) = N(x|0,I ⊗K) where the stacking is placing each column of X one on top of another as x =   x:,1 x:,2 ... x:,q   Column Stacking ⊗ = For this stacking the marginal distribution over time is given by"
419,unknown,the block diagonals. For this stacking the marginal distribution over time is given by the block diagonals. For this stacking the marginal distribution over time is given by the block diagonals. For this stacking the marginal distribution over time is given by the block diagonals. For this stacking the marginal distribution over time is given by the block diagonals. Two Ways of Stacking Can also s
420,unknown,"x =   x1,: x2,: ... xT,:   p(x) = N(x|0,K ⊗I) Row Stacking ⊗ = For this stacking the marginal distribution over the latent dimensions is given by the block diagonals. For this stacking the marginal distribution over the latent dimensions is given by the block diagonals. For this stacking the marginal distribution over the latent dimensions is given by the block diagon"
421,unknown,"For this stacking the marginal distribution over the latent dimensions is given by the block diagonals. For this stacking the marginal distribution over the latent dimensions is given by the block diagonals. Observed Process The observations are related to the latent points by a linear mapping matrix, yi,: = Wxi,: + ϵi,: ϵ∼N ( 0,σ2I ) Mapping from Latent Process to Observed Wx1,: Wx2,: Wx3,: x1,: "
422,unknown,"Wx3,: x1,: x2,: x3,: W 0 0 0 W 0 0 0 W × = Output Covariance This leads to a covariance of the form (I ⊗W)(K ⊗I)(I ⊗W⊤) + Iσ2 Using (A ⊗B)(C ⊗D) = AC ⊗BD This leads to K ⊗WW⊤+ Iσ2 or y ∼N ( 0,WW⊤⊗K + Iσ2) Kernels for Vector Valued Outputs: A Review Foundations and TrendsR⃝ in Machine Learning Vol. 4, No. 3 (2011) 195–266 c⃝ 2012 M. A. ´Alvarez, L. Rosasco and N. D. Lawrence DOI: 10.1561/2200000036"
423,unknown,"DOI: 10.1561/2200000036 Kernels for Vector-Valued Functions: A Review By Mauricio A. ´Alvarez, Lorenzo Rosasco and Neil D. Lawrence Contents 1 Introduction 197 2 Learning Scalar Outputs with Kernel Methods 200 2.1 A Regularization Perspective 200 2.2 A Bayesian Perspective 202 2.3 A Connection Between Bayesian and Regularization Points of View 205 3 Learning Multiple Outputs with Kernel Methods 20"
424,unknown,"3.1 Multi-output Learning 207 3.2 Reproducing Kernel for Vector-Valued Functions 209 3.3 Gaussian Processes for Vector-Valued Functions 211 4 Separable Kernels and Sum of Separable Kernels 213 4.1 Kernels and Regularizers 214 4.2 Coregionalization Models 217 4.3 Extensions 228 Kronecker Structure GPs ▶ This Kronecker structure leads to several published models. (K(x,x′))j,j′ = k(x,x′)kT(j,j′), whe"
425,unknown,"where k has x and kT has i as inputs. ▶ Can think of multiple output covariance functions as covariances with augmented input. ▶ Alongside x we also input the j associated with the output of interest. Separable Covariance Functions ▶ Taking B = WW⊤we have a matrix expression across outputs. K(x,x′) = k(x,x′)B, where B is a p ×p symmetric and positive semi-deﬁnite matrix. ▶ B is called the coregion"
426,unknown,"▶ We call this class of covariance functions separable due to their product structure. Sum of Separable Covariance Functions ▶ In the same spirit a more general class of kernels is given by K(x,x′) = q∑ j=1 kj(x,x′)Bj. ▶ This can also be written as K(X,X) = q∑ j=1 Bj ⊗kj(X,X), ▶ This is like several Kalman ﬁlter-type models added together, but each one with a different set of latent functions. ▶ W"
427,unknown,"functions. ▶ We call this class of kernels sum of separable kernels (SoS kernels). Geostatistics ▶ Use of GPs in Geostatistics is called kriging. ▶ These multi-output GPs pioneered in geostatistics: prediction over vector-valued output data is known as cokriging. ▶ The model in geostatistics is known as the linear model of coregionalization (LMC, Journel and Huijbregts (1978); Goovaerts (1997)). ▶"
428,unknown,"▶ Most machine learning multitask models can be placed in the context of the LMC model. Weighted sum of Latent Functions ▶ In the linear model of coregionalization (LMC) outputs are expressed as linear combinations of independent random functions. ▶ In the LMC, each component fj is expressed as a linear sum fj(x) = q∑ j=1 wj,juj(x). where the latent functions are independent and have covariance fu"
429,unknown,"covariance functions kj(x,x′). ▶ The processes {fj(x)}q j=1 are independent for q , j′. Kalman Filter Special Case ▶ The Kalman ﬁlter is an example of the LMC where ui(x) →xi(t). ▶ I.e. we’ve moved form time input to a more general input space. ▶ In matrix notation: 1. Kalman ﬁlter F = WX 2. LMC F = WU where the rows of these matrices F, X, U each contain q samples from their corresponding functio"
430,unknown,"time (Kalman ﬁlter) or spatial location (LMC). Intrinsic Coregionalization Model ▶ If one covariance used for latent functions (like in Kalman ﬁlter). ▶ This is called the intrinsic coregionalization model (ICM, Goovaerts (1997)). ▶ The kernel matrix corresponding to a dataset X takes the form K(X,X) = B ⊗k(X,X). Autokrigeability ▶ If outputs are noise-free, maximum likelihood is equivalent to ind"
431,unknown,"equivalent to independent ﬁts of B and k(x,x′) (Helterbrand and Cressie, 1994). ▶ In geostatistics this is known as autokrigeability (Wackernagel, 2003). ▶ In multitask learning its the cancellation of intertask transfer (Bonilla et al., 2008). Intrinsic Coregionalization Model K(X,X) = ww⊤⊗k(X,X). w = [ 1 5 ] B = [ 1 5 5 25 ]"
432,unknown,"Intrinsic Coregionalization Model K(X,X) = ww⊤⊗k(X,X). w = [ 1 5 ] B = [ 1 5 5 25 ] Intrinsic Coregionalization Model K(X,X) = ww⊤⊗k(X,X). w = [ 1 5 ] B = [ 1 5 5 25 ] Intrinsic Coregionalization Model K(X,X) = ww⊤⊗k(X,X). w = [ 1 5 ] B = [ 1 5 5 25 ] Intrinsic Coregionalization Model K(X,X) = ww⊤⊗k(X,X). w = [ 1 5 ] B = [ 1 5 5 25 ] Intrinsic Coregionalization Model K(X,X) = B ⊗k(X,X). B = [ 1 0 "
433,unknown,"Intrinsic Coregionalization Model K(X,X) = B ⊗k(X,X). B = [ 1 0 .5 0.5 1 .5 ] Intrinsic Coregionalization Model K(X,X) = B ⊗k(X,X). B = [ 1 0 .5 0.5 1 .5 ] Intrinsic Coregionalization Model K(X,X) = B ⊗k(X,X). B = [ 1 0 .5 0.5 1 .5 ] Intrinsic Coregionalization Model K(X,X) = B ⊗k(X,X). B = [ 1 0 .5 0.5 1 .5 ] LMC Samples K(X,X) = B1 ⊗k1(X,X) + B2 ⊗k2(X,X) B1 = [ 1.4 0 .5 0.5 1 .2 ] ℓ1 = 1 B2 = [ "
434,unknown,"LMC Samples K(X,X) = B1 ⊗k1(X,X) + B2 ⊗k2(X,X) B1 = [ 1.4 0 .5 0.5 1 .2 ] ℓ1 = 1 B2 = [ 1 0 .5 0.5 1 .3 ] ℓ2 = 0.2 LMC Samples K(X,X) = B1 ⊗k1(X,X) + B2 ⊗k2(X,X) B1 = [ 1.4 0 .5 0.5 1 .2 ] ℓ1 = 1 B2 = [ 1 0 .5 0.5 1 .3 ] ℓ2 = 0.2 LMC Samples K(X,X) = B1 ⊗k1(X,X) + B2 ⊗k2(X,X) B1 = [ 1.4 0 .5 0.5 1 .2 ] ℓ1 = 1 B2 = [ 1 0 .5 0.5 1 .3 ] ℓ2 = 0.2 LMC Samples K(X,X) = B1 ⊗k1(X,X) + B2 ⊗k2(X,X) B1 = [ 1"
435,unknown,"LMC in Machine Learning and Statistics ▶ Used in machine learning for GPs for multivariate regression and in statistics for computer emulation of expensive multivariate computer codes. ▶ Imposes the correlation of the outputs explicitly through the set of coregionalization matrices. ▶ Setting B = Ip assumes outputs are conditionally independent given the parameters θ. (Minka and Picard, 1997; Lawr"
436,unknown,"▶ More recent approaches for multiple output modeling are different versions of the linear model of coregionalization. Semiparametric Latent Factor Model ▶ Coregionalization matrices are rank 1 Teh et al. (2005). rewrite equation (??) as K(X,X) = q∑ j=1 w:,jw⊤ :,j ⊗kj(X,X). ▶ Like the Kalman ﬁlter, but each latent function has a different covariance. ▶ Authors suggest using an exponentiated quadra"
437,unknown,"characteristic length-scale for each input dimension. Semiparametric Latent Factor Model Samples K(X,X) = w:,1w⊤ :,1 ⊗k1(X,X) + w:,2w⊤ :,2 ⊗k2(X,X) w1 = [ 0.5 1 ] w2 = [ 1 0.5 ]"
438,unknown,"Semiparametric Latent Factor Model Samples K(X,X) = w:,1w⊤ :,1 ⊗k1(X,X) + w:,2w⊤ :,2 ⊗k2(X,X) w1 = [ 0.5 1 ] w2 = [ 1 0.5 ] Semiparametric Latent Factor Model Samples K(X,X) = w:,1w⊤ :,1 ⊗k1(X,X) + w:,2w⊤ :,2 ⊗k2(X,X) w1 = [ 0.5 1 ] w2 = [ 1 0.5 ] Semiparametric Latent Factor Model Samples K(X,X) = w:,1w⊤ :,1 ⊗k1(X,X) + w:,2w⊤ :,2 ⊗k2(X,X) w1 = [ 0.5 1 ] w2 = [ 1 0.5 ] Semiparametric Latent Factor"
439,unknown,"Gaussian processes for Multi-task, Multi-output and Multi-class ▶ Bonilla et al. (2008) suggest ICM for multitask learning. ▶ Use a PPCA form for B: similar to our Kalman ﬁlter example. ▶ Refer to the autokrigeability effect as the cancellation of inter-task transfer. ▶ Also discuss the similarities between the multi-task GP and the ICM, and its relationship to the SLFM and the LMC. Multitask Clas"
440,unknown,"LMC. Multitask Classiﬁcation ▶ Mostly restricted to the case where the outputs are conditionally independent given the hyperparameters φ (Minka and Picard, 1997; Williams and Barber, 1998; Lawrence and Platt, 2004; Seeger and Jordan, 2004; Yu et al., 2005; Rasmussen and Williams, 2006). ▶ Intrinsic coregionalization model has been used in the multiclass scenario. Skolidis and Sanguinetti (2011) us"
441,unknown,"intrinsic coregionalization model for classiﬁcation, by introducing a probit noise model as the likelihood. ▶ Posterior distribution is no longer analytically tractable: approximate inference is required. Computer Emulation ▶ A statistical model used as a surrogate for a computationally expensive computer model. ▶ Higdon et al. (2008) use the linear model of coregionalization to model images repre"
442,unknown,"coregionalization to model images representing the evolution of the implosion of steel cylinders. ▶ In Conti and O’Hagan (2009) use the ICM to model a vegetation model: called the Shefﬁeld Dynamic Global Vegetation Model (Woodward et al., 1998). Approximations Neil D. Lawrence GPRS 25th–27th February 2015"
443,unknown,Outline Gaussian Processes Multiple Output Processes Approximations Dimensionality Reduction Latent Force Models Outline Gaussian Processes Multiple Output Processes Approximations Larger Datasets Non Gaussian Likelihoods Link Functions Laplace Approximation Expectation Propagation IVM Sparse Expectation Propagation Dimensionality Reduction Latent Force Models Approximations in GPs ▶ Two main chal
444,unknown,"▶ Computational complexity and storage of exact inference O(n3) and O(n2) respectively. ▶ Non Gaussian likelihoods making requisite integrals intractable. ▶ In this section we address these challenges. Bayes Rule and Gaussian Processes ▶ So far we have focussed on joint Gaussians and exploited their properties. p(y) = N ( y|0,K + σ2I ) This is derived from y(xi) = f(xi) + ϵi where f ∼N (0,K) and ϵ"
445,unknown,"f ∼N (0,K) and ϵ∼N ( 0,σ2I ) ▶ Let’s remind ourselves of principles of probabilistic inference. Gaussian Processes: Extremely Short Overview -6 -4 -2 0 2 4 6 0 2 4 6 8 10"
446,unknown,Gaussian Processes: Extremely Short Overview -6 -4 -2 0 2 4 6 0 2 4 6 8 10 Gaussian Processes: Extremely Short Overview -6 -4 -2 0 2 4 6 0 2 4 6 8 10 Gaussian Processes: Extremely Short Overview -6 -4 -2 0 2 4 6 0 2 4 6 8 10 -6 -4 -2 0 2 4 6 0 2 4 6 8 10
447,unknown,"Classical Bayesian Inference ▶ The way we can perform inference in Gaussian systems is special (properties of multivarate Gaussians). ▶ Classically we need to declare a prior, p(f). ▶ Combine it with a likelihood, p(y|f), p(f|y) = p(y|f)p(f) p(y) ▶ The easy bit is the multiplication on top. Normally the tough bit is p(y) = ∫ p(y|f)p(f)df it just happens to be trivial for the joint Gaussian case .."
448,unknown,"Bayesian Inference, i.i.d. Likelihood ▶ Or for i.i.d. likelihood, p(f|y) = ∏n i=1 p(yi|fi)p(f) p(y) ▶ If p(yi|fi) = N ( yi|fi,σ2) inference is trivial because yi = fi + ϵi, ϵ i ∼N ( 0,σ2) . ▶ In approximate GPs we will return to the more general formulation. Variational Compression (Lawrence, 2007; Titsias, 2009) ▶ Complexity of standard GP: ▶ O(n3) in computation. ▶ O(n2) in storage. ▶ Via low ra"
449,unknown,"▶ Via low rank representations of covariance: ▶ O(nm2) in computation. ▶ O(nm) in storage. ▶ Where m is user chosen number of inducing variables. They give the rank of the resulting covariance. Variational Compression (Lawrence, 2007; Titsias, 2009) ▶ Complexity of standard GP: ▶ O(n3) in computation. ▶ O(n2) in storage. ▶ Via low rank representations of covariance: ▶ O(nm2) in computation. ▶ O(nm"
450,unknown,"▶ O(nm2) in computation. ▶ O(nm) in storage. ▶ Where m is user chosen number of inducing variables. They give the rank of the resulting covariance. Variational Compression (Lawrence, 2007; Titsias, 2009) ▶ Complexity of standard GP: ▶ O(n3) in computation. ▶ O(n2) in storage. ▶ Via low rank representations of covariance: ▶ O(nm2) in computation. ▶ O(nm) in storage. ▶ Where m is user chosen number "
451,unknown,"They give the rank of the resulting covariance. Variational Compression ▶ Inducing variables are a compression of the real observations. ▶ They can live in space of f or a space that is related through a linear operator ( ´Alvarez et al., 2010) — could be gradient or convolution. ▶ There are inducing variables associated with each set of hidden variables, xi. Variational Compression II ▶ Important"
452,unknown,"the likelihood independent across the data. ▶ It turns out that this allows us to variationally handle uncertainty on the kernel (including the inputs to the kernel). ▶ It also allows standard scaling approaches: stochastic variational inference Hensman et al. (2013), parallelization Gal et al. (2014) and work by Zhenwen Dai on GPUs to be applied: an engineering challenge? Inducing Variable Approx"
453,unknown,"▶ Date back to (Williams and Seeger, 2001; Smola and Bartlett, 2001; Csat´o and Opper, 2002; Seeger et al., 2003; Snelson and Ghahramani, 2006). See Qui˜nonero Candela and Rasmussen (2005) for a review. ▶ We follow variational perspective of (Titsias, 2009). ▶ This is an augmented variable method, followed by a collapsed variational approximation (King and Lawrence, 2006; Hensman et al., 2012). Au"
454,unknown,"Augmented Variable Model: Not Wrong but Useful? Augment standard model with a set of m new inducing variables, u. p(y) = ∫ p(y,u)du y Augmented Variable Model: Not Wrong but Useful? Augment standard model with a set of m new inducing variables, u. p(y) = ∫ p(y|u)p(u)du y u Augmented Variable Model: Not Wrong but Useful? Important: Ensure inducing variables are also Kolmogorov consistent (we have m"
455,unknown,"consistent (we have m∗other inducing variables we are not yet using.) p(u) = ∫ p(u,u∗)du∗ y u u∗ Augmented Variable Model: Not Wrong but Useful? Assume that relationship is through f (represents ‘fundamentals’—push Kolmogorov consistency up to here). p(y) = ∫ p(y|f)p(f|u)p(u)dfdu y f u u∗ Augmented Variable Model: Not Wrong but Useful? Convenient to assume factorization (doesn’tinvalidate model—th"
456,unknown,function as worst case). p(y) = ∫ n∏ i=1 p(yi|fi)p(f|u)p(u)dfdu yi fi u u∗ i = 1 ... n Augmented Variable Model: Not Wrong but Useful? Focus on integral over f. p(y) = ∫ ∫ n∏ i=1 p(yi|fi)p(f|u)dfp(u)du yi fi u u∗ i = 1 ... n Augmented Variable Model: Not Wrong but Useful? Focus on integral over f. p(y|u) = ∫ n∏ i=1 p(yi|fi)p(f|u)df yi fi u∗u i = 1 ... n Variational Bound on p(y|u) log p(y|u) = log
457,unknown,"∫ p(y|f)p(f|u)df = ∫ q(f) log p(y|f)p(f|u) q(f) df + KL (q(f) ∥p(f|y,u)) (Titsias, 2009) ▶ Example, set q(f) = p(f|u), log p(y|u) ≥log ∫ p(f|u) logp(y|f)df. p(y|u) ≥exp ∫ p(f|u) logp(y|f)df. Variational Bound on p(y|u) log p(y|u) = log ∫ p(y|f)p(f|u)df = ∫ q(f) log p(y|f)p(f|u) q(f) df + KL (q(f) ∥p(f|y,u)) (Titsias, 2009) ▶ Example, set q(f) = p(f|u), log p(y|u) ≥log ∫ p(f|u) logp(y|f)df. p(y|u) "
458,unknown,"∫ p(f|u) logp(y|f)df. p(y|u) ≥exp ∫ p(f|u) logp(y|f)df. Optimal Compression in Inducing Variables ▶ Maximizing lower bound minimizes the KL divergence (information gain): KL (p(f|u) ∥p(f|y,u)) = ∫ p(f|u) log p(f|u) p(f|y,u)du ▶ This is minimized when the information stored about y is stored already in u. ▶ The bound seeks an optimal compression from the information gain perspective. ▶ If u = f bou"
459,unknown,"▶ If u = f bound is exact (f d-separates y from u). Choice of Inducing Variables ▶ Optimizing the bound directly not always practical. ▶ Free to choose whatever heuristics for the inducing variables. ▶ Can quantify which heuristics perform better through checking lower bound. Factorizing Likelihoods ▶ If the likelihood, p(y|f), factorizes p(y|u) ≥exp ∫ p(f|u) log n∏ i=1 p(yi|fi)df. ▶ Then the boun"
460,unknown,"▶ Now need a choice of distributions for f and y|f ... Factorizing Likelihoods ▶ If the likelihood, p(y|f), factorizes p(y|u) ≥exp ∫ p(f|u) log n∏ i=1 p(yi|fi)df. ▶ Then the bound factorizes. ▶ Now need a choice of distributions for f and y|f ... Factorizing Likelihoods ▶ If the likelihood, p(y|f), factorizes p(y|u) ≥exp ∫ p(f|u) n∑ i=1 logp(yi|fi)df. ▶ Then the bound factorizes. ▶ Now need a choi"
461,unknown,"▶ Now need a choice of distributions for f and y|f ... Factorizing Likelihoods ▶ If the likelihood, p(y|f), factorizes p(y|u) ≥exp ∫ p(f|u) n∑ i=1 log p(yi|fi)df. ▶ Then the bound factorizes. ▶ Now need a choice of distributions for f and y|f ... Factorizing Likelihoods ▶ If the likelihood, p(y|f), factorizes p(y|u) ≥exp n∑ i=1 ∫ p( fi|u) logp(yi|fi)df. ▶ Then the bound factorizes. ▶ Now need a ch"
462,unknown,"▶ Now need a choice of distributions for f and y|f ... Factorizing Likelihoods ▶ If the likelihood, p(y|f), factorizes p(y|u) ≥exp n∑ i=1 ∫ p( fi|u) logp(yi|fi)df. ▶ Then the bound factorizes. ▶ Now need a choice of distributions for f and y|f ... Factorizing Likelihoods ▶ If the likelihood, p(y|f), factorizes p(y|u) ≥ n∏ i=1 exp ∫ p( fi|u) logp(yi|fi)df. ▶ Then the bound factorizes. ▶ Now need a "
463,unknown,"▶ Now need a choice of distributions for f and y|f ... Factorizing Likelihoods ▶ If the likelihood, p(y|f), factorizes p(y|u) ≥ n∏ i=1 exp ∫ p( fi|u) logp(yi|fi)df. ▶ Then the bound factorizes. ▶ Now need a choice of distributions for f and y|f ... Factorizing Likelihoods ▶ If the likelihood, p(y|f), factorizes p(y|u) ≥ n∏ i=1 exp ⟨log p(yi|fi)⟩ p( fi|u) ▶ Then the bound factorizes. ▶ Now need a c"
464,unknown,"▶ Now need a choice of distributions for f and y|f ... Factorizing Likelihoods ▶ If the likelihood, p(y|f), factorizes p(y|u) ≥ n∏ i=1 exp ⟨log p(yi|fi)⟩ p( fi|u) ▶ Then the bound factorizes. ▶ Now need a choice of distributions for f and y|f ... Gaussian p(yi|fi) For Gaussian likelihoods: ⟨log p(yi|fi)⟩ p( fi|u) = −1 2 log 2πσ2− 1 2σ2 (yi −⟨fi ⟩)2− 1 2σ2 (⣨ f2 i ⟩ −⟨fi ⟩2) Gaussian p(yi|fi) For G"
465,unknown,"⟨log p(yi|fi)⟩ p( fi|u) = −1 2 log 2πσ2− 1 2σ2 (yi −⟨fi ⟩)2− 1 2σ2 (⣨ f2 i ⟩ −⟨fi ⟩2) Implying: p(yi|u) ≥exp ⟨log ci ⟩N ( yi|⟨fi ⟩,σ2) Gaussian Process Over f and u Deﬁne: qi,i = varp( fi|u) (fi ) = ⣨ f2 i ⟩ p( fi|u) −⟨fi ⟩2 p( fi|u) We can write: ci = exp ( −qi,i 2σ2 ) If joint distribution of p(f,u) is Gaussian then: qi,i = ki,i −k⊤ i,uK−1 u,uki,u ci is not a function of u but is a function of X"
466,unknown,"Lower Bound on Likelihood Substitute variational bound into marginal likelihood: p(y) ≥ n∏ i=1 ci ∫ N ( y|⟨f⟩,σ2I ) p(u)du Note that: ⟨f⟩p(f|u) = Kf,uK−1 u,uu is linearly dependent on u. Deterministic Training Conditional Making the marginalization of u straightforward. In the Gaussian case: p(u) = N(u|0,Ku,u ) ∫ p(y|u)p(u)du ≥ n∏ i=1 ci ∫ N ( y|Kf,uK−1 u,uu,σ2) N(u|0,Ku,u )du Deterministic Traini"
467,unknown,"Making the marginalization of u straightforward. In the Gaussian case: p(u) = N(u|0,Ku,u ) ∫ p(y|u)p(u)du ≥ n∏ i=1 ciN ( y|0,σ2I + Kf,uK−1 u,uKu,f ) Deterministic Training Conditional Making the marginalization of u straightforward. In the Gaussian case: p(u) = N(u|0,Ku,u ) ∫ p(y|u)p(u)du ≥ n∏ i=1 ciN ( y|0,σ2I + Kf,uK−1 u,uKu,f ) Maximize log of the bound to ﬁnd covariance function parameters, L "
468,unknown,"L ≥ n∑ i=1 log ci + log N ( y|0,σ2I + Kf,uK−1 u,uKu,f, ) Deterministic Training Conditional Making the marginalization of u straightforward. In the Gaussian case: p(u) = N(u|0,Ku,u ) ∫ p(y|u)p(u)du ≥ n∏ i=1 ciN ( y|0,σ2I + Kf,uK−1 u,uKu,f ) Maximize log of the bound to ﬁnd covariance function parameters, L ≥ n∑ i=1 log ci + log N ( y|0,σ2I + Kf,uK−1 u,uKu,f, ) Deterministic Training Conditional Ma"
469,unknown,"Making the marginalization of u straightforward. In the Gaussian case: p(u) = N(u|0,Ku,u ) ∫ p(y|u)p(u)du ≥ n∏ i=1 ciN ( y|0,σ2I + Kf,uK−1 u,uKu,f ) Maximize log of the bound to ﬁnd covariance function parameters, L ≈log N ( y|0,σ2I + Kf,uK−1 u,uKu,f, ) ▶ If the bound is normalized, the ci terms are removed. ▶ This results in the projected process approximation (Rasmussen and Williams, 2006) or DT"
470,unknown,"Rasmussen, 2005). Proposed by (Smola and Bartlett, 2001; Seeger et al., 2003; Csat´o and Opper, 2002; Csat´o, 2002). Deterministic Training Conditional Making the marginalization of u straightforward. In the Gaussian case: p(u) = N(u|0,Ku,u ) ∫ p(y|u)p(u)du ≥ n∏ i=1 ciN ( y|0,σ2I + Kf,uK−1 u,uKu,f ) Maximize log of the bound to ﬁnd covariance function parameters, ▶ If the bound is normalized, the "
471,unknown,"▶ This results in the projected process approximation (Rasmussen and Williams, 2006) or DTC (Qui˜nonero Candela and Rasmussen, 2005). Proposed by (Smola and Bartlett, 2001; Seeger et al., 2003; Csat´o and Opper, 2002; Csat´o, 2002). Fully Independent Training Conditional Deﬁne c′ i to be c′ i = ci exp   y2 i qi,i 2   = exp   qi,i(y2 i −σ−2) 2   Then rewrite the bound: n∑ i="
472,unknown,"n∑ i=1 log c′ i + log N ( y|0,σ2I + diag ( Qf,f ) + Kf,uK−1 u,uKu,f ) where Qf,f = cov ( ﬀ⊤) p(f|u) = Kf,f −Kf,uK−1 u,uKu,f In FITC the log c′ i terms could be negative or positive. yi,j wj xi σ2 i = 1 ... n j = 1 ... p Gaussian Processes: Extremely Short Overview -6 -4 -2 0 2 4 6 0 2 4 6 8 10"
473,unknown,Gaussian Processes: Extremely Short Overview -6 -4 -2 0 2 4 6 0 2 4 6 8 10 Gaussian Processes: Extremely Short Overview -6 -4 -2 0 2 4 6 0 2 4 6 8 10 Gaussian Processes: Extremely Short Overview -6 -4 -2 0 2 4 6 0 2 4 6 8 10 -6 -4 -2 0 2 4 6 0 2 4 6 8 10
474,unknown,"GP Regression Analytical tractability of the posterior distribution is assured: ▶ Gaussian prior: f ∼N (0,Kﬀ) ▶ Gaussian likelihood: n∏ i=1 p(yi|fi) = N ( y|f,σ2 i I ) ▶ Gaussian posterior: p(f|y) ∝N (f|0,Kﬀ) N ( y|f,σ2 i I ) Bernoulli Distribution ▶ A mathematical switch allows us to write a probability table as a function. P(Y = 1) = π P(Y = 0) = (1 −π) ▶ Write as a function P(Y = y) = πy(1 −π)1"
475,unknown,P(Y = y) = πy(1 −π)1−y ▶ Can think of this construction as a “mathematical switch”. Known as the Bernoulli distribution. ▶ Widely used in classiﬁcation algorithms: πparameter is made to be dependent on “inputs”. Binomial Distribution ▶ Generalization of Bernoulli to multiple trials. ▶ Jakob Bernoulli: black and red balls in an urn. Proportion of red is π. ▶ Sample with replacement. Binomial gives 
476,unknown,"reds, y, from S extractions P(y|π,S) = S! y!(S −y)!πy(1 −π)(S−y) ▶ Mean is given by Sπand variance Sπ(1 −π)."
477,unknown,"0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 -5 0 5 10 15 20 25 P(y|S,π) y Figure : The binomial distribution for π= 0.4 and S = 20. Mean is shown as red line, 2 standard deviations are magenta. The Gamma Density ▶ Density over positive real values. p(y|a,b) = ba Γ(a) ya−1 exp (−by) = G ( y|µ,σ2) ▶ Mean is a b and variance is a b2 . ▶ Also available in multivariate as the Wishart (positive deﬁnite matrices)."
478,unknown,"deﬁnite matrices). Gamma PDF I 0 1 2 3 0 0.5 1 1.5 2 2.5 p(y|a,b) y, height/m Figure : The Gamma PDF with a = 127 and b = 75. Here it represents the heights of a population of students and constrains them positive. Gamma PDF I 0 1 2 3 -0.5 0 0.5 1 1.5 2 2.5 p(y|a,b) y, height/m Figure : The Gamma PDF with a = 127 and b = 75 alongside a Gamma PDF with a = 3 and b = 3. Categorical Distribution Multi"
479,unknown,"Multiple outcomes, example: die roll. die role probability y 1 π1 [1 0 0 0 0 0] 2 π2 [0 1 0 0 0 0] 3 π3 [0 0 1 0 0 0] 4 π4 [0 0 0 1 0 0] 5 π5 [0 0 0 0 1 0] 6 π6 [0 0 0 0 0 1] P(y) = ∏k i=1 πyi i Multinomial Distribution ▶ Generalization of categorical to multiple trials. ▶ Generalization of binomial to multiple outcomes. Proportion of each colour ball is now πi. ▶ Sample with replacement. Multinom"
480,unknown,"Multinomial gives the distribution of number of each of k different balls, y, from S extractions P(y|π,S) = S! ∏k i=1 yi! k∏ i=1 πyi i ▶ Mean for each colour is given by Sπi and variance Sπi(1 −πi)."
481,unknown,Distributions as Functions ▶ Probability distribution with a simple table can be limiting. ▶ The Poisson Distribution — a distribution a a function ▶ First published by Sim´ eon Denis Poisson(1781-1840) in 1837. ▶ Deﬁned over the space of all non-negative integers. ▶ This set is countably inﬁnite: impossible to summarise in a table! ▶ The Poisson distribution is therefore deﬁned as P (y|µ) = µy y!
482,unknown,"y! exp (−µ). (2) where y is any integer from 0 to ∞, and µis a parameter of the distribution. A Poisson with µ= 2 ▶ To work out the probability of y in a Poisson µ= 2 we can start ﬁlling a table. ▶ The values in a table are computed from (2) y 0 1 2 ... P (y) 0.135 0.271 0.271 ... Table : Some values for the Poisson distribution with µ= 2. 0 0.1 0.2 0.3 -5 0 5 10 15 20 25 P(y|µ) y Figure : The Poi"
483,unknown,"0.3 -5 0 5 10 15 20 25 P(y|µ) y Figure : The Poisson distribution for µ= 2. Mean is given by µ(red line), standard deviation is given by √µ(magenta lines show 2 standard deviations). Gaussian Noise 0 1 2 -3 -2 -1 0 1 2 3 4 p (f∗|X,x∗,y) Figure : Inclusion of a data point with Gaussian noise. Gaussian Noise 0 1 2 -3 -2 -1 0 1 2 3 4 p (f∗|X,x∗,y) p (y∗= 0.6|f∗ ) Figure : Inclusion of a data point wi"
484,unknown,"Gaussian Noise 0 1 2 -3 -2 -1 0 1 2 3 4 p (f∗|X,x∗,y) p (y∗= 0.6|f∗ ) p (f∗|X,x∗,y,y∗ ) Figure : Inclusion of a data point with Gaussian noise. Classiﬁcation Noise Model Probit Noise Model 0 0.5 1 -4 -2 0 2 4 p(yi|fi) fi yi = −1 yi = 1 Figure : The probit model (classiﬁcation). The plot shows p (yi|fi ) for different values of yi. For yi = 1 we have p (yi|fi ) = φ(fi ) = ∫ fi −∞N(z|0,1) dz. Ordina"
485,unknown,Ordinal Noise Model Ordered Categories 0 0.5 1 -4 -2 0 2 4 p(yi|fi) fi yi = −1 yi = 1yi = 0 Figure : The ordered categorical noise model (ordinal regression). The plot shows p (yi|fi ) for different values of yi. Here we have assumed three categories. Null Category Noise Model Classiﬁcation with a Missing Category 0 0.5 1 -4 -2 0 2 4 p(yi|fi) fi yi = −1 yi = 1yi = 0 Figure : The null category nois
486,unknown,Figure : The null category noise model (semi-supervised learning). The plot shows p (yi|fi ) for different values of yi. Here we have assumed three categories. Non-linear Response Functions ▶ Non Gaussian likelihood: p(yi|fi) = Φ( fi) ▶ Exact computation of the posterior is no longer possible analytically. p(f|y) = p(f) ∏n i=1 p(yi|fi)∫ p(f) ∏n i=1 p(yi|fi)df Link Functions ▶ Take the output of ou
487,unknown,"▶ Take the output of our function, f(·) use as: ▶ Success probability in binomial distribution. ▶ Rate function in Poisson likelihood. ▶ shape parameter of Gamma distribution. ▶ Problem: f(·) deﬁned over real line. ▶ Needs to be squashed down to 0-1 or constrained positive. Link Functions ▶ Log link function, model the log rate. log λ(x) = f(x) ▶ Logit link function, model the log odds. log π(x) l"
488,unknown,log(1 −π(x)) = f(x) Generative Model ▶ From a generative perspective we often naturally think of the inverse link: λ(x) = exp( f(x)) π(x) = 1 1 + exp(−f(x)) ▶ Can make some assumptions of the link function clearer. For example log additive link function: log λ(x) = f1(x) + f2(x) is a product of functions: λ(x) = exp( f1(x)) exp(f2(x)) Example: Logit/Probit Link Function 20 15 10 5 0 5 10 15 2020 1
489,unknown,15 10 5 0 5 10 15 20 Latent variable Realizations of fi 20 15 10 5 0 5 10 15 20 0.0 0.2 0.4 0.6 0.8 1.0 Likelihood function p(yi =1|fi) =Φ(fi) Laplace Approximation ▶ Second order Taylor expansion at mode of log likelihood. ▶ First suggested by Laplace for his English dice example. ▶ How Laplace independently (of de Moivre) reinvented the Gaussian density. Laplace Approximation log p(f|y) = log p(
490,unknown,log p(f|y) = log p(y|f) + log p(f) + const log p(f|y) = log p(y|f) −1 2f⊤K−1 ﬀ f ▶ Find MAP estimate ˆf. This is mean of Gaussian approximation. ▶ Find Hessian of this system. ▶ Covariance of approximation is −H−1. H =   d2 log p(y|f) d fid fj   ij −K−1 ﬀ Expectation Propagation: General Case ▶ Exact (intractable) posterior: p(f|y) = p(f) ∏n i=1 p(yi|fi)∫ p(f) ∏n i=1 p(yi|fi)df ▶ EP po
491,unknown,"q(f|y) = ∏K i=1 ti( fi) ZEP Expectation Propagation: Gaussian Approximation Consider the special case: p(yi|fi) ≈ti( fi) = ZiN ( ˜µi|fi,˜σ2 i ) Here Zi is a scaling factor so ti is unnormalized. If p(f) ∼N ( 0,Kf,f ) . No approximation needed. EP Posterior Approximation q(f|y) = ∏n i=1 t( fi)p(f) ZEP = N(f|µ,Σ) Site functions provide “fake Gaussian observations” with target value ˆµi and observati"
492,unknown,"i . ZEP = n∏ i=1 Zi ∫ n∏ i=1 N ( ˆµi|fi,ˆσ2 i ) p(f)df EP Posterior Approximation q(f|y) = ∏n i=1 ZiN ( ˆµi|fi,ˆσ2 i ) p(f) ZEP = N(f|µ,Σ) Site functions provide “fake Gaussian observations” with target value ˆµi and observation variance ˆσ2 i . ZEP = n∏ i=1 Zi ∫ n∏ i=1 N ( ˆµi|fi,ˆσ2 i ) p(f)df Site approximations ▶ Given initial site approximations: tj( fj) for j , i. ▶ Need to set ti( fi) ≈p(yi"
493,unknown,"p(yi|fi)p(f) ∏ j,i tj( fj) ≈p(f) n∏ j=1 tj( fj) p(yi|fi) ∫ p(f) ∏ j,i tj( fj)d fj,i ≈ ∫ p(f) n∏ j=1 tj( fj)d fj,i p(yi|fi)q\i( fi) ≈N ( fi|ˆµi,ˆσ2 i ) ˆZi Cavity Distribution q\i( fi) = ∏ j,i t( fj)p(f) ∫ ∏ j,i t( fj)p(f) df Tilted Distribution ˆpi( fi|yi) = p(yi|fi)q\i( fi) ˆZ where ˆZi = ∫ p(yi|fi)q\i( fi)d fi Minimization of the KL divergence ˆµi,ˆσi = argminˆµi,ˆσi KL (p(yi|fi)q\i( fi) ˆZ ∥N ("
494,unknown,"ˆZ ∥N ( fi|ˆµi,ˆσ2 i )) This is the KL between tilted distribution and marginal of approximation. Since the approximation is Gaussian, KL is minimal when: ▶ ˆµi = ⟨fi⟩p(yi|fi)q\i( fi) ▶ ˆσ2 i = ⟨fi⟩2 p(yi|fi)q\i( fi) −˜µ2 i Scale of Site Approximation ▶ Since the approximation is un-normalized, we set scale as follows: ˆZi = ∫ p(yi|fi)q\i( fi)d fi Classiﬁcation Noise Model Probit Noise Model 0 0.5"
495,unknown,"Classiﬁcation Noise Model Probit Noise Model 0 0.5 1 -4 -2 0 2 4 p(yi|fi) fi yi = −1 yi = 1 Figure : The probit model (classiﬁcation). The plot shows p (yi|fi ) for different values of yi. For yi = 1 we have p (yi|fi ) = φ(fi ) = ∫ fi −∞N(z|0,1) dz. Classiﬁcation 0 1 2 3 -3 -2 -1 0 1 2 3 p (f∗|X,x∗,y) Figure : An EP style update with a classiﬁcation noise model. Classiﬁcation 0 1 2 3 -3 -2 -1 0 1 "
496,unknown,"p (f∗|X,x∗,y) p (y∗= 1|f∗ ) Figure : An EP style update with a classiﬁcation noise model. Classiﬁcation 0 1 2 3 -3 -2 -1 0 1 2 3 p (f∗|X,x∗,y) p (y∗= 1|f∗ ) p (f∗|X,x∗,y,y∗ ) Figure : An EP style update with a classiﬁcation noise model. Classiﬁcation 0 1 2 3 -3 -2 -1 0 1 2 3 p (f∗|X,x∗,y) p (y∗= 1|f∗ ) p (f∗|X,x∗,y,y∗ ) q (f∗|X,x∗,y) Figure : An EP style update with a classiﬁcation noise model. Or"
497,unknown,"Ordinal Noise Model Ordered Categories 0 0.5 1 -4 -2 0 2 4 p(yi|fi) fi yi = −1 yi = 1yi = 0 Figure : The ordered categorical noise model (ordinal regression). The plot shows p (yi|fi ) for different values of yi. Here we have assumed three categories. Ordinal Regression 0 1 2 3 4 -3 -2 -1 0 1 2 3 p (f∗|X,x∗,y) Figure : An EP style update with an ordered category noise model. Ordinal Regression 0 1"
498,unknown,"Ordinal Regression 0 1 2 3 4 -3 -2 -1 0 1 2 3 p (f∗|X,x∗,y) p (y∗= 0|f∗ ) Figure : An EP style update with an ordered category noise model. Ordinal Regression 0 1 2 3 4 -3 -2 -1 0 1 2 3 p (f∗|X,x∗,y) p (y∗= 0|f∗ ) p (f∗|X,x∗,y,y∗ ) Figure : An EP style update with an ordered category noise model. Ordinal Regression 0 1 2 3 4 -3 -2 -1 0 1 2 3 p (f∗|X,x∗,y) p (y∗= 0|f∗ ) p (f∗|X,x∗,y,y∗ ) q (f∗|X,x∗"
499,unknown,Figure : An EP style update with an ordered category noise model. Null Category Noise Model Classiﬁcation with a Missing Category 0 0.5 1 -4 -2 0 2 4 p(yi|fi) fi yi = −1 yi = 1yi = 0 Figure : The null category noise model (semi-supervised learning). The plot shows p (yi|fi ) for different values of yi. Here we have assumed three categories. Semi-supervised Learning 0 1 2 3 -3 -2 -1 0 1 2 3 p (f∗|X
500,unknown,"Figure : An EP style update with an null category noise model. Semi-supervised Learning 0 1 2 3 -3 -2 -1 0 1 2 3 p (f∗|X,x∗,y) p (y∗= 0|f∗ ) Figure : An EP style update with an null category noise model. Semi-supervised Learning 0 1 2 3 -3 -2 -1 0 1 2 3 p (f∗|X,x∗,y) p (y∗= 0|f∗ ) p (f∗|X,x∗,y,y∗ ) Figure : An EP style update with an null category noise model. Semi-supervised Learning 0 1 2 3 -3 -"
501,unknown,"0 1 2 3 -3 -2 -1 0 1 2 3 p (f∗|X,x∗,y) p (y∗= 0|f∗ ) p (f∗|X,x∗,y,y∗ ) q (f∗|X,x∗,y) Figure : An EP style update with an null category noise model. Predictions ▶ Predictive distribution of q( f∗|y) is also Gaussian: ⟨f∗ ⟩ q( f∗|y) = k⊤ ∗ ( Kf,f + Σt )−1 ˜µ var (f∗ ) = k∗,∗−k⊤ ∗ ( Kf,f + Σt )−1 k∗ Example: People who speak an indigenous language 92.5 92.0 91.5 91.0 90.5 90.0 89.5 89.0 longitude 17."
502,unknown,17.5 18.0 18.5 19.0 19.5 20.0 20.5 21.0latitude Less than 100 speakers At least 100 speakers Example: People who speak an indigenous language 93 92 91 90 89 longitude 17.5 18.0 18.5 19.0 19.5 20.0 20.5 21.0latitude 0.050 0.100 0.1500.200 0.200 0.250 0.300 0.350 0.400 0.400 0.450 0.450 0.450 0.500 0.500 0.500 0.500 0.500 0.550 0.550 0.6000.600 0.600 0.650 0.650 0.700 0.750 0.750 0.800 0.8000.850 0.
503,unknown,0.8000.850 0.8500.900 Computational Complexity ▶ Major problem for Gaussian processes is the high computational complexity. ▶ O(n3) computation and O(n2) storage. For multioutput case O(n3p3) computation and O(n2p2) storage. ▶ Motivates sparse and low rank approximations. The Informative Vector Machine Reduce Complexity ▶ Including n data points through EP still leads to an O ( n3 ) complexity. ▶ 
504,unknown,"complexity. ▶ IVM algorithm resolves these problems with a sparse representation for the data set. ▶ Inspiration: the support vector machine. ▶ IVM use a simple selection heuristic to incorporate m most informative points (Lawrence et al., 2003; Seeger, 2004; Lawrence et al., 2005). ▶ Computational complexity: O ( n3 ) to O ( m2n ) . ▶ Infromation theoretic (Chaloner and Verdinelli, 1995) criteria"
505,unknown,"criteria used to select points. Data Point Selection Entropy Criterion ▶ Original IVM criterion inspired by support vectors being those that reduce the size of the ‘version space’ most. ▶ The equivalent Bayesian interpretation is volume of the posterior: measured by entropy. ▶ Entropy change associted with a data point is simple and quick to compute. ▶ For jth inclusion of ith data point: ∆Hj,i = "
506,unknown,"2 log ⏐⏐⏐Σj,i ⏐⏐⏐ + 1 2 log ⏐⏐⏐Σj−1 ⏐⏐⏐ = −1 2 log ⏐⏐⏐⏐I −Σj−1diag ( νj )⏐⏐⏐⏐ = −1 2 log ( 1 −νj,iςj−1,i ) . (3) IVM Parameter Updates Optimising Kernel Parameters ▶ Need to express the marginal likelihood for optimization. ▶ Seeger (2004) achieves by expressing the likelihood in terms of both the active and inactive sets. ▶ We simply express the likelihood in terms of the active set only. ▶ Given"
507,unknown,"only. ▶ Given the active set, I, and the site parameters, m and β, optimise approximation wrt kernel parameters using gradient methods. ▶ Active set and kernel parameters are interdependent: active set is reselected between optimisations of kernel parameters. Results Toy Problems ▶ Two toy data sets for classiﬁcation with probit noise. First uses an ARD set up and one irrelevant direction. ▶ A sec"
508,unknown,▶ A second demonstation: sampled 500 data points uniformly from a unit square in two dimensions. ▶ Sample then made from a GP prior of a function at these points. ▶ This function was ’squashed’ by a cumulative Gaussian and a class assigned according to this probability. IVM Classiﬁcation Classiﬁcation −4 −2 0 2 4−10 −5 0 5 10 0 0.2 0.4 0.6 0.8 10 0.2 0.4 0.6 0.8 1 Figure : Contours: Red solid line
509,unknown,"0.2 0.4 0.6 0.8 1 Figure : Contours: Red solid line at p (y|x) = 0.5 , blue dashed lines at p (y|x) = 0.25 and p (y|x) = 0.75. Active points are blue dots. Left: data sampled from from a mixture of Gaussians. Right: Data uniformly sampled on the 2–dimensional unit square. Class labels are assigned by sampling from a known Gaussian process prior. Ordered Categories Ordered Categories ▶ Two results "
510,unknown,Ordered Categories Ordered Categories ▶ Two results from two problems on ordered categorical data. ▶ First example the categories are separable linearly. ▶ Second example: sampled ordered categorical data in polar co-ordinates. Ordered Categories Toy Problems −3 −2 −1 0 1 2 3 −10 −5 0 5 10 −20 −10 0 10 20 −20 −10 0 10 20 Figure : .Left: a linear solution is found. Right: this categories in this ex
511,unknown,"example were sampled in polar co-ordinates. USPS digits Large Data Set ▶ USPS digit data set of 16 ×16 greyscale images. ▶ Contains 7291 training images and 2007 test images. ▶ Three different kernels with the IVM algorithm. ▶ For each data-set we use a ‘base kernel’ consisting of a linear part, a white noise term and a bias part. ▶ Three variations on this base kernel were then used: it was chang"
512,unknown,and ﬁnally a variant of the RBF ARD kernel. ▶ Set m = 500. USPS digits Classiﬁcation error % 0 1 2 3 4 5 6 7 8 9 Overall RBF 0.65 0.70 1.40 1.05 1.49 1.25 0.75 0.60 1.20 0.75 4.58 MLP 0.55 0.70 1.49 1.20 1.64 1.25 0.80 0.60 1.20 0.75 4.78 RBF ARD 0.55 0.60 1.49 1.10 1.79 1.20 0.80 0.60 1.20 0.85 4.68 Table : Table of results on the USPS digit data. A comparison with a summary of results on this da
513,unknown,"summary of results on this data-set Sch¨olkopf and Smola (2001, Table 7.4) shows that the IVM is in line with other results on this data. Furthermore these results were achieved with fully automated model selection. Incorporating Invariances Virtual Support Vectors ▶ Invariances present: rotations, translations. ▶ Could augment the original data set with transformed data points. ▶ This leads to a "
514,unknown,"points. ▶ This leads to a rapid expansion in the size of the data set. ▶ Sch¨olkopf et al. (1996) suggest augmenting only support vectors. ▶ Augmented points known as ‘virtual support vectors’. ▶ This algorithm gives state-of-the-art performance on the USPS data set. USPS with Virtual Informative Vectors Virtual Informative Vectors (Lawrence et al., 2005) ▶ Sch¨olkopf et al. (1996): biggest improv"
515,unknown,"translation invariances. ▶ Applied standard IVM classiﬁcation algorithm to the data set using an RBF kernel combined with a linear term. ▶ Took the active set from these experiments and aumented it: ▶ original active set plus four translations: up down lweft and right ▶ results in an augmented active set of 2500 points. ▶ Reselect active set of size m = 1,000 for ﬁnal results. Performance on USPS "
516,unknown,Classiﬁcation Error % 0 1 2 3 4 0.648 ±0.00 0 .389 ±0.03 0 .967 ±0.06 0 .683 ±0.05 1 .06 ±0.02 5 6 7 8 9 Overall 0.747 ±0.06 0 .523 ±0.03 0 .399 ±0.00 0 .638 ±0.04 0 .523 ±0.04 3 .30 ±0.03 Table : Experiments are summarised by the mean and variance of the % classiﬁcation error across ten runs with different random seeds. Results match those given by the virtual SVM but model selection was automati
517,unknown,"was automatic here. Posterior variance update ▶ Complexity is dominated by the computation of the posterior covariance: Σ = ( K−1 f,f + Σ−1 t )−1 Sparse EP ▶ q(f|y) is computed as before, but an sparse approximation is used instead of the exact covariance Kf,f. ▶ FITC approximation: O(nm2) Kf,f ≈Kf,uK−1 u,uKu,f + diag ( Kf,f −Qf,f ) ▶ DTC approximation: O(nm2) Kf,f ≈Kf,uK−1 u,uKu,f EP-FITC (genera"
518,unknown,"93 92 91 90 89 longitude 17.5 18.0 18.5 19.0 19.5 20.0 20.5 21.0latitude 0.050 0.100 0.100 0.150 0.200 0.2500.300 0.3500.400 0.450 0.500 0.550 0.6000.6500.700 0.700 0.750 0.7500.8000.8500.9000.950 EP-DTC Compatible with sparse variational approach: L= log N ( µt|0,Qf,f + Σt ) −1 2tr ( (Kf,f −Qf,f)Σti ) −ZEP Sparse variational + EP-DTC 93 92 91 90 89 longitude 17.5 18.0 18.5 19.0 19.5 20.0 20.5 21."
519,unknown,20.5 21.0latitude 0.0500.100 0.150 0.200 0.250 0.300 0.350 0.400 0.450 0.450 0.450 0.4500.500 0.500 0.500 0.550 0.550 0.600 0.600 0.650 0.650 0.700 0.700 0.750 0.800 0.8000.8500.900 This is a volume in PROBABILITY AND MATHEMATICAL STATISTICS A Series of Monographs and Textbooks Editors: Z. W . Birnbaum and E. Lukacs A complete list of titles in this series is available from the publisher upon requ
520,unknown,"MULTIVARIATE ANALYSIS K. V. MARDIA Department of Statistics. University of Leeds. Leeds. U .K . 1. T. KENT Department of Statistics. University of Leeds, Leeds. U .K . 1. M. BIBBY Faculty of Mathematics , The Open University. Milton Keynes , U .K. ACADEMIC PRESS Harcourt Brace & Company , Publishers London San Diego New York Boston Sydney Tokyo Toronto ACADEM1CPRESS UMITED 24/28 Oval Road, London "
521,unknown,"24/28 Oval Road, London NWl mx Urriled S IDles Edilion published by ACADEMIC PRESS. INC San Diego, CA~21Ol Copyright © 1979 Academic Press LImited Te.nth pritlting.1995 All Rights Reserved No part (l( this book Dlay be reproduced ill any (fnm by photostat. microfilm. or aoy other me3JlS. without written permission (rom the publishers. British Librar), Calaloguing in Publica/lOll Dolo Mardi.-, Kant"
522,unknown,"Multivariale analysIS. - (Pruoal.ilily IUUJ matbematical statistics). I. Multivariate anaJysis I. Title II. Kent. J T 111. B ibby, John IV. Se ries 519.5'3 QA27S 79-40922 ISBN ~12-471250-9 ISBN 0-12-471252-5 Pbk Printed in Great Britain by TJ Press (Potdstow) LId., Pacl~ow. Corn"" ,.11 To Neela Ma rdia ""Everything is Tetaled with every OIher thing. and this relation involves the! emergence of a rel"
523,unknown,"qllalities cannot be known a prior;, though a good number of them can be deduced from certain fundamental characteris­ tiCS."" - JSUlB philosophy The! lama Pliliosophy o( Non·Absohmsm by S. M ookcrjec:. q ..... Mahal anobis (1957) PREFACE Multivariate Analysis deals with observations on more than o ne variable where there i. some inherent interdependence between the: variable. With several texts al"
524,unknown,"With several texts already available in this area, one may very well enquire of tbe authors as to the need for yet another book. Mo st of the available books fall into two categories, either theoretical Or data analytic. The present book not only combines the two approaches but it also emphasizes modern development s. The choice of material for the book has been guided by the need to give suitable"
525,unknown,"has been guided by the need to give suitable mailer for the. beginner as well as illustrating some deeper aspects of the sUbject for the research worker. Practical examples are kept to the forefront and. wherever feasible. each technique is motivated by such an example . The book is aimed at final year under!,'Taduates and postgraduate students in Mathematic /Statistics with sections suitahle lor "
526,unknown,"and research workers. The book assumes a basic knowledge of Mathematical Statistics at undergraduate level. An eLementary course on Linear Algebra is also assumed. In particular. we assume an exposure to Matrix Algebra to the level required to read Appendix A. Broadly speaking, Chapters 1-6 and Chapter 12 can be de cribed as containing direct extensions of univariate ideas and techniques. The rema"
527,unknown,"remaining chapters concentrate on specifically multivariate problems which bave no meaningful analogues in the univariate case. Chaptcr 1 is primarily concerned with giving exploratory analyses for multivariate data and briefly introduces some 01 the important techniques, tools, and diagrammatic representations. Chapter 2 introduces various distributions together with some fundamental results, whe"
528,unknown,"together with some fundamental results, whereas Chapler 3 concentrates exclusively on normal distribution theory. Chapters 4-6 deal with prob­ lems in inference. Chapter 7 gives an over-view 01 Econometrics, whilst Principal Component Analysis, Factor Analysis, Canonical Correlation Analysis. and Discriminant Analysis are discussed from both theoretical and practical points of view in Chapters 8-1"
529,unknown,"and practical points of view in Chapters 8-11. Chapter 12 is on Mul ­ tivariate Analysis of Variance, which can be better understood in terms of PREFACE viii the techniques of previous chapters. The later chapters look into the presently developing techniques of Ouster Analysis, Multidimensional 'Scaling, and Directional Data. Each chapter concludes with a set of exercises. Solving these will nOI "
530,unknown,"only enable the reader to understand the material better but will also serve to complement the chapter itself. In general, the questions have in-built answers, but. where desirable, hints for the solution of theoretical problems are provided. Some of the numerical exercises are designed to be run On a computer, but as the main aim is On interpretation, the answers are provided. We found NAG roulin"
531,unknown,"answers are provided. We found NAG roulines and GLIM most useful, but nowadays any computer centre will have some suitable statistics and matrix algebra routines. There are three appendices, A. S, and C, which respectively provide a sufficient background of matrix algebra, a summary of univariate statis­ tics, and some tables of critical values. The aim of Appendix A on Matrix Algebra is not only "
532,unknown,"Algebra is not only to provide a summary of results, but also to give sufficient guidance to master these for students having little previous knowledge. Equations from Appendix A are referred to as (A.x.x) to distinguiSh them from (I.x.x), etc. Appendix A also includes a summary of results in n-dimensional geometry which are used liberally in the book. Appendix B gives a summary of important univa"
533,unknown,"The reference list is by no means exhaustive. Only directly relevant articles are quoted and lor a fuller bibliography we refer the reader to Anderson. D as Gupta, and Styan (1972) and Subrahmaniam and Subrahma­ niam (l1J73). The relerence list also serves as an author index. A suhject index is provided. The material in the book can be used in several different ways. For example, a one-semester el"
534,unknown,"example, a one-semester elementary cOurse of 40 lectures could cover the following topics. Appendix A; Chapter 1 (Sections 1.1-1.7); Chapter 2 (Sections 2.1-2.5); Chapter 3 (Sections 3.4.1.3.5,3.6.1. assuming results from previous sections. Definitions 3.7.1, 3.7.2); Chapter 4 (Section 4.2.2); Chapler 5 (Sections 5_1, 5.2.1a, 5.2.lb, 5.2.2a. 5.2.2b, 5.3.2b. 5.5); Chapter 8 (Sections 8.1, 8.2.1. 8."
535,unknown,"5.5); Chapter 8 (Sections 8.1, 8.2.1. 8.2.2, 8.2.5, 8.2.6, 8.4.3, 8.7); Chapter 9 (Sections 9.1-9.3, 9.4 (without details), 9.5, 9.6, 9.8); Chapler JO (Sections 10.1, 10.2); Chapler 11 (Sections 11.1, 11.2.1-11.2.3, 11.3.1. 11.6.1). Further material which can be introduced is Chapter 12 (Sections 12.1-12.3, 12.6); Chapter 13 (Sections 13.1, 13.3.1); Chapter 14 (Sections 14_1. 14.2). This material "
536,unknown,"14 (Sections 14_1. 14.2). This material has been covered in 40 lectures spread over two terms in different British universities. Alternatively. a one-semester course with more emphasis on foundation rather than applications could be based on Appendix A and Chapters 1-5. Two­ semester courses could indude all the chapters, excluding Chaplers 7 and 15 On Econometrics and Directional Data. as well a,"
537,unknown,"15 On Econometrics and Directional Data. as well a, the ,,,clions with ix PREFACE asterisks. Mathematically orientated students may like to proceed to Chapter 2, omitting the data analytic ideas of Chapter 1. Various new methods of presentation are utilized in Ihe book. For instance the data matrix is emphasized throughout, a density-free ap­ proach is given for normal theory_ the union intersecti"
538,unknown,"proach is given for normal theory_ the union intersection principle is used in testing as well as the likelihood ratio principle, and graphical methods are used in explanation. In view of the computer packages generally available. most 01 the numerical work is laken for granted and therelore, except for a few particular cases, emphasis is not placed on numerical calculations, The style of presellt"
539,unknown,"calculations, The style of preselltation is generally kept descriptive excepl where rigour is found to be necessary for theoretical reslllts, whicb are then put in the form of theorems. If any details of the proof of a theorem are felt tedious but simple, they are then relegated to the exercises. Several important topics not usually found in multivariate texts are discussed in detail. Examples of "
540,unknown,"discussed in detail. Examples of such material include the complete chapters on Econometrics, Cluster Analysis, Multidimensional Scaling, and D irectional Data . Further material is also included in parts of other chapters: method s of graphical presentation, measures of multivariate skewness and kurtosis, the singular multinonnal distribution, various non-normal distributions and families of dist"
541,unknown,"non-normal distributions and families of distributions, a density-free approach to normal distribution theory, Bayesian and robust estimators, a recent solution to the Fisher-Beluens problem. a test of multinorrnality, a non-parametric lest, discarding of variables in regression, principal com­ ponent analysis and discrimination analysis, correspondence analysis, allometry, the jack-knifing method"
542,unknown,"allometry, the jack-knifing method in discrimination, canonkal analysis of qualitative and quantitative variables, and a test of dimensionality in MANOV A . It is hoped thaI coverage 01 these developments will be helpful for studeots as well as research workers. Tbere are various other topics which have oot been tOllched upon partly because of lack of space as well as our own preferences. such as "
543,unknown,"Control Theory , Multivariate Time Series, Latent Variable Models, Path Analysis, Growth Curves, Portfolio Analysis, and various Multivariate Designs. In addition to various research papers, we have been inlluenced by particular texts in this area, especially Anderson (195&), Kendall (1975), Kshirsagar (1972). Morrison (1976). Press (1972), and Rao (1973). All these are recommended to the reader. "
544,unknown,"The authors would be most grateful to readers who draw their attention to any errors o~ obscurities in the book, or suggest other improvements. January 1979 Kanti Mardia John Kent John Bibby ACKNOWLEDGEMENTS First of all we wish to express our gratitude to pioneers in this field. In particular. w~ should mention M. S. Bartlett, R . A. Fisher, H. Hotelling, D . G. Kendall. M. G. Kendall, P. C. Maha"
545,unknown,"W . S. Torgerson, and S. S. Wilks. We are grateful to authors and editors who have generously granted us permission to reproduce figures and tables. We are also grateful to many colleagues for their valuable help and comments , in panicular Martin Beale, Cltristopber Bingbam. Lesley Butler. Richard Cormack , David Cox. Ian Curry, Peter Fisk. Allan Gordon , John Gower , Peler Harris. Chunni Khatri,"
546,unknown,"Okell, Ross Renner . David Salmond . Cyril Smith. and Peter Zemroch. We are also indebted to Joyce Snell for making various comments on an earlier draft of the book which have led to considerable improvement. We should also express our gratitude to Rob Edwards for his help in various facets of the book. for calculations, for proof· reading, for diag­ rams, etc. Some of the questions are laken from"
547,unknown,"rams, etc. Some of the questions are laken from examination papers in British universities and we are grateful to various unnamed colleagues. Since tbe original sources of questions are difficult to trace, we apologize to any colleague who recognizes a question of his own. The authors would like to thank their wives, Pavan Mardia, Susan Kent, and Zorina Bibby. Finally our thanks go to Barbara Fors"
548,unknown,typing a difficult manuscript with great skill. KVM JTK JMB CONTENTS Preface Acknowledgements C hapter t-lntroduction 1.1 Objec1S and variables 1.2 Some multivariate problems and techniques 1.3 The data matru 1.4 Summary statistics 1.5 Linear combinations \.6 Ge<>metricai ideas 1.7 Graphical representations * 1.8 Measures of multivariate skewness and kurtosis Exercises and complements . ..... . C 
549,unknown,"Exercises and complements . ..... . C hapter 2-Basic Properties of Random Vedors 2.1 Cumulative distribution functions and probability density tioos ., .... . . 2.2 Population moments 2.3 Characteristic [unctions 2.4 Transformations 2,) The multinormal distribution 2,6 Some multivariate generalizations of univariate distributions 2.7 Families of distributions 2.8 Random samples . . 2.9 Limit theo'"
550,unknown,"2.9 Limit theo'rems Exercises and comp lements Chapter 3- onnal Distribution Theory 3.1 Characterization and properties 3.2 Linear forms . . . .."" . 3.3 Transformations of normal data matrices 3.4 The Wishart distribution ,...... vji X 1 2 8 9 13 16 l7 20 22 26 func- 26 28 33 35 36 43 45 49 51 53 S9 59 62 64 66 3.5 The Hotelling T 2 distribution .... . 3.6 Mahalanobis distance . .. .... . 3.7 Stat"
551,unknown,3.6 Mahalanobis distance . .. .... . 3.7 Statistics based on the Wishart distribution 3.8 Other distributions related to the multinormal Exercises and complements Chapter 4---Eotimation 4. t Likelihood and sufficiency 4.2 Maximum likelihood estimation 4.3 Other techniques and concepts Exercises and complements Chapter 5-Hypothesis Testing 5.1 Introduction ..... 5.2 The techniques introduced 5.3 Th
552,unknown,5.3 The techniques further iUustrated '5.4 The Behrens-FISher problem 5.5 Simultaneous confidence intervals 5.6 Multivariate hYJX>thesis testing: some general points '5.7 Non-normal data . . . . . . _ . . . . . . . . 5.8 A non-parametric test for the bivariate two-sample problem E xercises and complements ..... . Chapter &--Multivariale Regression Analysis 6.1 Introduction . . . . . . . . . 6.2 Ma
553,unknown,6.2 Maximum likelihood estimation 6.3 The general linear hypothesis . • 6.4 Design matrices of degene rate rank 6.5 Multiple correlation 6.6 Least squares estimatiOn 6.7 D iscarding of variables Exercises and complements Chapter 7-Eronometri.. ........ .... . 7.1 Introduction . . . . . . . . . .. .... . 7.2 Instrumental variables and two-slage least squares 7.3 Simultaneous equation systems 7.4 Si
554,unknown,7.4 Single-equation estimators 7.5 System estima tors 7.6 Comparison of estimators Exercises and complements Chapter 8-Principal Compo nent Analysis . . . . . . . 8.1 Introduction . . . . . . . . . . . . . . . . . 8.2 Definition and properties of principal components 8.3 Sampling properties of principal components . 8.4 Testing bypotheses about principal components 8.5 Correspondenoe analysis ....
555,unknown,8.6 Allometry-the measurement of size and sbape XII 73 76 80 8S 86 96 96 102 109 113 120 120 123 131 142 144 147 148 149 lSI 157 157 158 161 164 167 171 175 . . 180 185 185 186 191 199 203 208 208 213 2 13 214 229 233 237 239 xiii CONTENTS 8.7 Discarding of variables ........ 242 8.8 Principal compone nt anaJysis in regression 244 Exe rc.ises and complements 246 Chapter 9-Factor Anal}D 2SS 9.1 Int
556,unknown,Exe rc.ises and complements 246 Chapter 9-Factor Anal}D 2SS 9.1 Introduction . . . . 255 9.2 The factor model . . 256 9 .3 Principal factor analysis 261 9.4 Maximum likelihood factor analysis 263 9.5 Goodness of fit test 267 9.6 Rotation of factors . . . . . . . 268 9.7 Factorscores 273 9.8 Relationships between factor analysis and principal component analysis • . . . . . . . . . . 275 9.9 Analysi
557,unknown,"9.9 Analysis of covariance structures . , 276 Exercises and complements ...•. . 276 Chapter IO-Canonical Correlation Analysis 281 10.1 Introduction . . . ....... 281 10.2 Mathematical developm ent . . . . 282 10.3 Qualitative data and dummy variables 290 10.4 Qualitative and quanta live data 293 Exercises and complements 295 Chapter lI-Discriminllnt AnalY8is 11.1 Introduction ..... . 11 .2 Discrim"
558,unknown,11 .2 Discrimination when the populations are known 11.3 Discrimination under estimation 11.4 Is discrimination worthwbile? . . . ] 1.5 Fisber's linear discriminant function J 1.6 Probabilities of misclassificalion 11.7 Discarding of variables . . . . . . t 1.8 When does correlation improve discrimination? Exe rcises and complements ...... . Chapter 12-M ultivariate Anal} .. is of Variance . . . .
559,unknown,Chapter 12-M ultivariate Anal} .. is of Variance . . . . . 12.1 Introduction . . ............. . 12.2 Formulation o f multivariate one-way classification 12.3 The likelihood ratio principle . . . . . . . . 12.4 Testing fixed contrasts . . . . . . . . . . . 12.5 Canonical variables and a test of dimensionality 12.6 The union intersection approach 12.7 Two-way classification Exercises and complement
560,unknown,Exercises and complements Chapter 13-Clnster Analysis 13.1 Introduction . . . . 13.2 A probabilistic formulation 13.3 Hierarchical method s 13.4 Distances and simiJanl'ics 300 300 301 309 318 3 18 320 322 324 325 333 333 333 334 337 338 348 350 356 360 360 361 369 375 CONTENTS 13.5 Othe r methods and comparative approach Exercises and complements . . . . . . Chapter 14-Multidimensional Scalin~ 14.
561,unknown,Chapter 14-Multidimensional Scalin~ 14.1 Introduction ...... . 14.2 ClassicaJ solution .... . 14 .3 Duality between principal coordinate analysis and principal corn- ponent analysis . . . . . . _. . ........ . . 14.4 Oplimal properties of the classical solution and goodness of 6t J 4.5 Seriation .......... .... . 14.6 No n-metric methods . . . . . . . . . . . 14.7 Goodness of fit measure; Procrust
562,unknown,".] 4.8 Multi-sample problem and canonical variates Exercises and complements ......,.... )(IV 394 394 397 -lO-l 406 409 413 416 4J9 420 Chapter IS-Directional Data 424 15.1 Introduction 424 15.2 Descriptive measures 426 15.3 Basic distributions . . 421\ 15 .4 Distribution theory 435 ]5.5 Maximum lik.elihood estimators for the Von Mise$-Fi~her distribu- tion . . . . . . . . . . . . . . 437 15.6 Tes"
563,unknown,15.6 Test of uniformity; the Rayleigh test 439 15.7 Some other problems 441 Exercises and complements 446 Appendix A-Matrix Algebra 452 A .l Introduction 452 A .2 Matrix operations 455 A .3 Further particular matrices and types of matrix 460 A.4 Vector spaces. rank. and linear equations 462 A .5 Linear translormations . . . . . 465 A.6 Eigenvalues and eigenvectors . . 466 A .7 Quadratic lorms and 
564,unknown,A .7 Quadratic lorms and definiteness 474 • A .8 Generalized inverse . . . . . . 476 A .9 Matrix differentiation and maximjzation probJems 478 A.IO Geometrical idea. . . . . . . . . . 481 Appendix B-Univariate Statistics 486 B .1 Introduction ... .. 486 B .2 Normal distribution 486 B .3 Chi·squared distribution. 487 B.4 F and beta variables 487 B .S I distribution ..... 488 Appendix C-Tables ... .
565,unknown,"Table C. 1 Upper percentage pOints of the X: distribution 4<)() xv Table C.2 Tab leC .3 Tab le C.4 References . Upper percemage points of the t.. distribution . . . pper percentage points of the F"" ,.v) distribution Upper percentage points 00 of 8(p. 1) 1. '-'2)' the eigenvalue ofIB - O(W + B )I=Oforp = 2. List of Main otations and Abbreviations . Subject Index Author Index CONTENTS largest 491 49"
566,unknown,"CONTENTS largest 491 492 494 497 508 510 519 1 Introduction 1.1 Objects and V~bles I Multivariate analysis deals with data containing observations on two or more variables each measured on a set of objects. For example. we may have the set of examination marks achieved by certain students, or the cork deposit in various directions of a set of trees. or flower measure­ ments for different species o"
567,unknown,"ments for different species of iris (see Tables 1.2.1, 1.4.1, and 1.2.2, respectively). Each of these data has a set of ""variables"" (the examination marks, trunk thicknesses, and flower me asurem ents) and a set of ""ob­ jects"" (the students. trees, and flowers). In general, if there are"" objects. 0"", ... 0"", and p variables, Xl •.... X p • the data contains lip pieces of information. These may be "
568,unknown,"information. These may be conveniently arranged using an (II x p) ""data matrix"". in which each row corresponds to an object and each column corresponds to a variable. For instance. three variables on five ""objects"" (students) are shown as a (5 x 3) data matrix in Table 1.1.1. Table. 1.1.1 D ata matrix with five students as object..~ where X l =age in years at entry to university. x, = marks Oul of"
569,unknown,"first year, and X .3 = sex Variables Objects x, x, X3't 1 18.45 70 1 2 18.41 6S 0 3 18.39 71 0 4 18.70 72 0 5 18"".34 94 1 t 1 indicates male ~ 0 indicutes fem ale. MULTIVARIATE ANALYSIS 2 Note that the variables need not all be of the same type: in Tahlc 1.1. I. Xl is a ""'continuous U variable, X2 is a discrete variable, and ,t ""l i.o,,; a dichotomous variable. Note also that attribute, characteri"
570,unknown,"dichotomous variable. Note also that attribute, characteristic. description. item, measurement , and response are synonyms for ""variable"", wherca~ individual, observation, plot, reading, and unit caD be used in place of ""object"", 1.2 Some Multivariate Problems and Techniques We may now illustrate various categories of multivariate technique. 1.2.1 Generalizations of univariate techniques Most univ"
571,unknown,"Most univariate questions are capable of at least one multivariate generalization. For instance, using Table 1.2.1 we may ask, as an exam­ ple. ""What is the appropriate underlying parent distribution of examina­ tion marks on various papers of a set of students?"" ""What are the summary statistics?"" ""Are the differences between average marks on different papers significant?"", etc. These problems are"
572,unknown,"tions of univariate problems and their motivation is easy to grasp. See for example Chap ters 2-6 and 12. 1.2.2 Dependence and regession Referring to Table 1.2.1. we may enquire as to the degree of dependence between performance on different papers taken by the same students. It may be useful. for counselling or other purposes, to have some idea of how final degree marks (""dependent"" variables) ar"
573,unknown,"how final degree marks (""dependent"" variables) are affected by previous examination results or by other variables such as age or sex (""explana­ tory"" variables). This presents the so-called regression problem. which is examined in Chapter 6. 1.2.3 linear combinations Given exam ination marks on different topics (as in Table 1.2.1), the question arises of how to combine or average these marks in a "
574,unknown,"way. A straightforward me thod would use the simple arithmetic mean . but this procedure may not always be suitable. For instance, if the marks on some papers vary mo re than others, we may wish to weight them differently. This leads us to search for a linear combination (weighted sum) which is ""optimal'"" in some sense. If all the examination papers rail 3 INTRODUCTION Table 1.2. J Marks in open-b"
575,unknown,3 INTRODUCTION Table 1.2. J Marks in open-book and closed-book examination out of J 00 t Mechanics (C) Vectors (C) Algebra (0) Anatysis (0) Statistics (0) 77 82 67 67 81 63 78 80 70 81 75 73 71 66 81 55 72 63 70 68 63 63 65 70 63 53 tit 72 64 73 51 67 65 65 68 59 70 68 62 56 62 60 58 62 70 64 72 60 62 45 52 64 60 63 54 55 67 59 62 44 50 50 64 55 63 65 63 58 56 37 31 55 60 57 73 60 64 56 54 40 44 6
576,unknown,60 64 56 54 40 44 69 53 53 53 42 69 61 55 45 62 46 6J 57 45 31 49 62 63 62 44 6t 52 62 46 49 4t 6t 49 64 12 58 6J 63 67 49 53 49 62 47 54 49 56 47 53 54 53 46 S9 44 44 56 55 61 ~ 18 44 50 57 81 46 52 65 50 35 32 45 49 57 64 30 69 50 52 45 46 49 53 59 37 40 27 54 61 61 31 42 48 54 68 30 59 51 45 51 56 40 56 54 35 46 56 57 49 32 45 42 55 56 40 42 60 54 49 33 40 63 53 54 25 23 55 59 53 44 4 48 49 51 
577,unknown,42 60 54 49 33 40 63 53 54 25 23 55 59 53 44 4 48 49 51 37 41 63 49 46 34 46 52 53 41 40 to indicates open-book. C indk:atcs closed book . MULTIVARIATE ANALYSIS Table 1.2.1 Conrinued M echanics (C ) Vectors (C) 46 61 40 57 49 49 22 58 35 60 48 56 31 57 17 53 49 57 59 50 37 56 40 43 35 35 38 44 43 43 39 46 62 44 48 38 34 42 I~ 51 35 36 59 53 41 41 3 1 52 17 51 34 30 46 40 10 46 46 37 30 34 13 5 1 4
578,unknown,46 40 10 46 46 37 30 34 13 5 1 49 50 18 32 8 42 23 38 30 24 3 9 7 51 15 40 15 38 5 30 12 30 5 26 o 40 Algebra (0) Analysis (0) Statistics (O ) 46 38 41 51 52 3 1 45 48 39 53 56 41 47 54 33 49 42 32 50 54 34 57 43 51 47 39 26 47 15 46 49 28 45 4R 21 61 41 51 50 54 47 24 38 34 49 46 32 43 36 22 42 41 44 33 50 47 29 40 56 30 46 48 29 37 22 19 43 30 33 37 n ~ 52 35 31 50 47 36 47 29 17 36 47 39 45 15 
579,unknown,52 35 31 50 47 36 47 29 17 36 47 39 45 15 30 43 46 18 50 25 31 38 23 9 31 ~ ~ 4~ 26 40 36 48 15 43 33 25 5L 47 ~ 43 17 22 43 23 18 39 28 17 44 36 18 32 35 21 15 20 W 21 <) 14 4 5 INTRODUcnON in one group then principal compollent aMlysis and factor analysis are two tech.niques which can help to answer such questions (see Chapters 8 and 9). In some situations tbe papers may rail into m ore tban one
580,unknown,"group--for instance. in Table 1.2.1 some examinations were ""open­ boo k"" wh.i1e others were ""closed-book"". In sucb situations we may wish to investigate the use of linear combinations within each. group separately. This leads to the method known as canonical corTelatiOtt analysis, which is discussed in Cbapter 10. The idea of taking linear com binations is an important one in mul ­ tivariate analy"
581,unknown,"tivariate analysis. and we shall return to it in Section 1.5. 1.2.4 Assignment and dissection Table 1.2.2 gives three (50 x 4) data matrices. In each malTix the ""ob­ jects"" are 50 irises of species Iris setosa, Iris versicolollr, and Iris virginica, respectively. The ""variables"" are x, =sepaJ lengtb, X3 = petal length, X 2 = sepal w idth. X 4 = petal width. If a new iris of unknown species has mea"
582,unknown,"X 4 = petal width. If a new iris of unknown species has measurements x,=5.1, x.,= 3.2, x,=2.7. x4=0.7 we may ask to which species it belongs. This presents the problem of discriminallt analysis, which is discussed in Chapter 11. Howe er. if we were presented with the 150 observations of Table 1.2.2 in an unclassified manner (say, before the three species were established) then the aim could have b"
583,unknown,"then the aim could have been to dissect the population ioto homogene ous groups. This problem is handled by cluster analysis (see Chapter 13). 1.2.5 Building configurations In som e cases the data consists not of an (n X p) data matrix, but of 1n(n-1) ""distances"" between aU pairs o( points. T o get an intuitive feel for tbe structure of such data. a configuration can be constructed of n points in "
584,unknown,"points in a Euclidean space of low dimension (e.g. p = 2 or 3). H opefully the distances between the 11 points of the configuration will closely match the original distances. Th e problems of building and interpreting such configurations are studied in Chapter 14, on Inllitidimensional scaling. 1.2.6 Directional data Tbere are various problems w hich arise when the variables are directional-that i"
585,unknown,"directional-that is, the multivariate observations are constrained to lie on a hypersphere. For a discus.ion of these problems see Cha pter 15. Table 1.2.2 Measurements On three types of iris (after Fisher, 1936) Iris selosa Iris versicolour Jris virginica , , , , , • Sepal Sepal Petal Petal Sepal Sepal Petal Petal Sepal Sepal Petal Petal length width length width length width length width length "
586,unknown,5.1 3.5 1.4 0.2 7.0 3.2 4.7 1.4 6.3 3.3 6.0 2.5 4.9 3.0 1.4 0.2 6.4 3.2 4.5 1.5 5.8 2.7 5.1 1.9 4.7 3.2 1.3 0.2 1>.9 3.1 4.9 1.5 7.1 3.0 5.\1 2.1 4.6 3.1 1.5 0.2 5.5 2.3 4.n 1.3 6.3 2.9 5.6 1.8 5.0 3.6 1.4 0.2 6.5 2.8 4.6 1.5 6.5 3.0 5.8 2.2 5.4 3.9 1.7 0.4 5.7 2.8 4.5 1.3 7.6 3.0 6.6 2.1 4.6 3.4 1.4 0.3 6.3 3.3 4.7 1.6 4.9 2.5 4.5 1.7 5.0 3.4 1.5 0.2 4.9 2.4 3.3 1.0 7.3 2.9 6.3 1.8 4.4 2.9 1.4 0.
587,unknown,4.4 2.9 1.4 0.2 6.6 2.9 4.6 1.3 6.7 2.5 5.8 1.8 4.9 3.1 1.5 0.1 5.2 2.7 3.9 1.4 7.2 3.6 6.1 2.5 5.4 3.7 1.5 0.2 5.0 2.0 3.5 1.0 6.5 3.2 5.1 2.0 4.8 3.4 1.6 0.2 5.9 3.0 4.2 1.5 6.4 2.7 5.3 1.9 4.8 3.0 1.4 0.1 6.0 2.2 4.0 1.0 6.S 3.0 5.5 2.1 4.3 3.0 1.1 0.1 6.1 2.9 4.7 1.4 5.7 2.5 5.0 2.0 5.8 4.0 1.2 0.2 5.6 2.9 3.6 1.3 5.8 2.8 5.1 2.4 5.7 4.4 1.5 0.4 6.7 3.1 4.4 1.4 6.4 3.2 5.3 2.3 5.4 3.9 1.3 0.4 
588,unknown,5.4 3.9 1.3 0.4 5.6 3.0 4.5 1.5 6.5 3.0 5.5 1.8 5.1 3.5 1.4 0.3 5.8 2.7 4.1 1.0 7.7 3.8 6.7 2.2 5.7 3.8 1.7 0.3 6.2 2.2 45 1.5 7.7 2.6 6.9 2.3 5.1 3.8 1.5 0.3 5.6 2.5 3.9 1.1 6.0 2.2 5.0 1.5 5.4 3.4 1.7 0.2 5.9 3.2 4.6 1.8 6.9 3.2 5.7 2.3 5.1 3.7 1.5 0.4 6.1 2.8 4.0 1.3 5.6 2.8 4.9 2.0 4.6 3.6 1.0 0.2 6.3 2.5 4.9 1.5 7.7 2.8 6.7 2.0 5.1 3.3 1.7 0.5 6.1 2.8 4.7 1.2 6.3 2.7 4.9 1.8 4.8 3.4 1.9 0.2 6
589,unknown,4.8 3.4 1.9 0.2 6.4 2.9 4.3 1.3 6.7 3.3 5.7 2.1 5.0 3.0 1.6 0.2 6.6 3.0 4.4 1.4 7.2 3.2 6.0 1.8 5.0 3.4 1.6 0.4 6.8 2.8 4.R 1.4 6.2 2.8 4.8 1.8 5.2 3.5 1.5 0.2 6.7 3.0 5.0 1.7 6.1 3.0 4.9 1.8 5.2 3.4 1.4 0.2 6.0 2.9 4.5 1.5 6.4 2.8 5.6 2.1 4.7 3.2 1.6 0.2 5.7 2.6 3.5 l.0 7.2 3.0 5.8 1.6 4.8 3.1 1.6 0.2 S.S 2.4 3.8 1.1 7.4 2.8 6.1 1.9 5.4 3.4 1.5 0.4 5.5 2.4 3.7 l.0 7.9 3.8 6.4 2.0 5.2 4.1 1.5 0.1 
590,unknown,"5.2 4.1 1.5 0.1 5.8 2.7 3.9 1.2 6.4 2.8 5.6 2.2 5.5 4.2 1.4 0.2 Q,O 2.7 5.1 1.6 6.3 2.8 5.1 1.5 4.9 3.1 1.5 0.2 5.4 3.0 4.5 1.5 6.1 2.6 5.6 1.4 5.0 3.2 1.2 0.2 6.0 3.4 4.5 1.6 7.7 3.0 6.1 2.3 5.5 3.5 1.3 0.2 6.7 3.1 4.7 1.5 6.3 3.4 5.6 2.4 4.9 3.6 1.4 0.1 6.3 2.3 4.4 1.3 6.4 3.1 5.5 1.8 4.4 3.0 1.3 0.2 5.6 3.0 4.1 1.3 6.0 3.0 4.8 1.8 5.1 3.4 1.5 0.2 5.5 2.5 4.0 1.3 6.9 3.1 5.4 2.1 5.0 3.5 1.3 0.3 "
591,unknown,5.0 3.5 1.3 0.3 5.5 2.6 4.4 1.2 6.7 3.1 5.6 2.4 4.5 2.3 1.3 0.3 6.1 3.0 4.6 1.4 6.9 3.1 5.1 2.3 4.4 3.2 1.3 0.2 5.8 2.6 4.0 1.2 5.8 2.7 5.1 1.9 5.0 3.5 1.6 0.6 5.0 2.3 3.3 1.0 6.8 3.2 5.9 2.3 5.'1 3.8 1.9 0.4 5.6 2.7 4.2 1.3 6.7 3.3 5.7 2.5 4.8 3.0 1.4 0.3 5.7 3.0 4.2 1.2 6.7 3.0 5.2 2.3 5.1 3.8 1.6 0.2 5.7 2.9 4.2 1.3 6.3 2.5 5.0 1.9 4.6 3.2 1.4 0.2 6.2 2.9 4.3 1.3 6.5 3.0 5.2 2.0 5.3 3.7 1.5 0.2
592,unknown,"5.3 3.7 1.5 0.2 5.1 2.5 3.0 1.1 6.2 3.4 5.4 2.3 5.0 3.3 1.4 0.2 5.7 2.8 4.1 1.3 5.9 3.0 5.1 1.8 MULTIVARIATE ANALYSIS 1.3 The Data MatrL""( The general (n x p) data matrix with /I objects and p variables can be written as follows: Variables X; Objects 0 , Xii X,"" x,,, Table l.1.1 showed one such data matrix (5X 3) with five objects and three variables. In Table J .2.2 tbere were three data matrices"
593,unknown,"having 50 rows (objects) and four columns (variables). ote that these data matrices can be considered from two alternative points of view. If w e compare two columns, then we are examining the relationship between variables. On the otber hand. a compa ri.~on of two rows involves examin­ ing the relationship between different objects. For example, in Table 1.l.1 we may compare the first two colum n"
594,unknown,"1.l.1 we may compare the first two colum ns to investigate whether there is a relationship between age at entry and marks obtained. Alternatively, looking at the first two rows will give a comparison between two students ( ""obj ect~ "") , one male and one female. The general (II x p) data matrix will be deno ted X or X( .. x p). The elemen t in row i and column j is x,;. This denotes the o bservati"
595,unknown,"jth variable on the itb object. We may write the matrix X = (x,j)' Th e rows of X will be written x;, x~ .... , x~. Note that x, denote·s the ith rOw of.x written as a column. The columns of X will be written witb subscripts in parentheses as ""cO ' ""(2)' ...• ~"") : that is, we may write [ Xi] X = : = [:0:. .. , lt' "" .• ""cOIl, 9 I~ODUCTJON where ""-[J (i= 1.. .. , tI), [ XI,] ""ci) = x""; (j= 1. ... "
596,unknown,"""ci) = x""; (j= 1. ... ,p). Note that on the one hand "", is the p-vector denoting the p observa­ tions on the first object. while on the other hand ""cn is the tI-vector whose elements denote (he observations on the first variable. In multivariate anal:,..is, the rows """" .... x"" usually form a random sample whereas the columns X(I) •• •.• ""< pI do not. This point is emphasized in the notation by the"
597,unknown,"the use of parentheses. Clearly when /I and p are even moderately large, the resulting tip pieces of information may prove too num erous to handle individually. Various ways of summarizing multivariate data are discussed in Sections 1.4. 1.7.1.8. 1.4 Summary Statistics We give here the basic summary statistics and some standard notation. 1.4.1 The mean vedor and covariance matrix An obvious extens"
598,unknown,"An obvious extension of tbe univariate notion of mean and variance leads 10 the following definitions. The samp le mean of the ith variable is 1 • x, = - L x,;, n ~ = 1 and the sample variance of the ith variable is 1 "" s,,=- L (x,;- x,)'=si. say, i=I, ... ,p. '1 r--I The samp le covariance between the jlh and jIb variables is ( 1.4.1) (1.4.2) (1.4.3) MULTIVARIATE ANALYSIS 10 The vector 01 means, "
599,unknown,"The vector 01 means, (1.4.4) is called the sample mean vecror, or simply the ""mean vector', It represents the centre 01 gravity of the sample points X n r = 1"" ... n. Th e p X P matrix s= (s,), 0.4.5) with elements given by (1.4.2) and (1.4.3), is called the sample covariance macrix, O r simply the ""covariance matrix H . The above statistics may also be expressed in matrix notation. Corres· pondin"
600,unknown,"ponding to (1.4.1) and (1.4.4), we have l ' ~ 1 j=- L x,=-X 'l, It r-="" 1 ~1 ( 1.4.6) where 1 is a column vector of "" ones. Also SO that 1 .. 1 III S= - L (x, -i)(x, - x), = - L x,x:- n . n ,= 1 11 ,,,,,1 (1.4.7) This m ay also be written S = ! X'X - Xi' = !(X'X -! X 'll'X) It n n using (1.4.6). Writing B=I-!1l', "" where B denotes the centring matrix, we find that (1.4.R) w hich is a convenient ma"
601,unknown,"w hich is a convenient matrix representation of the sample covariance matrix, 11 INTR\:.oucnON Since H is a symmetric idempotent matrix (H = H '. H = lf') it follows that for any p-vector B , 1 1 a'Sa = - a'X 'H'BXa = - y/y ~ 0, "" n where y = UXa. Hence the covariance matrix S is positive semi-<lefinite (S ."" 0). For continuous data we usually expect that S is not o nly positive semi-definite, but"
602,unknown,"semi-definite, but positive definite if .,;;. p + 1. As in one-<limensional statistics, it is often convenient to define tbe covariance matrix with a divisor of II - I instead of n. Set 1 'ux n SU= --I X =--IS, n- '1- (1.4.9) [f the data forms a random sample from a muHivariate distribution. with finite second moments, then SI> is an tlllbiased estimate of the true covariance matrix (see Theorem 2"
603,unknown,"covariance matrix (see Theorem 2.8.2), The matrix "" M = ';-x:r.=X'X L... "" (1.4.10) ,~ I is called the maIYix of SW/IS of squares Qlld producls for obvious reasons'. The matrix nS can app ropriately be labelled as the matrix of correcled sums of squares and products. T he sample correlalio"" coefficient between the ith and the jth variables IS r'i = s;,/(s,s,). (1.4.11) U nlike Si"" the correlation "
604,unknown,"U nlike Si"" the correlation coefficient is invariant under both cbanges of scale and origin of the ith and jth variables. Clearly Ir;J""I. Tbe matrix (1.4.12) with r"" = 1 is called the sample correlation malrDc. ote that R ."" O. If R = I. we say that the variables are uncorrelated. U D = diag (s;), then S = DRD. (1.4.13) E xample 1.4.1 Table 1.4.1 gives a (28x4) data matrix (Rao , 1948 ) related to"
605,unknown,"related to weights of bark deposits of 2R trees in the four directions north (N), east (E ), south (S), and west (W ), It is fouod that i,=50.S36, x,= 40.179, i, =49.679, X. =45.179. MULTIVARIATE ANALYSIS 12 Tabl. 104.1 Weights of cork deposits (in centign.ms) for 28 trees in the four directions (after Rao, 1948) N E S W N E S W 72 66 76 77 91 79 100 75 60 53 66 63 56 68 47 50 56 57 64 58 79 65 70"
606,unknown,56 57 64 58 79 65 70 61 41 29 36 38 81 80 68 58 32 32 35 36 78 55 67 60 30 35 34 26 46 38 37 38 39 39 31 27 39 35 34 37 42 43 31 25 32 30 30 32 37 40 31 25 60 50 67 54 33 29 27 36 35 37 48 39 32 30 34 28 39 36 39 31 63 45 74 63 50 34 37 40 54 46 60 52 43 37 39 50 47 51 52 43 48 54 57 43 These means suggest prima-facie differences with m ore depo sits in the N-8 directions than in tbe E-W direction
607,unknown,"tion matrices are N E S W rom' 215.761 278.136 ""'.'''] 212.075 220.879 165.254 s = S 337.504 250.272 ' W 217.932 N E S W N[' 0.885 0.905 0883] R = 1 1 0.826 0.769 0.923 . 1 Since tbese matrices are symmetric, only tbe upper half need be printed. Comparing the diagonal terms of S, we note that the sample variance is largest in the soutb direction. Furthermore, the matrix R does not seem to have a """
608,unknown,"to have a ""circular"" pattern, e.g. the correlation between Nand S is relatively high while the correlation between the other pair of opposite cardinal points Wand E is lowest. 13 INTRODUCTlON 1.4.2 Measures of multivariate scatter The matrix S is one possible multivariate generalization of the univariate notion of variance. measuring scalier about the mean. However , some­ times it is convenient t"
609,unknown,"scalter. Two common such measUJ'es are (I) the generalized variance. lSI; and (2) the roral variarion. If S. A motivation for these measUJ'es is given in Section 1.5.3 .. For both measures, large values indicate a high degree of scatter about x and low values represent concentration about i. However . each measUJ'e reflects different aspects of the variability in the data. The generalized variance"
610,unknown,"plays an important role in maxim um likelihood estimation (Chapter 5) and the total variation L~ a useful concept in principal componen t analysis (Chapter 8). Exam ple 1.4.2 (M. Gnanadesikan and Gupta, 1970) An experimental subject spoke 10 different words seven times each, and five speecb measurements were taken On each ullerance. For each word we have five variables and seven observation5. The "
611,unknown,"variables and seven observation5. The (5 X 5) covariance matrix was calculated for each word, aDd the generalized variances were as follows: 2.9, ) .3, 641.6, 26828.8. 262404.3. 169.2. 3106.8, 6 t 7671.2, 6.7. 3.0. Ordering these generalized variances. we find that fOr this speaker the ,econd word has the least variation and the eighth word has most variation. A general point of interest for ident"
612,unknown,"word has least variation for a panicular speaker. 1.5 Linear Comhinations Taking linear combina tions of the variables is one of the most important tools of multivariate analysis. A few suitably chosen combinations may provide more information than a multiplicity of the original variables, often because the dimension of tbe data is reduced. linear transforma­ tions can also sLmplify the structure "
613,unknown,"interpretation of the data more straightforward. Consider a linear comb ination y, = a,x"" + .. + a""x"",. r= I ... ., n. 0.5.1) \otL'Ll TVARIATE Ai'o-,.\.L YSTS 14 where a, ..... a"" are given. From (1.4.6) the mean y of the v, i~ !!iven by 1 n y=-a' L x,.= s'i, tl • I ( 1.5.2) and the variance is given hy 2 _1~ _ ,_ I~, - _ , _ , S ,-- L. (y,- y) - - L. a(x,- x)(x,-x)a- aSa, n r~ l "",--t ( 1.5.3) wh"
614,unknown,"n r~ l "",--t ( 1.5.3) where we have used (1.4.7). In general we may he interested in a q-dimensional linear transforma­ tion y,.= AX ,Tb. r= l ..... n, ( 1.5.4) which may be written Y = XA' + 1b'. where A is a (q x p) matrix and b is a q-vector. Usually q'"" p. The mean vector and covariance matrix of the new object~ y, are given by j = AX + b. I n S, = - L (y,-y}(y,-yl'= ASA '. n, I If A is non-si"
615,unknown,"S=A 'S,(A') '. ( 1.5.5) ( 1.5.6) ( 1.5.7) Here are several important examples of linear transformations which are used later in the book. For simplicity all of the transformations are centred 10 have mean O. 1.5.1 The scaling transformation LeI Y,= D '(x, -x). r= 1 •...• II. where D =diag (sr). This transformation scales each variable to have unit variance and thus eliminates the arbitrar­ iness i"
616,unknown,"iness in the choice of scale. For example. if ""'"" measures lengths. then y,., will be the same whether X 'll is measured in inches Or metres. Note that S, =R. 1.5.2 Mahalanobis transformation If S > 0 then S-' has a unique symmetric positive definite square root S 112 (sec equation (A.6.15». The Mahalanobis transformation is defined by r= I. .... 11. (1..'i.II) 15 INfRODUcnON Then S. = I, so that "
617,unknown,"15 INfRODUcnON Then S. = I, so that this. ·transformation eliminates the correlation be­ tween the variables and standardizes the variance of each variable. 1.5.3 Principal component transformation By the spectral decomposition theorem, the covariance matrix S may be written in the form S=GLG' , ( 1.5.9) where G is an orthogonal matrix and L is a diagonal matrix of the eigeovalues of S, I, """" 12 """
618,unknown,"mation is defined by the rolanOIl W,=G'(x,-i). r=l, .... I .. (1.5.10) Since Sw =G'SG=L is diagonal, the columns of W. called principal components. represent ("",carre/a ,ed linear combinations of the variables. In practice one hopes to summarize most of the variability in the data using only the principal components with the higbest variances. thus reducing the dimension. Since the principal compo"
619,unknown,"reducing the dimension. Since the principal components are uncorrelated with variances I, ..... I., it seemS narural to define the ""overaU"" spread of the data by some symmetric monotonically increasing function of I"" .... '"", such as n If or L 4. From Section A.6, lSI = IL l = n I, and tr S = tr L = L I,. Thus the rotation to principal components provides a motivation for the measures of multivari"
620,unknown,"of multivariate scatter discu.~ in Section 1.4.2. Note that alternative versions of the tranSfornlations in Sections 1.5.1-1.5.3 can be defined using Su instead of S. Example 1.5.1 A crans/ormation of Ihe cork dala If in the cork data of Table 1.4.1. the aim is to investigate whether the bark deposits are uniformly spread, our interest would be in linear combinations such as Y, =N+S-E-W. Y2=N-S. y"
621,unknown,"Y, =N+S-E-W. Y2=N-S. y,=E-W . Here A= [: () -1 -~J o - I o - I From Example 1.4.1 and (1.5.5). we find that the mean vector of tbe transformed variables is f = (!U!57.0.857.1.000) MOL TIV ARlATE ANAL YS1S ]6 and the covariance matrix is -20.27 -25.96J 61.27 26.96. 99.50 Obviously. the mean of y, is higher than that of Y2. and h indicating possibly more cork deposit along the north~uth axi< than al"
622,unknown,"possibly more cork deposit along the north~uth axi< than along the east-west axis. However . V (y,) is larger. If we standardize the variables so that the sum of squares of the coefficients is unity. by letting z, =(N+S-E-W)/2, z) = (E-W)/.J2. the variances are more similar. V(z,)=31.03. V(Z 2)= 30.63. V{z,)=49.75. 1.6 Geometrical Ideas In Section 1.3 we mentioned the two alternative perspectives "
623,unknown,"be used to view the data matrix. On the ODe hand, we may be interested in comparing columns of the data matrix. that is the variables. This leads to the techniques known as R -techltiques. so-called because the correla­ tion matrix R plays an important role in this approach. R -Iechniques are important in principal component analysis. factor analysis. and canonical correlation analysis. Alternativ"
624,unknown,"correlation analysis. Alternatively. we may compare the rows of the data matrix, that is, the different objects. This leads to techniques sucb as discriminant analysis, cluster analysis. and multidimensional scaling, which are known as Q ­ techniques. These two approaches correspond to different geometric ways of rep­ resenting the. (n x p) data matrix. First. the columns can be viewed as p points"
625,unknown,"points in an It -dimensional space which we shall call R -space or ob/eet space. For example. the correlation matrix bas a natural interpretation in the object space of the centred matrix Y = HX . The correlation ri• is just the cosine of the angle 8;; subtended at the origin between the two corresponding columns, Note that the correlation coefficients are a mea.'ure of Similarity hct.:au..e their"
626,unknown,"their values are large when variables are close to One anolher. 17 INTRODUCTIO Second. the 11 rows may be taken as n points in p-dimensional Q-space or variable space. A natu.ral way to compare two rows x, and x, is to look at the Euclidean distance between them: 1Ix, -x.II'= (x, -x,),(x, -x,). An altemalive procedure is 10 transform the data by one of the transformations of Section 1.5.1 or Secti"
627,unknown,"transformations of Section 1.5.1 or Section 1.5.2. and then look at the Euclidean distance between the transformed rows. Such distances playa role in cluster analysis. The most important of these distances is the Mahalal10bis disrana D"" given by D~ = liz, -z,1I2 = (x, - x,),S-'(x, - x.). ( 1.6.1) The Mahalanobis distance underlies Hotelling's r"" test and the theory of discriminant analysis. Note t"
628,unknown,"lively be defined using S"" instead of S . 1.7 Graphical Rep£esentations 1.7,1 Univariate scatters Fo r p = I and p = 2. w e can draw and interpret scatter plots fnr the data, but for p = 3 the difficulties of drawing such plots can be ap­ preciated. For p> 3. the ta.~k becomes hopeless. although computer facilities exist which allow one to examine Ihe projection of a multivariate scatter onto any "
629,unknown,scatter onto any given plane. However . the need for graphical representa­ tions when p > 3 is greater tban for the. univariale case since the relation­ .hips cannot be understood by looking at a data matrix. A simple starting point is to look into univariate plots for the p variables side by side. For the cork data of Table 1.4.1. such a representation is given in Figure 1.7.1. which indicates th
630,unknown,"1.7.1. which indicates that the distributions are somewhat skew. tn this case. Ihe variables are mea ured in the same units, and therefore a direct N , __ -'-__ ""-__ .L .... I ... _ ..... _ ... _t-.-----1. ____ •• _--'1 .... _--''-_-'-_ !----'--' --'---:a ....... _ ..... _ .... .-L-... H.,.J''-_ _ ---'----'--.L.- f---'-, --..L..,-.-.Lw.--_.1._ ._._ ...... _ ... ....l. __ -'-__ ,., __ -'-_ N - -:':"
631,unknown,"N - -:':I;--::':'o-: __ .--U.-.- I .•• -...... ' •• -!':o----!=, --.,.0,1 :--,Lt c- .0 20 30 40 50 60 70 60 90 tOO 110 Figure. 1.7.1 Uniuarjate reprf;!Setitmion of 'he C{)ri< data of Table 104.1. MULTIVARlAffi ANALysrs 18 comparison of the plots is possible. In general, we should standardize the variables before using a plot. Figure 1.7.1 does not give any idea of relationships between the variabl"
632,unknown,"variables. However , one way to exhibit the interrelationships is to plot all the observations consecutively along an axis. representing different vari­ ables by different symbols. For the cork data, a plot of tbis type is given in Figure 1.7.2. It shows the very noticeable tTee differences as weU as 105 100 !!l 95 90 85 eo 75 ~ 0 "" 8 v $ 1!l "" 70 !!l 65 o !!l $ !!l 0 1!l En 1!l 0 60 "" $ $ "" 55 ~ "
633,unknown,"0 60 "" $ $ "" 55 ~ 0 e III "" 0 $ 0 50 0 0 "" 0 45 1lI 1!l "" 0 0 "" 40 ., "" QI 35 Q $ !ao Q ~$ ~~ g~:o I 1!l!!j ,,!!! @ ""0 O "" m!!!!!! ~ : $ I 11~' ~~~~~~ '11""'""1!''' 1"" 30 2 3 4 5 6' 7 8 9 10 111 2 3 Ie 15 16 i716 19202122 23-24252627 28 Figure 1.7.2 COIISecI4tive uniVlniate representation (after Pearson. J 956). \l =- I 0= E.!!3 =S, ~ =W. 19 INTROOUcnON differences in pattern which are associated wi"
634,unknown,"19 INTROOUcnON differences in pattern which are associated with the given ordering of the lrees. (Thai is, the experimenters appear to bave chosen groups of smaU-trunked and large-lrunked trees alternately.) We also observe that Ihe 15th tree may be an outlier. These features due to ordering would remain if the Mahalanobis residuals defined by (1.5.8) were plotted. Of course, the Mabalanobis trans"
635,unknown,"course, the Mabalanobis transformation removes the main effect and adjusts every variance to unity and every covariance to zero. 1.7,2 Bivariate scatters Ano lher way of understanding the interrelationships is to loo k into all p(p - 1 )/2 bivariate scatter diagrams of the data. For the cork data, with four variables. there are six such diagrams. A method of grapbing 4 variables in two dimensions "
636,unknown,"First, draw a scatter plot for two variables, say (N, E ). Then indicate the ""alues of Sand W of a point hy plOtting the values in two directions from E loo.ot 90.0 90.0 ""0.0 60.0 50.0 40.0 30.0 20.0 ,o.0 L-~~~n-~~~~~--<f=-~~~ __ ~~~~~N 10.0 20.0 30.0 40.0 50.. 6C.:""' 7C .O 80.0 90 .0 100.0 Figure 1.7.3 A glyph ""prese""I<i1'o"" of rhe CfJrk dara of Table lA.1. MUl..TIVARIATE ANALYSIS 20 the point (N"
637,unknown,"the point (N , E). Figure 1.7'.3 gives such a plot wher e W is plotted along the negative x axis and S is plotted along the negative y axis using each point (N. E) as the origin. The markedly linear scatter of the points shows a strong dependence between Nand E. A similar dependence between S and W is shown by the similar lengths of the two ""Ieg.'· of each point. Dependence between Nand S is refle"
638,unknown,"Dependence between Nand S is reflected by the S legs being longer for large N and a similar pattern is observed between Nand W. E and W , and E and S. This method of glyphs E. Anderson, 1960 can be extended to several variables. but it does not help in understanding the multivariate complex as a whole. A related method due to Chernoff (t973) uses similar ideas to represent the observations on each"
639,unknown,"face. 1.7.3 Harmonic CIln'es Consider the rth data point X;= (x"" •...• x"",). r = I. .... n. A recent promising method (Andrews. 1972; Ball and Hall. 1970) involves plotting the curve fL(I)= x,,/./2+X,2 sin I+X., cos I""""X,~ sin 21 ... X"" cos 21+. .. (1.7.1) for each data point x"" r = 1. ...• n. over the interval - 'IT';; I';; 'IT. Thus there will be n hannonic curves drawn in two dimensions. Two da"
640,unknown,"points are compared by visually studying the curves over [-7T. 7T]. Note that the square of the L, distance [ r f.(I) - fy(I)f dl. between two curves f.(I) and 'y(I) simplifies to 7T11x -yIF. which is proportional to the square of the Euclidean distance between 1< and y. An example of the use of harmonic curves applied to cluster analysis is given in Chapler 13, where clusters appear as curves bun"
641,unknown,"given in Chapler 13, where clusters appear as curves bunched together for all values of I. Some practical hints for the plotting of harmonic curves with a large number of objects are given in Gnanadesikan (I 977). Notice that harmonic curves depend all the order in which the variables are written down . Thus. their interpretation requires some care. *1.8 Measures of Multivariate Skewness and Kurto"
642,unknown,"In Section 1.4 we have given a few basic swnmary stal;,,'tics based on the first-and the second-order moments. In general a kth-order central 2 1 U'ITRODUcnO . moment for the variables i"" ...• ~ is . 1 "" f M?, ..... ,.)=-~ n (r . -r.)', 1 ..... .1. i..J ---ro. • .."",. n r- 1 ,- 1 where j, + ... + j. = k, j,"" O. r = 1. ... ,s. As with mean and variance, we would like extensions to the multivariate "
643,unknown,"would like extensions to the multivariate case of summary measures like b, = mils· and b. =m./s', the univariate measures of skewness and kurtosis. Using the invariant functions g"". = (x. - il'S"" '(x, -x). Mardia (1970a) has defined multivariate measures of skewness and kurtosis by (1.8.1) and 1 "" b •.• = - L g~ n ,_ I ( 1.8.2) The foUowing properties are worth noting: (1) bl.. depends only on the"
644,unknown,"(1) bl.. depends only on the moments up to third order, whereas b •.• depends only on the moment~ up to fourth order excluding the third-order moment~. (2) These moments are invariant under affine transfonnations y=Ax+b. (Similar properties hold for b, and bz under changes of scale and origin.) (3) For p = 1. b, .• = b, and bz .• = bz. (4) Let D. be the Mahalanobis distance between x. and i and le"
645,unknown,"let cos an =(:1. - i),S-'(x. - i)/D)), denote the coo;ioe of the Mahalanobis angle between (x. - i) and (It. - x). Then equations (1.8.1) aDd (1.8.2) reduce to 1 "" "" h, .• =, L L (D)), cos a,.,)' '1 ,""' I~ = I ( 1.8.3) and ( 1.8.4) MULTIVARIATE ANALYS1S 22 Although b, .• cannot be negative, we note from (1.8.3) that if the data points are unifonnly distributed on a p-dimensional hypersphere. then "
646,unknown,"bl.. =0 since L cos an =0 and D, and D , are approximately equal. W e should expect "" ... »0 if there is a departure from spherical symmetry. The statistic b2 .p will pick up extreme behaviour in the MahalaDobis distances of objects from the sample mean. The use of bl.p and b2 "" to detect departure from multivariate normality is described in Chapter 5. Exercises and Complements 1.4.1 Under the tra"
647,unknown,"y, = AI, +b. r=I ........ show that (i) y=Ai+ b. (ii) S, =AS xA ', where i. S. are the mean vector and the covariance matrix for the :It~ 1.4.2 Let 1 "" S(8)=- L (:It. -a)(x,-a)' n ~, be tbe covariance matrix about x = a. Show that S(8) = S+ (j( -a)(j( -a)'. (i) Using(A.2.3k)or otherwise. show that IS(a)1 = 1S!{1 + (i"" - a)'S-'\x-a)}. and consequently min IS(a)1 = lSI. a (ii) Show tbat min. tTS(a)="
648,unknown,">(iii) Note that ISI=Oli and trS=I1i are monotonica lly increasing symmetric functions o( the eigenvalues ', ....• 1,. of S. Use this observation in constructing other measures of multivariate scatter. (iv) Using the ineqUality g.;; a. where a and g are the arithmetic and geometric means of a set of positive numbers. show that lSI.;; (n-' ITS)"". 1.4.3 (a) Fisher (1947) gives data relating to the b"
649,unknown,"(n-' ITS)"". 1.4.3 (a) Fisher (1947) gives data relating to the body weight in kilograms (x,) and the beart weight in grams (x,) of 144 cats. For the 47 female cats 23 INTROD Uc:...IO;-'; the Sums and sums of squares and products are given by I = and X'X = X 1 [ 110.9] [265.13 1029 .62] '432.5 "" 1029.62 41164.71 . Sho w that the me3!l vector and covariance matrix are i and S given in Exa mple 4.2.3"
650,unknown,"Exa mple 4.2.3. (h) For the 97 male cats from Fisher's data. the statistics are X'l- [281.3] n , _ [ 836.75 2 - 1098 .3 a d X 2 X 1 - 3275.55 3275.55] 13056.17 . Find the meall vector and covariance matrix for the sample of male cats. (c) Regarding the 144 male and female cats as a single sample. calculate the mean vector and covariance matrix. (d) Calculate the correlation coefficient for each of"
651,unknown,"I.S.1 Let M = XX . where X is a data matrix. Show that 1.5.2 Show that the scaling transformation in Section 1.5. I can be written as Y = HXD -'. Use the fact that Y 'l = O. to show that y = O. S.= R . 1.5.3 Show that the Mahalanobis transformation in Section 1.5.2 can be written as Z '=(z, .... ,z""). Hence. following the method of Exercise 1.5.2. show that z = O. S, = I. 1.5.4 For the mark data i"
652,unknown,""" = (18.458. 74.400, 0,4(0). l o.0I59 -0.4352 -0.0252 J S = 101.8400 3.0400 0.2400 S-I = l76.7016 0.14U5 6.2742 J 0.0160 -0.1885 7.2132 l 8.7405 U.02U5 U.5521 J 0.lOt3 -0.0732 . 2.6274 - 17: = MULTIVARJATE ANALYS IS 1.6.1 If we may write where Writing g., = (x, - i)'S-'(x, - i), we see that Therefore D~ = g.,. + g., - 2g.,. (i) Show that L g., =0 and L g.,. = L trS-'(lI, - i)(lI, - i)'= n tr I. = "
653,unknown,"(ii) Therefore show that and L D~= np +lIg., or • 1"" , &u=- L""Dr ,-p n , L D~=21! 2p. r.s* I 1.8.1 (Mardia.1970a) Le t u..=M,..Is~s~, where 1 "" M .. =-L (x,,-x,)P(x,.,-x,)"". n ,_ I Show that bl.2 = (1-,2)-3[U~+ 1I ~3 + 3(1 + 2,2)( U ~2 + u~,) - 2,3U""""UOJ 24 + 6'{u"",(""'12 - 11,,) + U""'(""'2' - U '2)-(2 + ,')u""u2,)) and 25 INTRODUCDON Hence, for s, =s~= I, r=O. show that b,:J. = Mio ~ M;,J + 3Mi 2 + "
654,unknown,"b,:J. = Mio ~ M;,J + 3Mi 2 + 3~, and b2.2 = M.o + M .. + 2M 2"" Thus b,.2 accumul ates the effects of M 'h M '2, M oJ, M 30 while bz.z accumulates the effects of M n , M ... and M .o• 2 Basic Properties of Random Vectors 2.1 Cumulative Distribution F unctions and Probability Density Functions Let x = (x, ..... x.)' be a random vector. By analogy with univariate theory, the cumulative distriburion l"
655,unknown,"function F defined by (2.1.1) Two important cases are absolutely continuous and discrete distributions. A random vector x is absolutely continuous if there exists a probabiliry density funcrion (p.dJ.). 1(11.), such that F(x) = r~f(u) du. He re du = du , ... du"" represents the product of p differential elements and the integral sign d enotes p·fold integration. No te that for any mea surable set D"
656,unknown,"mea surable set D ~ R "", P (xeD) = L {(u)du (2.1.2) and [{(u) du = 1. (2.1.3) In this boo k we do no t distinguish notalionally between a random vector and its realizatio n. F or a discrele random vector 1<. the total probability is concentrated on 27 8ASlC PROPERTIES OFRA . DOM VECfORS a countable (or finite) set of points {x,;j = 1. 2, ... }. D iscrete random vectors can be handled in the above "
657,unknown,"tunction ((x,) = P(x = x,). i = 1,2, ... , } ((x)=O. otherwise. (2.1.4) the (discrere) p.d./. o( x and replace the integration in (2.1.2) by the summation P(xeD) = L I(x,). (2.1.5) , ..... eD However. most of the emphasis in this book is directed towa rds abso­ lutely continuous random vectors. The support S of "" is defined as the set S = {xe R"" :{(x» OJ. (2.1.6) Tn examples the p.d.f. is usually "
658,unknown,"Tn examples the p.d.f. is usually defined only on S with the value zero elsewhere being assumed. Marginal and conditional dislributions Consider the partitioned vector "" = (x,. x;,), where ""1 and ""> have k and (p - k) elements. respectively Ik<p). The function P(x,,,; ,,~) = F(x~, .... x~. ""' •.... 00) "" called the ma rginal cumulative distribueion {ullclio"" (marg ina! c.d.£.) of ',. In contras! F"
659,unknown,"be called tbe joint p.d.f. Let "" have joint p.d.L ((x). Then tbe ma rginal p.d.f. of l(, is given by the integral of f(x) over X2~ that is. ,,(x,) = f~ {(x,. x,) d1l.2 · (2.1.7) Ihe marginal p.d.f. of x"" f,(x,). is defined similarly. For a given value of Xl. say, X I =x~. the condifional p.d.f. of X2 is rroporlional to {(x~, x,), w here the constant of proportionality can be calculated from the fa"
660,unknown,"calculated from the fact that this p.d.f. mus t integrate to one. In other words, tbe conditional p.d.f. of X 2 given "", = x~ is I - 0) f(x~. xoJ {(X2 x, - "" 1 f,(""~)' (2.1.8) (/t is assumed that (, (X~) is non -zero.) The conditional p.d.!. of x, given x, = x~ can be defined similarly. MULTIVARfATE A ALYSI$ 28 In general, two random variables can each have Ole same m arginal distribution, even wh"
661,unknown,"distribution, even when their joint distributions are different. For in­ Slance, the marginal p.dJ.s of the following joint p.d.f.s, fIx""~ x~)= L and (Morgenstern, 1956) (2.1.9) {(XI' X2) = 1 +a(2x,-I)(2x,-1), 0<.>:"" -'2<1, - I~a~l. (2.LlO ) are both uniform, although the two joint distributions are different. Independence When the conditional p.dJ. f(x,1 x, = ll~) is tbe same for all values of x~"
662,unknown,"all values of x~, then w e say that x, and ""2 are statisrically independent of each other. In such situations, r(x2 1 '"" = .x~) must be f,{x2). Hence, the joint density must equal the product of the marginals, as stated in the following theorem. Theorem 2.Ll I! ""I and "" are staristically indepelldent then fIx) = f,(x')!2(x,). ote that the variables X , and X , arc independent for tbe p.d.!. given "
663,unknown,"by (2.1.9), whereas the variables io (2. I. LO) are dependent. 2.2 Population Moments In this section we give the population analogues of the sample moments which were discussed in Section 1.4. 2.2.1 Expectation and correlation If "" is a random vector with p.dJ. ((x) then the expee/alioll or mean of a scalar-valued function g(x) is defined as Erg(x» = [ g(x)!(x) dx . (2.2.0 We assume that all nece"
664,unknown,"We assume that all necessary integrals converge, so thaI the expectations are finite. Expectations have the following properties: (J) Linearity. E[a, g,(x)+a2 g,(x)]= a ,E [g,(x)]+ a2E [g2("")]. (2.2.2) (2) Partition, x'=(xj, x~) . The expectation of a function of x, may be 29 B AS IC PROPERTIES OF RANDOM VECrORS written in terms of the marginal distribution of x, as follows: E{g(x ,)} = J~ g(""I)!("
665,unknown,"E{g(x ,)} = J~ g(""I)!(x) dx = r g(x,){,(x,) dx"" (2.2.3) When fl is known, the second expression is useful for computation. (3) 11 X I aDd X 2 are independent and g.(xtl is a function of x, only (i = 1, 2), then (2.2.4) More generally, the expectation of a matrix-valued (or vector-valued) function of X , G (x) = (g.j(x» is defined to be the matrix E[G (x)] = (E [g.,(x)]). 2.2.2 l'opnlation mean vec"
666,unknown,"E[G (x)] = (E [g.,(x)]). 2.2.2 l'opnlation mean vector and covariance matrix The vector E(x) = J.l is called the popu!atioll m eall vector of x. Thus. ""'"" = [ xl(x) dx, i = 1, ... l p. The population mean possesses the linearity property E(Ax+b )= AE(x) + b. where A(q Xp) and b(q x I) are constant. The matrix E{(x-I1)(x- I1)'}=1:= Vex) (2.2.5) (2.2.6) IS called the covariance matrix of"" also known"
667,unknown,"<:ovariance or dispenjon matrix). For conciseness, write (2.2.7) 10 describe a random vector with mean vector 11 and covariance matrix 1':. More generally we can define the covariance between two vectors, ,,(p x 1) and y{q x 1), by the (p x q ) matrix C(x, y) = E{(x-11)(Y -vy}, (2.2.8) whe re 11 = E(,,), v = E(y). Notice the following simple properties of covariances. Let Vex) = 1:=(u,j). (1) U ';"
668,unknown,"(1) U '; = C(X;, X;), i'"" j; u"" = V(X;) = ut, say. (2) 1:=E(xx')-...... '. (2.2.9) (3) V{a'x) = a'V(x)a = L a,a;u"". (2.2.10) MIJLTIVARlATE ANALYSIS 30 for all constant vectors a. Since the left-hand side of (2.2.10) is always non-negative. we get tbe following result: (4) l>O . (5) V(Ax-.-b)=AV(x)A', (2.2.11) (6) C(x. x) = VIs). (2,2.12) (7) C(x. y) = C(y. x)"" (8) C(XI +X2. y)= ('(XI' y)+ C(X2. y)"
669,unknown,"(9) If p=q. V(~+y)= V(~) + C(x. Y) + C(y. x) + V(y). (2.2.14) (lU) C(Ax. By ) =AC(,q)B '. (2.2.15) (11) if x and yare independent then C(x.y)=O . However the converse is nor true. See Exercise 2.2.2. Example 2.2.1 Let f( )- {XI + X 2 , XJ' Xz ~ n. Then O~xL,~~l~ otherwise. (2.2.16) = rE(X,)]= [7112] I'- l.E(x,) 7112 ' I= [0'11 a12]=[ 111144 -11144]. 0',. 0'22 -11144 11/144 Correlation malT;x The p"
670,unknown,"0',. 0'22 -11144 11/144 Correlation malT;x The population correlation matrix is defined in a manner similar to its sample counterpart. Let us denote the correlation ooeflicient between the ith and jth variables by Pi,. so that p"" = a,1 <T .ai' i """" j. The matrix P= (p,,) (2.2.17) with p"" = 1 is called the populatioll correlation matrix. Taking .1 = diag(O',). we bave l'=.1-llA-'_ The matrix 1';;.0"
671,unknown,"diag(O',). we bave l'=.1-llA-'_ The matrix 1';;.0 because );;;.0, and .1 is symmetric. Generalized variance By analogy with Section 1.4.2. we may also define 31 BASIC PROPERTIES OF RANDOM VECTORS the population generalized uariance and IOtal variation as Il:! and tr I. respectively. 1.2.3 Mahaianobis space We now turn to the population analogue of the Mahalanobis distance given by (1.6.1). if x an"
672,unknown,"given by (1.6.1). if x and yare two points in space. then the Mahalallobis distance between X and y, with metric 1:. is the square root of £Ii(x. y) = (X_y)'~I(lI- y). (2.2.18) (The SUbscript I may be omitted when there is no risk of confusion.) The matrix .1: is usually selected to be some convenient covariance malTix. Some examples are as follows. (J) Let x-{iJ.,,);) and let y-(I'-2, .1:}. Then "
673,unknown,"(J) Let x-{iJ.,,);) and let y-(I'-2, .1:}. Then £1( .. "" 1'-2} is a Mahalanobis distance between the parameters. ]t is invariant under transforma­ tions of tbe form x---> Ax+b. y---> Ay+b. .1:---AIA ', where A is a non-singular matrix. (2) Let x-(I'-.l:). The Mahalanobis distance between x and 1'-, L1(x.I'-). is here a random variable. (3) Let :<-(1'-1.1:). y-(p.""I). The Mahalanobis distance betwe"
674,unknown,"and y is .1(x, y). 2.2.4 Higher moments Following Section 1.8. a kth-order central moment for the variables ' .. , .... ""'. is { . } II, 1.'_ E _ f, IJ.""."""",. - II (x,. lJ.,l , .-1 where i. + ... + j, = k, j, """" 0, r = 1. .... s, Further. suitable popu lation counterparts of the measures of multivariate skewness and kurtosis for random vector x-( .. , 1:) are. respectively, {31 .• = E{(x - 1'-)'.1"
675,unknown,"{31 .• = E{(x - 1'-)'.1:-1 (y -I'-W. {32 .• = E{(l1-.. )'rI(X-I'-W , (2.2.19) (2.2.20) where x and yare identically and independently distributed (see Mardia. I <)70a). It can be seen that thcse measures are invariant under linear transformations. MULTlVARlATE ANALYStS 32 Example ~.2.2 Consider the p.d.f. given by (2.1.10) which has unitorrn marginals on (0.1). We have where fL, = T'-'{l + (-I)')/"
676,unknown,"which is the rth central moment for the uniform distribution on (0, 1). Let ')In = I-YU~(]'2. Then Consequently, using these in the population analogue of Exercise 1.8.1, we have Ilu =0, IlL2 = 4(7 - 13p')/{S(1-p')'}. 2.2.5 Conditional moments Moments of x, 2 are ,-ailed conditional moments . 1n particular. E( ,I ""') and V ex, I ""') are the conditional m ean vector and the condi­ tional variance-c"
677,unknown,"tional variance-covariance matrix of x, given ""X2' The regressioll curve of x, O n ""2 is defined by the conditional expectation function E (x , I x,), defined on the support of x,. If this function is linear in X2, then the regression is called linear. Th e conditional variance function defines the scedosric curve of x, on "" 2 ' The regression of '"" on x, is called /tomoscedasric if V(x, I x,) is "
678,unknown,"Example 2.2.3 Consider the p.d.!. given by (2.2.16). The marginal density of x., is {,(x.,)= r (x, + x,) dx, = x.,+!, H ence the regression curve of x, on x., is I, I I' x,(X,+X2) E (x,lx,)= x,f(x, x,Jdx,= , dx, I , X 2 T l O<x,< 1. (3x2 + 2) 3(1 +2x,,)' O<x, < 1. 33 B ASIC PROPERTIES OF RANDOM VEcroRS This IS a decreasing function ot x."" so the regression is not linear. Similarly, so that Vex, I "
679,unknown,"Similarly, so that Vex, I x,) = (I +6x,+6 xW{18 (1 + 2x,) '}, O<x, < I. Hence the regression is not homoscedastic. • In general, if aU the specified expectations exist. then E(x,) = E{E(x, b(2»)' (2.2.21) However, note that the conditional expectations £ ('"" 1 xJ may all be finite, even when E(,,) is infinite (see Exercise 2.2.6). 2.3 Characterutic Functions Let :r be a random p-vector. Then the c"
680,unknown,"defined as the function <I>.(t) = E(e""')= J e""'f(,,) d"", teR·. (2.3.1) As in the univariate case, we have the following properties. (1) The characteristic function always exists, <1>.(0)= 1. and 1<I>.(t)lo;;; l. (2) (Uniqueness theorem.) Two random vectors have the same c.!. if and only if they have the same distribution. (3) (Inversion theorem .) If the c.f. <I>.(t) is absolutely integrable, then"
681,unknown,"has a p.d.f. given by f(x) = (2~). [ e-'h<l>.(t) dt. (2.3.2) (4) Partition, '"" = (r"" ,,~) . The random vectors "". and ""2 are indepen­ dent if and only if their joint c.!. factorizes into the product of their respective marginal c.f.s; tbat is if (2.3.3) MULTIVARJATE ANALYSIS 34 where t' = (I;, I;). (5) . 1 {C""-'+"" } E(x',' ... x~) = ., + .+, a j a 1 <1>.(1) I 1 .. t ,I . .. Ip 1=0 when this moment"
682,unknown,"1=0 when this moment exists. (For a proof, differentiate both sides of (2.3.1) and put 1=0 .) (6) The d. of the marginal distribution of x, is simply <P.(t,. 0). (7) IT "" and yare independent random p-vectoIS then the d. of the sum X + y is the product of the c.Ls of x and y. (To prove this. notice that independence implies E(e,n.-y,) = E(e,r')E(e""').) Example 2-3.1 Let a lx,. x,) be a continuous "
683,unknown,"Example 2-3.1 Let a lx,. x,) be a continuous positive function on an open set S £ R2. Let (Xl> X2) = a(x"" x,)q(O,. 0,) exp {x, 0, + x,o,l, xE S, be a density defined for (0"" O,)E{(O,. 8,): l/q(O"" 8,)<00) where l/q(8,.I1,,= J a(x"" x,)exp {x,II, + x,lIz} dx, dx, is a normalization constant. Since this integral converges absolutely and uniformly over compact sets of (0"" 0,). q can be extended by anal"
684,unknown,"uniformly over compact sets of (0"" 0,). q can be extended by analytic continuation to give <P <, . .,(t,. I,) = J e""'<'+""''''{(x,. x2 ) dx, dx, = q(8,. 8,)/q(8, + ill' 92+ il,). • We end this section with an important result. Theorem 2.3.7 (Cramer-Wold) The dislributioll of a random p-veclo, X is complelely determilled by the sel of all one-dimellSional distribulimlS of Ii'lear combinations t'x. w"
685,unknown,"Ii'lear combinations t'x. where t E R P ranges tllTough all fixed p-vectors. Proof Let Y = t'x and let the c.f. of y be d>,ls) = E[e""""j= E[e b ""']. Ciearly for s = 1, <1>, (1) = E[ e'h], wbich. regarded as a function of I, is tht: d. of E. • The Cramer-Wold theorem implies that a mUltivariate probability 35 BASIC PROPERTIES OF RA~lX)M VECTORS distribution can be defined completely by specifying tb"
686,unknown,"its linear combinations. 2.4 Transformations Suppose that fIx) is the p.dl. of x. and let x = u(y) be a tTansformation from y to x which is one-to-one except possibly on sets of Lebesgue measure 0 in the suppmL~ of x and y. Then the p.d.L of y is f[u(y)}]. (2.4.1) where J is the Jacobian of the transformation from y to x. It is defined by ) = absolute value of III. J=(Cx,). dY, (2.4.2) and we supp"
687,unknown,"dY, (2.4.2) and we suppose that J is never zero or infinite except possibly on a set of Lebesgue measure O. For some problems, it is easier to compute J from - I ny I J '= absolute valu"" of iI~ (2.4-3) using the inverse transformation y = u-'(,,). and then substitute fOT x in ternlS of y. (1) Unear tran.~formalion. Let )' ~Ax b. (2.4.4) where A is a non-singular matrix. Clearly, x = A -J(y - b). T"
688,unknown,"fore ox,/iJy, = £tOo. aDd the Jacobian of the transformation y to "" is (2.4.5) (2) Polar trailSformation. A generalization of the two-dimensional polar transformation x = r cos 8, to p dimensions is y = rsin /I, r>O. 0,.,0<211, ,.='0(9). 9 =(6, ..... 6p _,)'. (2.4.6) where .-, u, (9) = cos 8, n sin 6,. sin 60 =005 lip = I. r "" and j=l. .. p-2. ,>0. MULTIVARIATE ANALYSIS Table 2.4.1 Jacobians of so"
689,unknown,"Transfonnation Y to X X = y-I X = AY + B X = AYB X=AYA ' X = Y¥ Res triction 1'( p X p) aod non-singular (aU elements random) V symmetric and non-singular Y (p x p). A (pxp) non-singular. B (p xp) Y(p X q), A(p x p), and B (qxq) non-singular Y(p X p) symme tric, A (p x p) non-siogular Y lower triangular J~hian (absolute value) I -~ 36 The Jacobian of the transformation from (r,9) to x is 0 - 1 J ="
690,unknown,"0 - 1 J = , p - 1 IT sin P - I 8i- 1 . (2.4.7) Note that the transformation is one to one except when r = 0 or 0, = 0 Or -rr, for any j = I. ...• p-2. (3) Rosenblatt's transfomwtion (Ro senblatt, 1952). Suppose that x has p.d.L fix) and denote the conditional c.d.f. of x, given xl _ .. ,X,- l by F(x, I x,, ... ,x,-,), j = 1. 2, .... p. 'lbe Jacobian of the trartsformation x to y, where y, = F(x, I"
691,unknown,"the trartsformation x to y, where y, = F(x, I x,"" .. ,x,-,). i=l, ... ,p~ (2.4.!!) is given by {(x"" .... x,,). Hence, looking at the transfonnation y to x, we see that y"" ..• , y. are independent identically distributed uniform variables on (0. 1). Some other Jacobians useful in multivariate analysis are listed in Table 2.4.1. For their proof, see Deemer and Olkin (1951). 2.5 The Multinormal Distr"
692,unknown,"2.5 The Multinormal Distribution 2.5..1 Definition In this section we introduce tbe most important multivariate probability distribution. namely the multivariate normal di.tribution. U we write the 37 BASIC PROPERTIES OF RANDOM VECTORS p.d.f. of N(,.,..0'2), the univariate normal with mean ,.,. and variance ~2>O, as {(X) = {21f<T2}-Jn exp {-1{x - ,.,.)t.r}- '(x - ,.,.)}, then a plausible extension"
693,unknown,"f(x) = 12-rrII-1I2 exp t -1{x-,,)'I-1 (x-,,)}, (2.5.1) where .1:> O. (Observe that the cortstant can be also written l<2-rr)p/2III'I2}-'.) Obviously, (2.5.1) is positive. It will be shown below in Theorem 2.5.1 that the total integral is unity, but first we give a fonnal definition. Definition 2.5.1 The random vector x is said to have a p-variate normal (or p-dimensional multinormal or multivariat"
694,unknown,"(or p-dimensional multinormal or multivariate IIOnnal) distribution with mean vector"" and covariallce matrix .I if its p.d.f. is given by (2.5.1). We w rite,. - N p ("". I). The quadratic form in 2.5.1 is equivalent to o 0 L L 0'''("",- "",)(..,.- 1-';), 1_ 1 i-I Th e p.d.f. may also be written in terms of correlations rather than covariances . Theorem 2.5.1 LeI x have the p.d.f. givell by (2.5.1), a"
695,unknown,"Theorem 2.5.1 LeI x have the p.d.f. givell by (2.5.1), and let y = L 1J2(lt-,,), (2.5.2) w here LIn is lhe symmetric posilive-de/inik squar~ root of I-'. Then Yl"" .. , Y. are independent N(O , 1) variables. Proof From (2.5.2), we have (x-jl.j'L'(x-jl.)=Y'y. (2.5.3) From (2.4.5) Ihe Jacobian of the transformation y 10 x is III'n. Hence, using (2.5.1), the p.d.f. of y is (y) _ 1 -x""n. g -(2-rr)p/2e "
696,unknown,"(y) _ 1 -x""n. g -(2-rr)p/2e ' . N ote that since g(y) integrates to 1, (2.5.1) is a density. CoroUary 2.5.L.l If:r has the p.d.f. given by (2.5.1) then E(x) =,., Vex ) = J:. Prool We have E(y) = 0. V(y)=I. (2.5.4) (2.5.5) MULTIVARIATE A.""'lAJ YSIS From (2.5.2). ,,= 'I'!2y + IL Using Theorem 2.2.1, the result follows. • 3H 12.5.6) For p = 2, it is usual to write P'2 as P. -1 < P < 1. In this case t"
697,unknown,"becomes I {(x"" x,) = 2 (1 2)'12 x 1rU.U2 -p exp [ ] l(x'-IL,)2 2( 1_,.» a~ 2.5.2 Geometry We now look at Orne of the above ideas geometncally. The multivariate normal distribution in p dimensions has constant density on ellipses Or ellipsoids of the form (2.5.7) c being a constant. These ellipsoids are called the COlltOl<rS o[ the distribution or the ""ellipsoids of equal concentrations"". For "", = "
698,unknown,"distribution or the ""ellipsoids of equal concentrations"". For "", = O. these contours are centred at x = O. and when 'I = I the contours are circles or in higher dimensions spheres Or byperspheres. Figure 2.5.1 shows a family of such contours for selected values of c for the bivariate case. and Figure 2.5.2 shows various types of contour lor differing"", and I. The principal component transformation"
699,unknown,"The principal component transformation facilitates interpretation of the ellipsoids of equal concentration. Using the spectral decomposition theorem (Theorem A .6.4). writel:= rAr'. where A = diag (A, •...• Ap) is the matrix of eigenvalues of I. and r is an orthogonal matrix whose columns are the corresponding eigenvectors. As in Section 1.5.3, define the principal componenl Iralls{on/lutio/l by y"
700,unknown,"the principal componenl Iralls{on/lutio/l by y = [""(x -11-). In terms of y. (2.5.7) becomes so that the components of y represent axes of the ellipsOid. TIus property is illustrated in Figure 2.5.1, where y, and y, represent the major and minor semi-axes of the ellipse. respectively. 2.5.3 PropeJ1ies If x - N o("""" I). the following results may be derived. 39 BASIC PROPERTIES OF RA..'IDO\1 VECfORS "
701,unknown,"'. 6 4 2 OL-----~----~2~----~----~4------~----~6t_-+"" Figure 2.5.' Ellipses of eq14al conce,llfation for lhe hiL'ariare normal dislribUlion. .thow;""g 'he principal compot1cms )'1 and Y2. where tAo' = (3.3), (TIl = 3. (Tu = l~ ""22 = 3. Theorem 2.5.2 (2.5.8) Proof From (2.5.3) the left-hand side is L y;. where the y,' are indepen­ dent N(O, 1) by Theorem 2.5.1. Hence the result follows. • Using this"
702,unknown,"Using this theorem IVe can calculate the probability. of a point x falling within an ellipsoid (2.5.7). from chi-square tables, SIDce It amounts to calculating Pr (U '"" c'). Theorem 2.5.3 The c.f. of x is 4>,(t) = exp (it'"", - ~t'l:t) . (2.5.9) Proof Using (2.5.6). we find that 4>,(t) = E(e""') = e'''~ E(c'··'). (2.5.10) MULTIVARIATE ANALYSIS 40 -I -I o (0) (b) o (el (d l Figure 2.5.2 Ellipses of e"
703,unknown,"Figure 2.5.2 Ellipses of equal concePl:ration for the bivariate normal disrriburion with c=l. (a) ... = (1. I). l: = (~ ~). (b) jI.' = (0. O).l: = (~ ~). (e) v.·=(o.O),l:=C: -~). (d) v.'=(O,O),l:=e ~). 41 BAS IC PROPERTIES OF RANDOM VECTORS where 0 ' = t'I. til. (2.5.11) Since the y, are independent N(O,I ) from Theorem 2.5.1 , p p E(e''''') = n <p yJ"",) = n e--:12 = e-o•o12• (2.5.12) I- I i-I Sub"
704,unknown,"I- I i-I Substituting (2.5.12) and (2.5.11) in (2.5.10), we obtain the required result. • As an example of the use of c.f.s we prove the following result. Theorem 2.5.4 All IIOII-Irivial Iillear combinations of the elements of x are univariate normal. Proof Let s >'O be a p-vector. The c.f. of y =s'"" is <p,(t) = <I>.(IS) = exp {its'jL -ir2a':Ea}. whicb is the c.f. of a nonnal random variable with "
705,unknown,"a'Ia>O . • Theo rem 2.5.5 (3, .• =0, (32.., = p(p+2). Proof Let V = (X- jL)'X-'(Y-jL), where I< and yare i.i.d. N.c..,l:.). Then , from (2.2.19), (3l.. =E(V'). Howe ver, the distribution of V is symmetric about V =O, and therefore E(V')=O. From (2.2.20) and (2.5.8), (32 .• = E{(x;),)= p(p+2) . • The multinormal distnoution is explored in greater detail in Chapter 3 lL,ing a density-free approach. "
706,unknown,"lL,ing a density-free approach. ·2.5.4 Singular moltinormal distribution ['he p.d.f. of N,c.., l:.) involves X-'. However , if rank (l:.) = k < p, we can define the (singular) density of I< as (271)-1(12 {'( )' ( )} (A, ... A.) 1/2 el<p - , X- Il X- x-Il , (2.5.13) whe re (1) "" lies on the hyperplane N '(x- l1) = 0, where N is a pX(p-k) matrix such tbat N'l:= O, N 'N=I._k' (2.5.14) MULTIVARIATE AN"
707,unknown,"MULTIVARIATE ANALYSIS 42 (2) 1:- is a g-inverse of 1: (see Section A.S) and A "". "", >'. are the noD-zero eigenvalues of 1:. There is a close connection between the singular density (2.5.13) and the non-singular multinormal d;stribution in k dimensions. Theorem 2.5.6 Let y-N.(O. A ,), where A, = diag(A, ....• Ad. Then there exists a (p x k) column orrhononnal marrix 8 (that is, 8'8 = I.) such that "
708,unknown,"that I = 8Y + 11 (2.5.15) has the p.d./. (2.5.13). Proof The change of variables formula (2.4.1) has a generalization applicable to hypersurfaces. If x=4>(y), Y E R '. IERP, is a parametriza­ tion of a k-dimensional hypersurface in R P(k""'p), and g(y) is a p.d.!. on R', then"" has a p.d.!' on the hypersurface given by [(I) = g(4) -'(x» 10 '0 1-112, (2.5.16) where D =l>(x) = (CleMY ») I c3y, '-""' • "
709,unknown,"is a (p x k) matrix evaluated at y = 4>-'(""), and we suppose 10'01 is never zero. By the spectral decomposition theorem we can write 1: = r Ar', where A=diag(>. "" .... A.,O .... ,O) and f=(B:N) is an orthonormal matrix partitioned so that 8 is a (p x k ) matrix. Then 1: = 8A,8 ', 8'8 = I., B 'N = 0, and N'N = 1,,-•. Notice that N can be taken to be the same as in (2.5.14). The transformation"" = 8y"
710,unknown,"The transformation"" = 8y+"", parametrizes the hyperplaneN'(I-IL) = O. The p.d.l. of y is given by (2."".)-1<12 (1 ~ Y~ exp -~ l...-(A, ... A.)'12 A, ' Now r Y?/A,=y'A;'y=(x- "",)'8A~' B'(X-I1) and BA -;-'8' is a g-inverse of 1:. Also, for this transformation, D = B and IB'8I'I2= 11.1'12= 1. Thus, Lhe p.d.f. of ((x) takes the form given in (2.5.13). • Using Theorem 2.5.6, it is easy to show that many "
711,unknown,"non-singular multinormal carry over LO the singular case. Corollary 2.5.6.1 E(x)= "",. V(x) = 1:. • Corollary 2_5.6.2 c/>.(t)=exp{it'I1- WU}. • (2.5.17) 43 HA~I("" PROPERTlE$ OF RAMX>~r VECTORS CoroUary 2.5.6_3 If a is a p-t:eclOr and a'l:a> O, then s'x has a uni­ !'onate nomlat disrribunon. • 2.5.5 The matrix normal distribution Le t X(n x p) be a matrix wh ose "" rows. x~, ... ,x:., are independenU"
712,unknown,"dIStributed as N p ()l..1:). Then X has the matrix nonnal disrrib,llion and represents a random sample from N,,(I1.1:). Using (2.5.1), w e find that the p.d.f. of X is n f(X) =12."".l:I nl2 exp {-i r (X,-l1yr-'(X, - I1)} ,., 2.6 SQme Multivariate Generalizations of Univariate Distributions We j!lVe below three COm m On techniques which are used to generalize univariate distnbuLions to higher dimens"
713,unknown,"univariate distnbuLions to higher dimensions. However, caution must be e.\ercised because in some cases there is more than one plausible way to carry out the generalization. 2.6.1 Direct generaIizations Often a property used to denve a univariate distribution ha, a plausible (though not necessarily unique) extension to higher dimensions. (1) The simplest example IS the multivariate norma l where t"
714,unknown,"tenn (x - ,...)2/0'2 in the exponent of the one-dimensional p.d.f. is generalized to the quadratic form (x-I1)'1:-'{X-IL). (Z) U x-N.()I.,1:) and l~ =exp(x,). i= I ..... p, then 0 is said to bave a multivariate log-normal distributioll with parameters 11, 1:. (3) Lei x-Np ()I.,1:) and )'-X; be independe nt. and set U, =xj(Y/V)ll2. i = I, .. "" p. Then 0 has a multivariate Student's t-disrribution w"
715,unknown,"parameters 1L,1:, and., (Corni..h. 1954; Dunnett and Sobel. 1954). The case II = 1 is termed the multivariate eauch)' disrributioll. See Exercise 2.6.5. (4) The Wishan distributioll defined in Charrer 3 is a matrix generali­ zation of the X2 distribution. The p.dJ. is given in (3.8.1). MUL TIV ARIA TE ANAL VSlS 44 (5) The multivariate Pareto distribution (Mardia. 1962. 1964a) has p.d.L (. )-I[ • x"
716,unknown,"(. )-I[ • x,]-<.'Ol f(x)=a(a+I) ... (a+p-l) .I1 bi I-p+'~1 b. . Xi >b "" i = 1, ...• 17, (2.6.1) with parameters bi > 0, i = 1. ... ,17, and a> O. It generalizes the univariate Pareto distribution because its p.d.f. is given by a linear function of"" raised to some power. See Exercise 2.6.4. (6) The Dirichler disrribtlcion is a generalization to p dimensions of the beta distribution (see Appendix B)"
717,unknown,"f(x"" ... ,x.,) r(t ~) ( . ) .. _ 1 0 '_0 1"" n . - 1 p - L xt x,', n r(a,) .-1 ,-I i .. O • x,>O, i=I •...• p. L "",""'1. (2.6.2) ,-I where ai > O. i = O. I •.... p. are parameters. (7) The mulrinomial distribution is a discrete generalization to p dimen­ sions of the binomial distribution. For parameters a I, ...• ap (ai > O. i = I •...• p. I~- I a, = I) and n (a positive integer), tbe probabilities"
718,unknown,"are given by (2.6.3) o II, '"" O. i = 1, ...• P. L ""i = '"" '=1 otherwise. The mean and covariance matrix of this clistribution are given in Exercise 2.6.6 and its limiting behaviour for large n is given in Example 2.9.1. 2.6.2 Common components Let ffi be some family of distributions in one dimension and let "" 0. II .... ..... denote independent members of ffi. Set i = t •...• p. 45 BASIC PROPERTIE"
719,unknown,"i = t •...• p. 45 BASIC PROPERTIES OF RANDOM VECfORS rhen the distribution of "" is one possible generalization of ~. This approach has been used to construct a multivariate Poisson distribution (see Exercise 2.6.2) and a multivariate gamma distribution (Ramab han­ dran. 1951). 2.6.3 Stochastic generalizations Som etimes a probabilistic argument used to construct a distribution in ,)ne dimension ca"
720,unknown,",)ne dimension can be generalized to higher dimensions. The simplest example is the use of the multivariate central limit theorem (Section 2.9) to justify the multivariate normal distribution. As another example. consider a multivariate exponential distribution (Weinma n. 1966 ; Jobn­ 'on and Kotz, 1972). A system has p identical components witb times to failure Xl •••.• x.,. Initially. each compo"
721,unknown,"failure Xl •••.• x.,. Initially. each component has an independent failure rate ,1,0> 0. (If the failure rate were constant for aU time. the life time of eacb component would have an exponential distribution. fix. Au)= A o' exp (-X/Ao). x>O.) Ilowever. once a component fails. the failure rate of the remammg ,""mpo nents changes. More specifically. conditional on exactly k compo­ .Icnts having faile"
722,unknown,".Icnts having failed by time I> 0, each of the remaining p - k components has a failure rate A. > 0 (until the (k + l}th component fails). It can be ,hown that the joint p.d.f. is given by n .I.,' exp - L (p-i) (X(I_ll- x""Jl . (0-1 ) { O- I } j ... n 1-0 A, Xi>O' i=l, ... p, (2.6.4) where x (O) = 0 and x(1) ~ X(2) ~ ••• :s:;; x (p) are the order statistics. A different multivariate exponential dis"
723,unknown,"A different multivariate exponential distribution has been described by Mars hall and Olkin (1967). using a different underlying probability model (""''' Exercise 2.6.3). 2.7 Families of Distributions 2.7.1 lbe exponential family rhe random vector J: belongs to the general exponential family if its p.d.!. "" of Ihe form f(J:;6) =exp [aC\(6)+b""(J:)+ , ~ a.(6)bl(J:)], xeS. (2.7.1) where 6'=(6 ...... 6"
724,unknown,"where 6'=(6 ...... 6,) is th"" vcClOr 01 parameters. exp[ao(9)] is the MULTIVARIATE ANALYSIS 46 normalizing constant. and S is the support. If r=ll and a.(9)=8,\ljoO). we say that x belongs to the simple exponential family (se~ (2.7.4)). The general exponential family (2.7.1) includes most of the important UnI­ variate distributions as special cases, for instanoe the normal, Poisson, negative binom"
725,unknown,"negative binomial, and gamma distributions (Rao. 1973, p. 195). Example 2.7.1 Putting (b"" ...• b.)=(x"" ... , x"", xi, ... , x!. XIX"" x,x"" ... , x,,-lx,,) in (2.7.1). it is seen that N.(p., 1:). whose p.d.!. is given by (2.5.1), belongs to the general exponential family. Example 2.7.2 A discrete example is the logir model (Cox . 1972 ) defined by log P(xl ....• x,,) = a,,+ I a,x, + I Q,;X,X, + I ll,"
726,unknown,"defined by log P(xl ....• x,,) = a,,+ I a,x, + I Q,;X,X, + I ll,J'x,-'jX, + ... , 12.7.2) for X, = ± I. i = 1, ... , p. If a"" =(), a"". = 0, ... , the vadables are ,""depen­ dent. The logit model plays an important role in contingency tables where the variables Zi = ~(x, ~ I), taking values 0 or I. are of interest. The interpretation of parameters in (2.7.2) is indicated by i log {P(x, = I I X , ..."
727,unknown,"= a l +a I2x,+ ... - u,.x,,+a'23x2xJ+' "" (2.7.3) sinoe a23, etc., do not appear in this expression. Properties of expol1ential families (l) For the simple exponential family, [(x; 9) = exp r a,,(9) + bo(xl+ t 8ib, (x) l XES, (2.7.4) the vector (b,(x), .... bq(x» is a sufficient and complete statistic for 6. (2) Consider the set of all p.d.!.s g(x) with support S satisfying the constraints E{b, (xl"
728,unknown,"constraints E{b, (xl) = c"" i=I ..... q, (2.7.5) wbere the c, are fixed. Then the entropy E{ -log g(x)\ is maximized by the density f(x; 6) in (2.7.4), provided there exists II for which the constraints (2.7.5) are satisfied. If such a II exis!;;, the maximum is unique (see Kagan el aI., 1973, p. 409; Mardia, 1975a). 47 BASlC PROPERTIES OF RANDOM VECTORS The above maximum entropy property is very p"
729,unknown,"The above maximum entropy property is very powerful. For example, if we fix the expectations of """" ...• x"", x~ •... , x;, ""lX"" ... , x""-lx,,, ""E R"" then the maximum entropy distribution is tbe multinormal dis· tribution. Extended exponential family Let us assume that a randnm vector y is paired with an observable vector x. Then an extended family can be defined (Dempster, 1971, 1972) as (2.7.6) wh"
730,unknown,"(2.7.6) whe re at) depends on X, and the parameters 8"". If (x'. y')' has a multioor­ mal distribution, then the conditional distribution of y I x is of the form (2.7.6). Obviously, the conditional distribution (2.7.3) for the logit model i~ also a particular case. 2.7.2 The spberical family If the p.dl. of x can be written as [(x) = g(E'x) (2.7.7) then the di~tribulion of"" is said to belong to the"
731,unknown,"IS spherically symmetric. Note that r = (x'x) ,,2 denotes the Euclidean distance from X 10 the """"gin. Hence for all vectors x with the same value of r the p.d.f. given by 12.7.7) bas tbe same value. In other words. the equiprobability contours are byperspheres. Exam ple 2,7.3 [f x-N r(O, 0'21). then the p.d.f. of x is (21TO"")-c>12 exp (-ir2/0""). Hence, N r(O,0'21) is spherically symmetric. Exam pl"
732,unknown,"Exam ple 2.7,4 The multivariate Cauchy distribution with parameters 6 and I bas p.d.!. [(x) = "",-'"" 1)12 r{Mp + 1 n(l +x'x)-',-I)IZ, .H1d belongs to the spherical family (see Exercise 2.6.5). The following is an important property of spherically symmetric dis­ I ributions. Theorem 2.7.] The value of r= (x'x)11Z is srarisrically independenr of any ,cale-invariam [u""ction of l<. MULTTV ARIA TE ANALY"
733,unknown,"MULTTV ARIA TE ANALYSIS 48 Proof Using the polar transformation (2.4.6) and the Jacohian (2.4.7). we see that the joint density of (r,9), [--' ] r--'g(r')dr IT sinP -'8,_, dO, ... d9p _ "" ,-2 factorizes; hence rand 9 are statistically independent. ote that 9 is ""'Iifomlly distributed on a p-dirnensional hypersphere. A function hex) is scale-invariant if hex) = h(ax) for aU a> O. Setting a = Ilr, w"
734,unknown,"a = Ilr, we see that h(x)= II{ru(9)}=/I{u(8») depends only on 9. Thus , h (x) is independent of r. • This theorem can also be proved i.n a more general setting, which will be useful later. Theorem 2.7.2 Let X(n )< p) I"", a random matrix which when thought of as an np-vector XV is spherically symmetric. If heX) is any col .. mn-scale­ invariant [ .. nclion of X. then heX ) is indepetldelll of (r, •"
735,unknown,"invariant [ .. nclion of X. then heX ) is indepetldelll of (r, •...• ro). where j= L .... p. Proof Write R 2 = I ri-Then the density of X can be written as g(R 2 ) X n dx"". Transforming each column to polar coordinates, we get Thus (r, •.... ro) i~ independent of 9 = (9,,). A function heX) is column -scale -invariant if h(XD) = heX) for all diagonal matrices D >O. Setting D =diag(r,', .... r;'), w"
736,unknown,"h(X)= lI(r,u,(8,1) •... , r.II.(9,.,» = h(o,(9(1)"" .. ,uo (8,.,» depends only on 8. where each u/(8m ) is the function of the jth column of 8 defined as in (2.4.6). Thus, h (X) is statistically independent of (rio' ..• ro)' • The above discussion can be extended to elliptically symmetric dis­ tributions, [(x) = g«x -j.L)':I'"""" , (x -""'», by noting that y =2""2(X-""') is spherically symmetric. Often "
737,unknown,"Often results proved for the multinormal distribution Nr(O.O""() are true for all spherically symmetric distributions. For example . if the vector 49 BASt e PROPERTIES OF RA.NDOM VECTORS • x"" .... x"",+n) is spberically symmetric, then (2.7.8) w here F .... n is the F-variable (see Dempster , 1969; Kelker, 1972). 2..7.3 Stable distriblltions .\ univariate random variable x w itb c.d.!. F(x) is calle"
738,unknown,"Il, > 0. b2 > 0. there exists b>O and c real such that F(x/b,)*F(x/b,)= F((x-c)/b), w here * denotes convolution. This concept has been generalized to higher dime nsions by Levy (1937). A ralldom """"ctor "" is srable if every hnear combination of its components is univariate stable. Calix symmetric ""hout a if x-a and - (x- a) have the same distribution. Then a useful ,ubclass of the symmetric stable"
739,unknown,",ubclass of the symmetric stable laws is the set of random vectors whose ""f. is of the fonn ,. log <I>.(t) = ia't -! L (rn jt)an. (2.7.9) , -, whe re II < a ,,;2. m"""" 1. and 0 , is a (p X p) positive semi-definite matrix of m nstants for i = I, ... , m . Eq uation (2.7.9) gives the c.f. of a non­ ,m gular multivariate stable distribution if E O , > 0 (Press, 1972, p. 155). The Cauchy distribution "
740,unknown,"The Cauchy distribution and the multinormal distribution are the two "",nst impo rtant stable distributions. For further details and applications in ,.able portfolio analysis see Press (1972). C hapter 12 .. 2.8 Random Samples ("" Section 2.5.5. we met the idea of a random sample from the N .{j.l.:I) oIi\tribution. We now consider a more general situation. ""uppose that """" .... X n is a random sample"
741,unknown,"""uppose that """" .... X n is a random sample from a population with p d.f. fix; 9), where 9 is a parameter vector: that is, X ,' . •• ' Xn are "",dependently and identically distributed (Li.d.) where the p.d.!. of x, "" I (¥,; 6), i = 1, ... , II. (The function f is the same for each value of i.) We obtain now the moments of X, S and the Mahalaoobis distances, """"der the assumption of random sampling "
742,unknown,"""""der the assumption of random sampling from a popUlation with mean '"" 11,,1 covariance matrix :I. No assumption of normality is made in this ·.t·.:tion_ MUL TJV ARIA TE ANAL YS1S Theorem 2.8.1 E(i) = ... a""d Ve il = (1/,,)1:. Proof Since E (Xi) = .... we have _ 1 "" E(x)=-L.. E{x.)= .... "" Because V{Xi)= 1: and C(X i''';) = 0. i""j, Ve il = 12 r t vex,) + I C(l<;, Xi)] = ~1:, • n ~ - l j,,""; 11 The"
743,unknown,"Theorem 2.8.2 E (S) = {(n -J)/n}1:. Proof Since S =~ f x,r,-ii' n i- \ =.!. f (x,-... )(x,-"",),-(i-"",)(i-"",), n i- I and E{\\<, - "",)(x; - "",>1 = 0 for i,. j, we see that E (S) = n('!'-\)1:= n -ll:. • It n 11 No te that II E(Su) =-1 E(S) =1:. 11- so thaI Su is an unbiased estimate of 1:. 50 (2.8.1) (2.8.2) Theorem 2.8.3 Let G ={g.,.), wl1£r. g.,. = (x,-i)'S-'(x .• -i). Then whe re np E(G )=-H , II"
744,unknown,"whe re np E(G )=-H , II -1 Proof W e have the identities (see Exe rcise 1.6.1) I g.,. = 0 and L g"" = ""p. (2.8.3) 51 BASiC PROPERTfF..5 OF RANDOM VECTORS Under random sampling, the variables g"" are identically distributed, as are the variables g~(r "" s). Let their expectations be a and b. respectively. From the above identities we see that (n - J)b+a =() and nO =np. Inerefore a = p and b = - p/(n -"
745,unknown,"Inerefore a = p and b = - p/(n - 1), as stated in the theorem. • Note that D ! = g"" + g., - 2g.,.. Therefore, under the assumption of ran­ dom sampling, E(D!) = a + a -2b = 2pn/("" - I), (2.8.4) El<llIIIpJe 2.8.1 Using the student data from Table 1.1.1. where n = 5 and p = 3, we see that, under the assumpt ion of random samp ling, EI O~) = 2pn/(Il - J) = 7.5. In other words, each On should be aroun"
746,unknown,"2.74. Calculations sbow the matrix of Mahalanobis distances On to he. I).no 2.55 2.92 3.13 3.07 0.00 0.75 2.83 2.96 O.()O 2.09 2.50 0.(10 3.15 (I.OIl II can be seen thaI most of the observed values of D"" are indeed near , N. although 02J is substantially lower. Z.9 Limit Theorems \1thougb a random sample may come from a non·normal pOpulation. the "",,,,pIe mean vector i will often be approximately "
747,unknown,"I .• r~c samp le size II. l'beorem 2.9.1 (Central Limit Theorem) Let:l"" :12 "" • be an i""fi""ite ·""'Iamee of indepelldellt identically disrri(,ured ra,tdom vectors from a I,.,'rinatioll wirh mean '"" Gild variance 1:. Theil ,,-1/2 f (x, - ""') = '1'12(;: - ""') ~ N.(O. 1:) as /1 __ 00. ,~. ,,/,,'re ~ denotes ""convergence i"" discriburioll"". By an abuse of noration ."". also wrice. asymplOclCa((y. MULTIVA"
748,unknown,"MULTIVARIATE ANALYSIS 52 The following theorem shows that a transformation of an asymptoti­ cally normal random vector with small variance is also asymptotically normal . Theorem 2.9.2 If t is asymprotically normal with mean 11 alld covariallce matrix VIII, alld if f=(j, •... ,f.)' are real-valued [uncriolls which are differenriable at 11, then f(t) is asymptotically normal with mean f<lll alld co"
749,unknown,"covariance matrix D'VD ln. where d;, = a[/a~ evaluored at t="",. PToof FltSt we need the following notation. Given a sequence of ran­ dom variables {g.,} and a sequence of positive numbers {b.l, we say that g., = O .(b.) as tI ~ 00 if lim sup P (lb;;;'g.,.I>c)-+O as c -+ co; (2.9.1) ""_GCI ""';'It that is if. for all n large enough.lb;'g.,1 will with large probability not be too big. Similarly, say t"
750,unknown,"lim sup P(lb;;;'g.,.I>c)= 0 (or all c>O; (2.9.2\ "" __ m"""","" that is. if g.,lb • ....!'.... 0 as II -> 00. Note that 0.(·) and 0.(') are prob­ abilistic versions of the 0(·) and 0(') notation used for sequences of constants. Since each fi is differentiable at '"" we can write fIt) -1("",) = D '(t-"",) +lIx-l'llO(x -"",), where 11x-I'lF=I(x,-I-';Y. and where &(x-.. )-O as X--I1. The assumptions about t "
751,unknown,"1I&(t - "",)11 = o. (l) as n - 00. Thus. n 1/2[I(t) -I<Il)] = tI t/2D'(t -"",)+0.(l)~ N.(O. D'VD). • More general theorems of this type can be found in Rao (1973, p. 387). Example 2.9.1 Lei x be multinomially distributed with parameters n and a, where a,>O. i = 1, ...• p, and set z=(lIn)x. Let b= (a:12, ... . a~(2)' and w= (z:n, ... ,z!12)'. From Exercise 2.6.6, x can be written as a sum of n i.i.d."
752,unknown,"written as a sum of n i.i.d. random variables with mean a and variance matrix diag (a)-aa'. Thus, by the centrallirnit theorem. z- N.( a.;; [diag (a) -aa']) 53 BASrc PROPERTIES OF RANDOM VECTORS asymptotically. Consider the transformation given by Then t;(z)= z,'l2. i = I. .... p. ar.1 {ibi'. aZj 1:-. = D. i =- j, i'"" ;. so that. asymptotically, where }; =k diag (b, 'Hdiag (a) -aa'} diag (b,')= MI"
753,unknown,"Note that since I b~= I, l:b = 0 and hence }; is singular. Exercises and Complements 2.1.1 Consider the bivariate p.d.f. f(xl> x2) =2, O<x, <x,< 1. Calculate flex,) and [2(x,,) to show that x, and X l are dependent. l.L2 As an extension of (2.2.16), let [(x"" x,) = {y(X~ + x~). O. O<x,. X2< 1, otherwise. where <»0. (3)0. Show that y must equal [1/(<>+1)+1/«(3+1)]-' for I ( .) to integrate to one. C"
754,unknown,"probability of the followin,g events: (i) O<x"" X2<~' (ii) O<x, <!<x 2<1. (iii) 4<x,.x,,<1. (iv) O<x,,<!<x,<l. 2.2.1 (Mardia 1962, 1970b. p.91) Consider the bivariate Pareto dis­ tribution defined by fix. y) = c(x + Y -1) .-2. x,y>l, where p > O. Show that c must equal p(p+ I). Calculate the joint and marg mal c.d.f.s of x and y. If tl> I ~how that x and y each have mean MULTIV A.RlA~ A..~AL VSJS 5"
755,unknown,"1'/(1' -1). and if I' > 2 show that the covariance matrv< 'S g,ven oy {(P-l)'(p-2WI[~ ~]. Wh at happens to the expectations if 0 < 1'''';: I? If I' > 2 show thaI corr(x, y) = 111'. and that the generalized variance and total variation are (1'+ 1)/{(p-I)'\/1-2)'} and 2p/{(r-I)'(p-2)). Calculate the Tegression and scedastic curves of x on y. 2.2.2 For the random variables x and y with p.d.L -o:::<x."
756,unknown,"-o:::<x. y<ox. sbow that p = O. Hence conclude that l' = 0 does not imply independence. 2.2.3 If 1';' 2 and 1:( l' )( pI i~ the ef/uicorrelarion matrix. 1:= (1-a)l~all'. show tbat'l:;'O if and only if -(1'-1)-'''';:0';;:1 If -(,,-1) 1 <a < I, show that r' =(1-0) '[I-all +(p-l)a} '11']. If .... i = (8. 0') and jI., = 0 show that the Mabalanobis distance between them is given bv [ 1+(p-2)a ]112 4(jI"
757,unknown,"them is given bv [ 1+(p-2)a ]112 4(jI., . ....,)=1\ (I-a){l+(p - l)aj 2.2.4 (Frechet inequalities: Frechet, 195 1; Mardia. 1970c) Let x and y be random variable.< with c.d.!. H (x. y) and marginal c.d.!.s F(x) and G( y). Show that max (F - G - 1,0)"",;: H,;;: min (F. G). '2.2..5 (Mardia. 19<>7c. Mardia and Thompson, 1972) Let x and y be randolU variables with c.d.f. H(x, y) and corresponding margin"
758,unknown,"F(x) and G(}'). Show that C(x'. y')=rs L~ 1: ,,'-'v' '(H(II.II)-F(I<)G(I1)}du d., for r. s>O. Hence show that C(x. y)= r [[H(U, v)-F(u)G(v)}dl' duo 55 BASIC PROPERTIES OF RANDOM VECTORS 2.2.6 (Enis, 1973) Let x and y be random variables such that the p.d.f. of y is 1 g(y) = .J2 y-llZe-,IZ. y >U. and the conditional p.d.L of x given y is {(x I y)= (27T )-II'y""' e-,,·/2. Show that E[x I y] = 0 and h"
759,unknown,"Show that E[x I y] = 0 and henoe that EJE[x I y]] = O. but that neverthe­ less the unconditional mean of x does not exist. 2.5.1 Consider the bivariate distribution of x and y defined as follows: let II and u be independent N (O, I) random variables. Set x = u if uu ;. 0 w hile x = - u if I<U < 0, and set y = u. Show that (i) x and yare each N(O. I). but their joint distribution is nOt bivariate n"
760,unknown,"normal; (ii) x' anef y' are statistically independent. but x and yare no\. 2.6.1. Mixture of normals (Mard ia. 1974) leI </>(x; .... , 1:) be the p.d.f. of N .(I1, 1:). Consider the mixture given by the p.d.f. g,(x) = A<I>(x; .... ,.1:)+A'</>("";112,1:). where O<A< 1 and '\'=1- '\. Find a non­ singular linear transformation, y = Ax + h. for which the p.d.f. of y is given by • g,(y)=(A</>(y,-/1)+>.'"
761,unknown,", 2 where cb(·) is the p.d.1. of N (O , I), and /1' = ( .... ,-"""") '1:""'(11] -....,). Hence show that the joint cum ulant generating function log E{exp (t'}')} for y is p ~ L 1~+ log (A '+Ae.d ,,). 1_1 where the second term is the c.g.f. for the binom ial distribution. Using the cumulant function or otherwise. show that the m ultivariate measures of skewn ess and kurtosis given hy (2.2.19) and (2."
762,unknown,"for xl are MULTIVARIATE ANALYSts 56 and (32 .• = {,u'(1-6,u');14}/(t + ,u';12f+p(p + 2). 2.6.2 A multivariate Poisson distribution (Krlshnamoorthy . 1951: Ho l- gate, 1964) Let uo, u"" ... ,..., be independent Poisson variables with parameters Ao, A , -Ao, ... , A. - Ao. respectively. Write down the joint distribution of x. = u() + U;, i = 1 •... , P. and show that the marginal dis­ tributions of x"
763,unknown,"p.d.f. is given by a.£b'JI ~ x(l') y<"" A I' f(x y)=exp (-A -A +,1. )-L - . _ . ...2 I 1 2 0 x!y! .--0 a' b' r! ' where s=min(x,y), a=A,-A o, b=A 2-A o, A ,>Ao>O . A2>Ao>0 , and x(')= x(x-I) ... (x -, + 1). Furthennore, E(y I x)= b + (Ao/A ,)x. var (y I x)= b + {aAo/A rlx. 2.6.3 A multivariate exponential distn'bution (Marsh all and Olkin, 1967) In a system of p components there are 2· -1 types of "
764,unknown,"each of which is fatal to one of the subsets (i, .... , i,) of the p compo­ nents. A component dies when one of the subsets containing it receives a shock. The different types of shock have independent timing, and the shock which is falalto the subset (i"" ...• i,) has an exponential distribu­ tion with parameter A ~' .... representing the expected value of the dis­ tribution. Show that the lifetim"
765,unknown,"distributed as - log p es:> a) = f A,a, i- I + L A""I,max (a"",ai,)+ ... + A,.2. .... max (a , .... ,0.). 1,<1) 2.6.4 A mnl ti .. ariate P areto dt.tn'bution (Mardi.. 1962, 1964a) Consider the p.d.f. f(lI)=a(a+l) ... (a+p-t) n b, L b;-'x.-p+L , (• )-'(. )-< ••• ) - l 1- \ X; > bl' i = I, .... p, wbere a>O. bi>O. (i) Show tbat any subset of the components of s: has the same type of distribution. 57 B"
766,unknown,"distribution. 57 BA~IC PROPERTIES OF RA N DOM VECfORS (ii) Show that ("" )-P (x> c) = _L b, 'CO -p+ 1 • ,-, (iii) Let:l .. r = L. ...• n. be a random sample from this population. Show that 0 = min (11,) has the above distribution with the parameter a replaced by na. 2.6.5 Mulmariate Student's r and Cauchy distribution (Cornish. 1954 ; Dunnell and Sobel. 1954) A random vector has a multivariate r di"
767,unknown,"tribution if its p.d.L is of the form where Cp 1:£1 ,n retv + p)/2) ('lTV )""""n v/2) and II is known as the number of degrees of freedom of the distribution . (i) Let x,= y/(SIJv). j=I. ... ,P. where y - Nr(O,l). S2 _ X~ . and y and S are independent. Show that x has the p.d.!. g.(x; O. O. (ii) For II = 1. the distribution is known as a mulrivariate Callchy distribution. Show that its c.f. is exp {"
768,unknown,"exp {i,,'t- (I'It)'''). 2.6.6 Let e; denote the unit vectOr with 1 in the ith place and 0 elsewhere. Let y,. j = 1.2, ... , be a sequence of independent identically distributed random vectors such that P(y = e,l = 11;, i = I, ... ,p, where 11;>0. Ia,=l. (i) Show that E(y) = a, V(y) = diag (a)-aa'. (ii) Let Y =I~- LY i' Show tbat II has the multinomial distribution given in (2.6.3). (iii) Show that"
769,unknown,"in (2.6.3). (iii) Show that E(x) = lIa, Ve x) =n[diag (a) -aa']. (iv) Verify that [diag(a)-as']1 = 0, and hence deduce that V(lI) is singular. 2.7.1 Show that the only spherical distribution for which the components of II are independent is the spherical multinormal distribution. 2.8.1 In the terminology of Section 2.8. suppose E(g~) = c. E(g~ = d, r;e s. Using (1.8.2) and nc = L:' , £( ~~). show "
770,unknown,"\tULTIVARIAfE A .... ALYSIS 58 using lI(n-l)d {I·"" } lie + 2 E ,_,.~, g~=n. :show that 2 d =-{I- E(b2 .p )}. PI -1 2.8.2 (Mardia.1964b) Let U =(u , •.... u.)' be an (II Xp) data matrix from a p.d.!. flo ) and set Xi = min (Un). Yi = max (Un). ,-1.. .... "" r ~ 1.. "" (i) Show that the joint p.d.!. of (x. y) is (-I)"" 0'· { J.' {(u) dU}"" ax, ... ax,. ny,. . OJ, , (ti) Let R. = y,-X, denote the range o"
771,unknown,"Show that the joint p.d.!. of R , ..... R. IS f· f(-I}· ii"" fl'(U)dU}""] dx. ~ ax, ... ax,. ay, ... ayp ,_ ... 2.9.3 Karl Pearsoo-type inequality Let "",(x) be a non-negative func­ tion. Show that In particular. prIx - Il)'l;-'(X-Il) < E2l ~ 1-(h./E'. where (32./> is the measure of kurtosis for x. 3 Normal Distribution Theory 3.1 Characterization and Properties 3.1.1 The cenlral role of multh'llriat"
772,unknown,"There h."" been a tendency in multivariate analysis to assume that all random vcctors come from tbe multivariate normal or ""multinorma]'"" family of distributions. Among the rCaSOns for its preponderance in the multivariate context are the following: (1) The multinormal dist[ihulinn i, an ea.y generalization of its uni­ variate counterpart, and the multivariate analysis runs almost paral­ lel to the"
773,unknown,"lel to the corresponding analysis based on univariate normaliry. The same cannot be said of other mullivariate generalizations: different authors have given different extensions of the gamma , Poisson, and exponential distributions. and attempts to derive entirely suilable definitions have not yet proved entirely successful (see Section 2.6). (2) The multivariate normal distribution is entirely de"
774,unknown,and second moments-- a total of only ip(p + 3) parameters in all. Tnis compares with 2· - 1 for the multivariate binary or logit distribution (2.7.2). This economy of parameters simplifies the problems of estimation. (3) In the case of normal variables zero correlation implies indepen­ dence. and pairwise independence implies total independence. Again. other distributions do not necessarily have t
775,unknown,"Again. other distributions do not necessarily have these properties (see Exercise 2.2.2). (4) Uncar functions of a multinormal vector are themselves univariate normal. This opens the door to an extremely simple derivation of MULTIVARIATE ANALYSIS 60 multioormal theory. as developed here. Again, other distributions may not have this property, e.g. linear fuoctions of multivariate binary variables a"
776,unknown,"binary variables aTe nO! tbemselves binary. (5) Even when the original data is oot multinormal, one can often appeal to central limit theorems which prove that certain functions such as the sample mean are normal for large samples (Section 2.9). (6) The equiprobability contours of the multinormal distribution are simple ellipses, which by a suitable change of coordinates can be made into circles ("
777,unknown,"made into circles (or, in the general case, hyperspheres). This geometric simplicity. together with the associated invariance prop­ erties. allows us to derive many crucial prOperties through intui­ tively appealing arguments. In this chapter. unlike Section 2.5. we shall use a denSIty-free ap­ proach. and try to emphasize the interrelationships between different distributions without using their "
778,unknown,distributions without using their actual p.dJ.s. 3.1.2 A definition by characterization In this chapter we· shall define the multi normal distribution with the help of the Cramer-Wold theorem (Theorem 2.3.7). This states that the multivariate distribution of aoy random p-vector x is completely deter­ mined by the univariate distributions of linear functions sucb as a'x. where a may be any non-rand
779,unknown,"Definition 3.1.1 We my tflat x 'las a p-variate normal distribution if and only if a'"" is univariate nomtal for all fixed p-vectors s. • To allow for the case a = O. we regard constants as degenerate forms of the normal distribution. The above definition of multinormality has a useful geometric in­ terpretation. 1f x is visualized as a random point in p-dimensional space, then linear combmations s"
780,unknown,"then linear combmations such as a'x can be regarded as projections of x ooto a one-<limensional subspace. Definition 3.1.1 therefore implies that the projection of x onto all one-dime(lsiooal subspaces has a univariate normal distribution. This geometric interpretation makes it clear that even after x is transformed by any arbitrary shift, rotation, or projection, it will still have the property o"
781,unknown,"it will still have the property of normality. In coordinate-dependent terms, this may be stated mOre precisely as fnllows. (In this theorem and others that follow we will assume that matrices and vectors such a~ A, b, and care oon-random unless otberwise stated.) 61 NORMAL DISTRJBtrnON THEORY Theorem 3.1.1 If"" has a p-variare normal distribution, and if y = Al<+c, where A is any (qxp) matrix and c"
782,unknown,"q-varillfe nurmal distribution. • Proof Let b be any fixed q-vector. Then b'y=s'x+ b'c, wbere a=A'b . Since x is multinormal, s'x is univariate normal by Definition 3.1.1. Therefore b'y is also univariate normal for all fixed vectors b, and therefore y is multinormal by virtue of Definition 3.1.1. • Corollary 3.1_1.1 AllY subset of elements of a multinormal vector itself has a multinormal distrib."
783,unknown,"have lm;variate normal distri/)wions. • Note that the above theorem and corollary need not assume that the covariance matrix I is of full rank. Therefore these results apply also to the smgular multinormal distribution (Section 2.5.4). Also, the proofs do not use any intrinsic property of normality. Therefore similar results bold m principle for any other multivariate distribution defined in a sin"
784,unknown,"way. That is, if we 'were to say tbat x bas a p-variate ""M"" distribution whe oever a'x is univariate ""M"" for all fixed a (""M"" could be ""Cauchy"") then results analogous to Theorem 3.1.1 and Corollary 3.1.1.1 could be derived. However. before proceeding further, we must prove the existence of the multinormal distribution. This is done by showing tbat Definition 3.1.1 leads to the c.f. which bas alre"
785,unknown,"3.1.1 leads to the c.f. which bas already been referred to m (2.5.9) and (2.5.17). Theorem 3.1.2 If x is mu/tinorma/ with mea'l vector"" a'ld covariance matrix 1:(1:;;. 0), then irs c.f-is given by (3.1.1) Proof We follow the lines of the Cramer-Wold theorem (Theorem 2.3.7), and note that if y = t' x then y has mean t'"" and variance t'It. Since y is unh'ariate normal, y - N(t'"", t'It). Therefore fr"
786,unknown,"univariate nonnal distribution, the d. of y is q,.(s) = E (exp isy)= exp (ist',,-ts't'It). Hence the c.f. of "" must be given by <i>.(t)= E(exp it'x) = E(exp iy) = <i>.(l) = exp (it'j.l-~It). From Section 2.5, we see that (3.1.1) is indeed the C.r. of a multivariate MULTIVARIATE A.""'lALYSIS 62 distribution. Hence the multinormal distribution wIth m~an .. and covariance matrix X exisL~ and its c.f. "
787,unknown,"As in Section 2.5. we may summarize the statement. "", is p-variate normal with mean .. and covariance matrix r·. by writing x - N.(v..X). When the dimension is clear we may omit the subscript p. We can obtain the p.d.f. when X >O using the inver.;ion fonnula (2.3.2l. and it is given by (2.5.1). Theorem 3.1.3 <al Two jointly lIIultinonnal vectors are "",dependent if a""d only if rhey are uncorrelated"
788,unknown,"(bl For IwO jointly mullino,mal vectors. pair-wise independence of rheir components implies complete independence. • Proof The c.f. given in Theorem 3.1.2 factorizes as required only wben the corresponding submatrix of X is zero. This happens only when the vector.; are uncorrelated. • 3.2 Linear Fonns Theorem 3.1.1 proved that if x - N.(v..l:l and y = A.x+c. where A is any (q x p) matrix. then y b"
789,unknown,"(q x p) matrix. then y bas a q-variate normal distributIon. Now from Section 2.2.2 we know that the moments of yare Aj4+ c and Al:A·. Hence we deduce immediately the following results. Theorem 3.2.1 If .r·-N,,(I1.l:l and y = Ax + c. rh"""" y- N.(Aj4+t.Al:A') . • CoroUary 3.2.1.1 If x - N.(v..X)and X>O. rhen y = X -""2(X - .. )­ N.(O.I) and (x-.. )'r'<S- ,,)=LY~-X~ .• Corollary 3.2.1.2 If x - N.(IJ.. "
790,unknown,"Corollary 3.2.1.2 If x - N.(IJ.. 0'21) attd G(q x p) is any row-()rr/lOlI()n1wl malTix. i.e. sali$fying GG ' = I •. then Gr - N.(G ... ,,'I). • CoroUary 3.2.1.3 If x - N.(O. n and a is any nOll-zero p-veCfOf. rhel. a'~R8 has til<! srandard univariare normal dislTiburiOlI. • Corollary 3.2.1.1 shows that any normal vector can easily be convened into standard form. It also gives an important quadrati"
791,unknown,"has a cbi-squared distribution. From Corollary 3.2.1.2 we note thaI the standard multinormal distribution has a certain invariance under or­ thogonal transformations. Note that Cor'JUary 3.2.1.3 also applies if. is a random vector independent of x (see Exercise 3.2.4). A funher direct result of Theorem 3.1.3 is the following. 63 II..ORMAI DtSTRlBtrrtO .... THEORY Theorem 3.2.2 If"" - N.( ... X). th"
792,unknown,"Theorem 3.2.2 If"" - N.( ... X). then Ax and Bs are independenr if and only if Al:B' = O. • CoroUary 3.2.2.1 If"" - N r( ... 0""1) and G iI any ,ow·orthonormal mar· rix. then G"" is independent of (I-G'G)"". • If x is partitioned into two subvector.;. with, and s elements, respec· lively. then by noting two particular matrices which satisfy the conditions of Theorem 3.2.2. we may prove the followin)! T"
793,unknown,"Theorem 3.2.3 If"" = (x~. xi)' - N.c.., Il. rhen "" atld Xu = x, - l:""Xi,'x, have the (ollowing distributions and are statistically independent: X, ,-N,(v.,.,.l:"",) where (3.2.1 \ Proof We may write '"" = Ax where A = [I. OJ. and :r"" = Hx where B =[-X,.l:,:,J]. Therefore. by Theorem 3.2.1. "", and x,, are normal. Their moments are Aj4. Al:A '. BI1. and BXB' . which SImplify to the given exp re""lons. T"
794,unknown,"exp re""lons. To prove independence nOle that Al:B ' = O. and use Theorem 3.2.2 . • Similar r~ults hold (using g-mverses, for the case of singular dislTibu­ bans. The above theorem can now be used to find the condilional distnbutlon of X2: when XI 15 known. Theorem 3.2.4 Using the assumptions and IIorlltioll of Theorem 3.2.3. tIle condirional distribution of x, for a given vlll"",e of '"" is Proof Si"
795,unknown,"Proof Since ~2 .J Lt\ independent of XI' its conditional distrihution for a !!iven value of x, is the same as its marginal disfribution. which was Slated in Theorem 3.2.3. Now X2 is simply x,; plus X""I""',,,. and this term is constant when x, is given. Therefore the conditional distribution o( ""21 x, IS normal. and iL~ conditional mean is (3.2.2) The conditional covariance matrix of x~ is the same "
796,unknown,"l:"" ,. • MULTIVARIATE ANALYSIS 64 H the assumption of normality is dropped from Theorem 3.2.3. then '"" and x,., still have the means and covariances stated. Instead of being independent of each other, however, all that can be said in general is that '"" and x,., are uncorrelated. When p = 2 and X, and X, are both scalars. then the expressions given above simplify. Putting ui, ui, and pUt U2 in plac"
797,unknown,"find that :I.,2 J =lT~(1_p2). (3.2.3) so that the conditional distribution of X2 given X, is Example 3.2.1 If 1: is the equicorrelation matrix 1:= (l-p)l+pU', then the conditional distributions take a special form. Note that 1:11 and l:"" are equicorrelation matrices of order (r x r) and (s x s). respectively. Also 1:'2 = pl,l: and l:"" = pl.l:. Furthermore. we know from (A.3.2b) that 1:""t = (l-ptt["
798,unknown,"that 1:""t = (l-ptt[l-uU'], p 1 +(r-l}p Therefore. l:,tl:l,t =pU' 1:,; = p(l_p)'l(l_ ""r\U' = QU'. Substituting in (3.2.2\ we find that the conditional mean is and the conditional covariance matrix is l:"".1 =(1-p)1 +pt.t;-pa1.~l,.l: =(I-p)1-p(l-ro)11'. ote tbat the conditional mean is just the original mean,., with each element altered by the same amount. Moreover. this amount is propor­ tional to l"
799,unknown,"tional to l'(xt -11,), the sum of the deviations of the elements of "", from their respective means. If r = 1, then the conditional mean is 3.3 Transformations of Normal Data Matrices Let Xl, ... , Xn be a random sample from N{JL.1:). We caU X = (Xl> ... , x .. Y a data matrix from N(I1.1:). or simply a ""normal data 65 NORMAL DISTRlBUTION THEORY matrix"". In this section we shall consider linear fun"
800,unknown,"AXB, where A(m x II) and B(p x q) are fixed matrices of real numbers. The most important linear function is the sample mean i' = lI'tl'X, where A=n -'1' and B=Ip. The following result is immediate from Theorem 2.8.1. Theorem 3.3,1 If X("" x p) is a dala matrix from N p (I1,1:), muI if ni=X 'I, Ihet! j has lhe N.(J1, ,,"" 1:) distributiort . • We may ask under what conditions Y = AXB is itself a norm"
801,unknown,"matrix. Since y,) = I..6 a~x...pbol' clearly each element of Y i~ univariate normal. However, for Y to be a normal data matrix we also require (a) that the rows of Y should be independent of each other and (b) that each row should have the same distribution. The following theorem gives necessary and sufficient conditions on A and B . Theorem 3.3.2 If X (n x p) is a normal data matrix from N p (J1,"
802,unknown,"Y(m x q) = AXB . lhen Y is a normal dara matrix if at!d only if (a) A1 =a l for some scalar a. or B'j.I.=O, and (b) AN = III for some scalar II, or B'U =0. Whe,. boll. lhese conditiot!s are salis/ied then Y is a nomlQl data matrix from N.(aB'I1.IIB'1:B). • Proof See Exercise 3.3.4. • To understand this theorem . note that post-multiplication of X involves adding weighted variables. while pre-mUlti"
803,unknown,"adding weighted variables. while pre-mUltiplication of X adds weighted objects. Since the original objects (rows of X) are independent, the transformed objects (rows of Y) are also independent unless the pre­ multiplication by A has introduced some interdependence. This clearly cannot happen when A is kl, since then a = k and (3 = k', and both conditions of the theorem are satisfied. Similarly. al"
804,unknown,"satisfy the conditions required on A . We may also investigate the correlation structure between two linear transformations of X . Conditions for independence are stated in the following theorem. Theorem 3.3.3 if X is a data marrix from N(J1, :1:). and if Y = AXB at!d Z = CXD, lhen the elements of Yare ilJdepe""dellt of the elemePlts of Z if and only if eil/ter (a) B'I.D = 0 or (b) AC' = O. • Proof"
805,unknown,"Proof See Exercise 3.3.5. • MULTNARIATE ANALYSIS 66 This result is also valid in the situation where the rows of X do not bave the same mean; see Exercise 3.4.20. Corollary 3.3.3.1 Under the collditions of the theorem. if X = (X .. X 2) then X, is independent of X 2 1 = X 2 - X,k.,'""I'2' Also X, is a dala matrix from N(,..,. ""III) arid X 2 , is a data matrix from Nu...."" ""I,2'). where 1'2, and ""I,"
806,unknown,"and ""I,21 are defined ill 13.2.1). • Proof We have X,=XB where 8'=(1,0). and X 2.,=XD where D'= (-""I,,""I .... ,'.I). Since B'ro= 0 the result follows from part (a) of the theorem . • Corollary 3.3.3.2 Under the conditions of the theorem. i = n-1X'1 is independent of HX , and therefore i is independent of s= ,,-'X'HX. • Proof Put A =n-'I' and C=H = J-n 'll' in the theorem. Since AC'= O. the result "
807,unknown,"O. the result follows. • 3.4. The WIShart Distribution 3.4.1 lotroduction We now turn from linear functions to consider matrix-valued quadratic functions of the form X'CX. where C is a symmetric matrix. Among such functions the most important special case is the sample covariance matrix S obtained by putting C = II 'H, where H is the centring matrix. (However, other quadratic functions can also be"
808,unknown,"(However, other quadratic functions can also be used. for instance, in per­ muting the rows of X, Or in finding within-group and between group covariance matrices in regression analysis). These quadratic forms often lead to the Wishart dIStribution, which constitutes a matrix generalization of the univariate chi squared distribution, and has many similar properties. Definiooo 3.4.1 If M(p x p) can"
809,unknown,"Definiooo 3.4.1 If M(p x p) can be w,ittell M = X'X. where X (m x p) is a data matrix f,om Np(O . ""I). tlten M is said to have a Wishart distribution with scale matrix ""I and deg,ees of freedom parameter m. We wrile M- Wpf""I. m). When ""I= I., Ihe distribution is said 10 be in standard form . • Note when p= 1 that the W,(O'2 • m) distribution is given by x'x. where the elements of .(m x 1) are Li.d"
810,unknown,"the elements of .(m x 1) are Li.d. N1(O. ,,2) variables; that is the W,(O'2• m) distribution is the same as the O'2x;, distribution. The scale matrix ""I plays the same role in the Wishart distribution as 0'2 does in the O'2 x;, distribution. We shall usually suppose ""I> O. Note that the first moment of l\f is given by E[ M ]= f E[ .,.:l=ml: ,-, 67 NOR.""tAL DISTRfBUllON THEORY 3.4.2 Properties of W"
811,unknown,"Theorem 3.4.1 If M - W.(""I. m) and 8 is a (p x q) matrIX, then B 'MB ­ W.(B'""!.B. mI .• Proof The theorem foUows directly from Definition 3.4.1, since B 'MB = B'X 'XB = Y'Y where Y = XB and the rows of X are Li.d. Np(O. ""I). From Theorem 3.3.2. the rows of Y are i.i.d. N.(O, B'""IB). Therefore. using Definition 3.4.1 again. Y'Y has the stated distribution. • Corollary 3.4.1.1 Diagollal submatrices "
812,unknown,"Corollary 3.4.1.1 Diagollal submatrices of M tllemselves have a Wisllan distributioll. • Corollary 3.4.1.2 ""I ""'M""!.-II1_ Wpll. mI. • Corollary 3.4.1.3 If M- W.(1. m) and 8 (pXq) satisfies B ·8 = 1 •. then 8'MB W""II. mI . • The corollaries follow hy inserting particular values of B in the theorem. Note from Corollary 3.4.1.1 thaI each diagonal element of M has a O'~ X;, distribution. The following"
813,unknown,"has a O'~ X;, distribution. The following theorem generalizes and em­ phasizes this important relation.ship between the chi-squared and Wishart distributions. Theorem 3.4.2 If M - Well:. m). al\d a 1$ allY fixed p-vector .<ucla rlwr a'""!.a! n. tllen a'Ma/8 1:& - x: •. Proof From Theorem 3.4.1 we see that a'Ma- W,(a'""!.a. m), which is equivalent to the stated result. • Note that the converse of The"
814,unknown,"Corollary 3.4.2.1 m"" -O'.'x~. • Theorem 3.4.1 showed that the class of Wishart matrices is closed under the Iransformation M -> 8'MB. The Wishart family is also closed under addition. Theorem 3.4.3 If M,- Wl~:. m,) and M,- Wp(""I. m2). and if M , alld M, are indepelldenr, then M, + M2 - W,,(""I. m, + III.). • Proof We may write M , as X:X"" where X, bas m. independent rows taken from Np(O. ""I). i = 1"
815,unknown,"taken from Np(O. ""I). i = 1.2. But M,+M, X;X , -X~X2=X'X. MULTIVARIATE ANALYSIS 68 Now X, and X 2 may be chosen so as to be independent. in which case aU the (ml + m,) rows of X are i.i.d. N .(O, X) variables. The result then follows from Definition 3.4.1. • So far we have taken a normal data matrix X with zero mean , and derived a Wisbart distribution based on X'X. However, it is possible that ot"
816,unknown,"other functions of X apart from X'X also have a Wishart distribution. Clearly any matrix containing the sums of squares and cross-products from a subset of the rows of X also has a Wishart distribution. Such a matrix equals X'CX , where G. = 1 whenever the ith row of X is in the subset, and all other elements of C are zero. This matrix is symmetric and idempotent, whicb suggests tbe following theo"
817,unknown,"Theorem 3.4.4 (Cochran. 1934) If X(1l x p) is a data matrix from N.(O, X). and if C(II x II) is a symmetric marrix, thell (a) """",CX has the same distribution as a weighted sum of illdependem W.(X, 1) matrices. where the weigllts are eigenvalues of C; (b) X'CX has a Wishart distribu.tion if and only if C is idempOtent. in which case X'CX - W.(X, r) where r = tr C = rank C ; (c) If S=n -'X'HX is Ihe"
818,unknown,"(c) If S=n -'X'HX is Ihe sample covariance matrix, then nS ­ W.<l:. n -1). • Proof Using tbe spectral decomposition theorem (Tbeorem A.6.4). write "" C = L A1'Y(I)'Y(1) where 'Y(o'Ym = 8iJ (3.4.1) 1-' and A; and Ym are the ith eigenvalue and eigenvector of C, respectively. Using (3.4.1) we see that (3.4.2) Writing Y = f'X where r is orthogooal, it is easily seen from Theorem 3.3.2 that Y is a data "
819,unknown,"3.3.2 that Y is a data matrix from N.IO, 1:). Therefore y,y\, ... , y""y~ in (3.4.2) are a set of independent Wishart matrices each having rank one. This proves part (a) of the theorem. For part (b) oote that if C is idempotent and of rank r, theo exactly r of the Ai are non-zero. and each noo-zero A, equals 1. Also r = tr C. Heoce X'CX - Wo(I, r) as required. To prove (c) we note that H is idempot"
820,unknown,"FOI the proof in (b) that idem potence of C is in fact a necessary 69 NORMAL D1STRffil1TION TIlEORY condition for X'CX to have a Wishart distribution we refer the reader to Andersoo (1958, p. 164). • The above theorem is also valid when IL"" ° if C is a symm etric idempotent matrix whose row sums are 0 ; see Exercise 3.4.5. For an extension to the case where the rows of X do oot have the same mean "
821,unknown,"see Exercise 3.4.20. Using Exercise 3.4.5, and also results from Theorem 3.3.1 and Corol­ lary 3.3.3.2, we emphasize the following important results concerning the sample mean and covariance matrix of a random sample I"" ... ,:l,. from N.(IL.X) nS- W.(1:, n -1), i and S are independent. (3.4.3) For an alternative proof of (3.4.3). see Exercise 3.4.12. We turn now to consider pairs of functions such"
822,unknown,"and investigate the conditions under which they are independent. Theorem 3.4.5 (Craig. 1943 ; Lancaster, 1969 , p. 23) If the rOWS of X are i.i.d. N.b.,1:), and if C"" ... , C . are symmetric matrices. thell X'C,X, ...• X'C.X are joilltly illdependent if C,c. = 0 for all rf s. • Proof First consider the case k = 2 and let C, = C, C, =0. As in (3.4.2) we can write X'CX = I A,Y,-Y:, X'OX = I t/ljZ,zj"
823,unknown,"where y, =X'Ym and Zj =X'Ii(j)o ""('0 and 1iCf) being eigenvectors of C and O. r.espectively. with '\', and t/lj the corresponding eigenvalues. From Theorem 3.3.3 we note tbat y, and z, are independent if aod ouly if y(;)lim = O. Thus. the np -dimensional normal random vectors ( ' \/2y' , l12y ')' and (.,. ""'Z' .1,110Z')' witl be independent if 1\1 I •..•• 1\"" "" """"1 1,···,'1/-"" "" 'Yi l)li ~, =0 whe"
824,unknown,"'Yi l)li ~, =0 whenever '\',t/I, is non-zero; that is. if for aU i. j. (3.4.4) But If CO=O , then pre-mUltiplying by ""fl., and post-multiplying by Ii(v, gives A.t/I.""flw)li(v) = O. This holds for all II and v. and therefore (3.4.4) holds. Thus, since X'CX and X'OX are functions of independent normal np-vectors, they are independent. \tULTlVARlATE A.'iALYSIS 70 To deal with the case k > 2 notice th"
825,unknown,"\tULTlVARlATE A.'iALYSIS 70 To deal with the case k > 2 notice that normal lip-vectors which are pairwise independent are also jointly independent. (This may be easily proved in the Same way as Th eorem 3.1.3(b).) Hence. the matrices X'C,X . , = I •.... k. are jointly independent. • The converse of the theorem also holds. For a proof. the reader is referred to Ogawa (1949). A similar theorem gives"
826,unknown,"X 'CX to be independenl of a linear function like AXB (see Exercise 3.4.7). This theorem is also valid when the rows of X have different means; see Exercise 3.4.20. Note that Cralg's theorem does not require the C, to be idempotent. although if they are. and if 14 = O. then hy Cochran's theorem the Quadratic forms to which they lead are not only independent but also each have the Wishart di.tribut"
827,unknown,each have the Wishart di.tribution. An important decomposition of X'X wben 14 = 0 (and of X 'UX for general j1.) into a sum of independent Wi shart matrices is described in Exercise 3.4.6. This decomposition forms tbe basis for the multivariate analysis of vanance (see Chapter 12.) Tbese theorems can be easily extended to cover (II x II) matrices such as XCX ' (in contrast to (p x p) matrices sucb
828,unknown,"as XCX ' (in contrast to (p x p) matrices sucb as X 'CX). This is done by noting that if the rows 01 X are i.i.d N p (j1., X}. then in general the rows of X ' (that is the columns uf X) are not i.i.d. H owever, when jI. = 0 and I = I"", the rows of X ' are i.i.d. N. (0.1) vecton.. since in this case all the lip elements of X are i.i.d. Hence we can get the necessary extensions for tbe standard case"
829,unknown,"standard case (see Exercise 3.4.9), and Can thereby derive the relevant propenies for general X (Exercise 3.4.111). The special case where n = I leads to quadratic forms proper and is discussed in Exercise 3.4.11 -3.4.3 Partitioned Wishart matrices If M - W.fl:, 1M), it is often useful to partition Minto submatriccs in the usual way . For instance. we may want to divide the p original variables in"
830,unknown,"into two subgro ups, consisting of .ay a and b variables, respectively, where a +b = p. Suppose then that MIJ is (a x a), and M n is (b x b), where a ""T' b = p. We baye already nOled (Co rollary 3.4.1.1) that Mil and M n have Wishart distributions, although tbese distr,butrons are not in gene ral independent. Howev er, M II is independent of (3.4.5) Here M 22 .1 is in fact just m times the sample "
831,unknown,"(3.2.1). 71 SOR.\1AL OISTRIBUTION TIIEORY Note tbat when p equals 2. the matrix M may be written as (3.4.6) The matrix M '2 I simplifies in this context to M zz , = msi(l - r') (3.4.7) Various properties concerning the joint distribution of M il' M 12 , and M u , are proved in the following theorem . Theorem 3,4.6 Let M - W p(l:, m ), Ill> a. TI,en (a) M '21 lias ,lie Wh(l:., .. m - a) distribwion"
832,unknown,"(a) M '21 lias ,lie Wh(l:., .. m - a) distribwion and is /IIdependent of (M lI, M 12), and (b) If l:,,=O , then M zz- M 12 , = M "" M ,,'M 12 has tl,e W .(I"". a ) dis· triburiO'I, and M "" M II' M "", M "", and M 22., are jointly independent. Proof Write M = X 'X . where the rows of X (III Xp) are i.i.d. N..(O.l:) random vectors. Then M "", may be written (3.4.8) where P is the symmetric, idempotent ma"
833,unknown,"(3.4.8) where P is the symmetric, idempotent matrix defined by P = 1- X , M1,' X~ . Note that P bas rank (m -a) and is a function of X , alone. Since X ,P = 0 we see that X;PX , = X ;.,PX "", where X , , = X , - X ,X ,,'X12 • By Corollary 3.3.3.1. X ,., I X , is distributed as a data m atrix from Np(O, l:"" ,). There­ fore. using Theorem 3.4.4(b), for any given value of X"" tbe conditional distributi"
834,unknown,"distribution of M "", I X, is W .(I"" "" 1M - a}. But this conditional distribu­ tion is free of X ,. Therefore it is the unconditional (ma rginal) distribution, and moreover M '21 is independent of X"" Since P (I- P )=O. we see from Theorem 3.3.3 that, given X"" the matrices PX ,., and (I- P)X 2,,=X IM ,,'M,,-X ,l:,,'X 12 are independent. Hence, given X"" the matrices M "", =(PX ,,)'(PX 2.') and (I- P )"
835,unknown,"independent. But from the above paragraph, M "" I is independent of X,. Hence M H , is independent of (X "" (1- P iX ,,). Since M "" and M I2 can be expressed in terms of X, and (I-P)X "". we see that M n , is independent of (M,,, M d. Thus , the proof of part (a) of the theorem is completed . For part (b) note that M u - M "".1 = X,(I- P )X , = X,.,(I-P )X, , + 1:"", l:,,' M 12 + M""l:I: XI2 -1:"",X,: MI"
836,unknown,"= X,.,(I-P )X, , + 1:"", l:,,' M 12 + M""l:I: XI2 -1:"",X,: MII l:,~l:12 MULTIVARIATE ANALYS1S 72 (see Exercise 3.3.2). Now when :I12 is zero, the last three terms of this expression disappear. leavmg just Xi.,(I-P )X2 ., . Because (I- P) is sym­ metric, idempotent, and has rank G , the distribution of M2 2 -M ,2.J conditional upon a given value of X, is W.(l:22, a). Moreover , Craig's theorem (Theor"
837,unknown,"theorem (Theorem 3.4.5) implies that M 2 2., and M "" - M ,,., are indepen­ den t for any given value of X l> since P(I - P) = O. As. tbe conditional distributions of M u ., and Mn-M"" ., do not involve X"" we see that M 22.' and M 22 - M 2 ,., are (unconditionally) independent, and further. (M n "" M "" - M "" J) is independent of X , (and hence independent of M,,). Therefore in their unconditional joi"
838,unknown,"Therefore in their unconditional joint distribution. the three matrices MlI,M"" ."" and M' 2-M 22.' are 'independent of one another . • Recall from (A .2.4g) that M22=~-d. ,. Hence parts (a) and (b) of the tbeorem may also be written as follows. CoroDary 3.4.6.1 (a) (M n )- ,_ W b«l:2'r '. m -a) and is independent of (M"" , M 12)· (b) If:II2 =O then M 22 -(M22r'-w.(:I"", G) , GndM 21 -(M 22 )- ', M H "
839,unknown,"alld (M""2r' are joilltly independent. • Hence , when :I12 = 0, tbe Wishart matrix M 22 can be decomposed into the sum (M"" - M"" .J + (M22.J) , where the two components of the sum have independent Wishart distributions. Moreover , the degrees of free­ dom are additive in a similar manner. U tbe population correlation coefficient is zero then, for the bivariate case, part (b) of Theorem 3.4.6 leads t"
840,unknown,"of (3.4.6) and (3.4.7): m22 . ~ = ms~(1- ,2) - lT~X!- - l ' Pn2 2 -U1 22.1 =msi ,2- u~ xi · Mo reover, these three chi-squared variables are jointly statistically inde­ pendent. Theorem 3.4.7 If M - w. (:I, ""'), m > p, then (a) the ratio a'l:-lala'M-'a Itas tlte X~-p -, distribution for any fixed d · . / ""Iii 2""1 p-vector a an In partlcu a"" u nt -.. X"",,- p + 1 Jar 1= J • • •• P ; (b) m il i.t ind"
841,unknown,"(b) m il i.t independent of all the elements of M except """",. Proof From Corollary 3.4.6.1, putting b= 1 and a =p-l, we get (m ... t' -(upprlx;'_p.,· (3.4.9) This proves part (a) for the special case where a = (0, , ... O. I)'. For general 8, let A be a non-singular matrix whose last column equals 73 N ORMAL DISTRlBlIT10 THEORY a, and set N = A -1M(A -'),. Then N - W p(A-':I(A-l)"" m ). Since N -l="
842,unknown,"A'W I A. we see from (3.4.9) that (noo)-1 = (a' ~' ar' - [(A 'r'A) .. ]-lX;'_p_, = (a':I-la)-'X;'_p_,' For part (b), note that (M"" ')-' in Theorem 3.4.6 is independent of (M il' M n ). Hence for this particular case, m """" is independent of aU the elements of M except ""'po. By a suitable permutation of the rows and columns a similar result can be proved for all i. • For a generalization of this the"
843,unknown,"The following theorem describes the distribution of IMI. Theorem 3.4.8 If M - W.(:I. lit) and m """" p, then IM I is 11:1 limes p independent chi-squared random variables with degrees of freedom m. m- l, ... ,m - p+ 1. 'Proof We proceed by induction On p. Clearly the theorem is true if p = 1. For p > 1 partition M with a = p -land b = 1. Suppose by the induction hypothesis that IM ""I can be written "
844,unknown,"induction hypothesis that IM ""I can be written as Il: 11 I times p - I in­ dependent chi-squared random variables with degrees of freedom m , m - 1, ... , m - p T 2. B y Theorem 3.4.6, M IJ is independent of the scalar M"" .• -:Izz. , X~ -.+'· Since IM I = IM ""IIM 22.,1 and I:II = 1:I,,11:1:,2.,1 (see equation (A .203j)). the theorem follows. • Coronary 3.4_8.1 If M - w,;(1:, m ), :I> O, alld m>p ,"
845,unknown,"Coronary 3.4_8.1 If M - w,;(1:, m ), :I> O, alld m>p , then M>O witlt probability One. Proof Since a Chi-squared variate is strictly positive with probability one and I:II>O, it follows that IMJ > O. Hence, since by construction Mis p.s.d., all of the eigenvalues of M are strictly positive with probability one .• 3.5 The Hotelling T 2 Distribution We now tum to functions sucb as d'M"" ld, where d i"
846,unknown,"WIShart, and d and M are independent. For instance, d may be the sample mean , and M proportional to the sample covariance matrix (see equation (3.4.3» . This inlportant special case is examined in Corollary 3.5.1.1. We sball nOw derive the general distribution of quadratic forms such as the above. This work. was initiated by Hotelling (1931). MULTIVARJATE ANALYSI S 74 Definition 3.5.1 If '"" can b"
847,unknown,"MULTIVARJATE ANALYSI S 74 Definition 3.5.1 If '"" can be written as md 'M -'d where d and M are i""depe""dently distributed us N p(O,1) and Wp(l. m) , respectively, then we say tl1at '"" has the Hotel1ing 1""'-distribution with parameters p and m. We write a - 1""'-(P. m) . Theorem 3.5.1 If x and M are independently distrib«ted as Np(IJo,l:) and W.(l:. Ill). respectively. then (3.5.1) 'Proof If d~='I-1I"
848,unknown,"(3.5.1) 'Proof If d~='I-1I2(I-P.) and M*=~II2Mr""If2 . we see that d* and M* satisfy the requirements of Definition 3.5.1. Therefore '"" - T2(p, m ) where a = md*'M* -'d*= m(""-IL)'~I(""-IL). Hence the theorem is proved. • Corollary 3.5.1.1 If j and S <Ire the mean Deetar and covariance matrix of a sample of size n from N .(IL, 'I). atld S. = (n/(n -1»S, thell Proof Substituting M=nS, m=II - I. and X-"
849,unknown,"tbeorem. the result follows immediately. • Corollary 3.5.1.2. The T 2 staCistic is invariallt ullder any lion ·singular linear transformation x ..... Ax + b. • Of course the univariate r statistic also bas this property of invariance mentioned above. Indeed tbe square of the univariate r,. variable has the T2(l, m) distribution (see Exercise 3.5.1). In other words, the F, . ~ distribution and the "
850,unknown,"theorem extends the result. Tbeorem 3.5.2 r(p. m) = {mp/(m-p + I)}Fp.m_p+ I' (3.5.3) Proof We use the cbaracterization '"" = md'~'d given an Definition 3.5. L Write a = m (d'M -'d/d'd)d'd. Since M is independent of d we see from Theorem 3.4.7(a) that the 75 NO RMAL DISTRIBUTtO N THEORY conditional distribution of /3 =d'd/d'M -'d given d is X;'-p.l' Since this conditional distribution does not depen"
851,unknown,"conditional distribution does not depend on d, it is also the marginal distribution of /3. and furthermore /3 is independent of d. By Theorem 2.5.2. d'd -X ~ . so we call express a as a ratio of independent X ' variables: a = mX;/X!.-P,J = {mp/(m - p+ J)}Fp.rn-p.'· • Coronary 3.5.2.1 If i and S are the mean and covariance of a sample of size n from N.(""..l:). tlleu {en - p l/p}(x -11)'S-J(i -11) -"
852,unknown,"Proof The result follows immediately from (3.5.2). • Coronary 3.5.2.2 iM i/iM+dd' i-B(~(m-p + I), 4p) wllere B(-'·) is a beta variable. Proof From equation (A .2.3n) the given ratio equals m /(m + "",). Using the F distribution of ""'. and lhe univariate relationship between F and beta distributions. the resul1 follows. (See also Exercise 3.5.2 .) • Since d'd and ~ are independent cbi·squared statis"
853,unknown,"Since d'd and ~ are independent cbi·squared statistics, as shown in the proof of Theorem 3.5.2, their ratio is independent of their sum . which also has a chi·squared distribution. (See Theorem B . 4.1.\ The sum is d'd+/3=d'd(l+ ,1 , ). dM d Hence we have the following: Theorem 3.5.3 If d and M are independently distributed as Np (0, I) a,1d W,,(l. /11). respectively. tl1ell and IS dIstributed Ind"
854,unknown,"Theorem 3.5.3 is one extension of the univariate result that if d­ N(O.!) and u'-X', then d2 +u 2 is independent of d2 /u2 • Another generalization of the same result is given in Theorem 3.5.4, which requires the following lemma . Lemma Let W be a square symm etric (p x p) ralldom matrix and let :I be a random p-vector. If x is i""dependent of (g;Wg"" ... ,g;,Wg,.) for all non·random orrhogo/IQI mat"
855,unknown,"of W . MULTIVARIATE ANALYSlS 76 Proof Using the joint characteristic function of Wand x it is seen that if tr A Wis independent of x for every square symme tric matrix A, then W is independent of x. Now A may be written in canonical form as L A ,ll;gl say, where G = (g"" ... ,g.,)' is orthogonal. Then tr AW =tr (L Aigig: )w= L A,gfWg ,. If the conditions of the lemma are satisfied, then all of the "
856,unknown,"summation, and the summation ilseU, are independent of L Thus . tr AW is independent of j[ for all A. and therefore W is independent of x. • Theorem 3.5.4 If d and M are independently distributed as N. (O, I) and W.(l. m). respectively. tllen d'M -'d is independellt of M + dd'. Proof Let G =(g ...... g.,J' be an orthogonal matrix and consider the quadratic form s q, = g/(M + dd')g"" j = 1 •... ,p. "
857,unknown,"quadratic form s q, = g/(M + dd')g"" j = 1 •... ,p. W rite '. M = X;X, = Lx ,x! and d = sm_"" ,~ , where X =(Xj .xm.,), is a data matrix from N. (O.I). Set Y = XG' so that Y, = X ,G ' and Ym.' = Gx m•,. Then "",""I ."" .... q, = g/(M + dd'jg, = L (gfx.)'= L y~. Now Y is also a data matrix from N.(O.I). so. thought of as a p(m + 1)­ vector y v. it has the N.'m .lI(O, I) distribution, and hence is spheri"
858,unknown,"symmetric. Therefore. by Theo rem 2.7.2, (q, ..... q.) is statistically inde­ pendent of any column-scale-invariant function of Y . In particular. (q"" ... ,q.) is independent of Since this result holds for all orthogonal matrices G . we see from the above lemma that the theorem is proved. • 3.6 MahalaDobis Distance 3.6.1 The fwD-sample HoteUing T Z statistic The so-called Mahalanobi s distance bet"
859,unknown,"Z statistic The so-called Mahalanobi s distance between IWO populations w ith means 1-', and J-Lz, and common covariance matrix :I has already been defined in 77 NORMAL DlSTRmunON' THEORY Section 2.2.3. It is given by .1, where .1 Z = (I-', - jLz)'r ' ~, -J-Lz). (3.6.1) Of course, only rarely are the population parameters known . and it is usual for them to be estimated by the corresponding samp l"
860,unknown,"Suppose we have two samples of size n, and,."" where "",+ /I,""=n. Then the sample Mah alanobis distance. D , can be defined by (3.6.2) w here 5u = (II,S , + '0,52)/(11 - 2) is an unbiased estimate of :I. (The sample mean and covariance matrix for sample i, i = 1, 2, are denoted Xi and 5,.) The statL~tical distribution of D Z under one particular set of assumptions is given by the following theorem. "
861,unknown,"Theorem 3.6.1 If X, and X z are independent data matrices. and if the n. row s of X i are i.i.d. N.(I1;,Ii), i = 1, 2. then when 1-', = J-Lz and I, = x,. (n,,"";,,)DZ is a 1""'(P. n -2) variable. • Proof Since Xi -N.{p"" 11, '1:,), i = 1.2. the general distribution of d = i,-x2 is normal with mean I-',-J-Lz, and covariance matrix n,':I, + n;-'1:,. When 1-', = J-Lz and I, =1:,=I. d - N.(O. cl:). where"
862,unknown,"U M i="",S, then M ,-W .(1:,.n,-l). Thus. when :I,=1:,= I. M =( n -2)Su =M, + M 2 - W p(I , n -2). So cM - W .(cI, /1- 2). Moreover , M i s independent of d since X, is independent of 5i for i = 1.2. and the two samples are independent of one another. Therefore Simplifying the left-hand side gives the required result. • The quantity (3.6.3) (3.6.4) is known as HOle/li""g's two-sample 1'"" slaristic. "
863,unknown,"(3.6.3) (3.6.4) is known as HOle/li""g's two-sample 1'"" slaristic. Using the relationship (Theorem 3.5.2) between 1'"" and F statistics, we may also deduce that. under !be stated conditions. 1I ,,,,i ll- p - l) 2 ( 2) D - F •• ,,_,. n n - p , (3.6.5) MULTIVARIATE ~~AL-YSIS 78 *3.6.2 A deromposition o( MahaJanobis distance The Mahal anobis distance of ... from 0 is d ;= J.L' ~-l ..... (3.6.6) Partiti"
864,unknown,"Partition ... ' = ( ... , . 11-2). wbere ... , contains tbe first k variables. Then, using (A.2.40 and (A .2.4g) to partition :l:"". we can write (3.6.7) where 1'-2., = 1'-2-:'£""k,,' ... , and :'£,2., = :£,;,- :'£,,:£-;-;I I2' Note that <Ii = ... \:l:l,' ... , represents the Mahalannbis distance based on the first k vari· ables, sn the condition 1'-2., = 0 is equivalent to <I ~= <I~. Similarly, if "
865,unknown,"Similarly, if o - N.( .... I) and M - W p(I , m ) then the sample Mahalanobis distance (3.6.8) can be partitioned as (3.6.9) where Di =mu~M'llluJI M 22.1= M 22-M 2IM IJIMI2, and (3.6.10) (see Exercise 3.6.1). The following theorem gives the distribution of D~-D~ when ..... , =0. Theorem 3.6.2 If D ; and D i are as defined abooe alld 1'-2., =0 . then D~- Di: p - k m + D~ --m - p 1 FP_k."",_p+l~ alld"
866,unknown,"D~- Di: p - k m + D~ --m - p 1 FP_k."",_p+l~ alld is independenr of D~. Prool Suppose M = X 'X , where X is a data matrix from N.(O.:£) in­ dependent of o. By Theorem 3.4.6, M u ., - W p ., (l:.,..2.h m - k) and is independent of (M 2,. M,,). By hypothesis 0 is independent of M . Thus . M 22."" (M"". M u ), and 0 are jointly independent and hence M 22.l is independent of (M 2 1> M il' uJ. (3.6.11) Le"
867,unknown,"Let us exami ne the distribution of z in (3.6.10) conditional on 0 , and 79 NQ R.lI.iAL DISTRIBunON T H EORY (3.6.12) where II,., = u,-:'£""I,lu, is normally distributed with me an 1'-2., = 0 and covariance matrix :'£,2.,. By Theor em 3.2.3, 0 2.1 is independent of (0 "" X ) and hence (3.6.13) T he second term in (3.6.12), :'£""I'I'o"" is a constanr given u"" X, . To study M 2IM'1 I'U 1 fu 1.xl, write "
868,unknown,"w here X 2., is a data matrix from N p.d O. :'£"".,) independent of (X "" 0 ,). Then whe re b = X ,M ,,'o, and ~,~""' X, b = :'£,,I,,'o, are constants given (X "" 0 ,). Given (D •• X ,). X, ,b is a linear comb inatioo of the (statisti­ cally independent) rows of X 2., SO that X "" b I 0"" X , is normally distri­ buted with mean 0 and covariance m atrix (b'b):'£""., = (o'MI ,'o,):'£"".,. From (3.6.13), O 2"
869,unknown,"From (3.6.13), O 2., ,u"" X , is independent of X >. lb . Thus. add ing the two term s of z together. we see that z I 0 "" X ,- Np _dO. (I + uI M,,' u ,r~: 22-')' Let y = (1 +D~In )· ' nz. Since D i="",u,M ,,'o, is a function of 0, and X , only. As thi~ conditional distribution does not depend On (u "" X ,) it is also the ma rginal distribution. Furthermore. y is independent of (u,. X ,) and hence ind"
870,unknown,"Now y and D i; are functions of (M "". M "". oJ. so. by (3.6.11). M 22."" y, and D i; are jointly independent. Thus. from Theorem 3.5.1, D 2_ D 2 p k_(m_k) 'T'(p-k.m-k) m + D~ and further, this quantity is imlependent of D~. • MULTIVARIATE ANALYSIS 80 3.7 Statistics Based on tbe Wishart Distribution In univariate analysis many tests are based on statistics having indepen· dent chi-squared distributio"
871,unknown,"dent chi-squared distributions. A particular hypothesis may imply. say. that a - <T2X~ and b - 1T2X~, where a and b are statistics based on the data. If a and b are independent then, as is welllmown. alb is alf3 times an Fa .• variable, and al(a + b) has a beta distribution with parameters ~a and !f3. Neither of these functions involves the parameter CT. which in general is unknown. In the multiva"
872,unknown,"general is unknown. In the multivariate case many statistics are based on independent Wishart distributions. Let A - W .(I, III) be independent of B - w.cr. /I) where m """" p. Since /II """" p. A -, exists. and the non -zero eigenValues o( the matrix A -'B are the quantities of interest. Note that since A -'B i~ similar to the p.s.d . matrix A -,aBA -''', all of the non-zero eigenvalues will be posit"
873,unknown,"positive. Also, with probabitity 1, the numbe r of non-zero eigenvalues equals min (/I, pl. Further, the scale matrix 1: has no effect on the distribution of these ejgenvalues. so without loss of generality we may suppose 1:= I (see Exerci.e 3.7. n. For convenience denote the joint distribution of the min (n. p) non -zero roots of A -'B hy I/'(p. m . n). Then the following theorem gives an importa"
874,unknown,"important relationship between the I/' distributions for different values of the parameters. See also Exercise 3.7.3. Theorem 3.7.1 For 1/1 """" P and n, p """" 1. the I/'(p. Ill, 11) distribuiion is identical tn the 1/'(/1, 1/1 + 11 - p. p) distribution. • Proof First, note that the number of non-zero eigenvalUes is the sam e for each distribution. Note also tbat m """" p implies m + n - p """" n so the "
875,unknown,"latter distribution makes sense. Suppose /I """" p. The I/'(p, m, 11) distribution is the joint distribution of tbe non-zero eigenvalUes of A -'B , where A - W.(I, 1/1) independently of B - W .(J, n). Write B = X'X, where X (n x p) is a data matrix £rom N.(O.I) independent of A. Because 11 """"'p. XX' is a noo-<ingular (n x n) matrix (and so XX '>O) with probabiUty I. Define G = (XX ') ''''X. (3.7.0 T"
876,unknown,"Then G is a row orthonormal matri.x (GG ' = 1,,) and also XG 'G = X . Thus . which has tbe same eigenvalues as (G A -'G ')(GX 'XG')= C'O. say. (3.7.2) Rl NOR MA L DISTR IB UTION THEORY We shall now show that C- W .(I, m-Il- p) independently of 0 - W.(J, pl. The theorem will then follow. Since G is a function of X. and A is independent of X we find (see Exe rcise 3.4.19) that (GA -'G') 'I X = C I X"
877,unknown,"Exe rcise 3.4.19) that (GA -'G') 'I X = C I X - W.(I, 111 + /1 - pl. Since this distribution does not depend On X , it is also the unconditional distribution, and C is independent of X . Hence C is also independent of D = GX'XG '. Finally, since all 1117 elements of X are independent (0.1) random variables. X'(pXII) can be considered as a data matrix from N .(O.I). so that O = GX 'XG '=XX '-W .(J."
878,unknown,"O = GX 'XG '=XX '-W .(J. pl. Thus the result i. proved when 1I.s p. If ,,> p. then start with the I/'(n. m + /1 - p. p) distribution ""instead of the ""'(p. Ill. n) di~tribution in the above discussion. • The following result describes tbe dislribuuon of a generalizatIon of the F statistic. Theo rem 3.7.2 If A - W,, (~. m) and 8 - W r(1:. ,,) are independent and if In """" P and n """" p. then </> = IA "
879,unknown,"In """" P and n """" p. then </> = IA 1 BI='8 111 (3.7.3) II proportional to the product of p independent F uaria/)/"",. of wltkh tile ith lias degrees of freedom (n - i ... 1) and (111-i-I). • Proof From Theorem 3.4.R, IA I and IB I are each \1:1 times the product of p independent chi-squared variables. Therefore '"" is the product of (' ratios of independent chi-squared statistics. The ith ratio is x~"
880,unknown,"ratios of independent chi-squared statistics. The ith ratio is x~ -I-+ llx?"" -I- 1 i.e. (/I - i T 1 )/("", - i + l) times an F._._"" ... , ,statistic. Allowing i to varY from I to p tbe result follows. • T he multivariate extension of the beta variable will nOw be defined. De finition 3.7.1 Whell A - W. ,(1. /!I) a/ld B - W p (I.II) are independel1t. m """" p. we say that 1\ = IA IIIA + B I =\1+ A 'B "
881,unknown,"lias a Wilks' lambda distrihution with para""""'"""" P.!rI. and n. • The A family of distributions oCc urs frequently in the context of likelihood ratio tests. The parame ter '"" usually represents the ""c.Tfor"" M ULT rvARIATE .-\NAL YSIS 82 degrees of freedom and "" the ""hypothesis"" degrees of freedom . Thus ttl -II represents the ""lOtar' degrees of freedom. Unfortunately the notation for this stati tic"
882,unknown,"notation for this stati tic L< far from standard. Like the 1"" statistic. W ilks' lambda distribution is invariant under changes of the scale parame ters of A and B (see Exe rcise 3.7.4). The dL< tribution of /\ is given in the following theorem . Theor em 3.7.3 We have .. A(p. m .n)- n ""i. 13.7.5) ,., whe re Ur, .... u"" are /l illdependelll variables GIld "", -B (!<m ~; - p). 4p). i = I. .... n. • "
883,unknown,"i = I. .... n. • Prt)C)f Write B = X 'X . where the II rows of X are i.i.d. Np(O,1) variables. Let X , be the li x p) matrix e<>nsisting of the first i rows of X . and let M , =A + X:X •. i= I. ...... . ote that ~~= A. M .. = A + B . and M , = M l ,-+xjx;. ow write IAI I M ""I IM.,IIM ,I IM ._,I /\(p. m·Il)=IA + BI =IM .I=IM ,IIM ,I··· 1M .. I . Tb.is producl may be written as U l l~ 2'"" U n . w he"
884,unknown,"Tb.is producl may be written as U l l~ 2'"" U n . w here 1M . ,I u. = IM .I' i = 1. .... II. oW M . = M ._t + x.x:, and therefore. by Co roUa ry 3.5.2.2. w ith M ; , corresponding to M and X i corresponding to d. II, - Bti(m + i -p).1pl. i= 1. .... 11. It remains to be shown that the U, ~re statistically independent. From Theorem 3.5.4. M , is independen t of Since I~ is independent of '"" I' ., . •"
885,unknown,", M .. ,=M .+ I 'Xi ox:.,. k = 1 83 NOR..\tAL D ISTRIBlTflON THEO R Y it follows that u; is also independ ent of M. ~ "" M i_, ••••• M •. and hence independen t of I~ + , •...• u •. The result follows. • Theorem 3.7.4 The A (p."""" II) and A(n,,,,+ n-p.p) distributions are Ihe same. P roof Let A ,;;' ... ;;. A. denote the eigenValUes of A -'B in Definitio n 3.7 .1 and let k = min (II, p) denote the n"
886,unknown,"values. Then , using Sectio n A.6, we can write "" . A (p, 111, ,,)=II+ A-'BI-'= n (1+A.r'= n (1+ ,1.,)-'. (3.7.6) Th us A is a function of the non·zero eigenvalues of A -'B. so the result follow s from Th eorem 3 .7.1 • Spe cial cases o f these result< are as follow s: (a) T he statistics 11.( p, 111, I) and A(1, "", + J -p, p) are equivalent. and each corresponds to a single B {Hm - p + I), ~p} st"
887,unknown,"(b) A(p. m . 2) and"" (2. 111 + 2-p. p) are equivalent, and correspo nd to tbe prod uct of a B H( I11 - p+l).!p} statistic w ith an independent BH(1I1 -p+2), !p} statistic. From the relatio nship between (3 and F variables, functions of A (p. 111, 1) and A (p. 111.2) statistics can also be expressed in term s of the F distrihution as follows: 1-A (p, m .l) Al p, 111, 1) P F' ."" .... - p-I· lII- p +"
888,unknown,"lII- p + 1 I-A(1,m ,,,) /I A (l ) -- F. .. ,., , m .'1 m 1-,)11. (p, III, 2) ,) I\(P. m, 2) 1-,)A (2, til, ,,) "" ')A (2 ) - --I F, ... 2(m _ ',' . m , tl 111- (3.7 .7) (3.7.8) (3.7.9) (3.7.10) Fo rmu lae (3.7 .7) and (3.7.8) are easy 10 verify (see Exe rcise 3.7.5), b ut (3.7.9) and (3.7,10) arc m ore complicated (see Anderson. 1958 . pp. 195-196). MULTIVARJATE Al'I;ALY IS 84 For other values of n"
889,unknown,"For other values of nand p. provided m is large. we may use Bartletr's approximation: -(m -i(p -Il + l)l log A(p. m. 11)- X;. (3.7.11) asymptotically as m ...... ""'. Pearson and Hartley (1972. p. 333) tabulate values of a constant factor C(P. II. m - p+ I) which improves the approxi­ mation. An approximation which uses the F distribution with non-integer degrees of freedom is discussed in Mardia a"
890,unknown,"approximation based on Theorem 3.7.3 i. given in Exercise 3.7.6. The A statistic arises naturally io likelihood ratio tests. and this property explains the limiting X2 distribution in (3.7.11 ). Another impor­ taot statistic in hypothesis testing is the greatest Toot statistic defined below : Definition 3.7.2 Let A- W.(I. m) he ,""dependent of B - W.(I. II). wltere III '"" p. Tllell the largest eige"
891,unknown,"III '"" p. Tllell the largest eige""ualue II of (A + B ) 'B is culled the greatest loot statistic and irs distributioll is denoted lI(p. m. II). Note that lI(p. 'II. II) can also be defined as the largest Toot of the dete[minental equation IB -II(A+ B)I=O. If A is an eigenvalue of A -'B . then ,\/(1 +.1.) is an eigenvalue of (A + 8 )-'B (Exercise 3.7.8). Since this is a monotone [unction of A, II is"
892,unknown,"(A + 8 )-'B (Exercise 3.7.8). Since this is a monotone [unction of A, II is given by (3.7.l2) where A, denotes the largest eigenvalue of A - 'B . Since .I., > 0 we see that 0< 6 < I. Using Theorem 3.7.1 aDd (3.7.6) we easily get the following properties: (1) /I(P. m. II) and I/(n. It! + n - P. p) have the same distribution: (3.7.\3) (2) 0(1. III, II) i-A(l.m.n} "" (3.7.14) A(l. m, n) F ..... m~ 1-0"
893,unknown,"1-00. m .lI) '"" (3) 6(p. m . l) I-A(p, m. J) p (3.7.15) A(p.m.ll t FfI .... t,tl' 1-lI(p, m. I) m-p+ For p '"" 2. critical values of the 0 statistic must be fmuld from tables. 85 ORMAL DISTRIBUTION THEORY Table C.4 in Appendix C gives upper percentage points for p = 2. Pean;on and Hartley (1972. pp. 98-104. 336-350) give critical values for general values of p. Of coun;e the relation (3.7.13) can b"
894,unknown,"general values of p. Of coun;e the relation (3.7.13) can be used to extend the tables. As above. note that p = dimension, m = ""error"" degTees of freedom. and II = ""hypothesis"" degrees of freedom. J.8 O tber Distributions Related to the MultioorDIal Wishart distribution The Wishart distribution W.(I. III) has already been described. For refer­ ence. it~ p.d.f. (when 1.:> 0 and m '"" p) is given by t"
895,unknown,"t(M! 0.8.1 ) with respect to Lebesgue measure n.~. d""~. in R·'r<""'2. restricted to the set where M > 0 (see Anderson. 1958. p. 154.) {liverted Wishart di .• rriburioll (See Siskind. 1972 .) If M - W.(1.:. mJ where I>O and m ""'p. then U = M"""" ' is said to have an inverted Wishart distribution W ;'CI. III). Using the Jacobian from Table 2.5.1 we see thal its p.d.f. is (3.8.2) The expected value of U"
896,unknown,"(3.8.2) The expected value of U is given (see Exercise 3.4.13) by E(U) = 2"" /(m-p-I). (3.83) Complex """",Itillormal distribution (See Wooding. 1956; Goodm an. 1963: Khatri. 1965.) Let z = (x'. y')'- 2.«1-l;. I-l~)'. I ) where I '2(P X p) is a skew-symmetric matrix (I '2 = - :t,,). The n the distribution of w =x + iy is known as complex mutinormal. Nm r-central Wishart distribution (See James. 1964."
897,unknown,"(See James. 1964.) Let X be a data matrix from N.(I1.l:).l1iO. Then M = X 'X has a non-central Wishart distribution. MULTIVARIATE AN .. \LYSIS 86 Matrix T distribution (See K shirsagar, 1960 ; Dicke y. 1967.) Let X (n x p) be a data matrix from N.W , Q ) wbich is independent of M - W.(P , v). Then T = X 'M"""" /2 has the matrix T distribution . Matrix bela type 1 distributioll (See Kshir.;agar. 1961"
898,unknown,"(See Kshir.;agar. 1961 ; Mitra, 1969; Kh atri and Pillai, 1965.) Let M ,­ W .(l:. v,), i = 1, 2. Then (M , +M 2)-1I2M I(M , + M 2)-1/2 has the matrix beta type J distribution. Mat,i"" beta type If distribution (See Kshirsagar. 1960 : Kha tri and Pillai. 1965.) Let M , - W .<l:, v,), i = I. 2. Then M ,'f2M IMi'f2 has the matrix beta type 11 distribution. Exercises and Complements 3.2.1 U the rows of"
899,unknown,"3.2.1 U the rows of X are i.i.d. N .(O , n. then using Theorem 2.7.1 show that tr XX is independent of any scale-invariant functio n of X . 3.2_2 [f x- N p(""..l:). show that x and Gx have the same distribution for all orthogonal matrices G if and o nly if ,...= 0 and l:= u 2 1. How does this relate tn the property of spherical symmetry? 3.2.3 If x - N (O. ( 2 1). show that Ax and (1- A -A ) •• whe"
900,unknown,"gene ralized inverse satisfying AA -A - A . are independent and each has a normal distribution. 3_2.4 (a) If x- .(11.1:) and a is any fixed vector. show that f a'(x - ,...) Ja'l:a N(O. l). (b) 11 a is now a random vector independent of x for which P(a'1:a = 0) = 0, show tbat f - N(O . 1) and is independent of a. (c) H ence show that if x - N ,(O , I) then "".0""+ '"" log Ix,l [.'"" + (log Ix,I>'],/2 ("
901,unknown,"3.2.5 (a) The ordinary least squares coefficient for a line passing through the origin is given by b = L x,yiL x: = x'Y/x'x. If Y - ,,(0. I) and if \lORl'\iAL OISTRIBlIT IO:-' 'OlEORY x is statistically independent of y then show that a = x'y/(~'x)' n - (0. J) and is independent of x. (b) Supp ose x - ""(0 . I). Since b = a/(x'x)'"" w here a - N(O. I) and is Independe nt of x. and since x'''' -x~ w "
902,unknown,"Independe nt of x. and since x'''' -x~ w e deduce that n'''b - t •. This is the null distribution of the ordinary least squares coefficient (or the bivariate normal m odel. 3 .2.6 U~ing the covariance matrix ,how thaI the conditiunal distribution 0; (x,. x,) given x, ha' me""n vector [ILl - P'(X,-IL,). 1L,1 and covariance matrix 3.2.7 If X ,. X 2. X 3 are i.i.d. ,,(J1.l:) random variables. and if y"
903,unknown,")'2="""" + "" , y,=XJ+ X"" then obtain tbe conditional dislribution of y, I:iven y"" and of Y. given Y2 and y, 3.3.1 If '"" - p(Il.l:) and Ql:Q '(q x q) i, non-singular. then. given that Qx = q. show that the conditional distribution or "" is normal with mean "".+ l:Q'(Ql:Q ')-I(q - QIl) a nd ( ingular) covariance matrix 1: -l:Q '(Ql:Q ')-IQ1:. 3.3.2 If X 'I is as defined in Corollary 3.3.3.1. and if Q = "
904,unknown,"3.3.2 If X 'I is as defined in Corollary 3.3.3.1. and if Q = X .(X ;X ,) 'X ; • • how that (i) QX ,.I = QX , -X 1l:,,'1:,2. (ii) (1- Q)X 11 = (I- Q )X ,. Hence prove lhat (iii) Xi ,(I- Q)X ""= X ;(I- Q )X ,. (iv) X i.IQX 2.. = X iQX , - 1:,,1:, ,'M Il-M "" l:,I'l:12 + 1:""I II'M IIl:, Il:"". where M ij :::;. X~ XI ' ISee also E xercise 3.4.15.) MULTIVARJATE ANALYSts 88 3.3.3 Suppose that x - (IL. I) a"
905,unknown,"3.3.3 Suppose that x - (IL. I) and a is a fixed vector. [f r. IS the correlation between X, and a·x. show that r = (cD) -'12Xs where {' = a'Xs and D = Diag (:I). When does r = Xs"" 3.3.4 (Proof of Theorem 3.3.2) (a) Let X V be the tip-vector obtained by stacking the columns of X On top of one another (Section A.2.5). Then X(n x p) L. a random data matrix £rom N.{JL.:I) if and only if wbere ® denote"
906,unknown,"wbere ® denotes Kronecker multiplication (see Section A .2.S). (b) Using the fact that we deduce that (AXB) v is multivariate nnrmal with mean ami with covariance matTix (B '®A)(:I®J)(B'®A), = B""IB®AA '. (c) Therefore AXB is a normal data matTLs if and only if (i) A1=a 1 for some scalar a or B' ... = 0. and (ii) AA ' = JH for some scalar (3 or B'IB = O. (d) If both the above conditions are satisfi"
907,unknown,"data matrix from N.(aD' .... (3B·IB). 3.3.5 (Proof of Theorem 3.3.3) Suppose that X(n x p) is a data matrLs (rom N.( .... I). and that Y=AXB and Z=CXD . Then. using (*) from Exercise 3.3.4. show that y""ZV' = (B'® A)XVX v'(D'®C)'. But using Exercise 3.3.4 (a) we know that V(Xv) = I®I. Therefore qyY , ZY ']= (B'®A)(I®J)(O®C') = B ·:W®AC' . The elements of Y and Z are uncorrelated if and only if the "
908,unknown,"is tbe zero matrLs. I.e. if and only if either B ':IO=O or AC' =0. 3.4.1 If M - W .(I. m) and a is any random p.vector which satisfies a'Xs i 0 with probability one, and is independeO! of M. then a'Ma/a'Xs has the x!. distributioll. and is independent of a. 89 r-;ORMAL DISlJUBUTION THEORY 3.4.2 [f M - W .(I , m). show that b'Mb and d'Md are statistically inde­ pendent if b'Id= O. (Hint: use Theore"
909,unknown,"pendent if b'Id= O. (Hint: use Theorem 3.3.3.) Hence show tbat m "" and III;, are independent if CT,; = O. aud that wben I = I, tr M has the x~ •• distribution. Give an alternative proof of this result which follows directly from the representation of M as X 'X . 3.4.3 (Mitra. 1969) Show tbat the following conditions taken togetber are necessary (and sufficient) for M to have the W .(I , m) distrib"
910,unknown,"(a) M is symmetric, and if a':Ia=O tben a'Ma = O witb probability one; (b) for every (q x p) matrix L which satisfies LXL' = I. tbe diagonal elements of LML ' are independent x!, variables. 3.4.4 (MitT8. 1969) The converse of Theorem 3.4.2 does not hold. Tbat is. if a'Ta/a'l:a - x~ for all a. then T does not necessarily have a Wishart distribution. (a) eonslmelion Consider T = aM where M - Wp(I. n"
911,unknown,"ex - B(!!, len -f)). and a and M are independent. From Theorem 3.4.2. a'Ta/a'Xa is the product of independent B (Jf, len -m and x! variables. Hence. using the bint. show that a'Ta/a'Xs - xf. Thus T=aM satisfies tbe required property. (Hint: If X, • .••• x"" are Ll.d. N(O, 1) variables tben. using the fact tbat x'x is independent of any scale-invariant function. see Exercise 3.2.1. note that are ind"
912,unknown,"are independent variables with B Gf.1(n - f)) and X! distributions. respec­ tively. Hence tbe product of the above variables is L:!., xf. which has a X1 distribution.) (b) COItlradiction But T cannot have a Wisbart distribution. For if it does. it must have f degrees of freedom. In that case ri, = ~;/(~,l •• )'/2 would have the distribution of a sample correlation coefficient based on a normal sam"
913,unknown,"normal sample of size (f+ I). But r,; is also m""/(III,,IIIII)'I2. whicb has the distributiOn of a sample correlation coefficient based on a normal sample of size (II + 1). Hence we bave a contradiction. and T cannot bave a Wisbart distribution. 3.4.5 (a) If the rows of X are i.Ld . N.( ... , X ), and Y=X - l ... '. then the rows of 'V are i.i.d. N .(O.l:). Now X 'CX = Y 'CY + Y ·ClJL'+ ... l'CY+(l"
914,unknown,X 'CX = Y 'CY + Y ·ClJL'+ ... l'CY+(l'Cl) ...... '. M U L.TIVARIA T E A N A L Y S tS 90 If C is symmetric and ·idempotent. then Y'CY has a Wi shart distribution by virtue of Theorem 3.4.4. and X'CX is the sum of a Wishart distribu­ tion. two non -independent normal distributions. and a constant. How­ ever. a special case arises when the rows o[ C sum to zero. since then Cl = 0 and the final three 
915,unknown,"following result which geoeralizes Theorem 3.4.4. (b) If the rows of X are U.d . N .( .... I) and C is a symmetric matrix. then X 'CX has a W .(I. r) distribution if (and only if) (i) C is idempotent and either (ii) p. = 0 or (iii) Cl = O. In either case, = tr C . (c) (Proof of X'IIX - W.C~ ... - I) As a corollary to the above we deduce that if the rows o[ X ("" x p) are i.i.d. N .(p.. I ), then X "
916,unknown,"W .(I.II-1). and S=n -'X 'UX - W .(II-'I ,n - l) because the centTing matrix H is idempotent of rank (n - 1). and all its row sums arc zero. 3.4.6 (a) Let C, •...• C. be (II x .. ) symmetric idempotent matrices such tbat C ,+ ... + C k = I and let X (n x p) be a data matrix from N .t.... I ). Sbow that C , C, = 0 for if j. Hence if M , = X 'C , X , deduce that X 'X = M , + ... + M k is a decomposi"
917,unknown,"M , + ... + M k is a decomposition of X 'X into a sum of independent matrices. (b) If ... = O. show that M , -W .(.I. r,) for i = 1. .... k. where r, = rank (C,). Cc) For general.... if C , = 11 -'11', use Exercise 3 .4.5 to show that M , -W.(.I. r,) for i =2 •.... k. 3.4.7 11 the rows of X are i.i.d . N .(p..l:) and i[ C is a symmetric (II x II) matrix. then X 'CX and AXB are independent if eithe"
918,unknown,"matrix. then X 'CX and AXB are independent if either B 'l:= 0 or AC = O. (Hint: As in (3.4.2) we have X 'CX = ~ A,y,y;. where y. =X''Y(i' and C = I A; 'Y'n'Y~"" Now by Theorem 3.3.3. AXB is independent of It ,''''Y(,,x if and only if eitber 8 'I = 0 or A,""2A'YU'= O. The second condition holds for all i if and only if AC = O. H ence the result follows.) 3.4.8 If the rows of X are i.i.d. and liS = X "
919,unknown,"3.4.8 If the rows of X are i.i.d. and liS = X 'HX is statistically indepen­ dent of ni= X'1·. then the rows of X must have a multivariate normal distribution (Kagan et al.. 1973). However . note that i is not independent of X·X . 3.4.9 If the rows of X ("" x p) are i.i.d . N v(O.I). and if C and Dare symmetric (p X p) matrices. then the following results may be derived [rom Theorems 3.4.4 and 3.4.5"
920,unknown,[rom Theorems 3.4.4 and 3.4.5: (a) XCX '-W .. (I.r) if and only if C is idempotent. and r = trC; (bl XCX' and XDX ' are independent if and only if CD = 0 : (c) XCX' and AX 'B are independent if (and on ly if) AC = 0 Or B = O . 9 1 NO RMA L D IST RIBlITTON THEO R Y (Hint: the columns of X are the row s of X '. and these also are i.i.d.) 3.4.10 Extend the results of Ex ercise 3.4.9 to-the case of ge
921,unknown,"and show that if the rows of X are i.i.d . N p(O .l:) then (a) XCX' - W .. (I. r) if and only if Cl:C = C. in which case, = tr Cl:; (b) XCX' and XDX' are independent if and o nly if cm = 0 : (c) XCX ' and AX'B are independent if (and only if) AIC = 0 or b = O. (Hi nt: note that the rOWS of Xl:-I12 are i.i.d. ,,(0.1), and use Exercise 3.4.9.) 3.4.11 From Exercise 3.4.9 show thaI if x - p (j.L,'1) t"
922,unknown,"(a) x'Cx - X ; if and only if p.= 0 and CIC = C . in which case ,= trCI (as a special case. (X-p.),l:- I(X- p.)-X ;); (b) x'c.. and x'Dx are independent if amI o nly if CID = 0 : (c) x'Cx and Ax are independent if (and on ly if) AIC = O. 3.4.12 (Alternative proof that i and S are independent) Let X ("" x p) be a data m atrix from N pw.. I ) and let A ("" x n) be an orthogonal matrix wh ose last row "
923,unknown,"wh ose last row is given by D .. = ,,-Int. If Y = AX show that (a) the rows of Y are independent: (b) Y .. = "" ,n i - N .(n In p..l:): (c) y, - N .(O.l:) for i = I. .... ,,- 1: (d) ""S = X 'UX = L.:'~,I y,y:-W .(I .I1 - I) independ ently oli. 3.4.13 (Expectation of the inverted Wishart distribution) If M ­ W e(I, m I. m <> p + 2. show that E~')= I- '/( m - {J - I ) . (Hint: [f x - X;, It <> 3, then"
924,unknown,"(Hint: [f x - X;, It <> 3, then use its p.d.L to show that E (x-') = I/(n - 2). Also nOte that a'';-'a/a'M -'a - X~'-v- I for all constant vectors a.) 3.4.14 [f c - X;;' then. using the central limit theorem (Theorem 2.9.1) and the transformation theorem (Theorem 2.9.2), it is easy to . ee that log c has an asymptotic N(Iog m . 21m ) distribution as 111 --> 00. Deduce a corresponding result for th"
925,unknown,"corresponding result for the asymptotic distribution of log IM I. where M - W(l:. mi. 3.4.15 (aJ Let X be a data m atrix from .(0, l:). If X ,., =X ,-X Il:,;l:,> tben and F - W(l:22.I, m ). (Hint: use Theorem 3.4.1: See also ExercL,e 3.3.2.) MULTIVARIATE ANALYSIS 92 (b) F is independent of X , and of M "". (e) U F is the matrix obtained from F by substituting"" 'M for ""l:, then F = M 22 I defined in"
926,unknown,"3.4.16 If A and Bare (p x p) symmetric idempotent matrices of rank r and s and if AB = 0, show that, fnr x - N p(O, (T'I). x'AJr: 1 I x'(A+ B )x - 8 (ir"" s), and tp -r)x' Ax rx'(I-A )x - F, .• _,. 3.4.17 (a) Suppose that the elements of x are U.d. with mean O. variance I, and third and fourth moments I'-J and 1'-4' Consider the matri... M = xx', and show that V (rn,,) = (1'-4- 2)1>"" ~ I, where 0"" "
927,unknown,"where 0"" is the Kronecker delta and Il',k! = I if and only if i = j = k = I. and is 0 otherwise. (b) Suppose tbat the elements of X("" x p) are Li.d. with the moment~ given above. If M = X'X show that C (m ll' m.,) = n[(1'-4 - 3)/;,; .. + &..o~ + 0,,0,.]. (e) Using the fact that 1'-4 = 3 for N(O, I), show that if M - W .(""l:. n) then C(m , .. mkl) = n «T"",(Tii +0'110',.)· 3.4.18 (Alternative proof "
928,unknown,"3.4.18 (Alternative proof of Corollary 3.4.8.1) Let {(x) be a p.d.f. on R ' and let X = (X,,. ... x.)' be a random sample from [(x). Show that rank (X ) = min (n, p) with probability one, and hence, using (A.4.2e), show that if ,,:;;. p, then X 'X> 0 with probability one. 3.4,19 (Eaton. 1972, §8.26) (a) Note the following generalization of Theorem 3.4.7(a): if M - Wp (l:, m ) aod A (k x 1') has ra"
929,unknown,"(~' A' )""- W .«Ar""A ')-', III - p + k ). (Hint: If A = [1, OJ, then this theorem is simply a statement thaI (M "")-'_ w. «1:"")"", '"" - P -j-k), wbich has already been proved in Corollary 93 ORlYlA L DISTRlBtmQN THEORY 3.4.6.1. Now any (k x p) matrix of rank k can be written A = B[l..0]1'l:'/2 • where 8 (k x k) is non-singular and 1'= (1';, r~)' is or­ thogonal. (The first k rows, 1'"" of l' form aD o"
930,unknown,"thogonal. (The first k rows, 1'"" of l' form aD orthonormal basis for the rows of U -In The rows of 1'2 are orthogonal to the rows of U -'12, so Al;-'121',=o.) Let Y = n 'inM""l;-1J2l"". Clearly Y - W .(I, III). Now AM""A' = 8[1. OlY "" [J, O]'B ' = 8Y "" 8 ' where y ll is the upper left-hand submalrix of Y "" . Now (~I A') "" =(8')-I(V"")"" 8- '. But (y lI)""_ W .(I, III - p+ k). Therefore (AM -IA')-' - W.«"
931,unknown,"W .(I, III - p+ k). Therefore (AM -IA')-' - W.«88')-I, III - p + k). But 8 = u -If21""[J. 0]'= u -I121',. Thus, since U '1J2f,= 0 and I=r;1',+ f,1'2 , (8B')"" = (U "" A ')"".) (b) Hence deduce Theorem 3.4.7(a) as a special case. 3.4.20 Let the rows of X (n x p) be independenUy distributed x,­ N.( ..... , l:) for i = I, ... , II, where tbe 11; are not necessarily equal. (a) Show that Theorem 3.3.3 rema"
932,unknown,"(a) Show that Theorem 3.3.3 remains valid w ith this assumption on X . (b) Show that Craig's theorem (Theorem 3.4.5) remains valid with this assumption on X . (c) If E (CX ) = 0, show that parts (a) and (b) of Cochran's theorem (Theorem 3.4.4) remain valid with this assumption on X . 3.5.1 (a) Examine the case p = 1 in Theorem 3.5.1. Putting d­ N ,(O.0'2) and illS 2 _ (J'2X~ to correspond to d and"
933,unknown,"that a corresponds to (dis)', which is the square of a 1m variable. (b) If d - Np( .. ,J:) and M - W .(l:, rn) then we say tbat a=rnd~'d has a IIoll-celltral -r-distribution. Show that a = (3+ 2111,,'M"",,+ m .. 'M"" ... where (3 has a central -r-distribution and x- N.(O, 1:). (c) Under the conditions stated in part (b), sbow that a is proportional to a non-central F statistic, where the non-central"
934,unknown,"on ,..'1:-1 .. , the Mahalanobis distance from .. to the origin. 3.5.2 Using the assumptions of Theorem 3.5.1, show tbat if a = m d'M -'d' tJlen a d'M""d (a) - B (lp l(m - I' + 1» ' a+m 1 + d'M 'd 2 • 2 ' (h) III 1 8 (1(111-P + 1), 4p); a+m I + d'M -ld (e) IMJ III - 8 (1(111 - 1'+ I).!p). IM + dd'l a + m (Hint: use equation (A.2.3n),) MULTIVARIATE ANALYS IS 94 3.5.3 Show that il ma - T'(p, m) then "
935,unknown,"totic X; distribution as m ~ """". 3.6.1 Partition 11 = (p.;, lIZ)' and suppose 1: is also partitioned. Using equations (A.2.4g) and (A.2.4n, show tbat where 3.7.1 If A - W .Cl: m ) independently of B - W .<l:, II) and m ;;. 1', show that A -'B has the same eigenvalues as A *-'B *, where A * = 1:- 112A1:-In_ W .(l, m ) independently of B * =:r-""'BI-I12 - W .(l, II). Hence deduce that the joint distr"
936,unknown,"Hence deduce that the joint distribution of the eigenvalues of A -'B does not depend on I . 3.7.2 In Theorem 3.7.1, show that G = (XX ')-WX is row orthono rmal. Also show that XG 'G = X and that GX 'XG'=XX'. 3.7.3 (Alternative proof of Th eorem 3.7.1) Let X «m + II) X(n + 1'», where m ;;. 1', be a data matrix from n~p (0, I) and partition X 'X = M = (M II M "" ),,. \M21 M Z2 P Using Theorem 3.4.6, "
937,unknown,"\M21 M Z2 P Using Theorem 3.4.6, show thal (M""t' =M "" ., =M u - M 2IM ,:M 12 - W .(l.m ) independently of M 22-M 22.I=M 21 M1:M I2-W .(I,II). Thu s. using equation (A .2.4&>. the non-zero roots of M 22M 2IM 'IIM 12 = - M' IM Il have the '1'(1', m , II) distribution. Since M and M -' are sym­ metric, -M 'tM 12 =-M 12'M"" = -(M 2IM 12)"" which has the same non­ zero eigenvalues as - M 12M 21 (since AB"
938,unknown,"zero eigenvalues as - M 12M 21 (since AB , (BA )"" and BA all have the same non-zero eigenvalues). Interchanging tbe roles of 1 and 2 above, and applying Theorem 3.4.6 again. sbow tbat the roots of - M I2M 2I have the '1'(/1, m + II - p. p) distribution. Tbu s tbe '1'(1', m , /I) and '1'(11, m + II - 1', p) distributions are tbe same . 3.7.4 If A - W .(I, m ) and B - W .(I , II) are independent Wis"
939,unknown,"rices, show that IA I/IA + B I has the A(p, m , I.) distribution. 3.7.5 Verify the F distribution given in (3.7.7) and (3.7.8). 3.7.6 (a) (Bartlett,1947) Show that -{r-i(p + q + I)} log A(p, r-q,q) has approximately a X~ distribution for large I. (b) (R ao, 1951, 1973, p. 556) If A -A(p,l-q, q), then an alternative 95 NORJI.iAL D1STRTBlITION THEOJ(Y approximation uses illS -2,\ 1-A II. R =-----;-~"
940,unknown,"R =-----;-~ pq A II. ' where m=r -~(p+ q + l) . ,\ =~(pq-2) . and S2=(p'q2_4l/(P'+q2_5l. Then R has an asymptotic F distribution with pq and (ms - 2>') degrees of freedom . even though (ms - 2,\) need not be an integer. 3.7.7 Show that if In or II is less than p then the <I> statistic defined in Theorem 3.7.2 can not be used. 3.7.8 II A >O and B ;;.O alld Bx = AAx for some xi'O. then show that Bx "
941,unknown,"Bx = (A/(l + A»(A + B )x. H ence deduce that if A is an eigenvalue of A-lB . then A/(I + A) is an eigenvalue of (A + B l-'B . Added in proof 3.4.21 Let x - N.(O. I). Using the characteristic function of x, show that E(X,X ,XkX"") = UijUr.:r + O""ikU"" + iT,tU 4'"" for i. j, k.. I = I, ... ,p. H ence prove that C(x' Ax . -x'B x) = 2 tT (AIB1: ), where A and B are p x p symmetric matrices. Hint: pro\'e "
942,unknown,"the transformed variables y = I ""'"". 4 Estimation 4.1 Likelihood and Sufficiency 4.1.1 The likelihood function Suppose that x"" .... x"" i a random .ample (rom a population with p.d.r. {(x; 9), where 9 is a parameter vector. The likelihood function of the whole sample is "" L(X;9) = n {(x,; II). (.1.1.1) . , The log likelihood function is .. I(X: II) = log L(X; 9) = L log {(x,; 9). (4. 1.2) Given a s"
943,unknown,"Given a sample X both L (X; 9) and I(X ; 9) are considered as functions of the parameter O. For tbe special case n = 1. L(x; 9) = {(x; 9) and tbe distinction between the p.d.!. and the likelihood function is to be noted: fix; 9) is interpreted as a p.dJ. when 9 is fixed and"" is allowed to vary and it is interpreted as the likelihood function when. is fixed and 9 is allowed to vary. As we know, the"
944,unknown,"allowed to vary. As we know, the p.d.f. plays a key role in probability theory whereas the likelihood is central to the theory of statistical inference. Example 4.1.1. Suppose ., ..... x"" is a random sample from N p (,..1:). Then (4.1.l) and (4.1.2) become, respectively, L(X : ,. . I)=I21TII - ""lZexp{-~.t. (r.-,.)':1 '(1(,-11»), (4.1.3) 97 ESTIMATION and II n [(X: I1.I)=Jug L(X :9)=-2'logI21T1:I-i"
945,unknown,"and II n [(X: I1.I)=Jug L(X :9)=-2'logI21T1:I-i L (:<,- ,.)':1-'(x, - ,.). 1-' (4.1.4) Equation (4.1.4) can be simplified as (ollows. When the identity (x, - ,.)':1-'(x, -,.) = (x, - XYL '(X. -xl +(i - ,.)'1:-'(x - ,.) ... 2(i- ,.yr '("". - x), (4.1.5) is summed over the index i = 1 , .... II. the final tern, on the right-hand side van.ishes. yielding f (x, -,.Yl:""'(x, -,.) = f (x, -;c)'r'(x. - i)+"
946,unknown,"f (x, -,.Yl:""'(x, -,.) = f (x, -;c)'r'(x. - i)+ It (i-,.)':1-'(i - ,.). I- I "" .. I (4.1.6) Since each term (x,-x),:1-'("", - x) is a scalar, it equals the trace of itself. He nce (see Section A.2.2) (4.1.7) Summing (4.1.7) over the index i and substituting in (4.1.6) yields "" L (x, -,.)'r'( , , - ,.) = tr I-' {i (x, - x)(x, -x)'} + It (i - ,.)'r '(j - ,.l. ,-I Writing (4.1.8) .. L (x, -x)(x, - x)'"
947,unknown,"Writing (4.1.8) .. L (x, -x)(x, - x)' = liS . , and using (4.1.8) in (4.1 A) gives For the special case w hen 1:= 1 and ,.= 8 then (4.1.9) becomes np 11 n I(X: 9)= - -Zlog 21T-2' trS-2' (x-9),(i- 9). (4.l.l 0) 4.L2 Efficient scores and FISher's infonnation The score f""""ctioli or efficient score s = s(X : 9) is defined as a I a seX : 9) = all (ex: 9) L(X; II) 00 L(X: 9). (4.1.11) MULTIVARIATE ANALY"
948,unknown,"Of course. seX; 9) is a random vector which we may write simply as s. The covariance matrix of s is called Fisher's infonnation matrix. which we denote by F . Theorem 4.1.1 If seX ; 0) is the score of a likelihood function alld if t is any function of X and 0, then. under certain regularity conditi(!Ils, £(st) = ~ E(t') - E(itt') ao ao . (4 .1.12) Proof We have E(t') = J t'L dX . On differentiatin"
949,unknown,"E(t') = J t'L dX . On differentiating both sides with respect to 9 and taking the differentia­ tion under the integral sign in the right-hand side (assuming this opera­ tion is valid), we obtain ~E(t') = J""1:;!Lt'L dX+ J: LdX . Tbe result follows on simplifying and rearranging tbis expression. • CoroUaty 4.1.1.1 If seX; 9) is the score corresponding to a regular likeli­ hood function then E(5) = O"
950,unknown,"Proof Choose t as any constant vector. Then E (s)t' = 0 for all I. Thus. E (s) = 0 . • (4.1.l3) CoroUary 4.1.1.2 If sex; 9) is the score corresponding to a regular likeli­ hood function and if t is any unbiased estimator of 0 then E(st') = I. (4.1.14) Proof W e have E(t) = 0 and, since t does not involve 0, : = 0, • CoroUaty 4.1.1.3 If seX : 0) is 1I1e score corresponding ro a regular likeli­ hood"
951,unknown,"hood function and if t is an estimator such thaI E (t)=0 + b(9), Ihen E(st') = I + B where (8 )"" =ab""iJB,. Proof Use itt'/ao = I + B .• From Corollary 4.1.1.1 we ee tbat F = V (s) = E (sS). Applying Theorem 4.1.1 with t= s gives , (iW) (~I) F = E(ss)= -E ao = -E\aiiiiiY-. (4.1.]5) 99 ESTf tA TI ON Example 4.1.2 Supp ose x"" ... ,x. is a random sample from .(9.1). From (4.1.10), a seX : 9) = 00 I(X "
952,unknown,"From (4.1.10), a seX : 9) = 00 I(X ; 0) = n(x -9), iJs' ao = -nl. Hence, from (4.1.15), F = nl. Alternatively, F can be obtained as the covariance matrix of II(X - O). Example 4.1.3 For n = 1, tbe simple exponential family defined in Section 2.7.1 has likelihood function f(x; 0) = exp [ 0 0(0) + bo(x) +, t. B,b, (x)]. Hence • • I (x ; 9) = "" 0(0) + bo(s) + L B,b, (x), sex; 0) = ao;,0) + b. where b"
953,unknown,"where b' = (b ,(x) ..... b.(x» . U sing (4.1.15). F =-E(iJs) = _ a 2 ao(9) iJO' a9a9' . 4.1.3 The Cramer-Rao lower bound (4.1.16) Theorem 4.1.2 If t = reX ) is all ullbiased esrimaror of 9 based 0"" "" regular likelillood funcrioll. till'"" (4. l.I 7) wllere F is rile FISI,er in/ormation matrix. Proof Consider corr""[a , 'Y], where a=a't, 'Y = c's, and s is the score function. From (4.1.13) and (4.1.1"
954,unknown,"C[a. y ]=a'C[t.s]c= a'c, V(y) = c'V(s)c = c'Fc. Hence corr [a, y]=(a'c)'/{a'V (t)ac'Fc},.; I. (4 .1.18) M aximizing the left-hand side of (4 .1.18) with respect to c with the help of (A. 9.13) gives, for all a. a'r 'a/a'V (t)a,,;; I; that is a'{V (I)-F-')a30 for all a. whicb is equivalent to (4.1.17) .• MULTNARlATE ANALYSIS 100 Note that the Cramer -Rao lower bound is attained if and ouly if the e"
955,unknown,"the estimator is a linear function of the score vector. Example 4.1.4 A genetic example presented by Fisher (1970. p. 305) has four outcomes with probabilities (2 + 0)14, (1-0)14. (I - 0)14. and 014, respectively. If in "" trials one observes X. results for outcome i, where Lx. = "". then x=(x"" x"" :1:3 , x.l' has a multinom ial di tribution and the likelihood function is where c = n !I(x,! x,! X3! x"
956,unknown,"I(x; 0) = log c -"" log 4+ x. log (2 + 0) +(X2 + X3) log (1 - 0) + X 4 log O. The score function is given by al x. sex: 0) ao 2+0 X2+ X3 X4 L -0 ' O ' From (4.1.1S), ( -x, (x2+ x,) X4) F= - E (2+0)2. (1-0)' 02. n (1+20) {20(l-0)(2+ 0»), The Cramer-Rao lower bound is F -'. Here 4x./n is an unbiased estimator of 0 with variance 0(4 - O)/n wh ich exceeds rl. Hence 4x4 /"" does not attain the Cramer-Rao"
957,unknown,"does not attain the Cramer-Rao lower bound. Since the lower bound can only be obtained by a linear function of the score, any such function (which wiII involve O) cannot be a statistic. Hence no unbiased estimator can attain the lower bound in this example . 4.1.4 Sufficiency Sup pose X = (x"" ... , x,,)' is a sequence of independent identically distri­ buted random vectors wh ose distribution depe"
958,unknown,"buted random vectors wh ose distribution depends on the parame ter e, and let t(X) be any statistic. The statistic t is said to be sufficient for e if L(X: e} can be factorized as L(X; e)= get; e)h(X ), (4.1.19) where /, is a non-negative function not inVOlving e and g i~ a function of e and t. Note that (4.1.19) implies that the efficient score s depends on the data only through the sufficient st"
959,unknown,"Example 4.1.5 For the multinormal case we have from (4.1.9) taking r:;ELIO~ ---- .:...:;'1·R.. ... L - UFLA 101 ESTIMATION exponentials, L(X; ..... 1:) = 127r1:I-N'2 exp { -~ tr l:- I S-~(i-I1Yr l(i-I1J}. (4.1.20) In (4.1.19), taking heX) = I, we see that i and S are sufficient for 11 and 1:. Example 4.1.6 From Example 4.1.4. L(x: 0) = c(2+ 0)""(\-6)""'-<'0'-14"" -<'-<'."". Taldng g(t;8)=(2+0)'·(l-0)'"
960,unknown,"Taldng g(t;8)=(2+0)'·(l-0)'7'<'O<· and t'=(xl, x,+x3 • x.) in (4.1.19), we see that 1 is sufficient for O. Since It = x 1+ X2 + X , + x. is fixed, t+= (x .. x4 )' is also sufficient, as x., + X3 can be written in ternlS of 1+. Exam ple 4.L 7 For a sample of size It from the sinlple exponential fam ily as in Example# 4.1.3 the likelihood can be written as [(X: e)=exp [,lao(e)+ .t O,.t b;<xl) ] exp "
961,unknown,"[(X: e)=exp [,lao(e)+ .t O,.t b;<xl) ] exp [.t. bO(X i)]' I ~ t .= 1 , .. 1 , In (4.1.19). take get: e) as the first factor in this expression and f,(X ) as the second. The vector I, where is sufficient for 9. .. I, = L b,e_,), i""'l A sufficient statistic is said to be minimal sufficient if it is a function of every other sufficient statistic. The Rae-Blackwell theorem states that if the minima l "
962,unknown,"the minima l sufficient statistic is complete, then any unbiased estimator which is a function of the minimal sufficient statistic m ust necessarily be the unique minim um variance unbiased estimator (MVUE); that is, it will have a smaller covariance matrix than any other unbiased estimator. Example 4.1.8 Suppose that the It rows of X are i.i.d. N .(I1, I) vectors and we seek the best unbiased est"
963,unknown,"and we seek the best unbiased estimate of the quadratic function 0 = ,,'11 + I'll. The mean x is minimal ,sufficient for 11, and we consider a quadratic function of i, t = ai'i+b'x+c. Then E (/)= a(I1'I1 + ~) + b',,+c. If I is to be an unbiased estimator of O. then a = 1. b = I. and c = -apln. Since this unbiased estimator is a function of the minimal sufficient statistic ii, wh ich is comple te, "
964,unknown,"the minimum variance unbiased estimator of 9. MULTrvARlATE ANA L YSIS 102 4.2 Maximum Likelihood Estimation 4.2.1 General case The maximum likelihood estimate (m.Le.) of an unk nown parameter is that value of the parameter which maximizes the likelihood of the given observations. In regular cases this maximum m ay be found by differentia­ tion and, since I(X ; 9) is at a maximum when L(X: 9) i< at"
965,unknown,"tion and, since I(X ; 9) is at a maximum when L(X: 9) i< at a maximum , the equa tion (aI/ail) = s = 0 is solved for 9. The m.Le . e of 9 is that value which provides an overall maximum . Since s is a function of a sufficient statistic, then so is the m .l.e. Also, if the density f{x; 9) satisfies certain regularity conditions and if en is the m.Le. of 9 for a random sample of size n. then en is a"
966,unknown,"size n. then en is asymp totically normally distributed with mean 9 and covariance matrix F;' = (nF)', wbere F is the Fisher information matrix for a single observation (see, for example , R ao. 1973, p. 416 ). In particular. since V(O.) -+ 0,0. is a cOllsistent estimate of 6, i.c. plim 0"" = 6, where plim denotes limit in probability. ElUIlllple 4.2.1 T he m.Le. of II in Examp le 4.1.4 is found by"
967,unknown,"eq uation 5= 0, which becomes 1II1'+(2x,+2x,-x, + x.)8 - 2x"" = 0. T his qu adratic equation gives two rootS, one positive and one negative. Only the positive root is adm issible. Wri ting P. = XJII. i = 1. 2 ,3,4, the quadratic equation becomes 1I'+1I(2- 3p ,-p.)-2p. =0. Discarding the negative root of this quadratic equa tion, iI = a + (a2 + 2p4)'!2, where a =i(3p,+ p.)-1. Example 4.2.2 If (x"" .."
968,unknown,"Example 4.2.2 If (x"" .... x,,) is a random sample from Weinman 's p­ variate exponential distribution defined in ection 2.6.3. then the log likelihood is given by 0-' ( Il.) 11 -'1= - L log A, +-' • ,_41 )..,/ (4.2.1) where . nllj = (p - j) L {x.(j. \) - x.u,1 ;""""J with x,(I) the jth sma llest element of x, (i = I, .... II; j = 1, ... , P - 1), and x i(O) = O. Note that although the elements of x,"
969,unknown,"x i(O) = O. Note that although the elements of x, are not independent, the elem ents Il"" ... , Il._, are independent. Differentiating the log likelihood 103 E~""M""""'TI ON gives the element' si of the score s as Sj = alfaA, - - n (Ai - llj)fA ~. The m.Le .s are therefore A; = 8,. Since £ (s,)=O we see that A, is an unbiased estimate of Aj• Tn the special case when aIL the parameters are equal. . ay "
970,unknown,"equal. . ay A. then (4.2.1) becom es ""-'/=-PIOgA -Ci:' Il.)/A. (4.2.2) f- U Differentiating with respeGt: to A gives al lip (P-')/ ('~ ' )/ s=-=--+ n L Ii, A '=n L o,-pA A'. aA A {_"" f-O Solving s = 0 gives ~ =(""i:' Il.)/P. I- li A gain. since E (s)=O w e see that A is unbiased for A. 4.2.2 Multivariate nnrrnal case 4.2.2.1 Un collstrained case For the multivariat~ normal distribution w e have the"
971,unknown,"function from (4.1.9): We shall now show that ii= i. i =s (4.2.3) if II ;;, P + 1. ote that from Corollary 3.4.8.1 that s> 0 with probability 1. The parame ters here are J.l and 1:. Using new parameters J.l and V with V = 1:-', we first calculate a/fa"", and illfilV . Equation (4.1.9) becomes np II II U , I(X; J.l. V ) = -2 log 2TT + 210g iV I-2 tr VS - 2 tr V (X- J.l)(x - J.l) . (4.2.4) Using Sect"
972,unknown,"(4.2.4) Using Section A.9. (4.2.5) MULTIVARIATE A.'iALVSIS 104 To calculate iH/ilV, consider separately each term on the right-hand side of (4.2.4). From (A.9.3), ~ log IV I = {2 V jlV I, iluil V..IIV I. if j, i = j. where V'I is the iith cofactor of V . Since V is symmetric. the matrix with eleme nts Vi;/IV I equals V -' =1:. Thus From (A.9.4), and iJ log IV I = 21:-Oiag 1: av . iltrVS . --=2S - "
973,unknown,"iltrVS . --=2S - 013gS !IV Comb ining these equations we see that al II aV =2 (2M - OlagM). where M = 1:- S -(i-"",)(x-"",y. To find the m.l.e.s of '"" and l: we must solve 01 ill - = 0 and -=0 . OIL ilV (4.2.6) From (4.2.5) we see thal V(x-IL)=O . Hence the m .l.e. of IL is ,1 =x and from (4.2.6) al/av = o gives 2M - OiagM = O. This implies M = O. i.e. the m.l.e. of 1: is given by (4.2.7) Since ,1= "
974,unknown,"m.l.e. of 1: is given by (4.2.7) Since ,1= x. (4.2.7) gives t = S. Strictly spealcing, the above argument only tells u.s that - and S give a stationary point of the likelihood. In order to show that it and S give the ove raU maximum value, consider the following theorem. Theorem 4.2.1 For allY fixed (p x p) marrix A > O. {(1:) = 1'11-""'2 exp (~ tr r l A) is maximized over 1:>0 by '1=II-'A, and {(I"
975,unknown,"is maximized over 1:>0 by '1=II-'A, and {(II-'A)=I,,-'A ) ..ne-npt2. 105 ESTIMArJON Proof II is easily seen that {(II- ' AI takes the form given above. Then we can write log fen IA ) - log [('1) = ~ ""p(a -I-log g). where a = tr '1 -'A/IlP and g =1 11 'X • A I"""" are the arithmetic and ""eomet­ ric means of the eigenvalues of ./ '1: 'A . Note that all of thes; eigen­ values are positive from Corollar"
976,unknown,"values are positive from Corollary A.7.3.1. From Exercise 4.2.3. a - 1- 10gg;;.O. and hence f(II-'A );;'[(l:) for all '1> 0 . • If we maximize lh<: likelihtlod (.1.1.9) over l: for fixed IL, we find from Theorem 4.2.1 that 1: "" given by (4.2.7). Then since from (A.2.:1m) we see that the 1o!!. likelihood ~s maximized by Ii. = i. Alternatively. we could maximize ove< IL first (Exercise 4.2.11). (In "
977,unknown,"in general leave it to tb~ reader to verify that the stationary pouu; obtained are in fael overall m axima.) Note that the m .l.e. of 11 could have been deduced {rom (4.1.9) directly, whether I is known or not. Since l:"""">0. -(>:- "",)'l:""""(i- "",)"",O and is maximized when 11 = '\:. Thus I(X:IL. I ) in (4.1 9) is maximized at IL= '\: whether l: i constrained Or nOl. Consequently, Ii. = i when I is k"
978,unknown,"However. in case; where"", is constrained the m.l.e. of '1 will be affected. e.g. when 11 is known a priori, the m .l.e. of I i. found by soh-ing al/ilV = 0 and is given by (4.2.7). In the case where II = 1 and I = I is known . Stein (1956) showed that when p;;'3. m.l.e. ,1= i = x is an i""admissible estimator of IL under quadratic loss. For further discu""ion of this remarkable fact. see Cox and Hin"
979,unknown,"Hinkley (1974, p.447). -1.2.2.2 COlIsrraillls 011 rlie mea"" vee/or IL Consider the case where IL i; known to be proportional 10 a known vector, so ""'= kl'<J. For example. the elements of could represent a sam ple of repeated measurcmenLs. in wh ich case 11= k1. With 1: also MULTIVARJATE ANALYSIS known (4.1.9) becomes To find the m.Le. of k, we solve aI/ilk = O. This gives ftl'Ql:-I(i - kilo) = 0, "
980,unknown,"ftl'Ql:-I(i - kilo) = 0, i.e. the m .l.e. k of k is 106 (4.2.8) (4.2.9) Then k is unbiased with variance (n..o1:- l llorl; see Exercise 4.2.l. When 1: is unknown the two equations to solve for t and k are (4.2.7) and (4.2.9). Pre- and post-multiplying (4.2.7) by i-I and s-t respec­ tively, gives S-I =l:-I + l:-\r- "")(x-,,yS-'. Pre-multiplying (4.2. to) by ..0 and using (4.2.8) gives pQS -l = ,...."
981,unknown,"pQS -l = ,....,;1:-'. Thus. from (4.2.9), (4.2.10) (4.2.11) A further type of constraint is ~= r. where R and r are pre-specified. Maximizing the log likelihood subject to this constraint may be achieved by augmenting the log likelihood with a Lagrangian el<pression; thus we maximize where A is a vector of Lagrangian multipLiers and I is as given in (4.1.9). With 1: assumed known , to find the m.L"
982,unknown,"for which the solution to aJ· - = 0 a"" satisfies the constraint ~=r. From (4.2.5) Thus al· = ""1:-I(j-,,) _ nR'A. a.. i-,,= l:R'A. (4.2.12) Pre-multiplying by R gives RX - r=(Rl:R')A if the constraint is to be L07 satisfied. Thus. we take A = (Rl:R)- '(Ri - r), so ';'=ii-l:R'A = i-1:R'(Rl:R ')-'(Ri - r). When :t is unknown the m.Le. of"" becomes ,;. = ii- SR'(RSR ')-' (Ri- rj. See Exercise 4 .2.8. 4"
983,unknown,"See Exercise 4 .2.8. 4.2.2.3 COlIsfTainrs 011 1: ESTrMAnON (4.2.13) (4.2.14) First we consider the case where ""I = k""Io, where ~ is known . From (4.1.9), (4.2.15) where u = tr :t;;'S +(j- ""),l:.~'(ii -,,) is independent of k. If J.1. is known, then to obtain the m.Le. of k we solve al/ak = O. This gives - p/k +a/k 2 =0. Tbus the m.Le. of k is k = alp. If j.L is unconstrained, then we solve alliJj."
984,unknown,"alliJj.L=O and al/ak=O. These give k = alp and l1=i. Together these give • k = II :t;;'S/p. (4.2.16) The constraint 1:12 = 0, where :t'2 is an off-diagonal subma trix of 1:, implies that the two corresponding groups of variables are independent. The m.Le. of 1: can be lound by considering each subgroup separately and is t=[Sl1 0 ] Os,,· Exam ple 4.2.3 For 47 female cats the body weight (kgs) and h"
985,unknown,"weight (gros) were recorded; see Fisher (1947) and Exercise 1.4.3. The sampLe mean vector and covariance matrix are i = (2.36.9.20)'. s= [0.0735 0.1937] 0. t 93 7 1.8040' Thus j and S are the unconstrained m.Le 's for"" and.1:. However, if from other information we know that ,,=(2.5.10),. then tis given by (4.2.7). i-"" = (-0.14, -0.80)', i:S+(i-}(i-)'= [0.0931 0.3057] "" "" 0.3057 2.4440' MULTIVARLAT"
986,unknown,"MULTIVARLATE ANALYStS 108 If instead we assume """"= k(2.5. 10)"" then from (4.2.11). k =(2.5. IO)S-'(2.36. 9.20)'/(2.5, 10)S- '(2.5. 10)'. This gives k = 0.937, so the m .l.e. of"", is (2.34. 9.37)' and the m.l.e. for l: is now i = S +(0.02, -O.17)'{0.02. -0.17) which is quite close to S. If the covariance matrix was known to be proportional to _ [0.1 0.2] 1;0 - 0.2 2.0 with"", unconstrained. then fro"
987,unknown,"i = [0.D78 0.156] 0.156 1.562 ' 4 .2.2.4 Sample s wirh lillked parameters We turn now to consider situations with several normal samples, wher e we k now a priori that some relationship exists between their parameters. For instance we may have independent data matrices X , ..... X •. where the rows of X ;(n; x p) are U.d . Np( ..... l:.), i = 1 ..... k. The most common constraints arc (a) 1;, = .."
988,unknown,"constraints arc (a) 1;, = ... = 1;. or (b) 1;, = ... = l:. and """" = ... = ""' •. Of course if (b) holds we can tTeat all the data matrices as constituting o ne sample from a single pop ulation. T o calculate the m .l.e.s if (a) holds, note that. from (4.1.9). the log likelihood function is given by 1= -t L [n, log 127Tl: I + II, tr r '(S, +d,d:)]. (4.2.17) where S, is the covariance m atrix of the "
989,unknown,"where S, is the covariance m atrix of the ith sample. i = I, ... , k. and d; = i , - j.Li' Since there is no restriction on the population means , them .l.e. of j.L, is X. and setting /I = 111"" (4.2.17) becomes 1=-~nlogI2,,:1:I -~ trL'W. where W = Ln,S •. (4.2.18) DIfferentiating (4.2.18) with respect to l: and equating to zero gives l:=Il-IW . Therefore i = n-IW is the m.l.e. of l: under the cond"
990,unknown,"stated. 109 ESTIMATION 4.3 Other Techniqoes and Concepts 4.3.1 Bayesian inference Un til now the vector 9 has been regarded as fixed but unknown . How­ ""ver. a Bayesian approach would regard 9 as a random variable whose distribution reflects subjective beliefs in what the value of 9 is likely to be. A fairly complete exposition of the multivariate aspects of Bayesian inference appears in Chapte r "
991,unknown,"inference appears in Chapte r 8 of Box and Tiao (1973 ). Suppo se that before observing the vector x, our beliefs about the value of 9 could be represented by a prior distribution ,,(9). For instance, if 6 is a scalar whicb is believed to lie between 1 and 5 with a 95% probability. then we m ay take the prior distribution to be normal with me an 3 and variance I. since about 95% of this distributi"
992,unknown,This would lead to I 1T(8)= c-cxp {-itll-3l'l. v2-rr Iternatively. if OU i beliefs were oot sym m etric about 3. som e other prior distribution might be m ore suitable. For instance if (J is non-negative. then a gamma distribution could be used. The prior density. given by -rr(9). and the likelihood of the observed data f(x; 9) together define the so-called posterior distribution. which is the con
993,unknown,the cond itiooal distribution of 9 given x. By B ayes' theorem. the posterior density is given by 7T(9 1 x)= {1T(9)f(x; Ol} / J 7T(9)f(x; 0) d9°c ... (9)f(x; 9). (4.3.1) All B ayesian inference is based upon the posterior probability fuoction. 10 particular. the Ba yesian estimate o f 9 is given by the mean of the posterior density 1T(0 I x). Sometimes it is convenient to use an impr oper prior de
994,unknown,"(f ro(9 ) dO = w). Such priors are allowable as long as f 7T(O)f(x; 9) de <00. so that the posterior density is defined. E xample 4.3.1 Suppose that X =(x ...... "".y is a random sample from N. (p.,:1:) and that there is no prior knowledge concerning"", and :1:. Then a natural choice of prior distribution is the non-informative (or vague) prior based on Jeffreys' principle of invariance (Jeffreys, 1"
995,unknown,"principle assumes first that the location parameter"", and scale parameter 1: are independent, so 17("",,:1:) = 17("",),,(:1:). and second that 1T (,.) ex IF(p.)I'12. 1T(:1:) <X IF(:1:)1""2 • w here F("",) and F(:1:) are the information matrices for "", and :1:. Hence , MULTIVARlATE A l ALYSIS 110 using Exercise 4.1.12. we see that """"li>,:I) ex I:II-IP' 'Wl . (4.3.2) The marginal posterior of .... Can n"
996,unknown,"The marginal posterior of .... Can nOW be obtained by substituting for 7T( ..... :I) in (4.3.1) and integrating 7T( ..... }; 1 X) with respect to 1:. to give ."".( .... 1 X) '"" J 1:II-wP>l)flexp ( ~ tr };-'V) dl:, (4.3.3) where V = IIS+II(;;-.... )(i- .... )'. The function to be integrated here is similar to the p.d.f. of the inverted Wi hart distribution given 'in (3.8.2). Using the normalizing co"
997,unknown,"that expression. it is clear from (4.3.3) that 1T(p.1 X) '"" IV I-""Il· But Tberefore ."".( .... 1 X ) ex [l + (i - .... )'S-'(i - .... n-""'2. Thus the posterior di.tribution of .... is a multivariate t distribution with (n - p) degrees of freedom and parameters i and S (see Exercise 2.6.5). The mean of this posterior distribution is the sample mean i. so in this case the Bayes' estimate of .... is t"
998,unknown,"case the Bayes' estimate of .... is the same as the maximum likelihood estimate. The posterior distribution of 1: is obtained by integrating ?T(,., 1: I X ) with respect to ..... This leads to an inverted Wisbart posterior distribution, witb a mean of ""S/(II -p - 2) (see Exercises 3.4.13 and 4.3.1). Note that in this case the Bayes' estimate differs from the maximum likelihood estinlate. Example 4"
999,unknown,"estinlate. Example 4.3.2 When one wishes to incorporate illformative prior infor­ mation into the analysis tben a useful prior distribution is the ""natural conjugate prior"" (see, for exam ple. Press, 1972. p. 76). For example , in the case of a random sample from tbe N .li>, 1;) distribution, tbe density of x and S from (3.4.3) and (3.8.1) is f(i.. S; ,., 1;) ex {il:l-,n exp [-ill(i - .... yr'(i-."
1000,unknown,"x {ll:l-I""- ')/2 exp (--ill tr 1;-'S)}. (4.3.4) Then the conjugate prior for J.! and :I is obtained by thinking of (4.3.4) as a density in .... and :I after interpreting the quantities x. ""S, and"" -1 as parame ters of the prior distribution <1> , G > 0, and til> 2p -I, and adjust­ ing the proportionality constant so that the density integrates to 1. Thus, 11(,.,)';) cx {I -"""", exp (--i tr 1;-lG )}"
1001,unknown,"(4.3.5) 111 ESTIMATION The Bayes ' estimates of .... and I are. respectively (see Exercise 4.3.2), (<1> + ""i)/(1 +n) , {ns+ G + 1: n (x-<I>)(x-<I>y}j (It ~m -2p-2). (4.3.6) either of these estimates is tbe same as the corresponding maximum likelihood estimate, although they do indicate that tbe Bayesian proce­ dure leads to a weighted average of prior estimates and samp le-based estimates. 4.3.2 R"
1002,unknown,estimates. 4.3.2 Robust estimation of location and dispersion for multivariate distributions Estimators of location One possible way to e..timate the location of a multivariate sample is to treat each variable separately. For example. one can use the a-trimmed mean for each vat1able (Le. the mean after omitting a proportion ex of the sma llest and a proportion ex of tbe largest observations on eac
1003,unknown,"An alternative estimator based on each variable separately is the M­ estimator (maximum likelibood type estimator). For a univariate. ample X, ..... x. It is defined implicitly by .. L w(x,- T .. )=O , ,-, where oil is a given function. If the sample comes from a symmetric distribution F, and if ODe takes w(x) = max [-k. min (x, k)] where F(-k) = ex. then this estimator has the same asympto tic be"
1004,unknown,"a-trimmed mean (Huber. 1972 ). (Note that if oiI(x)=x. then Tn =i). An inherently multivariate technique to estimate the mean vector has been given by Ge .ntleman (1965) (also see, Barnard, 1976). For fixed k. J """" k """" 2. he proposed an estimator xt which minimizes .. L II xi- x~II·. /-, where IHI denotes the usual Euclidean norm. He recommends k=~ in practice to minimize the effect of outliers a"
1005,unknown,"practice to minimize the effect of outliers and to retain as m uch precision as possible. Note that One gets the usual mean vector when k = 2. Tukey has proposed a multivariate analogue to trimming called ""peel­ ing"" (see Barnett, 1976). This consists of deleting extreme points of the convex hull of tbe sample and either repeating this a fixed number of MULTIVAR.lATE ANALYSIS 112 times or until a "
1006,unknown,"MULTIVAR.lATE ANALYSIS 112 times or until a fixed percentage of the points has been removed. Very little is known about the behaviour of this procedure. EXllIIIpie 4.3.3 We illustrate the above techniques on the iris data of Table 1.2.2 for Iii< versicolour with x, = petal length and X, = sepal length. Figure 4.3.1 shows the set of convex hulls for this data. The mean vector after ""peeling once"", "
1007,unknown,"vector after ""peeling once"", i.e. excluding the extreme points (nine in aU) of the convex bull, is i:=(4.26, 5.91)'. This is not far distant from the sample mean vector i=(4.26, 5.94)'. The excluded points are listed in Table 4.3.1. The et-trimmed values for et = 0.04 are also shown in Fig. 4.3.1, i.e. the largest two and smallest two extreme values are trimmed for each variable. The trimmed mean "
1008,unknown,"variable. The trimmed mean vector is ~)=(4.27, 5.93)'. This is very close to tbe untrimmed mean i. Hence on this data the values of the three estimates are similar. However , the properties of the estimators are vastly different, especially on samples wbicb are contaminated in some way . Estimates of dispersion There are various univariate techniques for producing robust estimates of variance whic"
1009,unknown,"variance which can be extended to covariance. but in general a covariance matrix produced in this way is not necessarily positive definite. 1.0 5.0 L---;-3'-;.O,.......-L----'---'-----'---..,.5'-;. O,-~'--.. Pelol length FigUTe 4.3.1 Convex hull for rhe iris doro (1. "".,sic%ur variety) witl, 0= mean afrer ""peeling "" onct!, + = a-mmmed mean (0. = 0.04). 113 Table 4.3.] Extrem e values to be exclude"
1010,unknown,"robust estimates in the iris data of Table 1.2.2 VaJues excluded after peeling once (X I' xJ (4.7.7.0). (4.9, 6.9). (3.3,4.9). (4.4, 6.7) (5.0,6.7). (3.5, 5.7), (5.1. 6.0). (4 .5, 5.4), (3.V, 5.1) a-trimmmg , a =0.04 x,: 3.3. 5.0. 5.1. 3.0 x,: 7.0.1>.9.4.9,5.0 ESTIMA1l0. The following procedure looks promising (see Wilk er al., 1962). (l) Rank tbe observations X,(I = 1.. ... n) in terms of Euclide"
1011,unknown,"II"" -x'II, where f* is a robust estimator of location. (2) Take a subset R whose ranks are Ihe smallest IOO(l-a)% and comp ute the sum of squares and product. matrix: A"" = L (x, - x"")(x, -x*)', -.>'1< where <II should be small to ensure that A"" IS non-singular. (3) Rank all n observations in terms of (x, - x*)'A ;;'(x, - x*). (4) A subset T of the"" observations whos e ranks are the smallest 100(1 "
1012,unknown,"100(1 - /3)% are chosen and the robust estimalor of the dispersion matrix is s* n(l ~ 13) ~T (x, -x*)(x, - x*Y. '(ere the value of k is chosen to make S' sufficiently unbiased and /3 is a 'mall positive number. If /3 is small and n(1- /3) """" p, then S* is noo ­ ,ingular with probability one. More work needs to be done . however. to ,ecornmelld k, 0/. and /3 in practice. Gnanades ikan dnd Kettenrin"
1013,unknown,"Gnanades ikan dnd Kettenring (I972) and Huber (1972) bave made a .umparison of tbe many robust estimators available. but the subject is still '"" a developing state. Exe rcises and Complements -LL1 Let x COrne from the general exponential family with p.d.f. f(x; 0) = exp f a,,(O) ~ b,,(x)~ f a, (O)Mx)}. , , MULTI V ARIA TE A ALVsrS 114 (a) Show that the . core function is aa"" f (la, b ( ) _ OlT"" Ab"
1014,unknown,"aa"" f (la, b ( ) _ OlT"" Ab 5 = - + L. - ,x --+ , n& ,_, OIl OIl where A is the malrix whose itb column is adJiI6. and b' is the vector (hl(x) ..... bq(x» . (b) ate that ilaolae and A are both free of x. Therefore, since Ihe core has mean zero, prove that U A is a non-sinj!ular matrix. then E[b]=-A I oall (cl Comparing (a) and (h), prove lhallhe value of 9 for which s is zero (i.e. tbe m aximum lik"
1015,unknown,"(i.e. tbe m aximum likelihood estimalor) is also the value which equates b to il. expected value. (d) Show that the information matI u.. is given by (iI!<') a' a <, a' a F = - E ~ = - --"" - L --', E[Mx)]. iI9 OIl il9' , ,ilO OIl 4.1.2 (a) If x ha, the multinomial distribution. sO that Vex) = diag(a)-aa'. as shown in Section 2.6.1. then show that VIP''') = P' V(x)fJ= L /3 ~a, -(L /3,a,),. (bl Note "
1016,unknown,"(bl Note that the score, defined in Exa mp le 4.1.4 may be written as ., = l3'x. where a , = ~(2 + 8). u, = a, = ~(1 - II). and a~ = 0/4. and /3, = 1/(-1a,), (:I, = (:I. = -1/(4112) and 13. = 1 / (4a~) . Hence deduce that yes) = t..r a; I. 4.1.3 Show from (4.1.2) that the score of a et of independent random variables equals the sum of their individual scores. 4.L4 If I(X : 9) is as defined in (4.1"
1017,unknown,"4.L4 If I(X : 9) is as defined in (4.1.10) with t= it. verify Coro llary 4 .1.1.2. 4.1-5 If x, ..... Xn are independent variables whose distributions depend upon the parame ter 9. and if F, = E(S,5;) is the Fisher information matrix corresponding to X ,. then show that E (5,S;) = 0 and hence that F = F ,- ... + F"" is the Fisher informa tinn matrix correspond ing to the m atrix X = \X , ..... .)'. "
1018,unknown,"X = \X , ..... .)'. 4.1.6 Obtain tbe Fisher information corresponding, to the genetic exper­ iment of Example 4.1.-1 by ..quaring. the score and evaluating E(S2). Check that the result is equal to that obtained in Exan1ple 4.1.4. LIS ESTlMATIO>< 4.1.7. Show that if E(I) = 9 + b(9), where t does not depend on 9, then equatIon (4.1.18) becomes corr2 (a, 1') = {c'(I + B )al'/(a'Vac'Fc), B = (ilb;(9)t"
1019,unknown,"so m~x coIT' (a, 1') = a'(I + B)'~I(I+ B )a/s'Va . Hence show Ihat the Cramer-Rao lower bound can be generalized. 4.1.8 If A - W p (:£, m) show that its likelihood is given by L{A;:£ ) = cl:£I-""""2IA I(m-p - Il12 exp (4 tr:t-' A ). where c includes all thl! terms which depend only on III and p. Taking V= ~' as the parameter as in Section 4.2.2.1. the· log likelihood is therefore I(A; V) = log c +~1"
1020,unknown,"therefore I(A; V) = log c +~1Il l og I VI+~( I11 - p -I) log IA I-!trVA . W riting K =~IIl'l: -tA, use the resull~ of Section 4.2.2.1 to show that the SCO re is the matrix a/ U =-= 2K-DiagK BV . No te that, since E(A) = m:t, it is clear that U has mean zero. ~.1.9 If,, - N p (1L8. 0'2 1), where a is known. then show that'"" enters the likelihood function only through the bivariate vector t= (a'x, """
1021,unknown,"show that t is sufficient for (IL. 0'2). Hence show that wben the elements of }. are i.i.d. N(IL, 0'2). then (i, x'x) is sufficient for (IL.O''). ~.1.10 (Ge neralization of Example 4.1.4 and Exercise 4.1.2) If x = (.t"" .... xpJ' has a multinomial distribution with parameters a ,(9) ..... 11.(9), and N =Ix"" (a\ show that the score function is s= L X i aa,. aj 00 1 (b) show that the condition E[o)=O"
1022,unknown,"s= L X i aa,. aj 00 1 (b) show that the condition E[o)=O is equivalent to the condition r (aa;/il9)= 0, which i implied by the condition r a, = 1; (c) show that MULTIVARIATE ANALYSIS 116 and that the information matrix is F= _E(iJs')= Nt. (J.. ilaJ iJa,_~) ae a, ae il9' il9il9' -when 0 is a scalar this simplifies to ( 1 (ao,)' a 2 a.) {= NL ~ dO - ao~ . 4.1.11 Suppose that sex; 8) and F(O) are the"
1023,unknown,"2 a.) {= NL ~ dO - ao~ . 4.1.11 Suppose that sex; 8) and F(O) are the score vector and informa­ tion matrix with respect to a particular parameter vector 9. Let <I> = <1>(9). where <1>(-) is a one-to-one differentiable transformation. Then show that the new score vector and information matrix are. respectively, 4.1.U Let x, ..... x"" be a sample [rom N.(Il, 1:) with 11 known and p = 2. For simplici"
1024,unknown,"For simplicity take V = 1:-' as the parameter matrix. Put v' = (v"". V22. 0.,). Then . using (4.2.4), sbow that ( a'/ ) F(V) = - E aviIV Hence sbow that vi2 vIl - 2vI) Un This result can be generalized to the general case to give iFM I ex lVI-to"" \ '"" III'"" (see. for example. Box and Tiao. 1973). Hence. using ExercL~e 4.1.11 aDd the Jacobian from Table 2.5.1. show that IF(1:)1 ex l1:t"""" "". 4.L13 Sh"
1025,unknown,"IF(1:)1 ex l1:t"""" "". 4.L13 Show that the information matrix derived in Exercise 4.1.12 for p =1 can also be written rr~2 - 20""110""1 2 ) u;.., - 2U '22U12 ' - 20""22 0'12 2 0'2Z UI I + (.r~2 11 7 EsnMA nON 4.2.1 If 11= k"""", and ~ is the m.l.e. of k given by (4.1.9), show that H(k) = k and V(t<) = (n..:,1:-'"""",)-"" where n is the sample size. 4.2.2 Since i is independent of S and bas expectation k"""""" "
1026,unknown,"estimator k given in (4.2.11) is unbiased, and that its variance is n -, E {v.I,S-'l:S-' """",,(¢,S-' """"')'}. ~.2.3 If x;;. 0 then sbow that {(x) =x-l-Iog x has a minimum of 0 when x = 1. (The first derivative of x is positive when x> I. negative when x < 1. and zero when x = 1). Therefore show that x ;;.log x+ 1. If a and g are the arithmetic and geometric means of a set of positive numbe"""" then es"
1027,unknown,"numbe"""" then establish that a;;.l + log g and that equality bolds only wben eacb positive number equals unity. 4.2.4 Using the data in Example 4.1.3. show that if 11' 'was known to be proportional to (2,8). then the constant of proportionality would be k= l.17. 4.2.5 [f x- N.{p. . .,..'I). where ... is known to lie on the unit sphere (i.c. 11'11= I). show that the m.l.e. of j.I. is x/(X'X)""'. 4.2."
1028,unknown,"4.2.6 If Rp.= r and ... is given by (4.2.13). show Ihat E (j).) = j.I. and V(jl)= ,.-'(1:-:tR'(IU:R T'IU:). No(e that V(j).) is less than Vii) by a non-negative definite matrix. 4.2.7 If 1:= kl:"" and 11 is unconstrained. so that k is given by p -I Ir1:O'S, as in (4.2.16). show that E[IC] = (Il-I)k/n. 4.2.8 (Proof of (4.2.14)) [f Rp.= r and I is unknown, then the m.l.e. of j.I. is obtained by solvi"
1029,unknown,"j.I. is obtained by solving simultaneously ai' -=0 and Rp.= r. all Note that l:= S + dd' where d = l:R'(R:i:RT 'a and a= Ri - r. Also, note that Rd = a and therefore tR' = SR' + da' and Rl:R' = RSR ' + 88'. Thus . d = (SR' + <12')(88' + RSR ')-'a = SR '(RSR T'a. w here we have used (A .2.4f) with A = RSR ', B = D ' = a, and C = 1 for the second part, plus some simplification. But d = i-j).. Theref"
1030,unknown,"x-SR'(RSR')-'(RX-r). This is the same as the expression obtained in 14.2.13) when 1: is !mown . with the substitution of S for 1:. MULTlVARIATE ANALYSIS 118 4.2.9 Use (4.2.13) to sbow that when 11 is known a r>riori to be of the form (11;,0')', then the m.l.e. of 11, is i,- X 12l:;ix2 • 4.2.10 (Mardia , 1962, 1964a ) (a) Show that the log likelihood of a random sample (x"" y,)', .... (x., Y.)'. fro"
1031,unknown,"of Exercise 2.2.1 is given by ,,- ', = log p(r> + 1)-(p + 2)0. where ""0 = t log(x,+ y;- I). i- I (b) Show that maximum likelihood estimator of I' is given by p = a-I-4+ (a-2 +j)1I2. (c) B y evaluating ifl/ap2. show that the Fisher information matrix is ,,(21'2+ 21'+ I)/{p(p+l»)2 and hence show thai the asymptotic variance of Ii is ,,-'{p(p + 1))2/(21'2 + 21' + 1). 4.2.11 Show that for any value of"
1032,unknown,"4.2.11 Show that for any value of X > O. the multi normal log likelihood function (4.1.9) is maximized when 11 = i. Then . using Theorem 4.2.1, show that if l1= i. then (4.1.9) is maximized when X = S. 4.2.12 If X is a data matrix from N .{j1, 1:). where X =<>'[(1- p)l+p11'] is proportional to the equicorrelation matrix. show that the m.l.e.s are given by ,1 =i, 2 ( J)'2 L S;j. P P- (J"" i<J (Hint:"
1033,unknown,"( J)'2 L S;j. P P- (J"" i<J (Hint: using (A.3.2b). write X -' = al+ bll' and differentiate the log likelihood with respect to a and b.) 4.3.1 (Press, 1972 , p. 167) Show that if x, ..... x. are i.i.d. N .(I1.X ). then i and M = ""S have the joint density ISr""-·-2ln f(i. S ; 11, X ) ex IX I-n exp{-t"" trr'[S+ (i- .. )(i-"" n. If the vague prior .. ( ... 1:) ocll:I-'·+1W2 is taken. show that the posteri"
1034,unknown,"density can be written as 11'(11,1: I X ) oc I XI -( v~'- un exp ( ~ tr v-'r' I. where v= "" and V =[S +(ii-jl)(i-I1)']-'""-"" By integnlling 1I'(jl,XIX ) with respect to X and using the normalizing constant of the inverse 119 ESlTMATIO'\ Wis hart distribution. show that the posterior den,ity of jl is 7T( .. 1 X ) '"" {l + (i - ill'S '(i - Ill) -no i.e. 11 ha. a multivariate ( distrihution with parame"
1035,unknown,"i.e. 11 ha. a multivariate ( distrihution with parame ters i. S. and v =,,-p. By integrating 7T{j1. X I X ) with respect to 11. show that the posterior density of X is inverted Wish art. W ; '(115.,,- I) (see (3.8.2)). Hence show that the posterior m ean of 11 is i. and if ,,> I' - 2 show thalthe posterior mea n of 1: is ""S/("" - p - 2) (see Exercise 3.4.13). .\..3.2 (press, 1972, p. 168) (a) If th"
1036,unknown,".\..3.2 (press, 1972, p. 168) (a) If the informative prior from (4.3.5) for .. and 1: is taken. then show that the posterior density is 7T(I1.1: I X ) = 11:1-(""+'""-1)12 exp {-~ tr r '[""s + G + (11- '!»{I1- 4>)' - lI(jl- X)(jl- i)'lJ (b) Show that the posterior density of j1 is multIVariate ( with paramc ­ I~rs 4> + I1X a = ---, )+11 II--=-Il + 1]1 - p. (el Show that the posterior density 01 1: i, "
1037,unknown,"W:«II~ I)C .II+ m - p - l) (see equation (4.8.2)) (d) Hence show that the posterior mean of jl i, a. and ,f "" ~ m - 2p - 2 > 0 show that the posterior mean of 1: is (l ~ II lC /(1I + /II + 21' - 2). hce Exercise 3.4.13). 5 Hypothesis Testing 5.1 Introduction The fundamental problems of multivariate hypothesis testing may be attributed to two sources-the sheer number of different hypotheses that ex"
1038,unknown,"exist. and the difficulty in choosing between various plausible test statis­ tics. In this chapter we shall present two general approacbes based on the likelihood ratio rest (LRT) and union intersection test (UIT). respectively. On some occasions, the LRT and UIT both lead LO the same test statistics. buloon other occasions they lead to different statistics. The s~e~r ' number of possible hypothes"
1039,unknown,"The s~e~r ' number of possible hypotheses is well illustrated by the p-dimensional normal distribution. This has !p(p+3) parameters, and therefore quite apart from other hypotheses. there arc 2P (p·3\n hypoth­ eses which on ly specify values for a subset of these parameters. This function of p is large even for p as smaU as 3. Other hypotheses could specify the values of ratios between parameters "
1040,unknown,"specify the values of ratios between parameters or other functions, and further hypotheses could test linear or non-linear restrictions. Of course, in practice the number of hypotheses one is interested in is much smaller than this. The second source of fundamental problems mentioned in the opening paragraph concerns the difficulty of choosing between certain plausible test statistics, and the key"
1041,unknown,"test statistics, and the key question of whether to use a sequence of univariate tests Or whether some new multivariate test would be better. These problems are apparent even in the bivariate case. as shown in the following example. (See also Exercise 5.3.6.) El<l1IDpJe 5.1.1 To illustrate the difference between multivariate hypothesis-testing and the corresponding univariate alternatives, conside"
1042,unknown,"only the variables x. and x,. where x. =head length of first son and x,=head length of second son. 121 HYPOTHESIS TESTING from Frets' head data given in Table 5.1.1. For the complete data of Table 5.1.1, ,. =25, and i=(I85.72. 151.12.183.84, 149.24)' 50.753 66.875 44.267] 52.186 49.259 33.651 96.775 54.278 . 43.222 We assume initially that x. and xJ are independent. and that each is normally distr"
1043,unknown,normally distributed with a variance of 100. (The assumption of indepen­ dence is woefully unrealistic. and is advanced here purely for pedagogic Table 5 .1.1 The measurements on the first and second adult sons in a sample of 25 fam ilies. (Data from Frets. 1921.) First son Second son Head Head Head Head lengtb breadth length breadth t Q1 155 t79 145 1~5 149 201 152 1 ~I 148 185 149 11\3 153 181( 
1044,unknown,176 144 171 142 2(18 157 192 152 189 t50 190 149 197 159 189 152 188 152 197 159 J 92 150 187 J51 179 158 186 148 1H3 147 174 147 174 150 185 152 190 159 195 157 188 151 187 158 163 137 161 130 195 155 183 158 186 153 173 148 181 145 182 146 175 140 165 137 192 154 185 152 174 143 178 147 176 139 176 143 197 167 200 158 190 163 187 150 MULTIVARIATE AI'-.ALYSIS 122 reasons.) Suppose that we wish 10
1045,unknown,"reasons.) Suppose that we wish 10 test the univariate hypothc.<es (simul­ taneously) that both means are 182. i.e. H,:x,~ (182,100). They may be tested using the following z statistics: x,- 182 z 1= 101 25 = 1.1\6, x, - HI2 =2 = 101 25 = 0.92. Since Iz,\< 1.91> for i = I. 2. neither of the univariate hypotheses H, nor H z would be rejected at the 5% level of significance. ow let us consider the bi"
1046,unknown,"ow let us consider the bivariate hypothesis H ,:x,-N(185.IOO) and x,- (185.100). This hypothesis is true if and only if both H, and H z are true. There are various ways of testing H ,. One way would be to accept H , if and only if the respective z tests led to acceptance of both H , and H ,. Another approach would be to note that if both H, and H , are true then An acceptance re~ion I-ta,ed on z, "
1047,unknown,"An acceptance re~ion I-ta,ed on z, would lie between the two lines Z I + Z, = ±C . ote that the obsen'cd value of z, equals 1.966, which is just significant at the 5% level, even though neither z, nor 2 , were significant. Hence a multivariate hypothesis may be rejected even when each of its univariate components is accepted. A third approach is based on the observation that if H, and H 2 are true"
1048,unknown,"true, then This would lead 10 a circular acceptance region. In the above example, z i + zi = 4.306. wh ich is below the 95th percentile of the xi distribution A final possibility, 10 be considered in detail below, would use informa­ tion on the correlation between z, and 2z to derive a test statistic based on a quadratic form in z 1 and 2 ,. This leads to an elliptical acceptance region. Thus . de"
1049,unknown,"region. Thus . depend ing upon which te,t statistic is chosen. our accep­ tance region can be rectangular. linear. circular, or elliptical. These different acceptance regions can easily lead tu conflicting results. W e shall concentrate on developing general strategies and particular test statistics which can be used in many frequently occurring situations. Section 5.2 gives two systematic strateg"
1050,unknown,"Section 5.2 gives two systematic strategies. based On the likelihOOd ratio 123 HYPOTHESIS TESTING test (LRT ) and union intersection test (VIT). respectively. The tesls are introduced briefly in Section 5.2, with several examples of each. We find that for some hypotheses the LRT and UIT lead to identical test statistics. while for other hypotheses they lead to different statistics. Sections 5.3 an"
1051,unknown,"and 5.4 discu s more detailed applications of the LRT and UIT. Section 5.5 considers the construction of simultaneous confidence intervals, while Section 5,6 ma kes some points o[ comparison between different types of hypothesis testing. Throughout the chapter we shall use the tables in A ppendix C and also those in Pearson and Hartley (I972. pp. 98-1 16, 333-358). Sections 5.7 and 5.8 consider no"
1052,unknown,param etric tests. 5.2 The Techniques Introduced This section introduces the likelihood ratio test (LRT) and union intersec­ tion test (Urr) and gives a few applications. 5.2.1 The likelihood ratio test (LRT) We assum e that the reader is already [amiliar with tbe LR procedure from his knowledge of univariate statistics. The general strategy of the LRT is to maximize the likelihood under the null 
1053,unknown,"LRT is to maximize the likelihood under the null hypothesis H, .. and also to maximize the likelihood under the alternative hypothesis H ,. These main results are given in the [ollowing definitions and theorem . Definition 5.2.1 If tl,e distribwion of the randoll! sample X = (s"" ...• ll.)' depends upon a parameter vector O. and if H ,,:O E D o altd H, :O E D. are any tlVO hypotheses, then rhe like"
1054,unknown,"against HI is defined as A(x)=q/Lf. (5.2.1) whe re L~ is rhe largest value which tlte likelihood function takes ;"" region D ,. i =0.1. Equivalently, we may use the statistic - 210g'\ =2(if- I~). (5.2.2) where If = log L f and If. = log L~. In the case of simple hypotheses. where D o and D. each contain only a single point, the optimal properties of the LR statistic are proved in the well-known eym"
1055,unknown,"well-known eyman-Pearson lem m a. For LR properties when H "" and H, are composite hypotheses, see Kendall and Stuart (1973. p. 195).ln Ml JLTNARlATE ANALYSI 124 general one tends to favour HI when the LR statistic is low, and H"" when it is high. A test procedure based on the LR statistic may be defined as follows: Definition 5.2.2 The likelihood ralio lest (LRT ) of size"" for te.sting H "" against "
1056,unknown,"against HI l.as as its rejeclion region R ={x l A(X)<C} where c is determined so that sup P.(XE R )="". hn Q For the hypotheses we are interested in, the distribution of A does not. in fact. depend on the particular value of 0 E flo. so the above supremum is unnecessary. The LRT bas the following very important asymptotic property. Theorem 5.2.1 In the notalion of (S.2.1). if fll is a region in R q"
1057,unknown,"flo is an r-dimensiorlO/ subregion of fl .. tlten WIder suitable regularity conditions, for each 6 E flo, - 210gA has all asymptotic x!-, distribution asn---ioX . Proof See. for example, Silvey (1970. p. 113). • To illll$trate the LRT. we examine three hypotheses assuming that X = (x ...... """")' is a random sample from 0"""".1:). 5 .2.1"" The hypotlresis H o:).l= I'<J,:I known When 1: is known. then "
1058,unknown,"maximized log likelihood under H "" is I~ = ICJ.lo.l:) = -In log 121T ~ -4n tr:,;-I S -~n(i - ).I.o)'r'I(X- ).I.o). Since H I places no constraints on ).l. the m. I.e. of ).l is i and If=/(i.1:)=-{n logI21T~- tn tr:,;-IS. (S.2.3) Therefore, using (5.2.2), we get -2 log A = 2(1f -I~) = /I (i-I'<,)'1;-J(i-).I.o). (S.2.4) Now from Theorem 2.5.2 we know that this function has an exact X; distribution u"
1059,unknown,distribution under H o. Hen ce we have a statistic whose distribution is known . It can therefore be used to test the null hypothesis. (Note that in this case the asymptotic distribution given by Theorem 5.2.1. is also the small sample distribution.) Examp le 5.2.1 Consider the first and third variables of Frets' head 125 HYPOTIfESIS TESTING measurement data from Table 5.1.1 and tbe hypothesis con
1060,unknown,"Section S.1.I, name ly that (XI' X 3 )' - N 2 U-<o, 1:), where [182] and 1: = [100 0 ] ).1.0= 182 0 100 . Using (S.2.4) we deduce that - 2 log'\ = 2S(3.72, 1.84) [~.Ol ~.Ol ]e: ~!) = 4.31. Since this is below S.99, the 9Sth percentile of xi. we accept the hypothesis that ).l = ).1.0. In this case, we can find a confidence region for IJ-. and 1J-3 using Tbeorem 2.5.2. A 95% confidence region for th"
1061,unknown,"be given using the inequality I.e. 2S(185.72 -IJ-•. 183.84-/.'3) diag (0.01,0.0 1)(18S.72 - IJ-I) < 5.99. 183 .84 - IJ-, (185.72 - /.'1)'+ (183.84 -/.'3)2 < 23.96. Because 1: is assumed to be proportional to l. this gives a confidence region which is circular in the parameter space. A mOre useful way to express this confidence region in terms of simultaneous confidence intervals is given in Exerci"
1062,unknown,"simultaneous confidence intervals is given in Exercise 5.5.2. See also Section 5.5. 5.2.1b Tile IIypotllesis H o:).l = ).lo, 1: unknown (Holelling one-sample y2 test) In this case l: must be estimated under H o and also under H I' Therefore both hypotheses are composite. Using results from Seclion 4.2.2.1. we know that the m.l.e.s are as follows: under H o, ii.="""" and i = S+d d' where d=x-I'<J; wh"
1063,unknown,"whereas under H .. ii.= i and i = s. Now from (4.1.9) we deduce that It = 1().I.o, S+dd') = -in{p log 2."". + log lSI + log (1 + d'S-ld)+p}, (5.2.S) tI,fULTrvARlATE ANALYSIS 126 whereas If = I(i, S) is obtained by putting d = 0 in this expression. Thus . -Z log .!. =2(lf-I~)=lIlog(l+ d'S"" d). (5.Z.6) This statistic depends upon (11-1 )d'S-'d , which is known from CoroUary 3.5.1.1 to be a T '(p. /I-"
1064,unknown,"3.5.1.1 to be a T '(p. /I-I) statistic. often known as the H otelling one­ samp le 1"" statistic. Further. (n-p)/pjd'S-'d has an F."". .• distribution. Exam ple 5.2.2 Consider again the first and third variables of Frets' head data and let us test the hypothesis 1fo: (x"" x,)' - N,(p..,.1:) w here I'<l = (J 82.182)' and :1: is now assumed unknown . Using the numbers given in Examp le 5.1.1, w e test "
1065,unknown,"Examp le 5.1.1, w e test n -p , _, _ 23( ) [9 1.481 66.875]- '(3.72)_ Z -p-d S d -3' 3.72, 1.84 66.875 96.775 1.84 - I. 8 against the F2 .23 distribution. S ince F ' .23 :0 .05 = 3.44, we accept the nnll hypothesis. 5.2.1c The hyporltesis H "": 1: = 1:0"""" unktlowlI The m .l.e.s for this exam ple under H "" are p,= x: and 1:='I., and. under H "" p,=i and 1:= S. Th erefore 1:\'=I(i, 1:0)= -tn logI21Tl:"
1066,unknown,"and If = lei. S) = -~ n log 1 2'IT SI-~ lip. Therefore - 2 log A = 2(lf - m= n tr 1:;;' S - n log 11:;;' sl-lIp. (5.2.7) Note that this statistic is a function of the eigenvalues ot :1:0' S, and also that. as S approaches :1:0. - 2 log A app roaches zero. In (act, if we write a for the arithmetic m ean of the eigenvalues of 1:;; l Sand g for the geometric me an, so that tr1:O' S = pa and 11:O'sl=g"
1067,unknown,"comes - 2 Jog A = IIp(a -log g - 1). (5.2.8) One problem with the st.atistic given by (5.2.7) is that its distribution is far from simple. Anderson (1958, p. 265 ) finds a formula for the m oments of A, and also derives the characteristic function of -2 log A. under both H o and 1f,. K orin (1968) has expressed (5.2.8) as an infinite sum of chi-squared variables, for certain small values of 11 and"
1068,unknown,"Pearson and Hartley, 1972, pp. 111. 358). H owever, these results are not easy to use. The general result cited in Theorem 5.2.1 indicates, howeve r, 127 HYPOTHES IS TESTING that -2 log A given by (5.2.8) has an asympt otic X;;' distribution unde r H o, where '"" equals the numbe r of independen t parameters in 1:. i.e. m=~p(p+ I). Ex ample 5.2.3 Using the first and third variables of Frets' data f"
1069,unknown,"Tab le 5.1.1. we may test the hypothesis I =diag(IOO, 100), w hich wa s assumed in Example 5.1.1. H ere 1:"" S = [0.01 0 J [91.481 66.875] = [0.9148 0.6688] "" 0 0.01 66.875 96 .775 0.6688 0.9678' The eigenvalues of this m atrLx are given hy A I = J .61 1 and A,=O.272. The refore. a =0.94 13 and g =O.66 19. He nce from (5.2.8) we find that - 2 10gA = 17.70. This must be com pared asymp totically wit"
1070,unknown,- 2 10gA = 17.70. This must be com pared asymp totically with a X~ distribution . and we see q uite clearly that the hypothesis is rejected; that is. our original assump­ tion regarding the covariance matrix was false. This might perhaps have been expected from a cursory look at the sample covariance ma trix. which shows the presence of a strong correlation. Using the same data. aJ1d having reject
1071,unknown,"diag (J 00. 100). we might now exam ine the hypothesis that I = .... =[100 50J - 50 100' Ce rtainly judging from the sample covariance m atrix S, this hypothesis ~eems distinctly more plausible. It is found that :1:-' S = [0.7739 0.2465 '] "" 0.2818 0.8445 ' T he arithmetic and geomet ric means of the eigenvalues are a = 0.8092 and g = 0.7642. ote that tbese are far closer together than the values "
1072,unknown,"obtained previously, reflecting the fact that I-' S is closer to the form leI. Inserting the values of a and g in (5.2.8) we find that -2 log A = 3.9065, w hich is well below the 95% percentile of the X~ distribution. H ence we deduce that this revised value of 1: is quite plausible under the given assumpti ons, 5.2.2 The union intersection lest (UlT) Co nsider a random vector x. which has the N ."
1073,unknown,"Co nsider a random vector x. which has the N . (j,L.I) distribution and a non-random p-vector a. Then if y. = a'x. we know that y. - N,(a'"",. a'a). Moreover. this is true for all p-vector. a. MULTIVARIATE A ALYSlS 128 ow suppose that we wish to test the hypothesis H,,: ... = O. Then under H o we know that y. - N(O , s'a), a hypothesis which we may call Ho.. Moreover. Ho. is true for aU p-vecLOrs I"
1074,unknown,Moreover. Ho. is true for aU p-vecLOrs II. In othel words. the multivariate hypothesis H o can be written as the intersection of the set of univariate hypothesis Ho.; that is (5.2.9) The intersection sign is used here because all the H o. must be true in order for H o to be true. We call Ho. a component of HI/' oW let us consider how we would test the univariate hypothesis H Oo • One obvious way i
1075,unknown,"One obvious way is to use z, = Y./M. which has the standard oormal distribution. A rejection region for H"", based On z. would be of the form R. = {z.: lz.l > c}. where c is some arbitrary critical value. say 1.96. This rejection region for H , .. could also be wrillen (5.2.10) Hence we have a rejection region for each of the univariate hypotheses which together imply H . in (5.2.9). We turn now to"
1076,unknown,"We turn now to consider a sensible rejection region for the composite hypothesis H "". Since H "" is true if and only if every component hypothesis HOo is true. it seems sensible to accept H "" if and only if every component hypothesis Hu. is accepted; that is. we reject H o if allY component hypothesis is rejected. This le~d' to a rejection region for H "" which is the union of the rejection regions "
1077,unknown,"union of the rejection regions lor the component hypotheses; that is R =U R •. (5.2.1I) The union of the rejection regions given by (5.2.11) and the illrersecrion of component hypotheses formulated in (5.2.9) provide the basis of the union intersection strategy. which was due initially to Roy (1957). Definition 5.2.2 A IIIIi01l inrersectioll resl (UIT) fnr tile hypolhesis H "" is a lesl wllOse reje"
1078,unknown,"lesl wllOse rejeclioll region R call he "",rille II as in (5.2.1 J). where R. IS Ihe rejecrio"" region correspotldi1lg 10 a compo1lenl hyporllesis f:lo.. lI""d where H . can be wnne"" as ill (5.2.9). Applied to the above example. the union intersection strategy based on (5.2.10) leads to a rejection of H o if and only if any z; exceeds c2 ; that is, H o is accepted if and only if z;.;;c' for all z •. "
1079,unknown,"that H o is accepted if and only if 129 H YPOTIiESIS TESTING [n general the union intersection test often leads to the maximization or minimization of some composite test statistic such as z;. In this example , z; = y;/aJa = a'nr/a/a/a. This is maximized when s = x. so that max z; = ix. Hence in this example the UIT statistic would be xx:. whose distribution under H o is known to be X;. The method"
1080,unknown,"be X;. The method of constructing UITs has important practical consequ­ ences. [f the null hypothesis is rejected. then one can ask which of the component rejection regions R. were responsible. thus getting a clearer idea about the nature of the deviation from the DUU hypothesis. In the above example. if one rejects H o, then one can ask which linear combina­ tions a'x were responsible. In particu"
1081,unknown,"tions a'x were responsible. In particular one can look at the variables x, = efx on their own. For example it might be the case that some of the variables x, lie in R •. whereas the others are all acceptable. See Section 5.5 for further details. Unfortunately. the LRT does not have this properly. If ODe rejects the null hypothesis using a LRT. then one cannot ask for more details about the reasons"
1082,unknown,"the reasons for rejection. We shall now take tbe hypotheses used to illustrate the LRT procedure in Sections 5.2.1a-S.2.lc above and stndy them using UITs. As in those examples, we take X to be a matrix whose "" rows are i.i.d. N.( ... ,1:) vectors. Note tbat this implies that y = Xa is a vector whose 1\ elements are Li.d. N(,,' .... s'u ) variables. 5.2.2a The hypothesis H o: fL = 1'<>. 1: knowlI "
1083,unknown,"5.2.2a The hypothesis H o: fL = 1'<>. 1: knowlI (union inrersecrioll approach ) Under H o the elements of yare LLd. N(tJ."" <7;). where y = Xa. IL, = a' ... o and "" : = a'1:a. AD obvious test statistic based on y = a'x is z! = na'(i-!'<,)(i -!,<,),a/a'u. Using (5.2.11) we wish to reject H "" for large values of max z!= n (i- !,<,)'.I-I(x-!,<,). (5.2.12) . This chi-squared statistic for the UIT bas a"
1084,unknown,statistic in Section 5.2.1a. Thus. for this hypothesis. the UIT and LRT both lead to the same test statistic. However. this property is not true in general. as we shall see below in Section 5.2.2c. ote that the critical values for the UI test statistic should be calculated on the hasis of the distribution of the UI statistic. in tbis case a x; distribution. and not from the value 1.96 or MULTTVARI
1085,unknown,"MULTTVARIATE 'ALYS1S 130 (1.96)2 w hich relates to one of the component hypotheses of H "" rather than H o itself. 5.2.2b The hypothesis H o:fJ.=Jl<>.1: unknown (H otelling one -sample T ' test. ullioll intersection approach ) ODce m o re w e note that under H o the elem ents o( y are i.i.d. N (/J-.r, u~). H ow ever, in this example u; is unknown and therefore mu st be esti­ mat ed. An obv ious est"
1086,unknown,"(5.2.13) This leads to the test statistic (or H ,,,, namely I. J s;/(n - I) We note that , a'(x - ""',)(>: - Jl<>),a t;=( n - I) 's . a a ooce m ore a ratio of q uadratic forms. This time the UIT slalistic IS max 1;= (II- l)(ii- Ili,)'S-I(i:- Jl<» . (5.2. I 4) . ote thaI (5.2.14) is Hotelling's urle-sample T 2 statistic. which has already been met in Section S.2.lb. H ence once morc the U IT and LR"
1087,unknown,"procedures have led to the same test st a li~ti c. 5.2.2c Th e I,ypothesis H o: 1:= 1:"". fJ. unknown (llnioll intersection lesl) Once more the elements of y are i.i.d. (1-',. u ;). H owever, Ihis time we wish to examin e u; and see wh ether it equals u~= a'1:,,,,. An obvious estimator of u; is s; defined in (5.2.12). Th is leads 10 Ihe test stalistic u. -= '1s;lu~ ... = na' Sa/a ' ~-a and w e reje"
1088,unknown,"and w e reject flo. if U.~CI. or U .. ~ ~. w here C t. and c2• are chosen to make the size of the test equal to a. Since na'Sa ( ~- ' S) max ~= 11.1., ..., . • a '-""18 . tla'Sa _ I mtn~= IIA p Ct, S). (5.2.15) • a~la 131 HYPOTHES IS TESTING where At denotes the ith largest eigenvalue. we see that the crjtical region for the U IT takes the form Ap (1:O'S)<c, or A,(IO' S» c"" where c, and c, are chos"
1089,unknown,"Ap (1:O'S)<c, or A,(IO' S» c"" where c, and c, are chosen to make Ihe size of the test equal to a. However , the joiot distributio n of the roots of 1:- ' S is quite complicaled and its critical values have not yet been tabulated. Note that this UlT is not the same as the corresponding LRT which wa s obtained in (5.2.7). although both statistics depend only on the eigenvalues of 1:;;' S. 5.3 The Te"
1090,unknown,"5.3 The Techniques Further Illustrated In Ihis section we take various further hypotheses. and derive the corres­ pond ing L R T s and UrTs . 5.3.1 One-sample hypotheses on '"" W e have already seen above thaI the hypothesis"", = fJ.o leads to the same lest statistic using both the LRT and UIT principle, whether or not 1: is known . T he following general result allows us to deduce the LRT for a wid"
1091,unknown,"wide variety of hypolheses on fJ.. As before, we assume that x ,, ... , J."" is a random sample from p(fJ..l:). T heorem 5.3.1 If flo and H , are hypotheses "",hich lead 10 m.l.e.s .;. alld i. respectively. and if t1l£re are '10 constrainls on 1:, then the 1II.I.e.s of 1: are S ... dd' and S, respectively. whe re d = x-.;.. TI,e LR test for testing fro against H , is given by - 2 log .1.= nd'1:- ld "
1092,unknown,"alld -210gA = lIlog(l+ d'S-'d) if 1: is wrk'IOWrt. (5.3.2) Proof The proof i identical 10 that followed in deriving (5.2.4) and (5.2.6). • Un[o rtunately. no result of corresponding generality exists concerning the UI test. Therefore, UlTs will have to be derived separately for each of the following examp les . MULT[v.AR1ATE ANALYSIS 5.3.1a The hyporhesis H o: RI'-= •. 1: /mowr. (llypor#tesis of l"
1093,unknown,5.3.1a The hyporhesis H o: RI'-= •. 1: /mowr. (llypor#tesis of linear constraints) 132 Likelihood rariQ resr Here R and r are pre-specified. The m.1.e. of Ii under H o: R I'-= r is given by (4.2.13). The corresponding LR test is given by (5.3.l). where The LR test is therefore given by -2 log A = tI(Ri-r)'(Rl:R')-'(Ri- r). (S.3.3) Under H o !be rows of XR ' are i.i.d. Nq(r. Rl:R') random vectors. 
1094,unknown,"q < P is the number of elements in r. Therefore, by The orem 2.S.2. (5.3.3) has an exact X~ distribution under Ho. An impoltant special case occurs when I'-is partitioned, 11 =(1'-:, ~t, and we wish to test wh ether 11, = O. This hypothesis can be expressed in the form Rli = r on setting R = (I. 0) and r = O. In an obvious notation (5.3.3) becomes -2 log A = nii,l:l,'xj. Another special case occur"
1095,unknown,"Another special case occurs when we wish to test the hypothesis H o:l1= kli<. for some k, (S.3.4) where Ilu is a given vector. This hypothesis m ay be expressed in the form R,.. = O. if we take R to be a «p-1) x p) matrix of ranJ< p -1. whose rows are all orthogonal to Ilu. U nder flo, the mJ.e. k of k is given by (4.2.19) and k m ay be used to express the LR statistic without explicit use of the "
1096,unknown,"m atrix R. See Exercise 5.3.3. An alternative method of deriving the LRT in (5.3.3) can be given by using the methods of Section 5.2.1a to test the hypothesis R,.. = r for the transformed data matrix Y = XR '. This approach also leads to (5.3.3). Union inrersection rest A sensible union intersection test for this hypothesis can be obtained by applying the methods of Section 5.2.2a to the transform"
1097,unknown,"the transformed variables XR '. Since the meth ods of S.2.13 and S.2.2a lead to the sam e test. the UTT is the same as the LRT for this hypothesis. 5.3.1 b TIle #typothesis H o: RI'-= r,1: wlknowr l (hyporhesis of linear constraints) Likelihood rario rest Using (4.2.14) and (S.3.2) we know that -2 log A =11 log (l+ d'S-'d). (5.3.5) 133 HYPOTHESIS TESTING where d = SR '(RSR Y '(Ri-r). Note that thi"
1098,unknown,"where d = SR '(RSR Y '(Ri-r). Note that this test is based on the statistic (n -l)d'S-ld = (11 -1 )(Rii-r),(RS R ',-'(Ri-r), (5.3.6) which has a T 2(q, 11 -1) distribution under flo since Ri - N.(r. 1I-'R 1:R') independently of nRS R '-Wq(R1:R'. 11 - 1). For the hypothesis that 1'-, = 0, this T' statistic becomes (n-l)x,S'j'il. (5.3.7) Example 5.3.1 W e may use the cork tree data from Table 1.4.1 "
1099,unknown,"examine the hypothesis H o that the depth of bark deposit on n = 28 trees is the same in an directions. Let x denote the four directions (N, E , S, W J'. One way to represent the null hypothesis is RI'-= O, where R=[l -1 o I -1] - 1 o . o -I Here q=3. To test H o we use the statistic (n-I)j'S;'j=20.74-7""(3, 27). where j and Sy are tbe sample mean and covariance matrix for the transformed variables"
1100,unknown,"transformed variables Y = XR' , and were calculated in Example 1.5.l. T he corresponding F statistic is given by n-q_,..-._ 640 F --YO> y y = . - 3 2.'5. q . Since F us ,0 .0 , = 4.68, we conclude thai Ho must be rejected. Note that the null hypothesis for this example may also be expressed in the form H Q: ""'= kl, as in (S.3.4). U nion i11lersectioll lest As in the previous section, the UIT gives"
1101,unknown,"same test here as the LRT. 5.3.2 One -sam ple bypotheses on I Th eorem 5.3.2 LeI X , • ••• , x. be a random sample from N.(I'-, 1;). If H o and H, are Ityporlteses which lead 10 i and S as the m.l.e.s. for I , and if i is rite m .l.e. of Ii WIder borh hyporheses, Ihell rile LRT for resring H o against H, is given by - 210g A = .. p(a - log g -1), (5.3.8) MUL TJV ARIA TE ANAL YSlS 134 where a and g"
1102,unknown,"where a and g are rhe arithmetic and geometric means of rhe eigenvalues of i-Is. ""Proof Tbe proof is identical to that followed in deriving (5.2.7) and (5.2.8). • A similar result balds if II-is k.nown. Unfortunately, however , no corres­ ponding results are known for the union intersection test. 5.3.2a The hypotltesis H o::£= k!."", '"" unknown Likelihood ratio test The m.l.e. k was fouod in (4.2.1"
1103,unknown,"therefore given by (5.3.8), where a and g relate to tbe eigenvalues of Io' Sik. But in this particular case (5.3.8) conveniently simplifies. We may write 0 = aolk and g = golf<. where 0 0 and go are the arithmetic and geome tric means of tbe eigenvalues of :to'S. But from (4.2.16), k= p- I tr:to' S = an. Therefore, in fact, a = 1 and g = g""lao' Hence, froro (5.3.8), we get -2 log A = lip log (ao/g"
1104,unknown,"In other word , the likelihood ratio criterion for this hypothesis depends simply upoo the ratio between the arithmetic and geometric means of eigenvalues of :to I S. This result is intuitively appealing, since. '3.S S tends to k I.h the values of ao and go both tend to k, and the expression given by (5.3.9) tends to zero, juS! as one would expect when Ifo is satisfied. From Theorem 5.2.1, we know"
1105,unknown,"From Theorem 5.2.1, we know tbat (5.3.9) has an asymptotic X2 distribu­ tion with ~p(p + 1)-1 =~(p - l)(p + 2) degrees of freedom. Korin (1968) bas used the teChnique of Box (1949) to express -2 log A as a sum of chi-squared variates, and thereby found appropriate X' and F approxima­ tions. Union intersectiotl test T here seems to be no straightforward UIT for this hypothesis, but see Olkin and To"
1106,unknown,"Example 5.3,2 In the so-called ""test for sphericity"" we wish to test whether :I = kl. The LRT is given by (5.3.9), w here au and g. now relate to the eigenvalues of 5 in the following way: I ao=-L $ii, P For the first and third variables of Frets' data, which has already been used in Example 5.2.3. ,,=25. p=2, uo= 94.13, ISI=4380 .81. and go=66.19. Therefore, from (5.3.9), -210g A = 17.6, This ha "
1107,unknown,"tic X 2 distribution w ith i(p-l)(p+2) =2 degrees of freedom . Since the 135 HYPOTHESIS TESTING upper 5% critical value is exceeded, we conclude tbat:I does not have the stated (orm. 5.3.2b Tire Itypothesis H O ::I,2= O,,,, unknown Likelihood ratio test Partition tbe variables into two sets with dim ensions p, and P2 , respectively, P, + P2 = p. U:£12 = 0, the likelihood splits into two factors. E"
1108,unknown,"factors. Each (actor can be maximized separately over :£,,, """" and I n, ... ,. respectively. giving tl = (tl;, jJ.~ )' = i and i = (511 0). o S'2 Since the m.Le. of j.l under H o and H t is i, we know from (5.3.8) that the LRT for this hypothesis depends upon the eigenvalues of i -'S = [SII 0 J-' rSlt S,'] = [I S-;-lS\2J o S"" Ls"" S,2 S,iS21 I . Clearly Ir i -IS = p. and therefore the arithmetic me"
1109,unknown,"values. a, is one. Also, using (A.2.3j), Hence -2 log A = -n log (is,2-s,IS,iS""IIIS""il = -n log II - S2iS""S,I'S .. 1 • = - n log n (l-A,J. i= 1 (5.3.10) (5.3.11) where the A, are the non -zero eigenvalues of S,iS,IS,IIS'2' and k = min (P"" P2)' This result is intuitively appealing since if S "" is close to O. as it should be if If. is true, then -2 log A also takes a small value. ate that -2 log A c"
1110,unknown,"-2 log A can also be written in terms of the correlation matrix as II-R'iR2IR,;R'21. To find the distribution of the LR statistic, write M "" = ""S"", M n = ""Su, and M 22.1 = n(S,,-S,IS,IIS'2)' Then , from Theorem 3.4.6(b), we see that under H o, M 22.1 and M 22 - M 22.1 are independently distributed Wishart matrices, M "" .l - W"",(!.,2, ""-1-p ,) and M n - M 22.I- W .,(!""2, P,), Hence, provided "" -] """
1111,unknown,"A 2/"" = IM ,2.IIIIM n .1 +(M '2 - M 22.I)I-A (p,. n - l - p"" P,), (5.3.12) T he null hypothesis is rejected for small A 2J. and the test can be MULTIVARIATE ANALYSlS 136 conducted using Wilks' A. For the cases p, = 1, 2 (or P2 = 1,2) the exact distribution of A from (3.7.7)-(3.7.10) can be used. For general values o[ p"" P2, we may use BarUett's approximation (3.7.11): -(11 -1(p,..-p, ~ 3» log II-S"
1112,unknown,"-(11 -1(p,..-p, ~ 3» log II-S2;S""S""'S,,,-X;,p"" (5.3.13) asymptotically, for large n. As we shall see in Chapter 10 the LRT can be used in canonical correlation analysis. The exact distribution of (5.3.11) was investigated by Rotelling (1936), Girshick (1939), and Anderson (1958, p. 237). arain (1950) has shown that the LRT bas the desirable property of being unbiased. When one set of variables bas"
1113,unknown,"unbiased. When one set of variables bas just a single member, so that p, = 1 and P2 = p-I, these formulae simplify: in that case (5.3.11) is just -/I log II -R ;:;uu'[, where .. = R 20 is a vector. Now, using (A.2.3k). this equals - lIlog(l-.. ' R;:; u)=~ lIlog (I- R""R2;R2') = -n log (1 ~ R 2), where R is the multiple correlation coefficient between the fir.;t variable and the others (see Section "
1114,unknown,"The union i,llersecrion resr [f '"" and "", are the two subvectors 01 s, then independence between x, and x, implies that the scalars a'x, and b'x, are also independent, whatever the vectors a and b. Now an obvious way of testing whether or not the two scalars are independent is to look at their sample correlation coefficient, r. Clearly r = [C(a'"""" b'"",»)' (a'S""b)' V(a'"",) V(b'X2) a'Sllab'S2,b ' (5"
1115,unknown,"The UIT based on this decomposition uses tbe statistic max"" .. r'(a, b), which equals the largest eigenvalue of S,,'S'2S,;S'1> and also equals the largest eigenvalue 01 8;:;s""S""'S,,. (This result is proved in Theorem 10.2.1.) Since 8;::l.s""1>.,'S,, can be written as [M"" ., + (M,,-M 22 ,)]-'(M,,-M 22 .,), where M""=IIS,, and M"" - M, ,.,= nS""S,IS"" have the above independent Wishart distributions, we "
1116,unknown,"that the largest eigenvalue has tbe greatest root distribution, O(p"" n -1-p"" P,), described in Section 3.7. Writing the non·zero eigenvalues 01 8;:; s""S.,'S"" as A"" A"" ... ,A .. note that the UIT is based on A"" while the LRT derived in (5.3.!11 is based on n(1-A,). Example 5.3.3 The test statistics derived above may be used to examine 137 HYPOTHESIS TEST ! G whether there is a correlation between h"
1117,unknown,"137 HYPOTHESIS TEST ! G whether there is a correlation between head length and breadth measure­ ments of first sons and those of second sons. The matrices S"", 8."" and S"" are the relevant submatrices of the matrix S given in Example 5. I. I. and II = 25,17, =2, P2 = 2. The LRT and UIT both require evaluation of the eigenvalues of _, _, [0.3014 0.2006] S22S""S""S ,,= 0.47()6 0.3232 . Here A, = 0.62 li"
1118,unknown,"Here A, = 0.62 lil and '\', =0.0029. For the likelihood ratio te t (5.3.12) we require A2J .. = (1 - A ,)(1-A2)= 0.377. USing the asymptotic distribution (5.3.13). -(25 -D log (0.377) = 21.0 is to be tested against the 5% critical value ofax~, p. = x~ distribution. The observed value of the statistic is clearly significant, and hence the null hypothesis is rejected. To usc an exact distribution, n"
1119,unknown,"A""· - II (p,. n -1-1'"" p,)= II (2,22,2), and from (3.7.10), 21(t -..//1)/2,.111 = 6.60- F ... 2 . This value is significant at the 5% level and hence we still reject H o. For the union intersection test we require the largest eigenvalue of S2d S""s~: S12 ' which is ,1,,=0.6218. From Table C.4 in Appendix C with v,=n-p,-l= 22 and v,=p,=2, and «=0.05, the critical value is 0.330. Hence the null bypot"
1120,unknown,"5.3.2r Tile hyporhesis Ho: P = l. .... ""nknown Th e hypothesis of the last section may be generalized to consider the hypo lhesis that all the variables are uncorrelated with one anolher: Ihat is P = lor, equivalently, .I is diagonal. Uoder R o. the mean and variance of each variable are estimated separately, so that tl = i and i = diag(slI""' .. s"",,). Hence, using (5.3.8), it is easily seen that "
1121,unknown,"the correlation matrix R by the statistic - 2 log A =-11 log IR I. (5.3.15) which, by Theorem 5.2.1. has an asympto tic X2 distribution under HOI w ith !p(p + 1)-P = 1P(P - II degrees of freedom. Box (1949) showed that the X ' approximation is improved if n is replaced hy n'= 1I - 1-i(2p + 5)="" -H2p + 11). (5.3,161 MVLTIVARIA ""lC A ALY S IS 138 Th us. we shall use the test statistic II' log IRI- X"
1122,unknown,"II' log IRI- X ;(u-l\/2 (5.3.17) asymptotically. 5.3.3 Multi-sample hypotheses We now consider the situation of k independent normal samples. whos e likelihood was given by (4.2.17). W e shall use some results from Section 4.2.2.4 in deriving LRTs . 5.3.3a Tile hypolhesis H .: 11, = ... = Ilk givell Illar I , =. , . = I. (olle- wa y mulriuariale allalysis of variallce) Likelihood rario leSI (Wilks"
1123,unknown,"Likelihood rario leSI (Wilks' A test) The LRT of H . is easily derived from the result of Section 4.2.2.4. The m .l.e.s under H . are j and S. since the observations can be viewed under H . as constituting a single random samp le. The m .l.e. of 11; under lhe allemative hypothesis is in the ilh sample mean , and the m .l.e. of the common variance matrix is "" -'W , where W = L II,S, is lhe ""within-"
1124,unknown,"where W = L II,S, is lhe ""within-groups"" sum of squares and products (SSP) matrix and II = L 11,. U sing (4.2.17), the LRT is given by A. = {IW I/IIISIl"""" = fr'W I""I2. (5.3.19) H ere T = liS is the ""total"" SSP matrix. derived by regarding all the data malrices as if they constituted a single sam ple. In contrast, the matrix W is the ""within-groups"" SSP m atrix and 8=T - W = L 1I,(i, - x)(i, -x)' ("
1125,unknown,"may be regarded as the ""between-groups"" SSP matrix. H eoce, from (5.3.19). (5.3.21) The m atrix W -'B i an obv ious gene ralization of the univariate variance ratio. It will tend to zero if H "" is true. W e now find the distribution of (5.3.21). Write the k samples as 3 single data matrix. 139 HYPOTHES IS TESTING where X ,(II, x p) represents the observations from lbe ith sampl e. i = I. ... , k. "
1126,unknown,"I. ... , k. Let 1, denote the II-vector with I in the places corresponding to the ith sample and () elsew here. and set I, = diag (1.). Tben 1 = L I, and 1 = L I,. Let H , = I, -11 ,'1,1, represent the centring matrix for the ith sample, so lhat n,S, = X'H ,X . and set C ,=LH ,. C -, -'II' -'II' 2 - L,.tlj 11-"" • [t is easily verified lbat W = X 'C ,X and B = X 'C 2 X . Further, C , and C 2 are id"
1127,unknown,"are idem potent matrices of ranks n - k and k - I, respectively. and C ,C ,=O. Now under H o. X is a data ma trix from .(11. I ). Thus by Coc bran's theorem (Th eorem 3.4.4) and Craig'S theorem (Th eorem 3.4.5), W = X 'C ,X - W .(I ,II-k), B = X 'C ,X - W .(I , k - I), and. furthermo re, Wand B are independent. Therefore, provided It;;;' r+k. under H"". w here Wilks' A statistic is described in Sec"
1128,unknown,"U llioll ill/usecrion leSI Th e univariate analogue of H o would bc tested using the analysis of variance statistics L 11, (x, -i,)'/L II,S ~. w here i is the overall mean. The corresponding form ula for a linear combi nation Xa is L "". (a'(i., - X))'IL lI,a'S,8. Th is expression's maximum value is the large t eigenvalue of (L "",Sir L ,di; - i)(i, - i)' = W -'B , (5.3.22) where W and B were define"
1129,unknown,"where W and B were defined above . Thu the UT statistic [or this hypo thesis is the greatest root of W -'B , which is not the same as the LR statistic. H owever. note from (5.3.21) lhat the LRT is in fact based on [1 (I+A ,)-', where A, .. _., Au are the eigenvalues of W - 'B . Hence again we see the familiar pattern that the LRT and UIT are bolb functions of the eigenvalues 01 the same matrix, bu"
1130,unknown,"functions of the eigenvalues 01 the same matrix, but they lead to differ­ ent test statistics. Two-sample H otellillg T ' lesl (k = 2) Wh en k = 2, the LRT can be simplified in terms of the two-samp le Hot elling y> statistic given in Section 3.6 . In this case. B = 1I,(i, - i)(x, - x), + "",(i, -i)(x, -il'. MULTIVARIATE ANALYSts t40 But X,-X=(n.ln)d. where d= i,-i2. Also i2 -i=-tl,dftl. Therefore."
1131,unknown,"B = (,t, tl2l .. ) dd'. (5.3.23\ and II + W- 'B I =11+ (tI, tl2fn)W-' dd'i = J +(n,n.lll \d' W -' d. (5.3.24) The second term is, of course, proportional to the Hotelling two-sample T'-statistic, and we reject H n for large values of this statistic. Furthermore. W -'B is of rank one. and its largest eigenvalue is (5.3.25) (Corollary A.6 .2.J). Therefore the UIT is also given in terms of the two-sa"
1132,unknown,"two-sample Hotelling T2 statistic. Hence, although in general the LRT and UIT are different for this hypothesis, they are the same in the two-sample case. For numerical examples of these tests see Examples 12.3.1 and 12.6. L 5.3.3b The hyporhesis H. :1:, = ... =l:. (rest of hornogetleiry of covariances ) Likelihood rario resr (Box's M test) The m.l.e. of .. , is i, under both H. and the alternativ"
1133,unknown,"the alternative. Therefore -210gA . = n log Isl- I"", 10g1S.I= I ,,,log[s,'SI· (5.3.26) Us ing ThtOorem 5.2.1. this has an asymptotic chi-squared distribution with ~p(p + l)(k -1) degrees of freedom. It may be argued that if "", is small then (5.3.26) gives too mueh weight to the contribution of S. This consideration led Box (1949) to propose the test statistic M in place of that given by (5.3.26). "
1134,unknown,"that given by (5.3.26). Box's M is given by M = -y I (n, -1) log 15;;,'5""1. (5.3.27) where and S"" and S""' are the unbiased e.timators n S""=--kS, n- Box 's M also has an asymptotic X~"" ~ ' Xk-l)/2 distribution. Box 's approxi­ mation seems to be good if each n, exceeds 20, and if k and p do not exceed 5. Box also gave an asymptotic F distribution (see Pearson and 141 HYPOTHESIS TESTING HartltOy, 19"
1135,unknown,"HartltOy, 1972, p. 108). For p = 1. (5.3.26) reduces to the test statistic for Bartlett's test of homogeneity, viz. I 2_ '="" I .2 (5328) 11 og s t.. 11, og S, , • • whe re sf is the variance of sample i and S2 is the pooled estimate of the variance. No simple UIT seems to be available for this hypothesis. Ex ample 5.3.4 Jolicoeur and Mosimann (1960) measured the shell length, width. and height of "
1136,unknown,"length, width. and height of 48 painted turtles, 24 male and 24 female. The resulting mean vectors and covariance matrices for males and females. respectively. were as follows: [ 113.311J "" = 88.29. 40.71 [ 136.00] .2 = 102.58 , 51.96 Here "", = ""2 = 24, n =48. III,S, + 1lzS,)f II is S, = [ 132.99 75.85 35.82] 47.96 20.75 , 10.79 _ [432.58 259.87 161.67J S2- 164.57 98.99 . 63.87 The overall covaria"
1137,unknown,"63.87 The overall covariance matrix [ 282.79 167.86 9R.75] 106.24 59.87 . 37 .34 S = We have IS,I= 698.87, lS,I = 11 124.82, ISI=4899.74. while the corres­ ponding determinants of the unbiased estimators are IS"".I=794.05. 1~21 = 12639.89 , Is.1 = 5567 .03. Further. k=2 , p=3, and therefore -y=0.9293. Hence. from (5.3.27), M = 24.099. which is to be tested against the 5% critical value 12.6 provide"
1138,unknown,"provided by the x~ distribution. Since this is highly significant we con­ clude that the male and female population covariance matrices are not equal. 5.3.3c The hypothesis H , : 1', = ... = ... and '1:1 = ... = '1:. (test of com- plete homogeneiry ) Likelihood ratio rest Let us first discuss the reLationship between this hypothesis. H ,. and the hypotheses H . and H , which were considered in the"
1139,unknown,"the last two sections. Nme that H . is a ""conditional"" hypothesis: thaI is, MULTIVARIATE ANA.LYSlS 142 if L~, Lt, and L~ are the maximized likelihoods under the three hypotheses and if L' is the unconstrained maximum likelihood, then A""=L~/L·. Bul L~ and Lt are equal, and therefore a relationship exists between the A. In fact A< = .1."".1. •• and tbis observation enable. us to obtain"", directly fro"
1140,unknown,"from (5.3.19) and (5.3.26). We must have -2 log Ae = -2 log A, -2 log A. ="" log I~WI- I II, log IS,I. This statistic has an asymptotic chi-squared distribution with !p(k -1) x (p + 3) degrees of freedom. o simple un seems to be available for thIS hypothesis. *5.4 The Behrens-Fisher Problem The problem of testing equality of means with no assumption about the corresponding covariance matrices is an"
1141,unknown,"corresponding covariance matrices is an extension o( the Behrens-Fisher problem. The essential point~ appear in the univariate situation, where the confidence interval solution differs from those reacbed by the fiducial and the Bayesian approaches. An excellent discussion of the univariate case appears on Kendall and Stuart (1973. pp. 146-161). The multivariate problem is as follows. Suppose that "
1142,unknown,"problem is as follows. Suppose that X l and X , are independent random data matrices. where X,(n, xp) is drawn from N.(p."" 1:'>, i = 1. 2: how can we test the hypothesis that ""', = ....,? Equivalently, we may write li= ""', - ""', and test the hypolhesis HOI: li = 0 against the hypothesis H ,: li ,.cO. 5.4.1 The likelihood ratio approach The m.l.e.s of ("""",112, It. I2 ) are as follows. Under H , the"
1143,unknown,"maximized when 11, = x, and 1:, = S,. From (4.2.17) the maximized log likelihood is It = -Hn, log 18,1+ rlzlog is,I)-1r1p Jog 21T-lnp. Under HOI the m.l.e. of ""', the commo n mean. may be shown by differenti­ alion to satisfy .. =(n, iI' + rl2i;')-'(n,ii'x, + rl2i;'i!) (see (A.9.2)). and tbe m.l.es of 1:, satisfy t. =S, + d,d:. (5.4.1) (5.4.2) 143 HYPOTHESIS TESTING where d,=i,-':"",i=I.2, asin Sec"
1144,unknown,"(5.4.1) (5.4.2) 143 HYPOTHESIS TESTING where d,=i,-':"",i=I.2, asin Section 4.2.2.4. A degree of simplification of these equations is possible. but their main value is in suggesting the following iterative algorithm: (1) Start with the initial estimates 1, = S"" i = 1,2. (2) Find the corresponding estimale of '"" from (5.4.1). Call this iL. (3) Using .. in (5A.2), calculate new t, by I, =S, +(i, - iL"
1145,unknown,"I, =S, +(i, - iL)(i, - iL)'· (4) Return to (2) using the new I,. This iterative procedure may be repeated until covergence is reached. 5.4.2 Union intersection principle One approach 10 the problem consists in using a solulion to tbe univariate Fisher-Behrens problem as proposed by Yao (1965). First, we note that whe re Le t i, - N.( ..... r,), f,=n,-I, d=i,-i2• U=U, + U "" S; -w.(r,.f.). i = 1. 2."
1146,unknown,"S; -w.(r,.f.). i = 1. 2. i = 1, 2. U, = SJf.. i = 1,2,} r=r,+r2 • (5.4.3) (SA.4) (5.4.5) ote that U is an unbiased estimator 01 r. and. when the null hypothesis is true . let us assume that thc following statement is true: (5.4.6) where f is 10 be selecled. independently 01 d, so tbat for all p-veCIOr5 a, fa'Ua-(aTa)xt . Tben, as in Section S.2.2b, it can be shown that w. = (a'd)'/(a'Ua) -t;(f), ("
1147,unknown,"w. = (a'd)'/(a'Ua) -t;(f), (5.4.7) MUL llV ARIA n:. ANALYSIS 144 where t. (f) denotes a t statistic with f degrees of freedom, Further, w! = ma x w. = d'tr'd - T 2(p. fl. • where the maximizing a is given by s*=U -'d, (5A.8) (5,4.9) Welch (1947) has shown that although (5.4.6) is not in general valid, we have. approximately, where ~=~ (a'u ,a)2 +2-(a'U 2a)', f. f, a'Ua f. a'Ua (5.4,10) (5.4,11) On"
1148,unknown,"(5.4,10) (5.4,11) On substituting a' from (5.4.9) in (5.4.\1), we expect from (5.4.R) thaI. approximately, where -.!..=2-(d'U - J U,U -'d)2 +~ (d'U -'U 2 U -'d)2, f* f, d'U 'd f, d'U-'d Thus a test can be carried out. For other SOlutions to this problem . see Bennett (1951) and G, S. James (1954), 5.5 Simultaneous Confidence Intervals We now show through examples how the union intersection princip"
1149,unknown,"leads to the construction of so-called ""simultaneous confidence intervals"" for parameters, 5.5.1 The one-sample Botelling T ' case From (5,2,14) w e have for all p-vectors B . where t. = a'(i - "",)/[a'53/("" _1))""2 (5.5.1) (5,5,2) and y> is the one-sample Hotelling T 2 statistic. Since. from Theorem 3,5,2. T 2 is proportional to an F distribution. the upper 100a percentage point of y> is 145 HYPOTH"
1150,unknown,"point of y> is 145 HYPOTHESIS TESTING (5.5.3) whe re T~ ={(/l-l)p/(Il-p)}Fp, n _p~ (5.5.4) and F p,n-p,. is tbe upper 100a percentage point of Fp,n_p' Now if T 2 ,.. T;, then (5.5.1) implies for all p-vectors B . (5,5.5) On substituting (5.5,2) and (5.5.4) in (5.5.5). and inverting the inequality, we can rewrite (5,5.3) as Pr {a'p.E (a';;-b, s'j+ b) for all a} = I-a. (5.5.6) where h = [{a'Sa p/(Il"
1151,unknown,"where h = [{a'Sa p/(Il-p )\Fp,,,-un ll{l, l'hus (a'x - h. a'i +h) (5.5.7) '"" the 100(1 - a)% confidence interval for a'p.. Note that there is a probability of (l-a) that all confidence intervals for a· ... obtained by ,'arying a will be IrUe, Hence, they are called the simultaneous confidence Intervals for a'p.. (a) It can be seen that. for fixed a. we could obtain 100(1-a)% confidence intervals f"
1152,unknown,"confidence intervals for a'p. from t. given by (5,5.2) since it is distributed as Student's t with (n - I) d.f. ame ly, in (5.5,7), take h=[(s'Sa/(n-l)}F,""_"" .]lf2 since FI.""_'=(~_')"" (Of course. the confidence coefficient will cbange if a is varied,) (b) The irnultaneous confidence intervals may be used to study more specific hypo theses concerning p.. For example, the hypo thesis H o:Il= O is ac"
1153,unknown,"H o:Il= O is accepted at 100"",% level of significance if and on ly if every 100(1 - a)% simultaneous confidence interval contain~ the zero value. Thus if H o is rejected. there mW!t be at leasl one vector a for whid , the corresponding confidence inlerval does /lot contain zero. Hence this method allows us to examine which particular linear com binations lead H "" to be rejected, Ex ample 5.5.1 In "
1154,unknown,"Ex ample 5.5.1 In Example 5.3,1 the differences in cork depth in the lour directions were sum mar ized by the transformed variables Y=XR ' ilnd Y was assumed to be a data maLri. from 3(RIL, IU:R'), where RIl = v, say, Hotelling's y> test for the cork data led to the rejection of MULTIVARJATE ANALYSIS 146 Ho: V = O. We now examime which mean might have led to the rejection of Ho. U we put 8 =0,0,0)"
1155,unknown,"of Ho. U we put 8 =0,0,0)', then 8'58=S"". Using Example 1.5.1, "" = 28. p=3. y, = 8.8571, F ,.2S.Q.01 = 4.68. s"" = 124.123. so (5.5.7) gives the 99% confidence interval for v, as 0.51<v,< 17.21. Similarly, using the values a =(0, 1,0)' and a=(O, O. I)', respectively. gives the intervals -5.01 <v2<6.72 and - 6.48< v,<8.48. Hence only the interval for v, does not contain zero, and thus it is this par"
1156,unknown,"particular mean wh ich led to the rejection of the null hypothesis. In general, when the hypothesis of zero mean is rejected. there must exist at least one a for which the associated confidence interval excludes zero. However. it is not necessary that a be one of the vectors e, = (0,0, ...• 1,0, ... ,0)' as in the above example. 5.5.2 Th e two -sample Hotelling T 2 case As in Section 5 .5.1, we ca"
1157,unknown,"As in Section 5 .5.1, we can construct a simultaneous confidence interval for a'lI = a'w,,-,..,) using the two sam ple means X, and i2 based on sample sizes "", and ""2' A UTT test for 11 = 0 was developed in (5.3.25). Us ing the same argument as in Section 5.5.1. it is found that the 100(.1-a)% simultaneous confidence intervals for a'lI are given by a'(i, -X2) ±{a'(n ,S, + n2Sz)ap(n, + ~F •. m :.11"
1158,unknown,"where In = '"" + ""2-p-l. 5.5.3 Other exampl "" The union intersection approach can be used to give simultaneous confi­ dence intervals in more complicated situations. For example . SCIs can be evaluated for the one-way multivariate analysis of variance of Section 5.3.3a to examine w ruch group differences on w hich variables are impor ­ tan! in the rejection of the null hypothesis. These SCIs are di"
1159,unknown,"Section 12.6. 147 H YPQTHES1S TESTING More generally. simultaneous confidence intervals can be calculated for the parameters of a multivariate regression. See Section 6.3.3. 5.6 Multivariate Hypothes is Testing: Some General Po ints Having discussed the LRT and VIT, what conclusions can one draw? They are different procedures. and in general they may lead to different test statistics. An important"
1160,unknown,"test statistics. An important statistical property of LRT is the general asymptotic chi-squared result stated in Theorem 5.2.1. No such result exists for UlTs. However. the importance of this difference should not be over­ emphasized. since no corresponding small-sample result is known . In addition, the LRT depends upon the assumption of a specific distribu­ tional foml. This is not crucial to th"
1161,unknown,"tional foml. This is not crucial to the Ul procedure, which merely requires a ""sensible"" way 01 testing each of the component hypotheses. For example. the t statistic may be .ensible for many non-normal dis­ tributions. A mOre important advantage 01 the Ul approach is that, if H o is r""je~1. ed. it is a simple matter to calculate which of the component hypotheses led to the rejection of H "". This "
1162,unknown,"hypotheses led to the rejection of H "". This can suggest a particular way of am ending the hypothesis in order to accord more with the data. For instance, if the T' statistic devised under both the LR and VI approaches 'Nere ""significant"", then the implication w;ing the LR approach would be ,imply ""reject H o"". However, the UI app roach would indicate more than this. As well as knowing that H o sh"
1163,unknown,"this. As well as knowing that H o should be rejected, one could enquire which specific linear combinations a'x led to its rejection. We saw this properly illustrated in the discussion of simultaneous confidence intervals in Section 5.5. Another nice property o[ the LRT is that it sometimes leads 10 convenient factorizations. For example, in Theorem 3.7.3 it was shown that Wilks' A can be written a"
1164,unknown,"that Wilks' A can be written as the product of independent beta variates. From extensive studies it has become clear that the UI criterion is ,""sensitive to alternative hypotheses involving more than one non -zero eigenvalue. On the other hand. Wilks' criterion provides good protection over a wide range of alternatives (Schatzoff, 1966; Pillai and Jayachan­ dran. 1967 ). However , Wi lks' criterio"
1165,unknown,"dran. 1967 ). However , Wi lks' criterion in itself does not provide simul­ taneous confidence intervals, a leature which might in practice outweigh its theoretical power properties. Gabriel (1970) has examined theoretically the relationship between LR and UI tests. Other criteria have also been proposed, including tbe MULTIVARJATE ANALYSIS following functions of the eigenvalues: Pillai (1955) Gir"
1166,unknown,"Pillai (1955) Giri (1968) ~>, ; L A, 1+.1., ' Kiefer and Schwartz (1965) n C ::1.;) , 148 Mardia (1971) has investigated the effect of non-normality upon Pillai's criterion. He found that it is fairly robust when used for testing hypoth­ eses on means. but not when used for hypotheses on covariance matrices, These properties are reminiscent of similar results from robustness studies on univariate "
1167,unknown,"on univariate statistics. Of course there are further criteria which can be used for assessing multivariate tests-the criterion of maximal invariance is one (Cox and Hinkley, 1974. pp. 157-170). *5.7 Non-no rmal Data We have SO far assumed that the parent population is multinormal. For non-normal populations when the form of the density is known, we can proceed to ohtain the likelihood ratio test "
1168,unknown,"proceed to ohtain the likelihood ratio test for a given testing problem. Tbe asym ptotic distribution of the likelihood ratio criterion is known through Wilks' Iheorem (see (or example Exercise 5,7.1). However. the exacl distribution is not known in general. Since multinormal theory is so developed. it is worlhwhile 10 enquire which tests of multi normal Iheory can safely be applied to non-normal "
1169,unknown,"data, Initially. of cour e. a test of multinormality should be applied. We shall now describe two such tests. For a review of various tests of multinormality. see Mardia (1978). In Theorem 2.5.5 it was shown that, for the multinormal distribution. the multivariate measures of skewness and kurtosis have values Il, .• =0 and Il, .• = pip + 2). Mardia (l970a) has shown that the sample counter­ parts "
1170,unknown,"parts of the measures given in Section 1.8. b""p and b, .• , have the following asymptotic distributions as "" -+ '"" when tbe underlying popu la­ tion is mullinormal: knb"".-x;' where f=~p(p+t)(p+2), (5.7.1) and (I>, .• - p(p + 2)}/{8p(p + 2)/n )'12 - N(O, I). (57.2) 149 HYPOTHESIS TESTING These statistics can be used to test the null hypothesis of multinormality. One rejects tbe null bypothesis for "
1171,unknown,"One rejects tbe null bypothesis for large bj •• and for botb large and small values of b, .•. Critical values of these statistics for small samples are given in Mardia (1974). Exam ple 5.7.1 Consider the cork data of Table 1.4.1, where n = 28 and p = 4. From (1.8.1) and (1.8.2) it is found that bl.4 =4.476 and b, .• =22 .957. The n kob,,4 = 20.9 is clearly not significant when compared to the uppe"
1172,unknown,"5% cfiticai value of the ;do distribution. From (5.7.2) we find that the observed value of Ihe standard normal variate is -0.40, which again is not significant. The main advantage of these tests is tbal if the hypotheses of Il = 0 and ll2.p = p(p+2) are accepted then we can use the normal theo~' ;ests nn mean vectors and covariance matrices because Mardia (I970a, 1971, 1974. 1975a) has shown the f"
1173,unknown,"(a) The size of Hotelling's T'-test is overall robust to non -normality. altbough both HOlelJing's one-sample T'-tcst and the two-sample test for II I ¥ O 2 are more sensitive 10 (31.. than to Il""p' If II, = II, the two-sample lesl is bardly influenced by (31.. or ll2.p' (b) Tests on covariances are sensitive, and sensitivity is measured by {3, ... Thus, broadly speaking, in t~e presence of non-no"
1174,unknown,"{3, ... Thus, broadly speaking, in t~e presence of non-normality, the normal theory tests on means are influenced by Ill.. whereas tests On covariances are influenced by Il, .•. J[ the hypotbesis of multinormality is rejected, one can sometimes take reme dial steps to transform the data to normality, e.g. using the log transformation. Another possibility is to use a non-parametric test. but the fi"
1175,unknown,"the field of multivariate non-parametric methods is not a rich one (see Puri and Sen, 1971). This fact partly results from the lack of a natural way to order multivariate data. However, we give a bivariate non­ parametric test in the next section. 5.8 A Non -parametric Test for tbe Bivariate Two-sample Problem uppose ""lI' ; = 1, ' .. , 11"" and X2,. j = 1. .... ""2t are independent random sam ples f"
1176,unknown,"sam ples from populations with c.d.f.s F(x) and G (s), respectively. Sup ­ pose j is the mean vector of tbe combined sample. Let tbe angles made MULTIVARIA TE AN ALYSIS 150 witb the x, axis in the positive direction by the vectors (x,; -x). i = 1 ....• It"" and ("" ; - x). j = I, ...• r~. be ranked in "" single sequence. Let (r, •.... rn) and (rj ..... r~,) be tbe ranks of the angles in the first and"
1177,unknown,"seoond samp les, respectively. Let N ="" I + "" 2' We replace tbe angles of the first and seoond samples by y "" = {cos (27!1'JN ), sin (27Tr,/N»),. i = I ....• """" (5.8.1) and y,; = {cos (2rrrj/N), sin (27frj/N»)'. i = 1. .... /12' (5.8.2) T his means that the angles are replaced by the ""uniform scores"" on a circle. For these observations it can be shown that y= o, (5.8.3) L et T2 be H otelling's two"
1178,unknown,"L et T2 be H otelling's two-sam ple J'2 statistic for the y given by (3.6.4). and define U (N- I)J'2 n,(N - ll(, --)'S;-'(- _-) (N-2) Oz J , y y, y . (5.8.4) (The second equality foUows because YI-y,=y,-y+y-y,=(N/n,)x (y,-y).) Consider the hypotheses H o: F(x) = G(x ) and H, : F(x) = G(x+8) , 8 "" O. Since we expect y, - Y to be shifted in the direction Ii when the alternative hypothesis is true. U"
1179,unknown,"substituting (5.R.3) in (5.8.4). we gel U 2(N - 1) {( ~ 27fr,)2 (~ . 27fr,)2} L. cos- + i.J Sm-- . n l'12 i- I M i_ I N (5.8.5) W e reject H o for large values of U. As """" "" 2 ..... ""'. II ,III, -> (3. 0< (3 < 1. it is found under H "" thal, asymp­ toticaUy, u ..... xi. (5.8.6) Example 5.8.1 The distance between the shoulders of the larger left valve (x) and the length of specimens (y) of Bairda ok"
1180,unknown,"two geological levels are given in Table 5.8.1. Here 11 ,=8 , ""2 = 11, N = 19. It is found that the ranks "" for level 1 are 5, 13, 14. 15, 16, 17, 18, 19. and "" 27fr, t... cosN = 2.9604, L Sin 2;:'= - 3.6613 . IS 1 HYPOTHESIS TESI1NG Ta ble 5.8.1 D ata for the geological problem (Shave,. 1960) Level I Level 2 x I' x V 63L 1167 682 1257 606 1222 631 1227 1\82 1278 631 1237 480 1045 707 1368 606 115"
1181,unknown,"606 1151 631 1227 556 1172 682 1262 429 970 707 1313 454 1166 656 1283 682 1298 656 1283 672 1278 '""alues of x and yare in micromelres. Co nsequenlly U given by (5.8.4) is 9.07, whicb is significant since the 5% value of X~ is 5.99. We note tbe following points. Like T 2, U is invariant under oon­ singular linear transformations of the variables Xli and x"". The Pitman's asymptotic efficiency of th"
1182,unknown,"asymptotic efficiency of this test relative to H otelling's J'2 test for the normal model is 7f/4. The U test is strictly non -parametric in the sense that the null distribution does not depen d on the underlying distribution of the variables. This test was proposed by Mardia (1967b). For critical values and extensions see M ardia (1967b . 1968, 1969. 19na,b) and M aroia and Spurr (1977 ). Another"
1183,unknown,"M aroia and Spurr (1977 ). Another non -parametric test can be constructed by extending tbe M ann-Whitney test as follows. First combine the two groups and rank the observations [or each variable separately. Then use J'2 for the resulting ranks. This test is not invariant under all linear transformations, but it is invariant under n:ionotone transformations of each variable separately. For further"
1184,unknown,"For further no n-parametric tests. see Puri and Sen (1971). Ex ercises aod Complements 5.2.1 Derive a likelihood ratio test for the hypoLhesis 11'11 = I, based on o ne observation x from N pu., ,,21), where ,,2 is known. 5.2.2 Show that l:O'S. a. g, and -2 log A take the values indicated in E xam ple 5.2.3. MULTIVAR IATE ANAL yStS 5.2.3 [f M is a (p x p) matrix, distributed as W""(:~. n ), where 1:"
1185,unknown,"5.2.3 [f M is a (p x p) matrix, distributed as W""(:~. n ), where 1:= rtll 0] p, L 0 l:"" p, 152 and M is partitioned conformably. show that F = M "" M ,,'M' 2 and G = M 22 - M 2.MltM ll are distributed independently as W .,(:l:,2, p,) and W """"(l:z,, n- p,). Using the fact that a'Fa and a'Ga are independent X' variables for all a, derive a two-sided union intersection test. based on the F-ratio, of t"
1186,unknown,"hypotbesis that :£ is in [act of tbe form given above. S.2.4 (a) Show that if Ex ample 5.2.1 is amended so thaI ,,= [100 50] - SO 100' then - 2 log A = 3.46. and H o is still not rejected. (b) Show that with :£ given above, the 95% confidence region [or 1', and j.I, is elliptical, and takes the form (185.72 - j.I,l' + (183.84 - j.I,)2 - (185.72 - 1'-,)( 183.84 - 1',) .. 17.97. S.2.S Many of the te"
1187,unknown,"S.2.S Many of the tests in this chapter have depended upon the statistic d'S-'d, where S is a (p x p) matrix. Show that in certain special cases this statistic simplifies as follow s: (a) If S is diagonal, then d'S-'d = I; + ... -L I;. where I, = dj,ls"". (b) When p = 2, (c) When p = 3. d'S-'d is given by I ~( I - r~,l""1' li(l-r ~,) + tj(1-r~,) - 2I,I,u""., -21,I'UI3.l - 21,I3 Un 1 1 + 2rl2r1)r23 -r"
1188,unknown,"5.3.1 (Rao, 1948. p. 63) ( al Consider the cork-tree data from Exam ­ ple 5.3.1 and the contrasts ZJ =(N -t-S)-{E + W) , ~2= S-W. 153 HYPOTHESIS TESTING Show that z, = Y"" "" = y,. and Z2 =1(Y'-}'l + Y,), where y,. y"" y, were defined in Example 5.3.1. (b) Show that the sample mean and covariance matrix for z = (Zl> Z 2, zJ)' are [ 124.12 59.22 Z = (8.86. 4.50. 0.86)' and S = 54.89 -20.27] -27.29 . 6"
1189,unknown,"-20.27] -27.29 . 61.26 (c) Hence show that the value of T' for testing the hypothesis E(z) =0 is T 2= 20.736 and the value o[ F is 6.40. Show that this is significant at the I % level. (Note that the value of T' obtained in tbis exercise is the same as that obtained in Example 5.3.1, thus illustrating that l' is invariant under linear transformations.) 5.3.2 (Seal, 1964 , p. 106) {a> Measurements "
1190,unknown,"5.3.2 (Seal, 1964 , p. 106) {a> Measurements of cranial length {x,> and cranial breadth (x2 ) on a sample of 35 mature female frogs led to the following statistics: [22.860] i = 24.397 ' T est the hypothesis that j.lJ = ,....,. S, = [17.178 19.710] 23 .710 . (b) Similar measurement~ on 14 male frogs led to the statistics [21.821] i = 22 .843 . 17.731] 19.273 . A gain test the hypothesis that j.I, "
1191,unknown,"(c) Show that the pooled covariance matrix obtained from the above data is s n,S, + 112S, ,~ I + ""2 [ 17.173 19.145] . 22.442 Use this to test the hypothesis that the covariance matrices of male and female populations are the same . (d) Test the hypothesis that III =,...."" staring your assumptions carefully. 5.3.3 (a) Consider the hypothesis H II:Il= k"""", where 1: is known. From (4.2.9), the m .l."
1192,unknown,"(4.2.9), the m .l.e. of k under H o is given by k = 11""r ' i1...,l.:-IIlo. Us ing (5,3.1). show that the LRT is given by -2 log A = ni':£-'{1:-{J1<'.:£-'Ilo)-'""""jJQ}I-'i. Using Section 5.3.1a, deduce that -2 log A - X~- I exactly. MULTIVARJATE A.'.IALYSIS \54 (b) Consider now the hypothesis H o :IL= k"""". where I is ullknowlt. From (4.2.11) the m .1.e. of k under H .. is given by f. = 1J.(,S"" x/IJ."
1193,unknown,"Using (5.3.2). show that -2 log A = 11 log (1 ~d'S-'d), where d'S"" d = XS "" (S-(W,S ' '""""r I fLolLo}S"" i. Using Section 5.3.1 b, show that (It -l)d'S"" d - T""(p - 1, II - 1). 5.3.4 U AG , Ab' and Ac are as defined as in Section 5.3.3c and if m., m., and m , are tbe respective asymptotic degrees of freedom. show that m c = rna + m,.. Explain. S.3.S Let X be a data matrix from N .(IL, I ). (a) Show t"
1194,unknown,"(a) Show that tbe LRT for tbe hypothesis tbat x I is uncorrelated with x2 , .•. ,Xv is given by A =(1-R ')"""", where R2= R ' 2R2~ R'l is the squared multiple correlation coefficient between Xl and X 2, . ..• ~. (b) Show that tbe LRT for the hypothesis that I = (T2{(1- p)1+ p 11') is given hy A =fiSi/[v""(1-r)o-1(1 +(p-t)r)'!}""Il. where v is the average of the diagonal elements of S and vr is the ave"
1195,unknown,"of the off-diagonal elements. (See Exercise 4.2.U and also Wilks. 1962. p. 477.) 5.3.6 (a) Rao (1966, p. 91) gives an anthropometric example in wbich each of two variables taken separately gives a ""significant"" difference between twu communities, whereas the T' or F test utilizing both variables together does not indicate a significant difference. Community 2 r statistics (or each charaCter Sample"
1196,unknown,"Sample size 27 20 Mean length (in millimetres) of Femw Humerus 460,4 444.3 2.302 335 .1 323 .1 2.214 The pooled unbiased estimate of the common covariance matrix can be constructed from the information in Ra o (1966) as S = [561.8 380.7] • 343.2 . 155 HYPOTHESIS TESTING Show that each of the individual I statistics is significant at the 5% level (using 45 degrees of freedom). However. if one tests"
1197,unknown,"(using 45 degrees of freedom). However. if one tests wbether the popula­ tion means of femur and humerus are the same in both communities, show that the resulting F statistic takes the value 2.685. whicb is not significant at tbe 5% level (using two and 44 degrees of freedom). (b) Rao sbows that the power of the T"" test depends upon p, the number of variables. and d •. tbe Mahalauobis distance in "
1198,unknown,"between the twn populations. He gives charts which illustrate the power functions for values of p between 2 and 9. It can be seen from these charts that, for a given sample size, the power can decrease when the number of variables increases from p to (p + q ), unless the increase in Mahalanobis distance d;+o - d; is of a certam order of magnitude. In the example given in paTt (a), the D2 for femur"
1199,unknown,"example given in paTt (a), the D2 for femur aJone was D i = 0.4614, while that for femur and humerus together was 0.4777. Such a small increase led to a loss of power with samples of the given size. Comment on this behaviour. 5.5.1 Following Section 5.5.1. show that the simultaneous confidence intervals for S'(IL,-""""') in the two-sample case are given by (5.5.7). 5.S.2 (a) In the one-sample case w"
1200,unknown,"5.S.2 (a) In the one-sample case when I is known. use the approach of eclion 5.5.1 to show that simultaneous 100(1-a)% confidence intervals fO T 8',. are given by { r } 112 a'IL E s'i± ~ (a' l:a) x!.v for all a. (b) Show that a',. lies in the above confidence interval for aU a if and only if IL lies in the 100(\-0')% confidence ellipSOid of Example 5.2.1, n(,.-x)':t""'(IL-X)< X~~ . mi nt: see Corol"
1201,unknown,"(c) Construct a corresponding confidence ellipsoid for tbe one-sample case when l; is unknown . S.7.1 Consider a random sample of size II from Weinman's p-variate exponential distribution (Section 2.6.3) and consider the null bypothesis flo: AO= A1 = ... = .... .,. The maximum log likelihood may be found from Exa mple 4.2.2. Show that -2 log A = 2np log (ii/g), where ii and g are arithmetic and ge"
1202,unknown,"asymptotic distribution of -2 log A. MULTIVARIATE A ALYSIS 156 5.7.2 Fo r the cork data of Table 1.4.1. show that the measures of skewness and kurtosis of Section 1.8. taking one variable at a time, are 0.6542 2.6465 E 0.6925 2 .732 0 s 0.5704 2.8070 whereas taking twO variables at a time. they are NE 1.214 7.512 NS 1.019 6.686 NW 1.138 7.186 ES 2.192 9.359 EW 0.965 6.89/\ W 0.195\ 2.2200 SW 0.935"
1203,unknown,"6.686 NW 1.138 7.186 ES 2.192 9.359 EW 0.965 6.89/\ W 0.195\ 2.2200 SW 0.935 7.S35 What can be concluded regarding nonnality of variables in the lower dim ensions? Can Ihe large values of /),,2 aDd /)2.2 [or ES be partially ascribed to the larger values of /)1.I and /)2.1 for E and S. respectively? 5.7.3 For the cork data (Table 1.4.1) let y, = N + S - E-W . Y2=N- S. YJ= E - W . Show that b1.3 = 1"
1204,unknown,"b1.3 = 1.777. b2 .; = 13.558. Test the hypo thesis of normality. 5.8.1 Show that 27T i ~ . 21Ti L cos-= L. SIO-=O . ...... 1 N .""""1 N and ~ 2 2r.i ~ . 22rri 1 N L cos - = L SIn - = - . ,,,,I N i-l N 2 Hence prove (5.8.3). Verify the second equality in (5.8.4) and prove (5.8.5). 5.8.2 Apply Hotelling's T' test to data in Table 5.8.1 and compare the conclusions with those in Example 5.8.1. 6 Multiva"
1205,unknown,"6 Multivariate Regression Analysis 6.1 Introduction Consider the model defined by Y = XB + U , (6.1.1) where Y (1l x p) is an observed matrix of p response variables on each of n individuals, X (n x q) is a known matrix, B (q x p) is a matrix of unknown regression parameters, and U is a matrix of unobserved random distur­ bances whose rows for given X are uncorrelated, each with mean 0 and common "
1206,unknown,"bances whose rows for given X are uncorrelated, each with mean 0 and common covariance matrix :E. When X represents a matrix of q ""independent"" variables observed on eacb of the n individuals, (6.1.1) is called the multivariate regression model. (If X is a random matrix then the distribution of U is assumed to be uncorrelated with X.) When X is a design matrix (usually Os and 1s), (6.1.1) is calle"
1207,unknown,"(6.1.1) is called the getleral linear model. Since the math ematics is the same in both cases, we shall not emphasize the difference, and we shall usually suppose that the first column of X equals 1 (namely X = (1, X ,) to allow for an overall mean effect. For simplicity, we shall treat X as a fixed matrix throughout the chapter. If X is random , then aU likelihoods and expectations are to be inte"
1208,unknown,"expectations are to be interpreted as ""conditional on X"" . The column s of Y represent ""dependent"" variables which are to be explained in terms of the ""independent"" variables given by the columns of X. Note that so that the expected value of Yo; depends on the ith row of X and the jth column of the matrix of regression coefficients. (The case p = 1, where r..ruL TIV ARJA TE ANALYSIS 158 there is o"
1209,unknown,"there is only one dependent variable. is the familiar multiple regressioll model which we will write as y =X~ + u.} In most applications we shall suppose that U is normally distributed, so that U is a data matrix from N.(O. I ). (6.1.2) where U is independent of X. Under the assumption of normal errors, the log likelihood ior the data Y in terms of the parameters B and I is given from (4.1.4) by I"
1210,unknown,"from (4.1.4) by ICB. I) = -1"" log 12"".:£1-1 tr (y - XB )r'(y - XB )'. (6.1.3) 6.2 Maximum Likelihood Estimation 6.2.1 Maximum likelihood estimators for B and I We shall now give the m.l.e.s for B and I in the multivariate regressIOn when II;;"" P +q. Tn order for the estimates of B to be unique we shall suppose that X has full rank q. so that the inverse (X'X)-l exists. The situation where rank(X) "
1211,unknown,"situation where rank(X) < q is discussed in Section 6.4. Let (6.2.1 ) Then P (II x II) is a symmetric idempotent matrix of rank n - q which projects onto the subspace of R "" orthogonal to the colum ns of X. (See Exercise 6.2.1.) In particular. note that PX = O. Theorem 6.2.1 For the log likelihood {ullction (6.1.3). the m .l.e.s of B and :£ are B = (X'X )-' X'V (6.2.2) and 1:=n-'V 'pV . (6.2.3) Pr"
1212,unknown,"and 1:=n-'V 'pV . (6.2.3) Proof Let y = xB = X(X'X} -'X'Y (6.2.4) and V=Y-xB=PV (6.2.5) denote the ""fitted"" value of V and the ""fitted"" error matrix, respectively. Then Y - XB = V + xB - XB , so the second term in the right-hand side of (6.1.3) can be written -1 tr .I-'(V - XB)'(Y - XB)=-1tr r'[iJ'v+ (B-B)'X'X(B - B )]. 159 MULTrv ARIA TE REGRESSION ANALYSIS Substituting in (6.1.3) we get I(B. I )"
1213,unknown,"I(B. I ) = -!n log 121T1I-ill tr r'i-~ tr 1-'(B- B )'X'X (B-B ). (6.2.6) Only the final term here involves B . and this is maximized (at the value zero) when B = B. Then the ""reduced"" log likelihood function is given by leB. I) = -!lIp log 21T-!1I (log III +tr :£-'1:). which by Theorem 4.2.1 is maximized when 1=1:. The maximum value of the likelihood is given from (6.2.6) by t(B 1:)=-4"" logI2"",1:I"
1214,unknown,"The statistics Band i are sufficient for Band .1: (Exercise 6.2.2), For aD efficient method of calculating Ji and 1:. see Anderson (1958, pp. 184- 187). Note tbat V'V = Y'Y - yy = V'PY. (6.2.S) In other words. the residual sum of squares and products (SSP) matrix (I'V equals the total SSP matrix Y'Y minus tbe fitted SSP matrix Y·Y. Geometrically. each column of Y can be split into two orthogonal p"
1215,unknown,First. Y(i) is the projection of Ym onto the space spanned by the columns of X. Second. 0(;) = PY (i) is the projection of Ym onto the space orthogonal to the columns of X . This property is illustrated for p = 1 in Figure 6.2.1. B u I/-:..:.....---~A Xfl o Figure 6.2.1 Geometry of multiple regression model. y = XIl ~ •. - MULTIVARIATE ANALYSIS 160 The true value of ~ = E(y) is denoted by the poin
1216,unknown,"- MULTIVARIATE ANALYSIS 160 The true value of ~ = E(y) is denoted by the point A . The observed value of y (the point B ) i~ projected onto the plane spanned by the columns of X to give tbe point A as an estimate of A. II we let Mil = X'X,M,,=Y'Y. and M 12 =X'Y, then we can write ... ...... J nX = U'U = M22 - M 2 ,M1, M'2' Thus the residual SSP matrix can be viewed as a residual Wishart matrix M 2"
1217,unknown,"M 22., conditioned on X, as in (3.4.5). 6.2.2 The distribution of B and I We now consider the joint distribution of the m.l.e.s derived in Theorem 6.2.1 under the multivariate linear model with normal errors. Theorem 6.2.2 Under Ihe model defined by (6.1.1) and (6.1.2) (a) Ii is unbiased for B, (b) E(U) = 0, (c) Ii and U are multivariale nannal, (d) Ii is slalistically independenl of U, and hence "
1218,unknown,"Proof (a) Inserting Y = XB+ U in the formula for B defined in Theorem 6.2.1, Ii = (X'X)-'X'(XB + U) =B + (X'X)-JX'U. Therefore, E(B)= B since E{U)=O. Hence (a) is proved. (b) We have U=Y - xB. Therefore E(U) = E(Y)-XE(B) = XB-XB =0. (c) We have U=PY=PU, (6.2.9) (6.2.10) where P was defined in (6.2.1). Also Ii = B + (X'X)-'X'U from (6.2.9). Hence both U and B are linear functions of U. Since U has "
1219,unknown,"normal distribution, the nOlIDaJjty of U and Ii follows immediately. (d) This follows by applying Theorem 3.3.3 to the linear functions defined in part (c) above. • Of particular importance in the above theorem is the fact the Ii and i are independent for normal data. More precise results concerning the distribution of Ii and :i: are given in the (oUowing theorem . Theorem 6.2.3 (a) The covariance"
1220,unknown,"G= (X'x r'. (b) ni-W .O:. n -q). 161 MULTIVARIATE REGRESSION ANALYSIS Proof (a) From (6.2.9) we see that ~,, -(3;, = &iIlu), where A = (X 'X )-'X '. Therefore C(i3;j' ~ .. )=aiE[u.i)u(IJa. = &i[U,1lJa. =uf/(AA').,. =uf/&' since AA ' = G. Therefore part (a) is proved. (b) From (6.2.10), n1 = U'U = UPU, where P is an idempotent matrix of rank (n -q). Since each row of U has mean 0, the result follow"
1221,unknown,"of rank (n -q). Since each row of U has mean 0, the result follows from Theorem 3.4.4. • Note that when X'X = 1, or nearly so, the elements of a column Ii(,) will tend to have a low correlation with one another; this property is consi­ dered desirable. 6.3 The General Linear Hypothesis We wish to consider bypotheses of the form C,BM, = D . where C, (g x q), M,(p x r), and D (g x r) are given matri"
1222,unknown,"M,(p x r), and D (g x r) are given matrices and C , and M, bave ranks g and T, respectively. In many cases D = 0 and M, = I, so tbe hypothesis takes the form C , B = O. No te tbat the rows of C , make assertions about the effect on the regression from linear combinations of the independetlr variables, whereas tbe columns of M , focus anention On particular linear combi nations of the dependenl var"
1223,unknown,"We shall follow the two-pronged approach begun in Chapter 5; that is, we shan consider the bypothesis from the point of view of the likelihood ratio test (LRT ) and the union intersection test (UIT). 6.3.1 The likelihood ratio test (LRT). Consider first the hypothesis C ,B=D (so M,=I). It is convenient to define some additional matnces before constructing the LRT. Let C,«(q -g) x q) be a matrix su"
1224,unknown,"C,«(q -g) x q) be a matrix such lllat C' = (C ;, C~) is a non-singular matrix of order q x q, and let B o(qx p) be any matrix satisfying C ,B o= D . Then the model (6.1.1) can be rewritten as Y . =Zd+U , (6.3.1) where Y .=Y - XB o, Z=XC -"" and d =(d~, d~)'=C(B- Bo) . The hypo thesis C ,B = D becomes d , = O. Partition C -I = (C"", Co» and let P , = 1 - X(2 )(C2)'X'XC2)r'C""'X' (6.3.2) represent the "
1225,unknown,"represent the projection onto the subspace orthogonal to the columns of XC (2). MUL lTV ARIA TE ANALYSIS 162 From (6.2.7) we see that the maximized likelihoods under the null and alternative bypotheses are given by i2."".n-'Y'.1', Y+I-.n exp (-1np) and 12."".n - 'Y~P +yxn12 exp (-1l1p), respectively. Thus the LR statistic is given by (6.3.3) No te that"" is a projection onto a subspace of P, (a vecto"
1226,unknown,"all the columns of X is orthogonal to the columns of XC"" ), so lhat Po = P , -P is also a projection. It can be shown that P, = X(X'X) -'Cj[C , (X'X )-'Cjj'C ,(X 'X)-'X' (6.3.4) (see Exercise 6.3.1). Also note that PY=PY~ because PX = O. This observation is useful in proving the following result. Theorem 6.3.1 The LRTof the hypothesis C,B = D for the model (6.3.1) has lest statistic A 2Jn=IY 'PY I"
1227,unknown,"which has A(p, n-q, g) distribution under the lIull hypothesis. Proof The formula for A 21n was derived above. For its distribution. Dote from (6.2.10) that Y'PY = U'PU under the null and alternative hypotheses. Also, under the null bypothesis, Y ':'P 2Y +=U' P 2 U (Exercise 6.3.2). Since P and P 2 are independent projections (PP2 = 0) of ranks n -q and g, resp~ectively, it follows from Theorems 3"
1228,unknown,"Y'PY and Y'.P,Y. have independent W.(l;, n-q) and W.(l;. g) dis­ tributions. The distribution of A 2/n follows. • Now let us consider the hypothesis C ,BM , = D for the model (6.1.1). From Exercise 6.3.3, we see that it is equivalent to study tbe hypothesis C,BM, = D for the transformed model YM , = XBM, + UM "" (6.3.6) where UM, is now a data matrix from N.(O, M;l:M, ) and 8M, contains qr paramete"
1229,unknown,"qr parameters. Define matrices and H=M,Y 'rP 2Y +M , = M,Y 'rX (X 'Xr'Ct[C, (X'X)-'C',j'C ,(X 'X)-'X'Y+M "" (6.3.7) E=M;Y'PYM , = M;YTI-X (X 'X) -'X']YM, (6.3.8) 163 MULTIVARIATE REGRESSION ANALYSIS to be the SSP matrix due. to tne regression and the residual SSP matrix, respectively. Then the LRT is given by the following theorem. Theorem 6.3.2 The LRT of the hypothesis CtBM , = D for the model (6"
1230,unknown,"(6.1.1) ;s givell by tile lest statistic A 2Jn = lEI/IB + E I-A(r. n-q, g). (6.3.9) Proof Apply Theorem 6.3.1 to the transformed model (6.3.6). • ote. that the LR statistic can also be written , IEi/IB + E I= n (1 Hit'. (6.3.10) where. A , ..... .1., are the eigenvalues of HE -'. Wh en p = 1 the m atrices in (6.3.9) are scalars and it is more usual to use the equivalent F statistic H /E -[g/(n -q)"
1231,unknown,"The null hypothesis is rejected for large values of this statistic. 6.3.2 The union intersection test cum The multivariate hypothesis C,BM, = D is true if and only if b'C ,BM,a = b'Da for all a and b. Replacing C , and M , by b'C, and M ,a in (6.3.7)­ (6.3.8). we see from (6.3.11) that each of these univariate hypotheses is tested by the statistic {b'C,(X'X)-'qb}{a'M;Y 'PYM ,a} (6.3.12) which has "
1232,unknown,"which has the (n -q)-'F,.n_q distribution under the null hypothesis for fixed a and b. Maximizing (6.3.12) over b gives a'Ha/a'EIl, (6.3.13) w hich has the [g/(n-q)]F, .• _ q distribution for fixed a (because Band E have independent Wishart distributions). Finally. maximizing (6.3.13) over a shows tbe VI statistic to be A,. the largest eigenvalue of BE -'. Let 8=A ,/(1 t-A,) denote the greatest ro"
1233,unknown,"The distribution of 8 = OCr, n -q, g) was described in Chapter 3. The null hypothesis is rejected for large values of e. If rank(M ,) = r = 1, then maxim izing over a is unnecessary and (6.3.13) becomes simpl y the ratio of scalars (6.3.11). Thus the UIT is equivalent to the LRT in this case. MULTIVARIATE Al ALVSIS 164 IT rank(C,) = g = 1. then maximizing over b is unnecessary. SiDce H and HE -' b"
1234,unknown,"HE -' bave rank 1 in this case. we see {rom (6.3.10) tbat tbe UIT and LRT are equivalent here also. The only non-zero eigenvalue of HE -' equals its trace. so from (6.3.7H6.3.8). .I., = tr HE -' = {C,(X'X)-lC W' C ,(X'X )-'X 'V..M ,E -'M;V ""X (X·X r'q . Letd = X(X'X )-'C ;. Underthe nullllypothesisM;V :d - Np(O , (d'd)M;:£M ,) (Exercise 6.3.4) and under both the null and alternative hypotheses E -"
1235,unknown,"E - W,(M ;:£M "" n - q) independently. Thus under the null hypothesis A, has tbe ( /l-q r'~ .. _q=[ '/(n-q- ,+ InF,. "" _. _.~, distribution. 6.3.3 Simultaneous con6dence intervals Suppose B is the true value of the regression parameter matrix and let V + = V - XB . Then the VI principle states that tbe probability tbat the ratio in (6.3.12) will be less than or equal to 9./0 - 9.) simultaneously fo"
1236,unknown,"all a and b for which the denominator does not vanish, equals \- 0:. Here O. denotes the upper "" critical value of the O(r, n - q, g) distribution. Rearranging gives the simultaneous confidence intervals (SCIs) P(b'C,BM ,8 e b'C,(X 'Xf'X'YM ,8 ± {~(8'Ea)[b'C,(X·X)-'q b]} ""'. for all a, b)-I-a. (6.3.14) 1 - 6. ote that b'C,(X'xr'X'YM ,lI = b'C,BM ,8 is an unbiased estimator of b'C ,BM ,lI. In some "
1237,unknown,"b'C ,BM ,lI. In some applications a and/or b are given a priori and tbe confidence limits in (6.3.14) can be narrowed. IT a is fixed then SCIs for b can be obtained by replacing 9./(1-9.) by [g/(II-q)]F .... _ •.•. If b is fixed then SCIs for a can be found by replacing 6./(1 - (0 ) by (0 - q)-'T;,,,-.;o = [r/(II - q - , + 1)] F,.,, _. _,~,;o. Finally. if both 8 and b are known. a single confidenc"
1238,unknown,"by replacing 9j(1-9.) by (n - q)-' FI."" _. ~' 6.4 Design Matri~es of Degenerate Rank In the design of experiments it is often convenient for symmetry reasons to consider design matrices X which are not of full rank. Unfortunately 165 M ULTIV ARIATE REGRESSION ANALVSIS the parameter matrix B in (6.1.1) is not well-defined in this case. The simplest way to resolve this indeterminacy is to restrict t"
1239,unknown,"subset of the columns of X. Suppose X (o x q) has rank k < q. Re arrange the columns of X and partition X = (X"" X ,) so that X ,(n x k) has full rank. Then X , can be wrinen in terms of X , as X ,=X,A (or some matrix A(k x(q-k». Partition B' =(B"" 80. If the regression model (6.1.1) is valid, then it can be reformulated in terms of X, alone as V = X ,Bi+u, (6.4.1) where Bf = B , + AB"" is uniquely d"
1240,unknown,"Consider a hypothesi.~ C ,BM .=D and partition C ,=(C lI,C 12). The hypothesis matrix C , is called leslable if, as a function of B. XB = 0 implies C ,B = O. (6.4.2) Then although B is indeterminate in the model (6.1.1). C, B = CIIB~ is uniquely determined. Also, note that the condition (6.4.2) does not involve M, . With a similar motivation. a linear combination d'B is called eSlimable if XB = 0 "
1241,unknown,"An alternative way to describe testability was given by Roy (1957). He showed that (6.4.2) holds if and only if (6.4.3) (See !O'<ercise 6.4.1.) This criterion is convenient in practice. If B~ is the m .Le. of Bi in the model (6.4.1) and if C, is [estable then the (unique) m.l.e. of C,B is given by • Thus, for a design matrix of non-full rank, [he estimation of estimable linear combinations and the"
1242,unknown,"linear combinations and the testing of testable hypotheses is most con­ veniently carried out using the model (6.4.1) instead of (6.1.1). EIample 6.4.1 (One-way multivariate analysis of variance). In Section 5.3.3a we considered k multinormaJ samples, Y""""""Y""I' Y"",+lI"""" Ynl-+""'l' ynl .... ~-+-I •••• ·Y"", where the ith sample consists of IIi observations from p("",+ T;, l:). i = 1 .... , k, and 0 = o"
1243,unknown,"MULTIVARIATE ANALYS IS 166 the form (6.1.1) using the design and parameter matrices 1 0 0 1 I 0 0 1 o 0 1 X = 0 1 o ) and B ' = ('1'"" ... , '1'"" 11). o 0 1 o 0 1 The matrix X (II X (k + I» has rank k, The indeterminacy of this paramet­ rization is usually resolved by requiring g T, = 0. H owever, in this section it is more convenienllo drop I' from Ihe regression; tbat is, partition X SO thaI X, c"
1244,unknown,"thaI X, contains the first k columns, and use the model (6.4,1). Since (X ;X lr' = diag (n~l) w e see thaI the m.l.e.s of T~,."" .... : in the model (6.4.1) are given by (T~,."", ~)' = (X ;X,)-'X ;Y = (y"" .... y.l', One way to express the hypothesis tbat '1'1 =."" = '1', for the roodel (6.1.0 is to use the «k - 1) X (k T 1» matrix C ,= 100 010 000 o - 1 0 o - I 0 1 -) 0 Partition C I = (CII, C ,,) so"
1245,unknown,"o - I 0 1 -) 0 Partition C I = (CII, C ,,) so thaI C II contains the firsl k columns. It is easily cbecked that tbe hypothesis C IB = 0 is teslable (and is equivalenlto the hypothesis T~ = ... = ... ~ for the model (6.4,1). The matrices Hand E 167 MUL nVARlA TE REGRESSlQ , ANAL YSIS defined by (6.3.7) and (6,3.8), w ith X and C, replaced by X I and C II• can be show n 10 equal Ban d W , respective"
1246,unknown,"be show n 10 equal Ban d W , respectively, the ""between-groups"" and ""within-groups"" SSP matrices given in Section 5.3.3a. (See Exercise 6.4.2.) Thus , as expected, the LRT and urr take the same forms as given there. As h varies through all (k-I)-vectors, C! ,b= c varies through aU k-vectors satisfying I~ c, = O. Such vectors are called contrasts and L CiT i = L C; T~ for all 11. Thus , [rom (6.3. "
1247,unknown,"L CiT i = L C; T~ for all 11. Thus , [rom (6.3. I 4), simultaneous confidence intervals for linear combinations a of conlrasts C between group m eans are given by C I T8 E c'Ya± --·-a'Wa L ~ , _ {O ' e'}""2 1- 0. ,_I n, where we have written '1"" =('1'"""".,'1',) and Y' = (j"" ... ,y,). and O. is the upper a critical value of 9(p, n - k, k - \). If a and/or c are known a priori, the adjustments describ"
1248,unknown,"the adjustments described al Ihe end of Section 6.3,3 are appropriate. 6.5 Multiple Correlation 6.5.1 The effect of the meaD In the m odel (6.1.1) it is some times convenient to separate the effect of the mean from the other ""independent"" variables, R ewrite the regression model as Y = !p.'+XB + U, (6.5.1) where (l,X ) now represents an (n x( 1+ q)) matrix of full rank. Without loss of generality "
1249,unknown,"loss of generality we shall suppose the columns of X are each centred to have mean 0, (If ""<,, is replaced by "" (I) - i;l, then (6.5.1) remains valid if ... i replaced by ... + L iiP "") Then [(I') ]-' (""-' 0 ) \X, (1, X ) = 0 (X'Xr' (6.5.2) because X 'l = O. Thus, by Theorem 6.2.3(a), ,1= y is independent of B = (X 'X )-'X'Y = (X'X )-'X '(Y - 1j'). If the mean of Y is estimated by y. then (6,5.1) "
1250,unknown,"then (6,5.1) can be written Y - 1j'= XB + W , (6.5.3) w here W = U - lu' denotes the centred error matrix which, thought of as a column vector, W V , has distribution N .p(O, l:®H ). Here H = l- n- 'l1'. (See Exercise 6.5,1.) ~flJLTIVARLATE ANALYSIS 6.5.2 Multiple rorrelatioo roefficieot Suppose p = 1 so that (6.5.1) becomes Y-Yl=XJHv . 168 (6.5.4) where v - N. (0, u 2 H) and again we suppose the "
1251,unknown,"centred to have zero mean . Define s=(;:: ~:) =n-'(Y';~1')(y_Y1, X) (6.5.5) to be the sample covariance matrix for y and the columns of X . Let R denote the corresponding correlation matrix. Since ~ = (X'X)-IX'y = (X'X)-'X'(y - 91)= SiiS,,, we see that tbe (sample) correlation between y and xli is given by R n = corr(y, XP ) = S12SiiS""/{s""S'2SiiS2,}'12 = {S'2SiiS,ds,,}II2. (6.5.6) Then R., .• is c"
1252,unknown,"Then R., .• is called the multiple cOrTelation coefficiellt of y with x. It can also be defined in terms of the correlations as R~ .• = R'2R;iR:.,. (6.5.7) An important property of the multiple correlation is given by the following theorem. Theorem 6.5.1 The (argest sample correlation between y and a linear combination of the columns of X is given by Ry-o, and is allained by the linear combination"
1253,unknown,"linear combination XIl. w/,e,e Ii = (X 'X)-' X'y is the regression coefficient of yon X . Proof Let ~ denote a linear combination of the columns of X. Then corr' (y, ~) = (JS'S,I?/{s""Ii'Y)· This function is a ratio of quadratic forms in Ii and. by Corollary A.9.2.2. its maximum is given by S""S2iS,dsll. and is attained when Ii = S2iS2' = (X 'X}-'X'y. • Ano ther common notation, which is used for a "
1254,unknown,"(X 'X}-'X'y. • Ano ther common notation, which is used for a single matrix X(II x q) to label the multiple correlation of :(0) with ""12) ••.• ' ""I.). is R,., ..... It is necessary to exercise some caution wben interpreting the multiple correlation, as the following example shows. EXllJIIpie 6.S.1 Consider the case when '23 = sin O. 169 MULTIVARIATE REGRESSION ANALYSIS These correlations will arise"
1255,unknown,"These correlations will arise wben 1«"". X(2). and """"I"" lie in the same plane in R "" and the angle between XII ' and X,2) is o. U II is smaU then r"" and r"" are both small. However. R, '2 = I. This example illustrates the seemingly paradoxical result that. for small II, :l(l) has a low correlation with both x,,, and )<,,), but the multiple correlation coefficient R '.'2 is at its max­ imum. The mult"
1256,unknown,"imum. The multiple correlation coefficient can be used to partition the total sum of square,:; as in (6.2.8). If Y = Xp denotes the filted value of y, and u=y -yl-~ denotes the fitted residual. then the total sum of squares (about the mean) can be expressed as y'y_lIy2 = Y'y+u'D~ which can also be written ns'=ns'R' + f1S'(I-R' ) 1 )' y-. r )I'"" . (6.S.8) Thus, the squared multiple correlation coef"
1257,unknown,"Thus, the squared multiple correlation coefficient represents the propor­ tion of the total Sum of squares explained by the regression on X. 6.S.3 Partial correlation coefficient It is sometimes of interest to study the correlation between two variables after allowing for the effects of other variables. Consider tbe multivariate regression (6.5.3) wilh p = 2 and centre the COlumns of Y so that y. "
1258,unknown,"y. = 9, = U. We can write each column of Y as i= 1, 2, where y, = X(X 'X)-'X'Yr.. is the ""fitted value"" of Y<o . and 0. .. is the ·'residual value"" of Y,o. Since u"". and 0."" are both residuals after fitting X. a correlation between Ym and y"", after ""eliminating"" the effect of X can be defined by (6.5.9) The coefficient '12 .• is called the (sample) paniat COrTelalion coefficient betweell Yr .. and"
1259,unknown,"To calculate tbe partial correlation coefficient, partition the covariance ma trix of Y and X as If we set ( S"" S= S21 S, S; ) (YI"") ~2 = n -, Y~) (Yrll' YIll> X). i.j=1. 2, MULTIVARIATE ANALYSIS 170 then (6.5.10) (See Exercise 6.5.5.) . . If we partition the correlation matrix R similarly. and set '11 = Til - r,R2~ r/' i. j = L 2. then 'I'. can also be written (6.5.11) Another notation, used for "
1260,unknown,"(6.5.11) Another notation, used for a single matrix X(n x q) to describe the partial correlation between x(I) and X(2) given S(3) ... ·' ~'fll ~'I'~~· . In general if X = (X .. X ,) is a partitioned data matrix wtth sample covariance matrix S. we can define the ""partial"" or '""residuar ' covariance matrix of XI after allowing for the effect of X , by SI.., = Sll-S I,S2iS,I' (6.5.12) Note that Sl1' "
1261,unknown,"Note that Sl1' can also be interpreted as the samp le conditional covariance matrix of X I given X"" (A population version of (6.5.12) for the multinormal distribution was given in (3.2.1).) Example 6.5.2 Observations on the intelligence (XI), weight (x,). and age (x,) of school children produced the following correlation matrix [ 1.0000 0.6162 0.8267] R = 1.0000 0.7321 . 1.0000 This in fact sugges"
1262,unknown,"R = 1.0000 0.7321 . 1.0000 This in fact suggests a high dependence (0.6162) between weight and intelligence. How ever, calculating the partial correlation coefficient '12-3 = 0.0286 show s that the correlation between weight and inteUigence is very much lower. in fact almost zero, after the effect of age has been taken into account. The mUltiple correlation coefficient between intelligence and the"
1263,unknown,"two variables is evaluated from (6.S.7) as R I."" = 0.8269. However . the simple correlation '13 is 0.8267, sO weight obviously plays little further part in explaining intelligence. • 6.5.4 Measures of correlation between vectors In the univariate regression model defined by (6.5.1). that is when p = 1, we have seen that the squared multiple correlation coefficient R ' repres­ ents the ""proportion "
1264,unknown,"ents the ""proportion of variance explained"" by the model, and. from 171 (6.5.8), MULTIVARJATE REGRESSION ANALYSIS 1-R'=o'il/y'y. (We assume y is centred so that y = 0.) It seemS reasonable to seek a similar measure for the multivariate correlation between matrices X and Y in the model Y = XB + U . Let us start by writing D = (Y'Ytlu'U. (Again. we assume that Y is centred so that the columns of Y h"
1265,unknown,"centred so that the columns of Y have zero mean.) The matrix D is a simple generalization of 1-R ' in the univariate case. Note that U'U = Y 'PY ranges between zero, when all the variation in Y is ""explained"" by the model. and Y'Y at the other extreme. when no part of the variation in Y is explained. Therefore 1-D varies between the identity matrix and the zero matrix. (See Exercise 6.5.7.) Any en"
1266,unknown,"zero matrix. (See Exercise 6.5.7.) Any ensible measure of multivariate correlation should range between one and zero at these extremes and this property is satisfied by at least two oft-used coefficients, the trace correla­ tion r,.. and the determinant correlation 'd. defined as follows (see Hooper. 1959. pp. 249-250), r~=p- I tr(l- D ). r1 = det(l-D ). (6.5.13) For their population counterparts."
1267,unknown,"For their population counterparts. see Exercise 6.5.8. Hotelling (1936) suggested the ""vector alienation coefficient"" (rA ). which in our notation is detD . and ranges from zero when U=O to one when U=Y. 1f d I •...• d. are the eigenvalues of 1-D then ,1=TId,. 'A=TIO-d,). From this formulation. it is clear that '0 or 'A is zero if just one d, is zero or one, respectively. but 'T is zero only if al"
1268,unknown,"or one, respectively. but 'T is zero only if all the d, are zero. Note that these measures of vector correlations are invariant under commu tation. viz. they would remain the same if we were to define D as equal to (U'U)(Y'yr 1 • 6.6 Least Squares Estimation 6.6.1 Ordinary Jeast squlU'es (OLS) estimation Co nsider the multiple regression model y = ~.,. u. (6.6.1) where u(n x 1) is the vector of di"
1269,unknown,"y = ~.,. u. (6.6.1) where u(n x 1) is the vector of disturbances. Suppose that the matrix X(n x q) is a known matrix of rank q. and let us relax the assumptions MULTIVARIATE ANALYSIS 172 about the disturbances, assuming merely that E(o) = 0, Vlo) =fi. (6.6.2) When 0 is not normally distributed then the estimation of 11 can be approached by tbe method of least squares. Definition 6.6.1 The ordinary"
1270,unknown,Definition 6.6.1 The ordinary least squares (OLS) estimator of 11 is given by (6.6.3) Differentiation shows that p minimizes the residual sum of squares (y-J(Il}'(y-)(Il). (6.6.4) hence the terminology. Note that Ii is exactly the same as the m.Le. of 11 given in Theorem 6.2.1 (with p = 1) under the assumption of normalitJ. and the assumption fi=u'J •. Note that E(Ii) = (X'X)-'X'(J(Il+ E o) = 11 s
1271,unknown,"so thaI p is an unbiased eslimate of II. Also V(tl) =(X 'X)-'(X'fiX)(X 'X)-'. In particular, if fi = u'I •• this formula simplifies to V(tl)=u'(X'Xr'. and in this case Ii has the following optimal property. (6.6.5) (6.6.6) Theorem 6.6.1 (Ga uss-Markov ) Consider the multiple regression model (6.6.1) and suppose Ihe disturbance renns are uncorrelaled with one another. V(o) = u 21 •. Then the OLS es"
1272,unknown,"another. V(o) = u 21 •. Then the OLS estimator (6.6.3) has a covariance malrix w hich is smaller than that of any other linear estimator. • In other words the OLS estimator is tbe best linear unbiased estimator (BLUE). lbe proof of this theorem is outlined in Exercise 6.6.2. 6.6.2 Generalized least squares estimation Wben 0 does not bave a covariance matrix equal to u'J, then. in general. the OLS "
1273,unknown,the OLS estimator is not the BLUE. However. a straightforward transfnr­ mation adjusts the covariance matrix so tbat OLS can be used. Let E(o)=O . V(u)=fi. 173 MULTIVARIATE REGRESSTON ANALYSIS Suppose that fi is known aod consider the transformed model Z=fi-'I2J(1l+V. where (6.1i.7) 'Since V(v) = I. this transformed model satisfies the assumptions of the Gauss-Markov theorem (lbeorem 6.6.1). Thus.
1274,unknown,"Gauss-Markov theorem (lbeorem 6.6.1). Thus. the best linear unbiased estimate of II is given by p = (X'fi-'X)-'X'fi-'I2Z = (X'fi-'X)-' x'a-', and the covariance matrix of P is V(tl) = (X'fi-'X)-'. (6.6.8) (6.6.9) The estimator defined by (6.6.8) is called the generalized least squares (G LS) estimator and is an obvious generalization of the OLS estimator. T he fact that the GLS estimator is the BL"
1275,unknown,T he fact that the GLS estimator is the BLUE can also be proved directly (Exercise 6.6.3). Note tbat (6.6.8) is not an estimator if fi is not known . However. in some applications. although a is unknown. tbere exists a consistent estimator il of fi. Tben il. may be used in place of fi in (6.6.8). 6.6.3 Application to multivariate regression Multivariate regression is an example where OLS and GLS l
1276,unknown,"'arne estimators. Write the multivariate regression model (6.1.1) in vector form , yV =X* 8 v +U"", (6.6.10) ""bere y"" = (Y(l) •...• Yip»' is obtained by stacking tbe columns of Y on top o f one another. and X· = I. ® X. The disturbance U"" is assumed to have m ean 0 and covariance matrix V(U V ) =fi =:I®I.,. rhen the GLS estimator of B V is given by B V = (X*'fi-'X*)-'X*'fi-'Yv = (1:®X'Xr'(:I-'®X')Y"
1277,unknown,"B V = (X*'fi-'X*)-'X*'fi-'Yv = (1:®X'Xr'(:I-'®X')Y v = ll®(X'X)-'X']yv. (6.6.11) Note that (6.6.10) does not depend upon I and hence defines an estimator wbether I is known or nol. In particular, the OLS estimator is obtained when 1: = I. so tbe OLS and GLS estimators are the same in this MLn.. TIV ARlA TE A""'AL YSIS 174 case. Further (6.6.11) also coincides with the m.Le . of B obtained in Theore"
1278,unknown,"Theorem 6.2.1 under the assumption of normality. 6.6.4 Asymptotic consistency of least squares estimators Let us examine the asymptotic behaviour of the GLS estimator as the sample size n tends to ""'. Let X"" 0.2 •... be a sequence of (fixed) ""indepen­ dent"" variables and let u,. u, .... be a sequence of random disturbances. Writing X=X. =(x~ •...• x~)' . 0=0 ... and P =P •• to emphasize the depen­"
1279,unknown,"dence on ,~ suppose thal lim (X~O;;' X. )-' =0. (6.6.12) Then E(Jl.)=P for all n and V(Jl.)=(X~O~'X.)- ' ..... Oas n ..... ""'.Hence Ii is eonsistenl. I! the ""independent"" variahles x, are random ratber than fixed. but still un correlated witb the random disturbances. then the results of Sections 6.6.1-6.6.2 remain valid provided all expectations and variances are interpreted as ""conditional on X """
1280,unknown,"E(u I X)= O and V(u I X)=O do nol depend on X . If we replace condition (6.6.12) by plim (X~O~'Xnr' = O. (6.6.13) (6.6.14) theo consistem.:y holus ill this situation also. (Here plim denotes limit in probability: that is. if A . is a sequence of random matrices and Ao is a constant matrix. then plim An = Ao or An ~ Ao if and only if for eacb 6.6.5. If or P(IIAn -AoII>£)->O as n ->""'. (6.6.15) "" > "
1281,unknown,""" > O. where IIAII = L L a~.) The proof is outlioed ID Exercise (6.6.16) (6.6. I 7) where 'It(q xq) is non-singular. then (X~O;;'X.)- ' tends to zero at rate n-' so that (6.6.12) or (6.6.14) holds. In this situation, witb some further regularity assumptions on X. and/or u •• it can be shown that in fact Ii. is asymptotically normally distributed. with mean P and covariance matrix 175 MULTIVARIATE "
1282,unknown,"175 MULTIVARIATE REGRESSION ANALYSlS n .''It-'. Note that (6.6.16) will hold. for example. if On = ""'I,, and X .. is a random sample from some distribution with mean 11 and covariance matrix 1:. lening '"" = 1:"""" 1111'. 6.7 Discarding of Variables 6.7.1. Dependence analysi~ Consider the multiple regre ion y=xP +"",l+ o and suppose tbat some of the columns of X("" x q) are ""nearly"" collinear. Since ra"
1283,unknown,"Since rank (X) equals the number of linearly independent columns of X and also equals the number of non-zero eigenvalues 01 X'X. we see that in this situation some of the eigenvalues of X'X will be ""nearly"" zero. Hence some of the eigenvalues 01 (X·X)-' will be very large. The variance of the OLS estimator Ii equals .,.'(X'X)-', and so at least some of these regression estimates will have large va"
1284,unknown,"Clearly. because of the high cOllincarity, some of the independent variables are contributing little to the regression. Therefore. it is of interest to ask how well the regression can be explained using a smaller number of independent variables. There are two important reasons for discarding variables here; namely (I) to IOcrease the precision of the regression estimates for tile retained variable"
1285,unknown,"variables. and (2) to reduce the number of measurements needed on similar data in the future. Suppose it has been decided to retain k variables lk < q). Then a natural choice of retained variables is the subset x"" ..... ' •• which max­ Imizes the squared multiple correlation coefficient (6.7.1) Tbis choice will explain more of the variation in y than any other set of k variables. Computationally. "
1286,unknown,"variables. Computationally. the search for the best subset of k variables is not simple, but an efficient algorithm has been developed by Beale et at. (1967). See also Beale (1970). The choice of k, the number of retained variables. is somewhat arbit­ rary. One rule of thumb is to retain enough variables so that the squared multiple correlation with y using k variables is at least 90 or 95% nf the"
1287,unknown,"MULTIVARIATE ANALYSiS 176 squared multiple correlation using al\ q variables (bUl see Thompson, 1978. for some test criteria). Further, aHhough tbere is usually just one set of k variables maximizing (6.7.1), there are often other k-sets which are ""nearly optimal"", so that for practical purposes the choice of variables is not uniquely determined. An alternative method of dealing with multiCOllinea"
1288,unknown,"An alternative method of dealing with multiCOllinearity in a multiple regression can be developed using principal components (see Section 8.8). Example 6.7.1 In a Study of"" = 180 pitprops cut from the Corsican pine tree. Jeffe"" (1967) studied the dependence of maximum compressive strength (y) on 13 other variables (Xl measured on each prop. The strength of pitprops is of interest because they are "
1289,unknown,"The physical variables measured on each prop were XI = the top diameter of the prop (in inches): x, = the length of the prop (in inches): x, = the moisture contenl of the prop, expressed as a percentage of the dry weight; X 4 = the specific gravity of tbe timber at the time of the test. X, = the oven-dry specific gravity of the timber: X6 = the number of annual rings at the top of the prop; x, = t"
1290,unknown,"x, = the number of annual rings at the base of the prop: x. = the maximum bow (in inches); """" = the distance of the point of maximum bow from the top of the prop (in inches); x"",=the number of knOI whorls; X II =the length of clear prop from the top of the prop (in inches): Xu = the average number of knots per whorl; x 13 = the average diameter of the knots (in inches). The columns o[ X have each "
1291,unknown,"The columns o[ X have each been standardized to have mean 0 and variance 1 so that X'X, given in Table 6.7. I, represents the correlation matrix for the independent variables. The correlation between each of the independent variables and y is given in Table 6.7.2. The eigenvalues of X'X are 4.22, 2.38, 1.88. 1.11. 0.9\. 0.82. 0.58. 0.44.0.35 ,0.19,0.05, 0.04, 0.04, and clearly the smallest ones ar"
1292,unknown,"For each value of Ie. the subset of k variables which maximizes (6.7.1) is given in Table 6.7.3. Note that if all the variables are retained then 73.1 % of the variance of y can be explained by the regression. U only k = 6 variables are retained, then we can still explain 71.6% of the variance of y, w hich is (O.716/0.731)X 100% or over 95% of the ""exp­ lainable"" variance of y. =~£ Ioo(::::>C de N"
1293,unknown,"de N::ICC""""'. o-.-.;ttn-f'lC...o 'X:...,. V)NV)N:.G~oNf'1 -X --CC - C-N- Ioo(cic~::::c~cocc I I' , r-""'fOO-~o.x:~..c'C...,. C""OC'-:::-C - ~o.Dr-- (""') o:;t ..... N('I- NV'l-.;t""OVJCC - l<::::co=6=ic:iccc-::;' I I MULTIVARIATE A 'ALYS IS Ta ble 6.7.2 Correlation be­ tween the independen t vari­ ables and y for pitprop data Variable Co rrelation I -0.419 2 -0.338 3 -0.728 4 -0.543 5 0.247 6 0.117 7 "
1294,unknown,"5 0.247 6 0.117 7 0.110 R -0.253 9 -0.235 10 -0.101 II -(J.05S 12 -0.117 13 - 0.153 178 Note that for this data, the optima l (k + I)-set of variables equals the optimal k-set of variables plus one new variable. for k = l. .... 12. Unfor­ tunately, this property does not ho ld in general. Further discussion of this data appears in Example 8.8.l. For the discussion of recent m ethods of discarding "
1295,unknown,"tests see Thompson (1978). Table 6.7.3 V ariables selected in multiple regression for pitprop data k Variables selected R ' I 3 0.530 2 3.8 0.616 3 3.6.8 0.684 4 3,6.8. II 0.695 5 l. 3. 6. B. II 0.706 6 1. 2. 3. 6, 8. II 0.710 7 l. 2. 3. 4. 6. 8, II 0.721 8 1. 2. 3. 4. 6. R. II, 12 0.725 9 1,2,3,4,5,6.8. 11.12 0.727 10 1. 2. 3. 4. 5. 6. 8. 9. 11.12 0.729 11 1. 2, 3. 4. 5. 6. 7,8.9.11. 12 0.729 12 "
1296,unknown,"12 1. 2. 3. 4.5.6.7.8.9. 10. II. 12 0.731 13 1.2.3.4,5,6,7.8.9.10,11,12.13 0.731 179 MULTIV A RIATE REGRESSION ANALYSIS 6 .7,2 Interdependence analysis Conside r now"" situation where q explanatory variables X are considered on their own. rather than as independent variables in a regression. When the columns of X are nearly collinear, it is desirable to discard some of the variables in order to red"
1297,unknown,"variables in order to reduce the number of measurements needed to describe the data effectively. fn this context we need a measure of how well a retained set of k variables. x ....... X i., explains the whole data set X. A measure of how well one rejected variable x;. is explained by the retained variables Xi, ..... lC •• is given by the squared multiple correlalion coefficient R~"" i ..... "". Thus"
1298,unknown,"coefficient R~"" i ..... "". Thus. an overall meas ure of the ability of the retained variables to explain tne data is o btained by looking at the w orst possible case; namely min R ~ ."" .... ll. ,. (6.7.2) where /0 runs through all the rejected variables. Then the best chOIce for the retained variables is obtained by maximizing (6.7.2) over all k -sct. or variables. Comp utationally this problem i "
1299,unknown,"6.7.1. See Beale et at. (1967) and Beale (1970). As in Section 6.7.1 the cho ice of k is somewhat arbitrary. A possible rule of thumb is to retain enough variables so that the minimum R 2 with any rejected variable is at least 0.50. say. FlITther, although tbe best choice of k variables is usually analytically well-defined. there are often several ""nearly optimal"" choices of k varIables maldng the"
1300,unknown,non-unique for practical purposes. Another method of rejecting variables in interdependence analysis based on principal components is discussed in Section 8.7. Exa mple 6.7.2 Let us carry out an interdependence analysis for the 13 explanatory variables of Jeffers' pitprop data of Example 6.7.1. For each value of k = 1. .... 13 the optimal k -set of variables is given in Table 6 .7.4. together with
1301,unknown,"together with the minimum value of the squared multiple correlation of any of the rejected variables with the retained variables. Fo r example. eight variables, nam ely the variables 3, 5.7.8.9, II. 12. 13. are needed to ~ xplain at least 50% of the variation in each of the rejected variables. Note from Table 6.7.4 that the best k-set of variables is not in general a subset of the best (k + I)-set"
1302,unknown,"Further discussion of this data is given in Example 8.7.1. MULTIVARIATE A ALVSlS 180 Table 6.7..J V3Iiables selected in interdependence analysb fOT pitprop data k Variables selected min R 2 wilh any rejected variable 9 2 4.10 3 1,5. 10 41,5.8.10 5 \.4.7.11.12 D 3.4,10, II. 12. 13 7 2.3.4.8.11.12.13 8 ].5.7.8.9. II, 12.13 9 2.4.5.7,8.9,11.12,13 10 \.3.5.6.8.9.10. 11.12.13 II 2.3.S.6.7.8.9.1O.11.12."
1303,unknown,"II 2.3.S.6.7.8.9.1O.11.12.13 12 1.3.4.5.6.7.8,9.10.11.12,13 13 1.2.3.4.5.6.7.8.'1.IU.11.12.1J Exercises snd Complements 0.002 0.050 0.191 0.237 0.277 0.378 0.441 0.658 0.751 0.917 n.9 19 n.927 6.1.1 (Box and Tiao. 1973, p. 439) For the mu luvariate regression model (6.1.1) wilh normal errors. show tbat Bayesian analysis with a non-informalive prior leads to a posterior p.d.L for B wh ich is propor"
1304,unknown,"tional to Inl:+(8 - B )'X'X(8 - B )1 nn . wbere i and 8 are defined in (6.2.2)-(6.2.3). Hence the posterior distribution of B ;"" a matrLx T distribution. 6.2.1 Show that the (n x Il) matrix P defined in (6.2.1) is symmetr ic and idempotent. Show that Pw = w if and only if "" is orthogo nal to all column s of X and that Pw = 0 if and only if .. is a linear combin ation of the columns of X. Hen ce I'"
1305,unknown,"orthogonal to the columns of X . 6.2.2 <a) From (6.2.6) show that I(B.l:) depends on Y o nly through 8 and l:. and hence shOlv that (8 . l:) is sufficient for (B . X). (b) Is 8 sufficient for B wben l: is known'! (c) 1 i sufficient for I when B is known? 6.2.3 Show the following from Theorem 6.2.3(a): (a) The correlation between ~"" and ~., is p""g""I(g""g,,) In. where P,' = u,d(Uj,(TII) 1,2 181 MULTI"
1306,unknown,"u,d(Uj,(TII) 1,2 181 MULTIVARIATE REGRESS ION ANALYSIS (b) The covariance between two rows o[ B is C $ "" ti.> = &.X . (c) Th e covariance between two columns of ti is C $ (il' til')) = O'""G. 6.2.4 Show from (6.2.9) that 8 - B = AU . w here A =(X 'X r'X ' and AA '=(X 'X r'=G . Hence, foUowing the ap­ proach adop ted in Exercise 3.3.4. show that 8 - B . thought of as a column vector, has the distrib"
1307,unknown,"(B - B )V - N (O, X®G) , thus confirming T heorem 6.2.3(a) and Exercise 6.2.3. 6.3.1 Verify formula (6.3.4) for the projection P""~ Hint: let Z = (Z "" Z z) = XC -' = (XC'''. XC ""') and let A = Z'Z. ote that CC -' = I im­ plies C ,C''' = I and C ,C'21=O. Show that P,- P = ZA- ' Z' - ZzA2~ Z; and that P 2 = ZA -'[I. O],(AlJ)-'[I, OJA -'Z '. H ence using (A.2.4g), show that P , -P and P , can both he "
1308,unknown,"[ A "" [Z ,.ZJ A 2 ' A I2 ]rZ'] A ,,(A "") 'A ,z LZ; . 6.3.2 In Theorem 6.3.1 verify that Y~ Pz Y + = UP ,u. 6.3.3 (a) Partition X =(X "" X z). Y =(Y ,. Y z) and B = (8"" B I,). X = (1:."" 1 12). \821 B 22 \x21 ~22 and consider the hypothesis H "":B ,, = O. Let L,(Y; B .1:) denote the likelihood of the whole data et Y under the model (6.1.1). which is given by (6.1.3). and let 1-,(Y ,: B "". B 2 ,.X,,) d"
1309,unknown,"Y, under the model (6.3.6). Show that ma x L, over (X, B ) such that B "" = 0 max L, over C£. B ) max 1-, over (X"". B z,) such that B "" =0 max L z over (I "", B II• B,,) and hence the LRT for H o is the same whether one uses the model (6.1.1) or (6.3.6). (Hint: Split the maximization of L, into two parts by factoriz­ ing the joint density of (Y I, Y zl as the product of the marginal density of Y , ("
1310,unknown,"Y , (which depends on B "". B z,. and I "" alone) times the conditional density of Y zl Y I (which wh en maximized over B 12, B n. I ,z. and I,z. takes the same value, whatever the value of B "", B 2 "" and III') (b) Carry o ut a similar analysis for the hypothesis C ,BM , = D . MVl TTVt\RJATE ANALYSIS lR2 6.3.4 If Z(1I X p) is a data ma trix from N ,,(O.1:). and a is a fixed n -vector. sbow that Z 'a"
1311,unknown,"6.4.1 If a hypothesis matrix C , is testable in the sense of (6.4.2), show thaI the parameter matrix C ,B is uniquely determined in the model (6.1. I) because EY = XB . Show that C , is testable if and only if R oy's criterion (6.4.3) holds. 6.4.2 Ln Example 6.4.1 show that H = Y'P2Y and E=¥ 'PY are the ""between-groups"" and ""within-groups"" SSP matrice , respectively. which were defined in Seclion "
1312,unknown,"were defined in Seclion 5.3.33. (H int: Set ""= 0 and N , = II, ..... -/I, for i = 1.. ... k. Consider the following orlbogonal basis in R"" . Let e = 1. Let f"" have +11(1-1) in the places N,_,+I. .... N, .-j-1. and -I in the ( ,_I + j)th place and 0 elsewbere for j = 2 ..... 11,: i = I ..... k. Le.t g, have I/N._, in the places I. .... I/N, , and -1/11, in the places N, ,+ I, .... ;, with 0 elsewhe"
1313,unknown,"with 0 elsewhere. for 1=2 ..... k. Then verify that P,e = P ,f"" = 0 and P ,g, = g,. so that • P,= L (g;gY'g,~ Similarly verify that Pe = Pg , =0 and P f""=f,, so lhal P = I-n-'U '-P!. Thus. verify that H and E are tbe ""between-groups"" and ""within-groups"" SSP ma trices from Section 5.S.3a.) 6.4.3 Give the form for the simultaneous confidence intervals in Exam­ ple 6.4.1 when the contrast c and/or th"
1314,unknown,"ple 6.4.1 when the contrast c and/or the linear combination of variables a is given a prio,i. 6.5.1 Let U be a data matrix frOID N p (I1.1:) and set W = HU = U-lii'. Show that E (w ;) = 0 for i = 1.. ... /I and that i = j, { ( \-11 ')£, C(w"" w ,) = E(w , w I) = ' ..- -II ~. iI-i. Hence . writing W as 'I vector (see S~cLio"" A.2.S). t1.,t1uce W ""­ N"",,(O.1:@ H ). 6.5.2 (Population mUltiple correlati"
1315,unknown,"N"",,(O.1:@ H ). 6.5.2 (Population mUltiple correlation coefficient) If x is a random p -vector with covariance matrix I partitioned a. (6.5.5). show that the largest correlation between x, and a linear comhination of X2,"""" x., equals (T I t~12l:;i ~21 . 6.5.3 (a) If R is a (2X2) correlation matrix show thaI R, ,=1,,,1. (h) If R is a (3 x 3) correlation matrix show that R i.n = ('i2 -+-r13 -2r 12' "
1316,unknown,"R i.n = ('i2 -+-r13 -2r 12' 1."".,.,)/( 1-riJ). 183 MULT IVARIATE REGRESSION ANALYSIS 6.5.4 Le t R be the correlation matrix of X (II x p). Show that (a) O ... R ,., p""'l; (b) R,., ... , =0 if and only if ""< ,, is uncorrelated with each of It2' ..... >; .. : (c) R,., ... "" = \ if and only if >;"" is a linear combination of .,,), ..... ,p"" 6.5.5 Verify formula (6.5.10) for the partial correlation coe"
1317,unknown,"show that it can be expressed in the form (6.5.11). 6.5.6 Let R be a (3 x 3) correlation m atrix. Show that (a) '12"" = ('12 - 'lJ,Z)I{( J - (;,)(1-ri,))'I2: (b) ' lD may be non -zero wh en "" 2=0 and vice versa: (c) '!2'3and '12 have different signs when '13 '23> T il> 0 or ' 13'23 < "" 2 < 0 . 6.5.7 (a) Sh ow that the eigenvalues d, .... , d"" of 1- D in (6.5.13) all lie between 0 and I. (b) If a an"
1318,unknown,"(b) If a and g are arithmetic and geometric m eans of d, ..... d .. show that ,:;-=a. (c) If a, and g, are arithmetic and geometric me ans of l- d "" .... I- d"". show thal a I = I-a and 'A = g~. (d) H ence show that '1""""~ and 'A""'(l- r~)p. 6.5.8 Let ~ = (' "" X 2) be a random (p + q )-vector witb known me an 11 and covariance m atrix ~. U X 2 is regressed 0 11 X I using then show that 4. = l:2iX,2 ."
1319,unknown,"then show that 4. = l:2iX,2 .• = 1- l:2iI,I1:.!1:,,· The corresponding population values of PT. the trace correlation, and PD. the determinant correlation. are given by and 6.6.1 If (6.6.1) holds. show that E(y) = ~. Show that the residual sum of squares (y - ~)'(y-~) is m inimized when Il = ~. given by (6.6.3). MULTIVARIATE ANALYSfS l84 6.6.2 If t = By is any linear eStimator of P and y salisfies"
1320,unknown,"6.6.2 If t = By is any linear eStimator of P and y salisfies (6.0. l), show that (a) t is unbiased if BX = 1 and hence confum that the OLS estimator is unbiased: (b) the covariance of t is Bfl.B', and hence confirm (6.6.5)-(6.6.6): (c) t=P+Cy. wbere C=B-(X'Xt'X'. and hence show that t is un­ biased if and only if CX =0. If now t is an unbiased estimator and n = ,,'In' show tbat tbe covariance matr"
1321,unknown,"matrix of t is E[(t-Il)(t- Il)'i= E[(t - ~)(t - ~)'] + E[( ~ - Il )(~ - Iln and hence that Ii has smaUer covariance matrix than any other linear unbiased estimator. (This proves the Gauss-Markov result. Theorem 6.6.1.) 6.6.3 If t= By is a linear unb iased estimator of II where y satisfies (6.6.1) and j3 is given by (6.6.8), show that t = j3+Cy where e = B-(X'n-'X)-'X'fl.-'. Hence show that t is un"
1322,unknown,"e = B-(X'n-'X)-'X'fl.-'. Hence show that t is unbiased if and only if ex = 0 and that, if t is unhiased. lhen its covariance matrix is not less than that of a. 6.6.4 If y -Nn(XII.n ) with known covariance matrix n, then show that the GLS estimator is the maximum likelihood estimator of II and that it is normal with mean II and variance (X'n-'X)-'. 6.6.5 To prove the consistency of the GLS estimato"
1323,unknown,"tions (6.6.13) and (6.6.14) it is necessary and sufficient to prove thaI plim ~n.' = (3, for eacb component. i = 1, .... n. (a) Show thaI E(ji""lx.)=Il. v(ji. I Xn)= (X~n;;-t x"" t'· (b) Let ,,~ denote the (i. ;)th element of (X~n~' X,,)- ', which is a function of the random matrix X •. Then . by (6.6.U), plim a~=O. Using Chebyshev's inequality, show that P(\/3n.,-(3;\>e I X,,) < a~/e2. (c) Show tha"
1324,unknown,"P(\/3n.,-(3;\>e I X,,) < a~/e2. (c) Show that for all e> 0, Il > 0 P(I ~n. , - (3; I > e) .. P (Ii3n.; - f3.J > E l a~ < Il)P(a~ < III + P( CT~;;' Il). Using (b) and (6.6.14) deduce that for any fixed e >0. the right-hand side of tbe above equation can be made arbitrarily small by choosing Il small enough and restricting f/ to be sufficiently large. Hence P(I ~n. , - (3; I > e) ~ o a~ n ~ <Xl so t"
1325,unknown,"o a~ n ~ <Xl so that plim 13 •. , = (3;. 7 Econometrics 7.1 Introduction Economic theory postulates relationships hetween economic variables. It is lhe purpose of econometrics to quantify these relationships. TypicaUy, an economic model might predict that one economic variable is approxi­ mately determined as a linear combination of other economic variables. The econometrician is interested in est"
1326,unknown,relationship and testing hypotheses about it. At first 'ighL this framework appears similar to the regression model of Chapter 6. However . several complications can arise in econometrics (and also in other fields) which require special treatment. (t) Heleroscedaslicity The variance of the error term might not be constant. This problem can somet imes be resolved by transforming the data. (2) Auloc
1327,unknown,"the data. (2) AulocorrelatiOlt If the data represents a series of observations taken over time then lhe error terms might be correlated with one another, in which case lime series techniques are appropriate. (3) Depeltdeltce In the usual multiple regression model. y=Xjl+u, it IS assumed that X is independent of the disturbance term u. However, in econometrics this assumption is often not justifiab"
1328,unknown,Tn this cbapter we shall only be concerned with compl ications arising from (3). For this purpose. it is convenient to distinguish two sorts of economic variables. Suppose we are interested in modelling an economic system. Elldogenous variables are variables measured within the system. Their values are affected botb by other variables in the system and also by MULnVARIATE Af\ALYSIS 186 variables o
1329,unknown,"variables outside the system. On the other hand. exoKellous uariables are variables which are measured owside the system. They can still affect the beha\'iour of the system, but are not themselves affected by fluctuations in the system. (Econom ists sometimes also consider the effect of lagged elldogellous variables. that is endogenous variables observed at an earlier time. These variables usually"
1330,unknown,time. These variables usually play the same role as exogenous variables. but [or simplicity we shall not discuss them here.) A typical equation in econometr ics tries to e.'plain one of the endogen­ ous variables in terms of som e of the other endogeno us variables and some of the exogenous variables. plus a disturbance term. An essential assumption in this model states that because the exogenous 
1331,unknown,"determined outside the system. they are ullcorrelared with the disturbance term . This assum ption is nor made for the endogenous variables: one usually assumes that aU the endogenous variables are correlated with the disturbance term. Example 7.1.1 (Kmenta , 1971. p. 563) The supply equation in a 'implified model I)f food consumption and prices is (7.1.1) where Q , =food consumptio n per bead, P,"
1332,unknown,"(7.1.1) where Q , =food consumptio n per bead, P, = ratio of food prices to general consum er prices, F, = ratio of farm prices to general consumer prices. A, = time in )lears (a trend term). and "", i~ a disturbance term. The subscript t = I •... ,20 represents observations over 20 years. Here Q, and P, are assumed to be endogenous; their values represent the state of tbe food market. On the other"
1333,unknown,"tbe food market. On the other hand F , and A, are assumed to be exogenous variables; their values can affect but are not affected by the food market. Of course. the constant term 1 allowing for the mean effect is always taken to be exogeno us. This exam ple is discussed further throughout the chapter. One of the purposes of econometrics in the above regression-like equation is to estimate the para"
1334,unknown,"equation is to estimate the parameters. However. because P, is assum ed to be correlated with u, the usual OLS regression estimates are not app ropriate. and other techniques mus t be sought. 7.2 Instrumental Variables and Two-stage Least Squares 7.2.1 Instrumental .ariahles (IV) estimation Unfortunately. when the independent variables are correlated with the random disturbances. the OLS estimator"
1335,unknown,"187 ECONOMETR IC'S Cons ider the regression-like equation y = X~ + D, (7.2.1) w here ). and 0 are (tl x 1). X is (11 x q). P is (q X 1). and X is correlated with u . An example in which tbe OLS estimator i3 is not consistent is outlined in Exercise 7.2.1. One technique which can be used to overcome this inconsistency is the me thod of instrumental variables (IV). Suppose we can lind a tie"", set of"
1336,unknown,"variables Z (II x q) with the same dimension as X , sucb that Z is uncorre­ lated with the disturbances u. Then Z can be used to construct a consistent estimate of p . M ore specifically. suppose the following assump tions are satisfied for X . D . and the IVs Z : E(u IZ)=O, V(u I Z ) = a'I. plim /I-'Z 'X = 'if. non-singular as /I -> ""'. plim tI 'Z 'Z = (i). non-singular as /1->""'. (7.2.2) (7.2.3)"
1337,unknown,"(7.2.2) (7.2.3) (7.2.4) Th us. the random disturbances are uncorrelated witb one anotber and with tbe instrumental variables Z: however. (7.2.3) states that Z is correlated with X . Tables 7.1.1 and 7.1.2 summarize the types of estimation to be con­ ,ide red io tbis chapter. Definition 7.2.1 The instrumental variable (N) estimator of 1:1. usi/lg rhe ;'.srrumelllS Z . is defined by P* = (Z 'X) 'Z ·"
1338,unknown,"P* = (Z 'X) 'Z ·y. (7.25) T heorem 7.2.1 For the model (7.2.1) satisfying a.~.~umptio""s (7.2.2)- 7.2.4), the instrumental variables esrinraror (7.2.5) is cOfuisrenl. i.e. plimp *= p . Table 7.l.1 General methods: of estimation fo r a single equation Method (1) Ordinary least squares (OLS) (2) Instrumental variables (IV) (3) Two -stage least squares Estimator Ii = (X'X)-'X 'y 11"" = (Z'X)-'Z 'y fl··"
1339,unknown,"11"" = (Z'X)-'Z 'y fl·· = CX' ~T'X' y Location in text (6.6.3) (7.2.5) (7.2.10) MULTIVARIATE ANAI.VSI::\ 18 Table 7.1.2 Methods of estimation for a simuttaneous system of equations Method (A) Single-equation methods (I) Indirect lea"" squares (ILS) (2) Two-stage least squares (2SLS ) (3) Limited information maximum likelihood (L1MU (B) System methods (I) Generalized least squares (G LS ) and Zellner"
1340,unknown,Zellner~s two-stage estimator for seemingly unrelaled regressions (2) Three-stage least squares (3SLS) (3) Full information maximum likelihood (FlML) Prool The IV estimator jl* ~-dl1 be expressed as jl* = (Z'X)- 'Z'(X(l+u) = jl+(Z'X) 'Z 'u. Location in (ext (7.3.19) (7.4.3) Section 1.4.2 (75.3) and (7.5.7) (7.5.9) Section 7.5.3 Thus to show consistency we must 'how that the second term converges i
1341,unknown,"in probability to 0 as ""~""'. Since by (7.2.3) plim (Il-'Z'X)-' = 'l'-'. it is sufficient to show that plim Il-'Z'u= O. oW from (7.2.2), E(,,-'Z'u I Z) = 0 and V(n -'Z'u I Z)="" -'Z' V(u I Z)Z = n-'u 2 Z'Z. (7.2.6) Since. from (7.2.4), plim ,,-'Z 'Z = e, this conditional variance tends to 0 at rate lin; hence (7.2.6) follows (see also Exercise 6.6.5). • Remarks (1) ole tbat although 11* is consisten"
1342,unknown,"Remarks (1) ole tbat although 11* is consistent, it is not necessarily an unbiased estimator of jl. We canno t evaluate the term E«Z'Xt 'Z'u) because X is correlated with u. (2) With some further regularity assumptions on X. Z , and D, it can be shown that 11* is asymptotically unbiased and normally distributed. jl* - N.<Il. n -'uz,p-'8('It')-'). In practice 'lr and 8 can be estimated by ,,-'Z'X a"
1343,unknown,"tively, and 0'2 can be estimated from the residual vector by u2 = (y - Xjl*)'(y - Xjl*)/("" - q). (7.2.7) 189 ECONO~ffiTRJCS ElCllIIIple 7.2.1 (Errors in variables) Consider the simple regression y= Q +/3i.+u. where i is a random sample from N(/J.. uil independent of u which is a random samp le from N(O, 0':>. Suppose. however, that we can only observe ii and y subject to measurement errors, i.e. w"
1344,unknown,"observe ii and y subject to measurement errors, i.e. we observe y = y+ 1' aDd x= x-w. where"" and w are random samp les from N(O, 0';) and N(O. 0';). and x. u, v and w are all independent. Then we are Irying to fit the regression line using the equation y=a + /3x+ £, £= u+ v-/3w. However. since C(E"" x,) = - /3,,::. j = I. .... ,n, the OLS estimator of /3 will not be consistent. Note that v and w ca"
1345,unknown,"variables"" whereas u is an ""error in equation"". Suppose we can find a variable z which is correlated with i.. hut uncorrelated witb u, v. and w . Then using (1. z) as instrumental variables for (1. :x), the rv estimators of /3 and ('( are foond from (7.2.5) to be a* =y-/3*i. (7.2.1<) where .I,. is the ample covariance between z and y. (See Exercise 7.2.2.) This estimate of jl is also found in fact"
1346,unknown,"This estimate of jl is also found in factor analysis (Chapter 9) in slightly disguised form when estimating the parameters for p =3 variables with k = I factor (see Example 9.2.1). Example 7.2.2 In a ""cross-sectional"" study of capital-labour substitu­ tion in the furniture industry (Arrow er al., 1961: Kmenta, 1971. p. 313), the authors examine the relationship )Ii = a + (3.\:1 + l~, where y = log"
1347,unknown,")Ii = a + (3.\:1 + l~, where y = log (value added ""'labour input). x = log (wage rate'"" price of product!. and , runs through different COUll tries. The data is given in Table 7.2.1. The OLS estimates for a and /3 lead to the estimated regression equation (with standard errors in parentheses underneath): y; = -2.28+0.84 x,. (0.10) (0.03) However , as the explanatory variables may contain errors of"
1348,unknown,"ment, it seems preferable to use instrumental variable estimation. The value of z = log (wage rate +price of product) for kniTting-mill products is an exogenous variable for the furniture industry and seems a plausible MULTIVARIATE ANAL YSIS 190 choice of instrumental variable. since it is unlikely to he correlated with measurement errors for x or with the disturbance term in the regression. Table"
1349,unknown,Table 7.2.1 Capital-labo ur suhstitution data (Krnenta. 1971. p.313) Country y x z Un ited States (l.7680 3.5459 3.4241 Canada 0.4330 3.2367 3.1748 New Zealand 0.4575 3.2865 3.1686 Australia 0.5002 3.3202 3.2989 Denmark 0.3462 3.1585 3.1742 Norway 0.3068 3.1529 3.0492 United Kingdom 0.3787 3.2101 3.1175 Colombia -O.llSS 2.6066 2.5681 Brazil -0.1379 2.4872 2.5682 Mexico -0.2001 2.4280 2.6364 Argent
1350,unknown,"Brazil -0.1379 2.4872 2.5682 Mexico -0.2001 2.4280 2.6364 Argentina -0.3845 2.3182 2.5703 The values of z are given in Table 7.2.1. The instrumental variable estimates from (7.2.7) and (7.2.8) (with estimated standard errors) lead to the equation y, = -2.30+0.84 ""'. (0 10) (0.03) 1t will be noted that IV estimates and OLS estimates are very sim ilar. Thu s. in this examp le. the m easuremeot error"
1351,unknown,"7.2.2 'Two-stage least squares (2SLS) estimation The instrumental variable m atrix Z ( .. x k) in Section 7.2.1 is assum ed to have the same dimension as the ""independent"" variable matrix X (n xq), i.e. k = q. H ow ever, if k > q then an extension of 1V estimation may be given using the meth od of two·stage least squares (2SLS ). This me tbod is defined as follows . First, regress X on Z ilsing th"
1352,unknown,"estimates to get a fitted vaJue of X , x = Z (Z'Z )-'Z'X. (7.2.9) Note that X (n x q) is a linear combination of the columns of Z . Second. substitute X for X in the original equation (7.2.1) and use OLS 191 to estimate ~ by /l'""= (X'X)-'X.'y = [X 'Z (Z 'Z )-IZ 'X rl(X 'Z )(Z'Z )-'Z 'y. T hen /l"" is called the 2SLS estimator of /l. ote that since X'X = X'X. w e can write Ii"" = (X'X)-IX 'y. ECONO(l"
1353,unknown,"ECONO(l.1ET RlCS (7.2.10) so that Ilu can be interpreted as an IV estimator using X as the instrumental variables lor X . In {act if k = q then Ii·' reduces to the IV estimator Il* in (7.2.5) (see Exercise 7.2.3). For general k;;. q it can be een that ~ •• is consistent, provided that Z IS uncorrelated with u, plim /I-I Z 'Z = 0 (k x k) non-singular, and plim /I -'Z 'X = w (k x tl) of rank q. (See"
1354,unknown,"q. (See Exercise 7.2.3.) We have not yet commen ted on tbe choice 01 instrumental variables in practice. intuitively, it is clear that we should cboose Z to be as highly correlated witb X as possible, subject to Z being uncorrelated with u. However, in practice it is ohen difficult to determine lhe COT relation between Z and 0 because the random disturbances u arc unobserved. 1n the context 01 mea"
1355,unknown,"1n the context 01 measurement errors (Examples 7.2. I and 7.2.2). the problem of choosing instrumental variables is often avoided by a~suming that the meas urement errors are ""negligible"" compared to the equ ation errors. so that OLS may be safely used. H owever , we shall see in the next section that there are situations in w hich lhe use of instrumental variables estimation does play an importan"
1356,unknown,"7.3 Simultaneous Equation Systems 7.3.1 Structural form Imagine an economic sy tern wh ich can be described by a collection of endogeno us variables y, •..•• y •. Sup pose this system can also be affected by a number o{ exogenous variables x"" .... x.' but these exogenous ,'ariables are not affected by cbanges in the endogenous variables. An economerric model of this econom ic system consists 01 a "
1357,unknown,simultaneous equations in which certain linear combinations of the en­ dogenous variables are approxim ately explained by linear comb inations of some of the exogenous variables. More specificaUy. consider matrices Y (11 x p) and X (n x q). The II components 01 Y and X might represent observations on tbe same unit over different periods of time (longitudinal sludies) or observations on different u
1358,unknown,"sludies) or observations on different units at the same period in time (cross-sectional studies). Suppose that Y and X satisfy p simultaneous MULTIVARIATE A ""J\LYSt$ 192 equations YB+xr = u. (7.3.1) Equation (7.3.1) is called the structural form of the model under study. Here B (p X p) = (/3,,) and r(q x pi = (-Y,j) are parameter matrices and U (n xp) is a matrix of disturbances. The jth column of"
1359,unknown,"the jth structural cquation on n observational units. j = I. .... p. The parameters in each equation are defined only up to a scaling factor. and this indeterminacy .s resolved by taking one clement in each <-alumn ~( t) equal to I. It is usually conve nient to take /3,j = I so that the jth equation can be rewritten as a regression expressing Ihe jlh endogeno us variable in terms of some of the ot"
1360,unknown,"variables. For example. the first equation can then be written (7.3.21 We <hall suppa e one column of X is set equal to 1 in order to allow for an overall mean effect In the specification of the model sOme of the parameters in B and r will be known to be zero. Thus. not all of tbe variables appear in every equation. Also, some of the equations may be exact identities: that is. the parameters are k"
1361,unknown,"parameters are known exactly and the error term< vanish identically for these equations. For the error term U =(u , •.... 0.)' we make the following assumptions for s, t = I. .... /I : E(o,)= O. Yeo ,) = 1:. e(o,. u,) = o. Thus the errors may be correlated between equations. but there is no correlation between different observational units. li r of the structural equations are exact identities. th"
1362,unknown,"equations are exact identities. then the corresponding entries in 1: are known to be O. and we suppose rank (1:) = p - r. When convenient we shall further suppose that U is multinormally distributed. An essential assumption of the model states tbal tl.e disturbance U is lI/1correlated witll tJoe exogenous variables X. However. in general, the disturbance U will be correlated with the endogenous V'"
1363,unknown,"any equation containing more than one endogenous variable. the OLS e timates of the parameters will not be appropriate aod other estimates must be sought. Example 7.3.1 Consider a simple fwo-equation model consisting of an income identity w=c.+z (7.3.3) 193 ECONOMETR ICS and a consumption function c =a+{3 w + n.. (7.3,4) Here w = income , c = consumption expenditure, and z = Don-consumption expend"
1364,unknown,"expenditure. In this model wand c are considered as endogenous vari­ ables (determined inside the model), wher eas z is considered as exogen­ ous (perhaps determined by government policy). All the vectors are of length II, representing observations on n units. This model can be easily put into the form (7.3.1) by setting Y =(w .c), Z =(1.z), U=(O .o) and [ I -131 B= _I I' r= [ 0 -1 -a] o· 1:= [0 0"
1365,unknown,[ I -131 B= _I I' r= [ 0 -1 -a] o· 1:= [0 0]. o (Tn 7.3.2 Reduced form If the structural model (7.3.1) is well-defined. the endogenous variables will be determined by the values of the exogenous variables and the random disturbances. In particular when U vanishes then Y has a unique value (as a function of X ) representing the equilibrium behaviour of the economic model. and any non-zero random di
1366,unknown,"(iolls away from this equilibrium. To solve (7.3.1) for Y we assume B is non-singular. Then (7.3.1) becomes Y = XTI + V, (7.3.5) where we set n = -rB ' (7.3.6) and V = UB -'. Equation (7.3.5) is called the reduced form of the model and n (q x p) is called the matrix of redllced parameters. ote that the row of V are uncorrelated each with mean 0 and covariance matrix (B -'),1:B '= 'It, say. Further"
1367,unknown,"(B -'),1:B '= 'It, say. Furthermore, the disturbances V are uncorrelated with X. so that (7.3.5) satisfies the assumptions of the usual multivariate regression model (6.1. I). Hence we may estimate the reduced parameters n using. for example. the usual OLS estimator (7.3.7) (We shall suppose throughout that X 'X is non-singular so that the reduced parameter matrix n is uniquely defined.) However ."
1368,unknown,"However . the interesting problem is how to transform an estimate of n into estimates of B and r. We shall discuss this problem in the next section. MUL Trv ARIATE ANALYSIS 194 Example 7.3.2 In Examp le 7.3.1, we can solve (7.3.3) and (7.3.4) for w and C 10 get Thus for this model all w = --+--z +--n, 1-/3 1- /3 1-/3 a /3 I c=--+--z+--u. 1-/3 1-/3 1-/3 1112]= [a/(1-/3) a/(1-/3)] ""'"" 1/(1- /3) /3/("
1369,unknown,"1112]= [a/(1-/3) a/(1-/3)] ""'"" 1/(1- /3) /3/(1- 13>' (7.3.8) (7.3.9) (7.3.10) Note that it is clear fIom (7.3.8) that wand u are correlated. sO that OLS estimation applied directly to (7.3.4) is not appropriate. 7.3.3 The identification problem Since we are supposing X'X is non-singUlar, the parameter matrix II for the reduced model (7.3.5) is uniquely defined. However , the parameters of interest"
1370,unknown,"of interest are B and r. sO the question here is whether we can solve (7.3.6) for B and r in terms of II. If so. the structural parameters B and r are well-defined. Equation (7.3.6) can be rewritten IIB = -r or IIIi,,,= - 'Y(j,' j = I, ... , p. (7.3.11) We shall attempt to solve (7.3.11) for B and r in terms of n one colum n at a time. No te that the jth column s lim and 'Y(,) correspond to the pa"
1371,unknown,"parameters for the jth structural equation in (7.3.1). For notational simplicity we shall look at the first equation; solving for Pm and 'Y(II and we suppose that this equation is not an exact identity. Write Ii = Po, and 'Y = 'YO)' Some of the components of Ii(p x 1) and 'Y(q x l) w ill he known to be O. Without loss of generality let the endogenous variables y, •...• y"" and the exogenous variabl"
1372,unknown,"be ordered in sucb a way that = ['Yo] ""Y O ' (7.3.12) Then Po and 'Yo include all the components of Ii and 'Y wbich have not been explicitly set to zero. Let p, and q, denote the lengths of P n(PI X l) and 'Yo(q , x I). respectively. 195 Partitioning lI(q x p) as lI=rnoo 1I0,], lnlO I111 ECONOMETRlCS (7.3.13) where IIoo is (q, X p,), we see that the equation IijI = -'Y become s Doo~o= -'Yo, D, oPo"
1373,unknown,"Doo~o= -'Yo, D, oPo= O. (7.3.14) (7.3.15) Note that if we can solve (7.3.15) for ~o, we can immediately get 'Yo from (7.3.14). Now (7.3.15) consists of q-q, equations, stating that lio(p, x I) is orthogonal to each of the rows of IIoo«q-q,)x p,). Thus, (7.3.15) can be solved uniquely for Po if and only if rank (IIw) = p, -1, (7.3.16) in which case ~o lies in the one-dimens ional subspace orthogona"
1374,unknown,"rows of IIlO • usually normalized by /3"" = 1. Since we are supposing that II is the true reduced parameter matrix for the structural parameters Band r, (7.3.15) holds; that is, tbe rows of 010 SIe p,-vectors lying in the subspace of R O, orthogonal to Po. Hence, it is always true that rank (lIlll)';; p,-L (7.3.17) Also, since II,. is a « q - q » X p,) matrix, it is always the case that q-q,i>rank("
1375,unknown,"q-q,i>rank(IIIO)' Thus it is convenient to distinguish the foUowing three mutually exclusive and exhaustive possibilities: (1) If rank (IIw)< p,-1. the first equation is said to be under­ identified. (2) If rank (II 10) = p,-1 and q -q) = p, -1. the first equation is said to be exactly identified. (3) If rank (IIIO) = p, -1 and q -q, > P, - 1, the first equation is said to be over-idenrified. An e"
1376,unknown,"An equation is said to be identified if it is either exactly identified or over-identified. 7.3.4 Under -identified equations To say tbe first equation is under-identified means that there is more than one set of values for the parameters Po and 'Yo which correspond to a given value of II. Thus, the parameters of the structllral model SIe not uniquely defined; different values for the structural p"
1377,unknown,MULTIVARIATE ANALYSIS 196 same model. Hence no estimation of the structural parameters is possible here. In this case the equation defining the structural model are incom­ plete and the econometrician mus t rethink his model. The problem is similar to the lack of uniquenes.' for the parameters in a regression when the design matrix is singular. We shall assume for the rest of the chapter that none
1378,unknown,"in the system is under-identified. 7.3.5 Exactly identified equations and indirect least squares (II.S) estimation If the first equation is exactly identified then the p, - I equations of (7.3.15) may be solved for the p, - 1 unknown eleme nts of 130. Note tbat in this case n '0 is a «p,-I) x PI) matrix of full rank p, - 1. Also. 'Vn may be found from (7.3.14). Suppose now that n is unknown . A na"
1379,unknown,"Suppose now that n is unknown . A natural estimate of n is the OLS estimate (7.3.18) Then , with probahility I. iT"", will have full rank p,- 1 (the same rank as 0 10), so that the equations ~ 11 = 1 (7.3.19) can be solved for Po and -y"". These estimates are called the illdirecr leasr squares (lLS) estimates of 13 .. and 'Vu. If regularity conditions are satisfied so that Ii is consistent, then Po "
1380,unknown,"7.3,6 Over-identified equations Suppose the first equation is over-identified. Then (7.3.15) consists of q - q, equations to be solved for the p,-1 unknown parameters of 130. where q -q, > p,-1. When n , .. is known exactly. this excess of equations is irrelevant. However, if n is unknown and is estimated by fJ in (7.3.6). then a problem arises. For in general il,o is a «q -q,) x pj) matrix of ful"
1381,unknown,"PI' wbereas n,o has rank p,-I. Thus the equations (7.3.20) are inconsistent and an exact solution for Ii. does not exist. Renee the method of lLS does not work bere and an approximate solution to (7.3.20) must be sought. Techniques to deal with this situation will be considered in Sections 7.4-7.5. 197 ECONOMETRICS 7.3,7 Conditions for identifiability Cond itions for the identifiability of the fir"
1382,unknown,"Cond itions for the identifiability of the first structural equation have been given above in terms of the reduced parameter matrix 0 ,0. Tn practice one is usually more interested in the structural param eters Band r, so it is useful to describe the conditions for identifiability in terms of these parame ters. Partition B = [13"" B,,] o B ,' (7.3.21) and define O «p - PI + q - ql) X (p -1) by O=lr"
1383,unknown,"O=lr:l (7.3.22) Then it can be shown that rank (ll w) = rallk (O )-(p - p;). (7.3.23) (See Exercise 7.3.2.) Thus , the identifiability conditions can be rephrased as follows. For completeness, we give tbe conditions for any struclUral equatioll whicb is not an exact identity. (The identification problem does not arise for exact identities in tbe system because there are no parameters to be estimat"
1384,unknown,"Cons ider the jtb structural equation of (7.3.1). Let p, and 'I, denote the number of components of 13(;) and 'VOl. respectively, wbich are not explicitly set to zero. Let B \;)«p-p,)X(p-l) and r'i'«q-q;)x(p-I) denote the matrices obtained £rom Band r by delering the jth column and by de/ering all the rows ~ . for wh ich i3'i"" O. and all the rows 'V, for which ),,;""""0, respectively. Set O lil=(B \"
1385,unknown,"which ),,;""""0, respectively. Set O lil=(B \""', r'i"")'«p-P;""'q-q,)x(p- l». Theorem 7.3.1 Su ppose the jtll strucrural eql<arron of (7.3.1) is not all exact idenriry. Tlten rltis equarion is (a) under-identified if rank (0 ''') < p - 1, (b) exactly idenrified if rank (0 ''')= p-l and q -q, =p, -1, (c) over-identified if rank (0 1;)= p -1 and q - qi> Pi -1. • Note that q - q, = P, -1 if and only if O"
1386,unknown,"Note that q - q, = P, -1 if and only if O li> is a square matrix. Also q-q;< p,-1 implies lhat rank (O Ul)<p-l, so tbat tbe jth equation is under-identified in this case. Example 7.3.3 Let us illustrate this theorem on the equations of Exa m­ ple 7.3.1. Only the consumption equation (7.3.4) is not an exact identity here. C hecking the second column of B and r we see that P2 = 2. q2 = 1 so MULTIVAR"
1387,unknown,"that B \2l is empty and I',"" = (-1). Thus 0 121 = (-1) is a (I X 1) square matrix of full rank, so this equation is exactly identified. Example 7.3.4 (Kmenta, 1971, p.563) Consider the following model 01 food consumption and prices: Q, =a.-a2P,+a.)D , +U n Q, = b, ~b, P,+b,F,+ b4 A, + "",2 (demand), (supply). wh ere D , i~ disposable income at constant prices and r ranges from I to 20 years. The su"
1388,unknown,"20 years. The supply equation has already been discussed in Example 7.1.1 and the other variables are defined there. The variables Q , and P, are endogenou s. whereas I, D ,. F"" and A, are exogenous. This model can be put into the form (7.3 .1) by setting Ll [ -al -b,] - a, 0 r= 0 -b,' o -b, ote tbat Q, has been interpreted as the ""dependent"" variable in both the demand and supply equations. II is"
1389,unknown,"II is easily checked that, in the notation of Theorem 7.3.1, D'''=C:J so that the demand equation is over-identified and the supply equation is exactly identified. In most practical situations the structural equations will be over­ identified. We now turn 10 methods of estimatinn for such cases, and shall give two cia ses of estimators-single-equation methods and system mell,ods. Single-equation m"
1390,unknown,"mell,ods. Single-equation methods give estimates for one equation at a time, making on ly limited USe of the otber equations in the system. whereas system methods give simultaneous estimates for all the parame­ ters in the system. 199 ECONOMETRtCS 7.4 Single-equation Estimators 7.4.1 Two -stage least squares (2SL.<;) A structural equation can be written a a regression-like equa tion by putting one"
1391,unknown,"putting one endogeno us variable on the left-hand side and all the other variables on the right-hand side, as in (7.3.2). Unfo rtunately, OLS is not usually applicable here because Some of the endogenous variables appear am ongst the explanatory variables, and the endogeno us variables are correlated with the disturbance term. One way to deal with tbis problem is the method of two-stage least squa"
1392,unknown,"is the method of two-stage least squares (2SLS ) given in Section 7.2.2. where we choose our instrumental variables from al/ the exogeno us variables. For notational convenience let us work 00 the first structural equation and take the first endogenous variable as the ""dependent"" variable, so (3"" = I. Th is equation can be written in the form of a regression as Y (I)= - Y:Jl:t:-X fl'VO+ U (I)' (7."
1393,unknown,"Here Y . and X "" denote those endogenous and exogenoll~ variahles. respectively. appearing in the first structural equation (other than YlII)' [n the notation of Section 7 .3, ~'= $ 0, 0') = (J, ~*. 0') and V "" = (y"",. V *). Since X "" is exogenous and hence uncorrelated with the random distur­ bance. it can serve as its own instrument. On the other hand. V . should be replaced in (7.4.1) by its fi"
1394,unknown,"y. = X (X'X )-'X'Y •. Af ter inserting, V. in (7.4.1) we get the new equation (7.4.2) where ii,,, = 0,,, -(Y * -y .) ~ •. U sing OLS on (7.4.2) we get the estima tor (7.4.3) which is called the two-stage least squares estimator of ~*. 'Yo. Note that (7.4.3) can be written [ - ~.J = (Z ""l.}-'Z 'Yco» - -Yo where Z= <Y •. x.,). Then the variance of $ .. , Yo)' given X can be estimated by MULTIVARIATE"
1395,unknown,"MULTIVARIATE ANALYSIS 200 where 6"" is an estimate of (7"" obtained from the residual vector by Il-p,+l The denominator here of course represents the degrees of freedom of the chi-squared distribution that the residual sum of squares would have if Y * were equal to Y *. that is if Y * were non-random given X . (f tbe first structural equation is exactly identified then the 2SLS estimator of 11. and "
1396,unknown,"estimator of 11. and 'Yo reduces to the n..s estimate (Exercise 7.4.J). 7.4.2 Limited information maximum likelihood (UML) Another method. which is applicable when the errors are norm ally distributed. is maximum likelihood estimation. In this section we look not at the likelihood of tbe whole system. but only at the likelihood of those endogenous variables. y"" ...• Y.,. occurring in the first str"
1397,unknown,"tion. We wish to obtain m.l.e.s for the structural parameters 11"" ,. 'V'I>' and Ull' For simplicity suppose that none of the first p, equations is an identity. Then the disturbance terms ¥ flh"" . ,vfp,J in the reduced equations are normally distributed with likelihood 12""w uol-..n exp (-~ tr ""'iH~ VoVCI)' where V ,,=(v'I>'"" "" v'o,,) and ""'00>0 . Wr iting V o in terms of the ob­ served quantities Y"
1398,unknown,"served quantities Y o= (y"") ....• Y'o,') and X=~, X,). /I x [q, +(q-q,l]. the likelihood becomes 12""""'001-"""" exp [ -~ tr w iiJ(Y o- X oII..,-X ,fi"",),(y 0-""""fioo - X,fillll]. (7.4.4) This quantity is 10 be maximized with respect 10 W ,,,,(p, x p, 1. D oo(q, x PI)' and filO«q-q,)xp,) subject to the constTaint that fi,o has r.ank P,-1. The constraints on D imposed by the other structural equatJons t"
1399,unknown,"system are ignored in this maximiZ,!'-tion; _ Denote the resulting m .Le.s by""'"",,, 000, D,o' Then the m.l.e.s of the structural parameters of interest are given by solving ii,clio = 0, normalized by ~"" = 1. say, and setting The details of this maximization are a bit intricate and will not be given ~ .. ""i RhL - trrLA I 201 ECONOMETRICS here. (See. for example . Dhrymes . 1970. pp. 344-34lS.) In p"
1400,unknown,"can be shown that ~o minimizes the ratio 110 Y ;'(I- Xo(X(,X.,)- 'Xu)Y 0110 1l0Y;,(1- X (X'X)-'X'lY oIlu (7.4.5) and that Yo and """" are then obtained by regressing the linear combina­ tion Y ,i-n(n xl) on ""',. Note that the numerator of (7.4.5) is the residual sum of squares for the regression of the linear combination Y c$ on Xo. and the denominator for the regression on X . For this reason the U"
1401,unknown,"the regression on X . For this reason the UML estimate of Il is also called the least variance rario (L VR) estimate. Since ""', is a submatrix of X. the ratio (7.4.5) is always greater than 1. If we write (7.4.5) as Il:'Allo/ll;.CIlo, it is easily seen that ~o is the eigenvector of C -' A corresponding to the smallest eigenvalue. which for convenience we have standardized by all = I. '. An explici"
1402,unknown,"all = I. '. An explicit estimate 01 the covariance matrix of <Il:,. Yo ,), can be given. but we shall not consider further details here. (See. for example. Kmenta. 197 1, pp .567-570.) It is clear that aside from the normalization constraint /311 = 1. the LlML estimator ii .. is a symmetric function of Y,I\ •.... Y'D,)' Hence it does not matter which endogenous variable is chosen as the ""dependent"
1403,unknown,"variable in the first structural equation. This invariance property is not shared by the 2SLS estimator. However. it can be shown (for example. Dhrymes. 1970, p.355) that. under mild regularity condition,. the 2SLS and LIML estimators are asymptotically equivalent (0 one another: thus they are both asymptoti­ cally unbiased with the same covariance matrix. When the first structural equation is exa"
1404,unknown,equation is exactly identified. the LIML estimator is identical to the 2SLS estimator (and hence the same as the ILS estimator). See Exercise 7.4.2. Exa mple 7.4.1 (Kmenta. 1971. pp. 564. 572-573) To iUustrate the 2SLS and LIML estimators let us return to the food consumption and prices model of Example 7.3.4. Twenty observatiolls were simulated from the multinormal distribution using the true par
1405,unknown,"0, = 96.5 - 0.25P, ~ 0.30D, + u"" 0, =62.5 + O.15P, + 0.201'; +O.36A, + II"" with covariance matrix for U, I and u"", = r3.125 3.725] I 3.725 4.645 . (demand). (supply), MULTIVARJATE ANALYSIS 202 Table 7.4.1 Data for food consumption and prices mode l Q, P, D . F, A . 98.485 lOO.323 87.4 98.0 1 99.187 104.264 97.6 99.1 2 102.163 103.435 96.7 ':/9.1 3 101.504 104.506 98.2 98.1 4 104.240 98.001 99.8 11"
1406,unknown,"104.240 98.001 99.8 110.8 5 103.243 99.456 100.5 108.2 Ii 103.993 101.066 103.2 105.6 7 99.900 104.763 107.8 109.8 8 100.350 96.446 96.6 108.7 9 1()2.820 91.228 88.9 100.6 10 95,435 93.085 75.1 81.0 1\ 92.424 98.801 76.9 68 .6 12 94.535 102.908 84.6 70.9 13 98.757 98.756 90.6 81.4 14 105.797 95.119 103.1 102.3 15 100.225 98.451 105.1 105.0 16 103.522 86.498 96.4 1 lO. 17 99.929 104.016 104.4 92.5 "
1407,unknown,"99.929 104.016 104.4 92.5 18 105.223 105.769 1111.7 89.3 19 106.232 113.490 127.1 93.0 20 Table 7.4.2 Estimators (with standard errors) for food consumption and prices modci True coefficient OLS 2SLS LIML 3SLS F1ML Demand equarioll , Constant 96.5 99.90 94.63 93.62 :3 ~ (7.9) (8.0) '"" ~ P -0.25 -0.32 -0.24 -0.23 '"" ~ ~ (0.10) (0.10) "" "" "" "" D 0.30 0.33 0.31 0.31 E E (0.05) (0.05) "" "" '"" '"" SuPVl}'"
1408,unknown,"Constant 62.5 58.28 49.53 52.11 51.94 (12.01) '"" (11.89) (12.75) ..J D 0.J5 0.16 0.24 '"" 0.23 0.24 '"" (0.10) ~ (0.10) (0.11) '"" F 0.20 0.25 0.26 "" 0.23 0.22 (0.05) E (0.04) (0.05) "" 0\ 036 0.25 0.25 '"" 0.36 0.37 (0.10) (0.1)7) (0.08) 203 ECONOMETR ICS Values for D, and F. are taken from Girschick and Haavelmo (1947) and the data is summarized in Table 7.4. L The estimated coefficienlS using variou"
1409,unknown,"The estimated coefficienlS using various procedures are sum marized in Table 7.4.2. Note that 2SLS was carried out using Q, as the dependent variable in each equation. Standard errors are not given for the (inconsistent) OLS estimators and it can be seen how inaccurate these estimates are compared with the other procedures. ote that 2SLS and LlML estimates for the supply equation are identical bec"
1410,unknown,equation are identical because this equation is exactly identified. (See Exercises 7.4.1 and 7.4.2.) The 3SLS and FIML estimatnrs will be described in the next eelion. 7.5 System Estimators 7.5.1 Seemingly unrelated regressinns Before we turn to general system estimators we first consider the special case when only olle endogeno us variable appears in each equalion. With a slightly different notat
1411,unknown,"model as j=l, .... p. (7.5.1 ) where X i(n x q,) denotes those exogenous variables occurring in Ihe jth equation. Note that since only exogenous variables appear on the right· hand side of each equation, this model is already in reduced form. Hence the assump tions of tbe multiple regression model are satisfied and OLS applied to each equation separately will give consistent estimates of the param"
1412,unknown,"parameters. However, since the disturbances are assumed correlated between equations, greater efficiency can be attained by treating the system as a whole. Write the model (7.5.1) as a single multiple regression (7.5.2) where y V = (Yi"", ... ,Yi.,)' denotes staclcing the columns of Y on lOp of one another and set X , 0 ... 0 0 X 2 ,,· 0 X= 0 0 '"" X. MULTIVARIATE i\NALYS1S 204 The covariance matrix"
1413,unknown,"MULTIVARIATE i\NALYS1S 204 The covariance matrix of U V is e (UV )=l:® I=O, say. If l: is known, then estimation of ~ in (7.5.2.) can be carried out by generalized least squares to give the estimate (7.5.3) (See Section 6.6.2.) Note that ~. is also the m.l.e. of ~ when the data is normaUy distributed. The OLS estimate ..i of ~ is given by (7.5.4) that is, i = I, ... , p. (7.5.5) There are two spec"
1414,unknown,"There are two special cases in which the GLS estimate ~ * and the OLS estimate A are the same: (a) X , = ... = X p. In this case (7.5.1) reduces to the multIVariate regres· sion (6.6.1). See Section 6.6.3. (b) 1: is diagonal. See Exercise 7.5.1. In general both the GLS and OLS estimators are consistent. but by the Gauss-Markov theorem ~. has a smaller covariance matrix and hence is preferred. In p"
1415,unknown,"preferred. In practice l: is not usually known, so that the GLS estimate (7.5.3) would appear 10 be of limited use. However , it is possible to estimate l: and to use this estimate of l: in (7.5.3) to give a modified GLS estimate. First, we estimate ~ by OLS, and use the resulting residuals to estimate l: by i;, where (7.5.6) and Using this estimate. n =t® l, in (7.5.3), we get the new estimate of"
1416,unknown,"(7.5.7) which is known as Zellner's two-stage estimator. Under mild regularity conditions the variance matrix of .i v can be consistently estimated by 205 ECONOMETRICS Example 7.5.1 (Kmenta , 197 1, p.527) Consider data on the invest­ ment performance of two firms, General Electric Company and Westing­ house Electric Company. over tbe period 1935-1954. Each firm's in· vestment (1) is related to th"
1417,unknown,"vestment (1) is related to the value of il~ capital stock (e), and the value of its shares (F). The assumed relationship is I, = aC, + 13F, +')' + u"" 1=1935 .... ,1954. The results for General Electric are as follows (standard errors m parentheses): (a) Using ordinary least squares, 1, = O.152e +0.0271=; -9.956. ttl (1261 IJt.31'1 (b) Using Zellner', two·stage method. I, = O. 139C, +0.038F, - 27.7"
1418,unknown,"The r,:slIlts for We stinghouse were as follows: (a) Using ordinary least squares. I, = O.092e, + O.053F, -0.509. 111 .1;1,., j~'lIlftl (EtOOl (b) Using. Zellner's two·stage method, I, = O.058C, + O.064F, - 1.25. IOH~~1 WUl~' P5S) It can be seen that in the case of each of the six coefficients, Zellner's estimate has a lower estimated standard error than does the ordinary least squares estimate. 7"
1419,unknown,"squares estimate. 7.5.2 Three-stage least squares (3SLS) The method of three·stage least squares involves an application of Zellner's estimator to the general system of structural equations. As in Section 7.4.1. write each of the structural equations as a regression·like equation. Y'i.=Z IIi(""+u,,ih j=l, ... ,p-r. Here Z , = (Y •. "" Xu ,) denotes those endogenous and exogenous variables (other tha"
1420,unknown,"(other than Yu.l appearing in the jth equation and 1i0)=( - P*.U)' -Yo.mY represents the corresponding structural coefficients. Also, r denotes the number of exact identities in the system which we omit from considera­ tion. MULTIVARIATE A ALYSIS This system can be written more compactly as y""'= ZA V + U V , where and similarly for A v and U V . The covariance matrix of U V is given by 206 say, wh"
1421,unknown,"V is given by 206 say, which suggests using the method oE generalized least squares. The correlation between the disturbance terms can be eliminated by malting the transformation (7.5.8) Since Z is correlated wilh U V , OLS is not appropriate here. However 2SLS can be used to replace Z in (7.5.8) by its fitted value after regressing on X ; that is, let Z, = (Y *~, X "".j). where and set z, o o This"
1422,unknown,"and set z, o o This substitution leads to the estimate of A v (Z'n-'Z)Z'n-'Y v. where n = I®1. Note that this application of 2SLS , which assumes a knowledge of I. is different {rom the single-equalion application of 2SLS in Section 7.4.1. O[ course in practice n is unknown , but it can be estimated by 0= l:®I. where .. . .. , .. 1/2 G"" i = (y(l) - Z ,8(i) (Y(il -Z,8u,)/[(n - p, + 1-qi )(n - PJ + "
1423,unknown,"G"" i = (y(l) - Z ,8(i) (Y(il -Z,8u,)/[(n - p, + 1-qi )(n - PJ + 1 - q,)] 207 ECONOMETRICS and g("" and gm are the corresponding single equation 2SLS estimators from Section 7.4.1. Then the estimator (7.5.9) is caUed tbe rhree-slage least squares estimator. Under mild regularity conditions it is consistent and the variance can be consistently estimated by For notational convenience we have used tbe "
1424,unknown,"by For notational convenience we have used tbe jth endogenous variable as tbe ""dependent"" variable in tbe jth structural equation. In fact any of the endogenous variables appearing in the jth equation can be chosen as the ""dependent"" variable. However, different cboices will in general lead to different estimates of the structural parameters (beyond the differences of scaling). Example 7.5.2 The a"
1425,unknown,"of scaling). Example 7.5.2 The application of 3SLS to the food consumplion and prices model of Example 7.4.1 is summarized in Table 7.4.2 with Q , chosen as the ""dependen'"" variable for both equations. ote that for the demand equation 3SLS gives the same estimales as 2SLS . This occurs because the supply equation is exactly identified and hence adds nO information when estimating the over-identifi"
1426,unknown,"information when estimating the over-identified demand equations. (See Exercise 7.5.2.) 7.S.3 Full information maximum likelihood (FIML) Maximum likelihood estimalion can also be applied to the whole system of equations. For simplicity suppose there are no exact identities in the system. Then the likelihood of the reduced form residuals is given by 121r'l'I-nn exp [ -1 tr NrJV 'V )] =121r'l'I-"""" e"
1427,unknown,"121r'l'I-nn exp [ -1 tr NrJV 'V )] =121r'l'I-"""" exp [-hr {'I'- '(y - Xll),(y - XII)}]. (7.5.10) This likelihood is then m aximized with respect to '1'> 0 and ll(qx p), satisfying the constraint llB=-r (7.5.11) for some pair of matrices B (p x p) and r(q x p). Each column of B usually normalized by /3 .. = 1, i = 1, ... , p and some of the elements of Band r are known to be 0 from the structural fo"
1428,unknown,"are known to be 0 from the structural form of lhe model. The resulting maximizing value of Band r is called the ft.1I infomla/ioll maximum likelihood (FIML ) estimator of lhe slruclural parameters. Unfortunately this maximization cannot be carried out analytically, and an iterative procedure must be used. ole that (7.5.10) is the likelihood MULTIVARIATE ANALYSIS 208 of all the equations in the sys"
1429,unknown,"of all the equations in the system, whereas (7.4.4) included only some of the equations, and that (7.5.11) is a more restrictive set of conditions on D than those given in Section 7.4.2. It can be shown that, under mild regularity oonditions, for large n the FlML estimator is asymptotically the same as the 3SLS estimator; hence they are both asymptotically unbiased with the same covariance matrix."
1430,unknown,"(See Dhryme s, 1970, p. 371.) Exam ple 7.5.3 Consider again the food supply and demand model of Example 7.4.1. The FIML estimates are given in Table 7.4.1. Note that the FIML and LIML estimates of the demand equation are identical because the supply eq uation is exactly identified and system methods offer no advantage over single equation methods in this situation. (See Exercise 7.5.3.) Also, the "
1431,unknown,"equation are similar to one another. 7.6 Comparison of Estimates To summarize this chapter, we compare the different estimators u ed in a simultaneous system of equations. First. OLS and GLS applied directly to the system generally lead to biased and inconsistent estimates. Single­ equalion methods such as 2SLS and L1ML are consistent but ignore the information provided by the system as a whole. O"
1432,unknown,"system methods such as 3SLS and FlML take this information into account and have smaller asymptotic variances than single-equation methods. However, system methods require larger samp le sizes, more complicated calculations, and mOre computing time. The asymptotic advantage of system methods persists in small samples to some extent, although the improvement over single-equation methods is not so m"
1433,unknown,"is not so marked here. However. system methods. especially FTML, are more sensitive to mis-specification of the model (for example omitting a relevant explanatory variable from some of the equations). 10 particular, 2SLS is more robust than the other methods to this sort of error. For more detailed comparisons, see. [or example, Kmenta (1971, pp. 381-385) or Dhrymes (1970. pp.372-380). Exercises a"
1434,unknown,"Exercises and Complements 7.2.1 (Inconsistency of OLS estimator) Consider the equation y = )$+ u. Suppose that the rows of X(n x q) are a random sample from some distribution with meaD 0 and covariance matrix I> 0, and suppose that 209 V(u I X )= 0-2 1., E(u lX)=X9, where 9 = (0, .... ,O.)':t 0 does not depend on n (a) Show that E(u)=O, V(u) = (,,2+ 9'1:9)1., ECONOMETRICS so that the random distur"
1435,unknown,"ECONOMETRICS so that the random disturbances have mean 0, constant variance, and are uncorrelated with one another, but are correlated with X . (b) Show that the OLS estimator of p satisfies (c) Show that EolIX)=fh-9, p = P + (X'X)-'X'u. vol I X ) = o-'(X 'X )-', T hus, using Exercise 6.6.5. deduce that plim P = P + 9, and hence that Pis inconsistent. 7.2.2 (a) In Example 7.2.1, show that the OLS "
1436,unknown,"respectively, of /3 and a are given by ~ = s,,/I'm a = y - ,lx /3*= •• ,/ • .,. a*=y-/3$i (OLS) , (IV). (b) Show that C(y"" x,) = /3(T~ and hence deduce that plim S = /30-11(0-1 ~ IT;), so that B is inconsistent if /3'"" O. (c) If /3'"" 0 and IL"" O. show that a is also inconsistent. (d) Supposing C(x,. z,)= u"" "",O and C(E,. z.)=O. show that /3* and a * are consistent estimates of /3 and a. 7.2.3 (a) "
1437,unknown,"7.2.3 (a) If the ""independent"" variables X (""X q) in (7.2.1) are regres­ sed on new variables Z (n x k) using the multivariate regression model x = zr+ v , where r(kx q) is the parameter m atrix, show that thc OLS e<timate of r is r = (Z 'Z)-'Z'X (see Section 6.6.3) and that the fitted value of X is given by (7.2.9). (b) Lf k = q and Z 'X is non-singular, show that the 2SLS estimator p'­ in (7.2.1"
1438,unknown,"in (7.2.10) reduces to the IV estimator 1\"" in (7.2.5). (c) For k;;. q, show that p.' is consistent, provided that Z is uncorrelated with D , plim ""-'Z 'Z = 9 (k x k) non-singular, and plim n- 'Z'X= W (k xq) of rank q. 7.2.4 Show that the 2SLS estimator 11·' in (7.2.10) is unaltered if Z( .. x k) is replaced by ZA, wher e A is a (k X k) non-singular m atrix. \1ULTIVARIATE ANALYSIS 210 7.3.1 Suppos"
1439,unknown,"\1ULTIVARIATE ANALYSIS 210 7.3.1 Suppose the jth structural equa tion of an e""onomie system in­ cludes only one endogenous variable. Sbow that this equation is iden· tified. If it is also supposed that all the exogenous variables are included. show thaI tbis equation is exactly identified. 7.3.2 Partition the structural parameter matrices B and r as in (7.3.21) and define D =(B '"" ri)' as in (7.3."
1440,unknown,"and define D =(B '"" ri)' as in (7.3.22). Partition B -' as _ , = rp""'"" p(lV] BloW) B (n ' where p,n, is a p,-vector. Expanding 88-1 = 1 and rn-I = n. show that 8 18 ,fI)=0. Hence I"".B (ll)=I1I0• (0. O )B -' = ro m in le-•• J. nIl Since B -' is non-singular, D . CO . 0 ) and the above m atrix all nave tbe same rank. H ence show that rank (0) = p - p , + rank (0 In). 7.3.3 Consider a m odel consisti"
1441,unknown,"7.3.3 Consider a m odel consisting of two simultaneous equations Y41l= a 1x.1)+u(1)' Y(1,=b1Y C2l+ b'2 ~ l + b""l. xn\+ Ucll' whe re YO)' YC!) are endogeno us and ~ nt Xc2 ) are exoge nou s. (al Describe the structural parameter matrices B and r for this model. (b) Write these equations in reduced form. (c) Show that the first equation is over-identified and the second is under-identified. (d) What"
1442,unknown,"(d) What happens to the identifiability if a term a~"",,) is added to the first equation? 7 ,4.1 Supp ose that the first structural equation of an economic system is exactly identified (so lbat p,-I = q - q,). _ (a) Using Exercise 7.2.4 show that the instrumental variables (Y •• X u) in the 2SLS estimator (7.4.3) can be replaced by X =(X "". X ,). Thu s. we can write - ~. = (I. O)(X'Y *. X'X.,)-'X'Y"
1443,unknown,"can write - ~. = (I. O)(X'Y *. X'X.,)-'X'Y""10 -.yo = (0, I)(X 'Y *' X 'x,.t'X 'Y<I"" in tbe same way that (7.2.10) can be sim plified to (7.2.5). 211 ECO N OMETRlCS (b) Show that (~:)~IJ = - (~o) = (~)(O.I)(X'Y*. X 'Xo t'X'y"""" where ( fI ~, fI ;0)' = (X'X)-'X '(Y(I), Y ,.) is tbe OLS estimate of the reduced parameter matrix and ~*, .yo are given in (al. (Hint: pre-multiply this equation by (X 'X ) "
1444,unknown,"equation by (X 'X ) and, using X (I, 0)' = Xo , sbow that it reduces to an identity.) (c) Hence deduce that the 2SLS estimates of 110 and Yo coincide with lLS estimates (7.3.19) in this situation. 7.4.2 If the first structural equation is exactly identified, show that the restriction rank (010) = P, - 1 is irrelevant in maximizing the likelihood (7.4.4). Hen ce deduce that the LIML estimates of p "
1445,unknown,the fLS estimates in this case. 7.4.3 (a) In Example 7.4.1. show that if the covariance matrix of the sltuctural form disturbances is 1:= p.125 3.725] 8.725 4.645 . then the covariance matrix of the reduced form disturbance~ is v=[ 4 -2]. -2 2 (b) Calculate the matrix of reduced form parameters using the equa­ tion n = -rB-J • 7.5.1 Cons ider the system o[ seemingly unrelated regressions (7.5.1). 
1446,unknown,"1: is diagonal. then show that the OLS and GLS estimates of the param eters are the sam e. 7.5.2 (a) Consider the following two seemingly unrelated regressions y"", = X ,11(1) + ""<I)' Ym = X ,llm + X 2Q + ""<2). whe re Xi X 2 = 0. Sbow tbat the OLS estimates of P'"" and Pm equal the GLS estimates. but the OLS estimate of Q does not in general equal the GLS estimate. Also. show that the first equation"
1447,unknown,"and the second is exactly identified Hint: let 1:(2 x 2) denote the known or estimated covariance matTix for the disturbances. Write X = [~' o X , MULTIVARIATE ANAL YS1S 212 Using (7.5.3), show that the GLS estimates of the parameters are (b) Consider a general two-equation economic system where the first equation is over-identified and the second is exactly identified. Show that the 2SLS estimate"
1448,unknown,"the 2SLS estimates equal the 3SLS estimates for the structural parame­ ters of the firs' equation (but not the second). (Hint: see Exercise 7.4.1.) (c) Generalize this result to a p-equation system with no identities, where the first equation is over-identified and all the otbers are exactly identified. 7.5.3 Consider a p-equation economic system with no identities, where the first equation is ove"
1449,unknown,"the first equation is over-identified and all the others are exactly iden­ tified. Show that the LIML and FIML estimates of the structural parame ­ ters of the firs' equation are equal. (Hint: Write the likelihood as L o(lloo, "" 10, 'l'oo)L,(llul, """". '1'), where the first factor is the density of V 0 = ("""" t, ... , "" (P')' and the second factor is the density of V I I Y o. Show tbat in this situa"
1450,unknown,"(7.5.11) merely states that rank (ll1O) = P, - t, and hence the second factor takes the same value when maximized over ""01> ""II' ""'0.' '1'"". for any values of the parameters in the first factor. (See Exercise 6.3.3.) The first factor is simply the likelihood (7.4.4).) 7.5.4 (Kmenta, 1971 , p. 530) A set of three ""seemingly unrelated regression equations"" is specified as Y CII = 0: 1 + (31X(U + U o"
1451,unknown,"Y CII = 0: 1 + (31X(U + U o ). y (2) = Q2 + fl2""<2l + ~2)' Yut = OJ + fl,ll("" + 0 (3). The covariance matrix of the disturbances is known . Consider the es­ timator of fl, obtained by using Zellner's method on all three equations, and the estimator obtained by using this method on just the first two equations. Compare tbe variances of the two estimators. 8 Principal Component Analysis 8.1 Introduc"
1452,unknown,"Analysis 8.1 Introduction Chapter I has already introduced the open/closed book data, which involved the scores of 88 students on five examinations. This data was expressed in Table L2.l as a m atrix having 88 rows and five columns. One question which can be asked concerning this data is how the results on the five different examinations should be combined to produce an overall score. Various answ"
1453,unknown,"produce an overall score. Various answers are possible. One obvious answe r would be to use the overall mean , that is the linear combination (x I ~ X2 I XJ -I x, + x,)/5. or, equivalently, I'x, where I is the vector of weights ~l =GHH)'. But can one do better than this? That fs one of the questions that principal component analysis seeks to answer. We call a linear combination I'x a standardized "
1454,unknown,"linear combination I'x a standardized linear combination (SLC) if I l~ = I. This teChnique was developed by H otelling (1933) after its origin by Karl Pearson (190 I). As a first objective principaJ component analysis seeks the SLC of the original variables which has maximal variance. In the examination situa­ tion this might seem sensible-a large variance ""separates out"" the candidates, thereby e"
1455,unknown,"candidates, thereby easing consideration of differences between them . The students can then be ranked with respect to this SLC. Similar considerations apply in other situations, such as constructing an index of the cost of living. More generally, principal component analysis looks for a few linear combinations which can be used to summarize the data, losing in the process as little information as"
1456,unknown,"process as little information as possible. This attempt to reduce dimen­ sionality can be described as ""parsimonious summarization"" of the data. For example, we might ask whether the important features of the MULTlVARlATE ANALYSLS 214 ope n/closed book data can be summarized by. say, two linear combin a­ tions. If it were possible. we would be reducing the dimen sionality from p =5 to p=2 . 8.2 D "
1457,unknown,"8.2 D efinition and Properties of Principal Components 11.2.1 Popolation principal component, Section 1.5.3 introduced the so-called principal component transforma­ tion in the cnntext of sample data. This was an orthogonal transformation which transforms any sel of variables into a set of new variables whicb are uncorrelated with each other. Here we give its population counterpart. Definition 8.2"
1458,unknown,"Definition 8.2.1 lf x is a random vecror with mea II j1 WId covariance matrix 1:. thell ti,e principal component transformation is tile Iralls(orma ­ riol! x-> y =r'(~ - j1). (8.2.l) ... lIere r is orthogollal, I""1:r = A is diagollal and A ,;;' A,;. ... ;;. A.;' O. The strict posilivity of tl,. eigellvalues A, is guarallteed if 1: is positive defi""ite. TI,is represelltation of I follows from Ille "
1459,unknown,"rheorem (TIleorem A.6.4). The ith principal component of x ma y be defined as Ihe i th elemenr of tile veClor y. namely as (8.2.2) Her"" 'YU ) L, rhe ith columll of r, and may be called rhe ith vecror of principal component load ings. Ti,e (uncrio"" y. may be called the last principal compO llellt of Y. Figure A .1O.2 gives a pictorial representations in two dimensions of the principal componen ts f"
1460,unknown,"Example 8.2.1 Suppo e tbat x) and X2 are standardized to have mean 0 and variance 1, and have correlation p. The eigenvalues of this covariance matrix are 1 ± p. If p is positive then the first eigenvalue is A) = 1 + p, and the first eigenvector is (I, 1). Hence the first principal component of x is y, =T""'(x\ +x,), which is proportional to the mea n o( the elements of x. Note that y, has variance"
1461,unknown,"variance (1 + pl, which equals the first eigenvalue. The second principal component corresponding to the second eigenvalue I-p is Y2 = T'f2(x,-x2 ). which is proportional to the difference between the elements of x. and has 215 PRTNCTPAL COMPONEr-.~ ANALYSIS variance (J - pl. wbicb equals tbe second eigenvalue. If p is negative. then the order of the principal componen ts is reversed. For a p-vari"
1462,unknown,"extension of this exam ple. see Example 8.2.2. In the above example l', and Y2 are uncorrelated. This is a special case of a m ore geoeral result, w hich is proved in part (c) of the following theorem . Theorem 8.2.1 If x - (11.1:) and y is as defined in (8.2.1), Illell (a) E(y,) = 0; (b) V (y,) = Ai: (c) C(y,. YI)=O. iF j: (d) V(l',);;' V(l',);. ... ;. V(Yp);'O: p (e) I V (y,)=trI : • (f) n V (y,"
1463,unknown,"• (f) n V (y,)=i1:i . i""""l Prool (a)-(d) follow from Definition 8.2.1 and the properties of the expectation operator. (e) follows from (b) and the fact that II I is the .<um of the eigenvalues. (f) follows from (b) and the fact that 11:1 is tbe product of the eigenvalues. • It is instructive to confirm tbe results of the above theorem with respect to Example 8.2.1. No te p articularly that V (y,)+"
1464,unknown,"to Example 8.2.1. No te p articularly that V (y,)+ V(Y2)=2, and that V(l',) x V(Y 2) = (J _p2), thus confirming parts (e) and (f) of the theorem. Section (d) of Theore m 8.2.1 states that l'\ has a larger variance than any of the other principal components. (Here and elsewhere we shall use ""larger"" som ew hat loosely to mean ""not sm aller"".) H oweve r, we sball now prove a stronger result. Theorem"
1465,unknown,"Theorem 8.2.2 No SL C of x has a variallce larger than A ,. tile variallce of the /irst principal component. Prool Le t the SLC be a'x. where a'a = 1. We may write (8.2.4) wbe re 'Y(1), ••.• 'Y(P) are the eigenvectors of 1:. (A ny vector can be wrinen in this form s ince the eigenvectors constitute a basis for R P.) oW , if a = a/x, then (8.2.5) MULTIVARIATE ANALYS1S 216 using the spectral decompo"
1466,unknown,"using the spectral decomposition of X (Theorem A .6.4). N ow Y(nYu> = Ii"". the Kronecker delta. Inserting (8.2.4) into (8.2.5) w e find that V (a) = LA ,c? (8.2.6) But we know that ~ c'=1 L., , (8.2.7) since a is given by (8.2.4) and satisfies a'a = I. Therefore. since A I is the largest of the eigenvalues. the maximum of (8.2.6) subject to (8.2.7) is '\,. This maximum is obtained wben c, = 1 and "
1467,unknown,"Therefore V(a ) is maximized when a=y(1). • For a multinormal random vector, the above theorem may be inter­ preted geome trically by saying that the first principal component repres­ ents the major axis of the ellipsoids of concentration. (See Figure 2.5.1). A similar argument shows that tbe last principal component of x has a variance which is sma Her than that of any other SLC. The intermediate"
1468,unknown,"components bave a maximal variance property given by the following theorem. Theorem 8.2.3 iJ a = a'x is (I SLC of x which is l"",coTTelated with the firSl Ie principal compOne""lS of x, lhen Ille variance of a is maximized whet! a is the (k + l)th principal component of x. Proof We may write a in the form given by (8.2.4). Since a is uncorre­ lated withy(,)"" for i = 1 •... , Ie. we know that a'Ym = "
1469,unknown,"lated withy(,)"" for i = 1 •... , Ie. we know that a'Ym = 0 and therefore that c,=O for i = I, ... , k. Therefore. using the same argument as in (8.2.6) and (8.2.7). the result follows. • Example 8.2.2 We now extend the bivariate example of Example 8.2.1. Suppose that X is the equicorrelation matrix, viz. X= (I-p)l+pU'. If p>O then A ,=i-(p - l)p and A, = A]= .•. = Ap =l-p. The eigenvector correspo"
1470,unknown,"corresponding to A, is 1. Hence if p > 0, the first principal componenl is proportional to p -'l'x. tbe average of the x. This may be taken as a measure of overall ""size"", while ""shape"" may be defined in terms of the other eigenvectors wbich are all orthogonal to 1 (i.e. vectors containing some negative signs). (For a further discussion of size and shape see Section 8.6.) • 1t is clear that a'x al"
1471,unknown,"Section 8.6.) • 1t is clear that a'x always has the same variance as -a'l<. and therefore that in terms of variance we are uninterested in the difference between the two vectors 8 and -8. As mentioned before. we consider only SLCs . so th,1t La ~ = 1. However. some authors scale a differently. e.g. in such a way that L a; is equal to the corresponding eigenvalue. 2i? PRlf'..""C £PAL CO l\.{P()NENT "
1472,unknown,"2i? PRlf'..""C £PAL CO l\.{P()NENT AN ALYSIS 8.2.2 Sample principal components We turn now to consider the sample. based counterpart of the previou.< section. thus building on Section 1.5.3. Let X = (x ••...• ,,"")' be a sample data matrix, and a a standardized vector. Then Xa gives 11 observations on a new variable defined as a weighted sum of the columns of X . The sample variance of this new vari"
1473,unknown,sample variance of this new variable is a'Sa. where S is the sample covariance matrix of X . We may then ask which such SLC has the largest variance. ot surprisingly the answer is that the SLC with largest variance is the first principal component defined by direct analogy with (8.2.2) as (8.2.8) Here 1:<1) is the standardized eigenvector corresponding to tbe largest eigenvalue of S (i.e. S =GLG' 
1474,unknown,eigenvalue of S (i.e. S =GLG' ). This result may be proved in the same way as Theorem 8.2.2. Similarly. the ith sample principal component is defined as y(o) = (X -li')Il(;). and these comp onents satisfy the straightfor­ ward sample extensions of Theorems 8.2.1-8.2.3 (see Exercise 8.2.5). Putting the principal components together we get y = (X - li')G . Also X -li' = YG' since G is orthogonal; th
1475,unknown,"y = (X - li')G . Also X -li' = YG' since G is orthogonal; that is. G has transformed one (11 x p) matrix (X -ti') into another one of the same order (Y). The covariance matrix of Y is given by where Sy = 1I-ly'RY = II-'G '(X - li')'H(X - li')G = tI-'G 'X 'HXG = G 'SG = L. tbat is, the columns of Yare uncorrelated and the variance of Ym is ~. The rth element of Y(1)' y"". represents the score of the"
1476,unknown,"component on the rth individual. In terms of individuals we can write the principal component transformation as Often we omit the subscript r and write y; = g(.,(x-i) to emphasize the transformation rather than its effect on any particular individual. MULTIVARlATE ANA. LYSts 218 Example 8.2.3 Consider the open/closed book examination data (rOm Table 1.2.t. It has covariance matrix 302.3 125.8 100."
1477,unknown,"302.3 125.8 100.4 105.1 116.1 170.9 84.2 93.6 97 .9 s= 111 .6 ltO.8 120.5 217.9 153.8 294.4 and mean i' = (39.0, 50.6, 50.0. 46.7. 42.3). A spectral decomposition of S yields principal components y, = 0.51x, +0.37.>:.,+ 0.35x, +0.45x. + 0.53.<,-99.7. Y2= 0.75x, + 0.21x. -0.08x,-0.30x, -0.55x,+ 1.5. y, = -0.30.<, +0.42x2-+0.15x, +0.60x.-0.60x., -19.S. y. = 0.30x,- 0.78x2 -O.OOx, + 0.52x, -0.18x,+ 1"
1478,unknown,"y,= 0.08x, +0.19x2 -0.92x, +0.29x, +0.1 5x,+ 13.9, with variances 679.2, 199.8, 102.6, 83.7, and 31.S, respectively. Note that the first principal component gives positive weight to all the variables and thus represents an ""average"" grade. On the other hand, the second component represent a contrast between the open-book and closed-book examinations, with the first and last examinations weighted m"
1479,unknown,Of course the difference in performance due to the two types of examina· tion ;s also confounded with the differences between the individual subjects. *8.2.3 Further properties of principal components Tbe most important properties of principal components have already been given in Theorems 8.2.I-S.2.3. along with the corresponding state· ments for sample principal components. We now turn to examin
1480,unknown,"other useful properties, each of which is briefly summarized as a heading at the beginrung of the relevant section. (a) The sum of the first k eigenvalues divided by the sum of all t',e eigenvalues. (.I., + ... + .I.. )/(.1., + ... + .I..). represents the ""proportioll of total variation"" explained by the first k principal components. The ""total variation"" in this statement is tr I, and the rationa"
1481,unknown,"statement is clear from part (e) of Theorem 8.2.1. A justification for the use of trl: as a measure of multivariate scatter was given in Section 1.5.3, 219 PRlNCJPAL COMPONENT ANALYSIS and the proportion 01 total variation defined here gives a quantitative measure of the amount of information retained in the reduction from p to k dimensions. For instance, we may say in Example 8.2.1 that the first"
1482,unknown,"component ""explains"" the proportion 4(1 + p) of the total variation, and in Example 8.2.2 the proportion explained is {I +(p -1)p}/p. In Example 8.2.3 the first principal component explains 679.2/1097.1 =61.9% of the total variation. and the first two components SO.l %. (Compare this with the fact that the largest of the original variables comprised only 302.3/1097.1 =27 .6% of the total variation"
1483,unknown,"and the largest two only 54.4%.) (b) The principal compone lllS of a ralldom vector are 1101 scale·illvariallt. One disadvantage of principal component analysis is that it is /lot scale invariant. For example, given three variables, say weight in pounds , height in feet, and age in years, we may seek a principal component expressed say in ounces. inches, and decades. Two procedures seem feasible: "
1484,unknown,"feasible: (a) multiply the data by 16.12. and t\;, respectively, and then carry out a principal component analysis; (b) carry out a principal component analysis and then multiply the element' of the relevant component by 16, 12. and .'0. Unfortunately procedures (a) and (b) do not generally lead to the same result. This may be seen theoretically for p = 2 by considering 1=( u~ fWl:2.). PU1U2 U2 wh"
1485,unknown,"1=( u~ fWl:2.). PU1U2 U2 where p >0 . The larger eigenValue is .I., =i(~+~)+1d, where d = {(O'i-0'~f+40'fO'~p2}'n. with eigenvector proportional to (a"" a.)=(0'~-0'1+d, 2fXT,O'') (8.2.9) (see Exercise 8.1.1). When 0',/0'2 = 1. the ratio u2/a, given by (8.2.9) is unity. If O'c = 0'2 and the first variable is multiplied by a factor k, then for scale·invariance we would like the new ratio a.,lu, to be"
1486,unknown,"changing 0', to kO', in (8.2.9) shows that this is not tbe case. Alternatively. the lack of scale·invariance caD be examined empirically as shown in the following example. (See also Exercise 8.2.4.) Example 8.2.4 From the correlation matrix of the open/closed data. the first principal component (after setting the sample mean to 0) is found to be 0.40u, +0.430.2 +0.50""3 +0.46 .. _ + 0.440.5 , MULTI"
1487,unknown,"be 0.40u, +0.430.2 +0.50""3 +0.46 .. _ + 0.440.5 , MULTIVARiATE ANALYSJS 220 Here U"" ... , Us are the standardized variables. sO thaI t~ = :o;/s,. Re­ expressing the above componen ts in terms of tbe original variables and standardizing, we get Y ,= 0.023x I +0.033x,+ 0.048x] + 0.031 X4 +O.026x,. or y! = 13.4y, = 0.31x, + 0.44x2+0.64x3 +0.42x. +0.35xs· The first component of the odginal vadables ob"
1488,unknown,"The first component of the odginal vadables obtained from the covariance matrix is given in Example 8.2.3 and has different coefficients. (The eigenvalues are also different-they account for 6l.9, 18.2,9.3,7.6, and 2.9% of the variance when the covariance matrix is used and for 63.6, 14.8, 8.9, 7.8, and 4.9% when the correlation m atrix is used.) Algebraically, the lack of scale in variance can be"
1489,unknown,"Algebraically, the lack of scale in variance can be explained a~ follows. Let S be the sample covariance matrix. Then if the itb variable is divided by di' the covariance matrix of the new variables is OSO , whe re 0 = diag (di'). However , if x is an eigenvector of S, then 0 -'"" is 1101 an eigenvector of OSO . In other words, the eigenvectors are not scale invariant. The lack of scale-invariance "
1490,unknown,"invariant. The lack of scale-invariance illustrated above implies a certain sensitivity to the way scales are chosen. Two ways out of this dilemma are possible. First, one may seek so-called ""natural"" units, by ensuring tbal all vari­ ables m easured are of the same type (for instance, all heights or all weights). Alternatively, one can standardize all variables so that they have unit variance, an"
1491,unknown,"unit variance, and find the principal components of the correlation matrix ratber than the covariance matrix. The second option is the one most commonly employed, although this does complicate questions of hypothesis testing---see Section 8.4.4. (c) If Ihe covariance marrix of x has rank r< p, rhen Ihe roral variarion of .I carl be ellrirely explailled by rhe firsr r prillcipal compollellls. This "
1492,unknown,"This follows from the fact that if I has rank r, then the last (p - r) eigenvalues of 1: are identically zero. Hen ce the result follows from (a) above. (d) The vecror subspace spanned by rhe firsr k pri>lcipal compOI.ellls (1';; k < p) has smaller mean square deviarion from rhe popularion (or sample) variables rhan a>lY olher k -dimensional subspace. If 1-(0,1:) and a subspace HeR"" is spanned by "
1493,unknown,"bill, ... ,h,.\> then by projecting"" onto this subspace we see that the squared distance d' from x to R has expectation 221 PRINCIPAL COMPONENT ANALYSIS k E(d2) = E(x'x) - L E(h~,)<). 1-' Let 'u, = 1""11,1), j = 1. ... , k. We have • E(d') = tr l:,-L E(f(I)Y) r~' p • = trl:,-L L ti,}., (8.2.10) i=] j=1 since the ',1) are also orthonormal. Exercise 8.2.7 then shows that (8.2.10) is minimized when t."
1494,unknown,"is minimized when t.; = 0_ i = k + 1, ...• p. for each j = 1, .... k; that is when the 1m span the subspace of the first k principal components. Note that the case k = L is just a reformulation of Theorem 8.2.2. (e) As a special case of (d ) for k = P - 1, rhe plane perpendicular ro rl.e las/ principal componenr has a smaller meall square deviarion from the popularion (or sample) variables rhan al"
1495,unknown,"(Of course, the plane perpendicular io the last principal component equals the subspace spanned by the first (p -1) principal compon ents.) Consider the multiple regression x.= p'x.+u of x, on (x"" ... , Xp )'= ""2' say, where u is uncorrelated with .I,. The linear relationship x, = ~'s., defines a plane in R "" and tbe parameter ~ may be found by minimizing E(d') where d = lx, -~' x,1 is the distanc"
1496,unknown,"E(d') where d = lx, -~' x,1 is the distance between the point"" and the plane, with d measured in rite direcrion of rl.e ""dependelll variable"". On the other hand, the plane perpendicular to the last principal componen t can be found by m inimizing E(d'), where d is measured perpendicular ro rhe plane. Thus principal component analysis represents ""orthogonal"" regression in contrast to the ""simple"" r"
1497,unknown,"As an example where this point of view is helpful, consider the following bivariate regression-like problem, where both variables are subject to errors. (This problem bas already been discussed in Example 7.2.1. using the metbod of instrumental variables.) Example 8.2.5 Suppose an observed vector :<;=(x"", x,.), r= 1, ... , n, is modelled by Xf""l =~+B'l. X ,2 = a + (3f. + e,2, (8.2.1I ) where f. is"
1498,unknown,"where f. is the (unknown) ""independent"" variable, E"" - N(O, T~), E,,­ N(O, T il, and the ES are independent of one another and of the !;s. The problem is to find the linear relationship (8.2.11) between x, and X2, that is to estimate a and {3. However, because x. is subject to error, a simple regression of X2 on x, is not appropriate. MULTIVARIATE ANALYSIS 222 There are two ways to approach this p"
1499,unknown,"MULTIVARIATE ANALYSIS 222 There are two ways to approach this problem. First the ~s can be viewed as "" unknown additional parameters to be estimated. in which case (8.2.11) is called a functional relations/lip. Alternatively, the ~s can be regarded as a sample from N("""" 0""), where JJ. and 0'2 are two additional parameters to be estimated. in which case (8.2.11) is called a strucrural relations/lip"
1500,unknown,"strucrural relations/lip. (See Kendall and Stuart, 1967 . Chap. 29.) If the ntio between the error variances A = T2/T, is known . then the parameters of both the functional and structural relationships can be estimated using maximum likelihood. In both cases it is appropriate to use the line through the sample mean (x"" x2 ) whose slope is given by the first principal component of the covariance ma"
1501,unknown,"first principal component of the covariance matrix of (A 'I2XL , x,). (With this scaling }he error variance is spherically symmetric.) Explicit form ulae for Ii and f3 are given in Exercise 8.2.8. Remarks (I) Unfortunately, if the error variances are completely un­ known, it is ""ot possible to estimate the parameters. In particular, maximum likelihood estimation breaks down in both cases. Ln the f"
1502,unknown,"lional relationship case the likelihOOd becomes infinite for a wide cboice of parameter values, whereas in the structural relationship case there is an inherent indeterminacy in the parameters; that is, different parameter values can produce the same distribution for the observatio""-•. Thus , in this situation more information is needed before estimation can be carried out. For example. if it is p"
1503,unknown,"can be carried out. For example. if it is possible to make replicated observations, then information can be obtained about the error variances. Alternatively. it may be possible to make observations on new variables which also depend linearly on the unknown e. The use of one additional variable x, in a structural relationship was discussed in Example 7.2.1 in terms of instrumental variable estimat"
1504,unknown,"(2) More generally. the linear relationship in (8.2.1 I) can be ex­ tended to any number of dimensions p >2 to express variables x"" ... ,x"" each as a linear function of an unknown ~ plus an error term, where all the error terms are independent of one another. If the ratios between the error variances are known, then the above comments on the use of the first principal component remain valid, but a"
1505,unknown,"first principal component remain valid, but again, with unknown error variances, the estimation of the parameters of tbe functional relationsbip breaks down here also. (However , the structural relationsbip with unknown error variances can be considered as a special case of factor analysis. and estimation of the parameters is now possible. See Chap ter 9. especially Exercise 9.2.7.) 8.2.4 Correlat"
1506,unknown,"8.2.4 Correlation structure We now examine the correlations between x and its vector of principal components. y. defined in Definition 8.2. L For simplicity we assume that 223 PRINC1PAL COMPO~'E.""T ANALYSIS x (and therefore y) has mean zero. The covariance between x and y is then E(xy') = E(nT) = l:r = rAr'r= rA. Therefore the covariance between x, and Y, is y""A,. Now x, and y, have variances 0'"" "
1507,unknown,"variances 0'"" and Ai' respectively. so if their correlation is Pi, then (8.2.12) When l: is a correlation matrix. 0'"" = I so p"" = Yi, AI' We may say that the proportion of variation of x, ""explained"" by y is 2 Th . ' Pi,. en smce the elements of yare uncorrelated. any set G of compo- nents explains a proportion (8.2.13) of the variation in x,. The denominator of this expression represents the vari"
1508,unknown,"variation in X, which is to be explained. and the numerator gives the amount explained by the set G. When G includes all the components the numerator is tbe (i. i)th elem ent of rAT"", whicb is of course ju 1 iT ... so that the ratio in (8.2.13) is One. ote that the part of the total variation accounted for by the compo ­ nents in G can be expressed as the sum over all p variables of the proportion"
1509,unknown,"proportion of variation ill each variable explained by the components in G, wbere eacb variable is weighted by its variance: that is. • L Ai = L (TIIP!., (8.2.14) I£G 1""""'1 See Exercise 8.2.9. 8.2.S The effect 01 ignoring some component, In the open/closed book data of Example 8.2.3, the final two components explain barely 10% of the tOlal variance. It therefore seems fair 10 ask what if anything "
1510,unknown,what if anything would be lost by ignoring these compone nts completely. This question can be broken down into several parts. by analogy with those considered in Section 8.2.4. First we might ask what aTe the values of the correlation coefficients. which by anaiogy with (8.2.12) are given by . MULTIVARIATE ANAJ .YSIS 224 Calculations for the openJclosed book data lead to the foUowing values for 'H
1511,unknown,"for 'H and rr"" (Note that the rows of the second table sum to 1 except for rounding error.) Component i fli 2 3 4 5 1 0.758 0.609 -0.175 0.156 0.026 2 0.734 0.224 0.322 0.548 0.081 Variable 3 0.853 -0.136 0.139 -0.003 -0.493 4 0.796 -0.288 0.409 0.321 0.109 5 0.812 -0.451 -0.354 -0.094 0.050 Component j f~ 1 2 3 4 5 1 0 .574 0.371 0.030 0.024 0.001 2 0.539 0.050 0.104 0.300 0.007 Variable 3 0 .727"
1512,unknown,"Variable 3 0 .727 0.018 0.019 0.000 0.243 4 0.634 0.083 0.168 0.103 0.012 5 0.660 0.204 0.125 0.009 0.002 [n other words. the first component for thIS data explains (0.758), = 57.4% of X,. 53.9% of ~, 72.7% of x."" 63.4% of x •. and 66.0% of x,. The first two componenls taken together explain 94.5% of X"" etc. Note also that whereas the last component explains only a small proportion of the variable"
1513,unknown,"the variables I, 2, 4, and 5, it accounts for (0.493)2 = 24.3% of the varia­ tion in variable 3. Hence ··throwing away"" the last component is in fact rejecting much more information about variable 3 than it is about tbe others. Similarly, ""throwing away"" the last two components rejects most about variables 2 and 3, and least about variable 5. Anotber example using these correlations is given in Ex"
1514,unknown,"A common practical way of looking at the contributions of various principal components (due to CatteU, 1966) is to look at a ""scree graph"" such as Figure 8.2.1 (that is, one plots A, versus j). Such a diagram can often indicate clearly where ""large"" eigenvalues cease and ""small"" eigen­ values begin. Various other ""rules of thumb"" for excluding principal components exist, including the following: ("
1515,unknown,"(a) include just enougb components to explain say 90% of the total variation; (b) (Kaiser) exclude those principal components whose eigenValues are less than the average, i.e. less than one if a correlation matrix has been used. 225 PRINCIPAL COMPONENT ANALYSIS It has been suggested that when p.;; 20, then Kaiser's criterion (b) tends to include too few components. Similarly, Cattell's scree test "
1516,unknown,include too many components. Thus a compromise is often used in practice. The isotropy test (Section 8.4.3) can also give some indication of the number of components to include. Example 8.2.6 Consider the Jeffers' (1967) pine pitprop data of Exam­ ple 6.7.1. A principal component analysis of tbe correlation matrix yields the eigenvalues and eigenvectors given in Table 8.2.1 together with the assoc
1517,unknown,associated percentages and cum ulative percentages 01 variance explained. A look at Cattell's scree graph (Figure 8.2.1) suggests that three or six components should be retained whereas Kaiser's criterion suggests the num ber four. Note tbat seven components are needed to explain 90% ot tbe variance. Jeffers decided to retain the first six components. In this example the first six principal compon
1518,unknown,"natural physical interpretations. For eacb component j consider tbose variables which bave relatively bigh positive or negative weighting (say tbe variables i for which !:Ij'"" 0.70 max. &oj) as constituting an index of the com bined action, or contrast, of the original variables. (However. it must be warned that this approach does not always lead to meaningful interpretations.) The first component"
1519,unknown,"Percen.toge ot varIance e""plQJned ~ each i!tgen...alUe \a/.1 • Figure 8.2.1 Scree grapil for Ih. Corsica"" pilprop daUJ of Table 8.2.1 MULTIVARIATE ANALYStS "" cl r-a: __ OON\oON...oO-O- 0"", V'lU""J'C;f~ """" OOOOOOOO ocoocicicicidcicoc 0 t' , I ~ ..... C""'l~OOO"":f'V'lVl\OcC- "":f' M'O;fV'lll1('10COOOCCC 0 ococidciddddcicio C I I I I gC1~~=:;;~a~~~:g~ ~ cicodcdci~ci6ccio ci I I I I "" - r-'-OOOOO"">:::O\C"
1520,unknown,"I I I I "" - r-'-OOOOO"">:::O\C - ~C:1'H"""" 0- M('>IO ..... O- - CCr-M-M 0000000000000 C I I I I 1 'C;fNCN - V'l- '<;f'O----~ -OOClO-C\Cr-CON- ""':f oooocioccoocicici ci I , I I '"" \0 CO or, M- \11 0 '"" M r-:c f""'- M N --C'JO-cOOOC----o 00 .occdcid:;;cic:iciocici ci I I I I I 'OO ..... V""l\C.ot..NNO""IO-o"":f'COO--=--t""'lr.- MN-- ..... r""""l..c:::: :r-. d cc6cioocidciddci ci I I I I I I I o\OOOVlVl'CIr-O"
1521,unknown,"o\OOOVlVl'CIr-O\O - OOC 0 ..... ceo 00(""'1-(""'100 MM 6dcidcdcic:icicicicid I I I I I I I N~ :! ~~~~~N~8g~ ~ cooc::;;ciciciciciocc I 1 , I I I I NC\~\Or- - O\""'NV') - -..t - co M - V""', -..t-O--CNMMM ~! dccidodc:icicici~cc N I I I I I C-Nr->COOOOl.,oOO-N- """"I V'O;f-""- CN-..t('lMMO-- N ccdcicioocicicccc .o:j I I I I I I I I I I "" - M ~ ~ ~ ~ ~ ~ ~ ~ k ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <'10 --: 0 -- 000 r--:c 00"
1522,unknown,"<'10 --: 0 -- 000 r--:c 00 226 227 PRJNCfPAL COMPONENT A.p,JAl YSIS 1,2,6,7,8,9, and 10, and represents the overaU size of the prop. The second component, weighting highly 3 and 4, measures the degree of seasoning. The third componen t, weighting 5 and 6. is a measure of the rate of growth of the timber. The fourth, fifth, and sixth components are most easily described as a measure of variable 11,"
1523,unknown,"12, and an average of variables 5 and 13, 8.2.6 Graphical representation of principal components The reduction in dimensioos afforded by principal component analysis can be used graphically. Thus if the first two compo nents explain ""most"" of the variance, then a scattergram showing the distribution of the objects on these two dimensions will often give a fair indication of the overall distributio"
1524,unknown,"distribution of the data. Such plots can augment the graphical methods of Section 1.7 on the original variahles. Figure 8.2.2 shows the 88 individuals of ope n/closed book data plotted on the first two principal components of the covariance matrix (Examp le 8.2.3). ote that the variance along the y, axis is greater than the variance along the Y2 axis. I, 60 50 '0 30 20 -10 • - 10 • /-20 t , -30 +."
1525,unknown,"60 50 '0 30 20 -10 • - 10 • /-20 t , -30 +. -40 - 50 -60 • • , '+ + • • + • , FIgure 8.2.2 The 88 individl4als of rite open/dused book data plotted on the first two principal comvonel1ls. MULTIVARIATE ANALYStS 228 As well as sbowing tbe distribution of objeers on the first two principal components. the variables can also be represented on a similar diagram, using as coordinates the correlations de"
1526,unknown,"using as coordinates the correlations derived in Section 8.2.5. Example 8.2.7 The values of r'l for the open/closed book data were calculated in Section 8.2.5. Hence variables 1-5 can be plotted on the first two principal axes, giving Figure 8.2.3. Note that all of the plotted points lie inside the unit circle and variables I and 5. which lie fairly close to the unit circle, are explained better b"
1527,unknown,"nents than variables 2,3,4. 0.8 0 .7 0 .6 I. 0 .5 0 .4 0.3 0 .2 ? D.' o f-"",o""""., --;00"".2'-;;0"""".0""---'0:'-: •• '-;;0"""".S:-;;0""'.6;-C;;0""'. 7;-;0""'.8;;--;0""'. 9;;-'""'I . O'--~ - 0 .1 -0.2 -0.3 •• -0 .4 5 . -0.5 -0.6 -0.7 -0.8 Fig~'e 8.2.3 Correlations betwee"" the """"nables and principal compo"".nts for the open/ closed book dlUa. 229 PRINCIPAL COMPONENT ANAL vs's 8 .3 Sampling Properties of Princip"
1528,unknown,"8.3.1 Maximum likelihood estimation for normal data The small-sample distribution of eigenvalues and eigenvectors of a covariance matrix S is extremely complicated even wben all parent correlations vanish. One reason is that the eigenvalues are lIoll-rational functions of the elements of S. However, some large sample results are known, and many useful properties of the sample principal components "
1529,unknown,"for normal data stem from the following maximum likelihood results. Theorem 8,3.1 For nonnal data wllell the eigenvalues of 1: are distiller, tire sample prillcipal cQmpOllenlS and eigenvalues are the maximum likeli­ hood estimators of the corresponding population parameters. Proof The principal componenlS and eigenvalues 01 l: are related to 1: by means 01 a one-to-one lunction, except when 1: ha"
1530,unknown,"eigenvalues. Hence the theorem follows from the invariance of maximum likelihood estimators under one-to-one transformations. • When the eigenvalues of 1: are not distinct the above theorem does not hold. (This can easily be confirmed for tbe case l:=a 2 1.) Tn such cases there is a certain arbitrariness in defining the eigenvectors of 1:. Even if this is overcome, the eigenvalues of S are in gene"
1531,unknown,"this is overcome, the eigenvalues of S are in general distinct, so the ejgenvalues and eigenvectors of S would not be the same function of S as the eigenValues and eigenvectors of 1: are of :to Hence when the eigen­ vectors of 1: are not distinct, the sample principal components are nor maximum likelihood estimators of their population counterparts. How­ ever, in such cases the following modified "
1532,unknown,"Theorem 8.3.2 For Iwrmal data, when k> 1 eigenvalues of 1: are not distiller, but rake the COl11mOn value A. then (a) r/,e m.l.e 0/ A is T, rhe arirllmeric mean of the correspolldillg sample eigenvalues, and (b) the sample eigenvectors corresponding to tile repeated eigelwalue are m.l.e.s, although riley are 1101 tlte U/lique m.l.e.s. Proof See Anderson (1963), and Kshirsagar (1972, p. 439). • Alt"
1533,unknown,"Although we shall not prove the above theorem here, we may note that (a) appears sensible in the light of Theorem 8.3.1, since if A, is a distinct eigenvalue then the m .Le. of A, is I"" and since if the last k eigenvalues are equal to A, then MULTIVARJATE ANALYSIS 230 and trS=I,+ ... + I._.+ kT. Part (b) of the theorem also appears sensible in the light of Theo rem 8.3.1. Example 8.3.1 Le t l:=(l-"
1534,unknown,"8.3.1. Example 8.3.1 Le t l:=(l-p)I+pll', p >O. We know from Example 8.2.2 that the largest eigenvalue of 1: is A, = 1 + (p- l)p and that the other (p -1) eigeDvalues aU take the value A = (1-pl. Suppose we assum e nothing abou t 1: except that the smallest p- l eigenvalues are equal. Then the m.l.e. of A, is I"" the first sample eigenvalue of S. and the m.l.e. of ~ is T =(trS- I,)/(p - I). Note th"
1535,unknown,"T =(trS- I,)/(p - I). Note that the corresponding estimates of p. namely (l,- l)/(p- J) aDd 1- T are not equal. (See Exercise 8.3.3.) 8.3.2 Asymptotic distributions for normal data (8.3.21 For Large samples the following asymptotic result. based on the central limit theorem , provides useful distributions for the eigenvalues and eigen. vectors of the sample covariance matrix. Theorem 8.3.3 (Anders"
1536,unknown,"Theorem 8.3.3 (Anderson , 1963) LeI 1: be a posi/ive-de!illlte matrix with dislinct eigenvalues. Let M - Wp(l:, III) and sel V = III -1M . Consider speclTal decompositions l: = rAT"" WId U = GLG ', and let ~ and I be Ihe vectors of diagonal elemenls in A alld L. 11rell Il,e fol/owing asymptotic dislributions hold as III ..... ""': (3) 1- ptA, 2A~/ m); Ihat is Ihe eigellvalues of are asymplOlicall}' "
1537,unknown,"normal, unbiased, and illdependent, with I, ha ving vMiance 2A N m . (b) Il<.,-Np{-y"" V ,/III). where Ihat is, rile eigelweclOrs of U are asymptolically normal and UIl­ biased, and have the staled asymptotic covariallce matrix V Jm. (c) TI,e covariance between til. rth element of 1:<"" aPld the sth elemellt of Il<j) is - A,A ,yoY ';! 111 (A, - A, J'. (d) The elements of I are asymptotically indepen"
1538,unknown,"G . 231 PRlNcrPAL COMPONENT ANALYSIS PrOilf We shall only give the proof of parI (a) here, and we shall use the 0.0 and 0.(-) notation given in (2.9.1) and (2.9.2). If M - Wp(l:, m ), then M* =r'Mr - Wp(A, m); that is. M * has a Wishart distribution with diagonal scale matrix. Set U* = m -IM"" . Then using Exercise 3.4.17 it is easily seen that E(u~) = Ai' V(u~) =2m- 1Af, E(u~)=O, V(u~)=m- IA''\i' "
1539,unknown,"for i4 j. Since M * can be represented as the sum of m independent matrices from Wp(A, I), it follows from the ceDtral limit theorem that if we write u' = (uf!> ... ,u!J' and A = (,I."" ... ,Apl', then (8.3.3) and Hence i# j. (8.3.4) Thus , the roots, I,> ... > 1., of the polynomial IV""-111 converge in proba bility to the roots. A, > ... > Ap , of the polynomial IA -III; that is U:-li ~O, i=I, ... "
1540,unknown,"U:-li ~O, i=I, ... , p. To gel more precise bounds on u~ - t., use (8.3.4) in the standard asymptotic expansion for determinants (A.2.3a) to write • \U *-vI\= n (ut-0)+0.(11'-'). (8.3.5) where 0 is any random variable bounded above in probability 0 = Op(1). lD particular, because 4.;;trU*=trU for all i =1, ... , p, it is clear from (8.3.4) that Ii = 0 .(1). SiDce I"" ... , lp are the eigenvalues of"
1541,unknown,"0= 4 in (8.3.5) yields p n (uj;-l,) = O .(m- l ), i= 1, .. "", p. Now the true eigenvalues A, are distinct so for i ~ j, u~ -I, is bouDded in probability away from O. Hence tbe nearness of the above product to 0 is due to only one lactor, U;, - 4; that is i=l, ... tp· MULTIVARIATE ANALYSIS 232 Inserting this result in (8.3.3) gives In ""'(1- >.) = "",'/2(0-- >')+ Q,(m - 'i2) ~ N V(O, 2A ') because th"
1542,unknown,"because the second term, a .(m -II'), converges in probability to 0. Hence part (a) is proved. • For a sample of size 01 from a multinormal distribution, the unbiased covariance matrix Su = (n -1)-' nS satisfies the assump tions 011 U in tbe above theorem with OIl = 01 - 1 degrees of freedom. Thus. the eigenvalues I, > ... > l"" of Su with eigenvectors Iklh ···, Ikp) have the asymptotic distributio"
1543,unknown,"distribution stated in the theorem. Of course, asymptotically, it does not matter whether we work with S or Suo Example 8.3.3 Let I, .... , Ip denote tbe eigenvalues of Su for a sample of size n. From part (a) of the theorem we deduce tbat, asymptotically, I, -N(A .. 2,1.:1(11-1)). Therefore, using Theorem 2.9.2, asymptotically. log I, -N(lo.g A"" 2/(n - t». This le ad~ to. the asymptotic confidenc"
1544,unknown,"This le ad~ to. the asymptotic confidence limits log A, = log 4 ± z(2/(n _1)""2. where z is the relevant critical value from the standard normal distribu­ tion. For instance, consider the open/closed book data from Example 8.2.3 where"" = 88. Putting z = 1.96 gives (4e-O .291 • 4eO.297 ) as an asymptotic 95% confidence interval for A,. Use of the five sample eigenvalues of Su, 687.0.202.1, 103.7, S4"
1545,unknown,"687.0.202.1, 103.7, S4.6, and 32.2, leads to the following intervals: (510.925). (150,272), (77.140). (63,114). and (24,43). Example 8.3.4 Theorem 8.3.3 can also be used for testing hypotheses on the eigenvectors of I. Consider, for example, the null hypothesis that the ith eigenvector 'Y(i) takes a given value r, where r'r = 1 and we suppose that A, has multiplicity L If the null hypothesis 'Y(i)"
1546,unknown,"part (b) of Theorem 8.3.3, asymptotically. (n _l)'n(I:(,)-r)-N(O , V,). Note that V , has rank (p -1); its eigenvectors are 'Y(n with eigenvalue 0 and 'Ym with eigenvalue (,1.,,1. ~' + Ai' Ai - 2)- ' for j'"" i. Thus, (see Section A.S). a g -inverse V~ is defined by the same eigenvectors, and by eigenvaJues zero and Aj A. il+ A ~1 A. f- 2 ; thatis~ V i = I ('\,,1.,' + ,1.;-' ,1.,- 2N m 'Y;j) = .I.,"
1547,unknown,"233 PR INCiP AL COMPONENT AN A.L YSlS Hence, by a modified version of Theorem 2.5.2 on Mahalanobis distances (see specifically Exercise 3.4.11), asymptotically, (n -l)(g"", -r),(A,r' + A;'I-21)(1:(,) - 1') - X~ ' I' (8.3.6) Let W,-=~ S- '+I;'S-21 denote the sample version of V ,-. Since (n - 1)'12(1:(,,-r) = 0 .(1) and V ;-= Wi ~op(J), tbe limiting distribution in (8.3.6) remains the same if we rep"
1548,unknown,"(8.3.6) remains the same if we replace V~ by W i. As 1:(,) i, an eigenvector of W~ witb eigenvalue 0, (8.3.6) implies. asymptotically, (8.3.7) 8.4 Testing Hypotheses about Principal Components 8.4.1 Introduction It i' often useful to have a procednre for deciding whether the first k principal components include all the important variation in x. Clearly one would be happy to ignore the remaining (p"
1549,unknown,"would be happy to ignore the remaining (p - k) components if their corresponding popUlation eigenvalues were all zero. However , this hap­ pens only if I has rank k, in which case S must also have rank k. Hence this situation is only trivially encountered in practice. A second possible alternative would be to test the hypothesis that tbe proportion of variance explained by the last (p-k) component"
1550,unknown,"than a certain value. This possibility is examined in Section 8.4.2. A more con~enient hypotbesis is the question whether the last (p - k) eigenvalues are equal. This would imply that the variation is equal in all directions of the (p-k)-dimensional space spanned by the last (p-k) eigenvectors. This is the situation of isotropic varia/ion, and would imply that if one component is discarded, then a"
1551,unknown,"that if one component is discarded, then all the last k components should be discarded. The hypothesis of isotropic variation is examined in Section 8.4.3. In all these sections we assume that we are given a random sample of normal data of size n. 8.4.2 The bypothesis that (A, +. , . + A.)/(A, + ... + A.)=t/J Let I"" ... , Ip be the eigenvalues of Su and denote the sample coun terpart of ;JJ by .? "
1552,unknown,"of ;JJ by .? = (1,+ ... -r 1.)/(1, + ... + ~). We seek the asymptotic distribution of .j,. From Theorem 8.3.3 we know that tbe elements of I are asymptotically normal. Using Theorem 2.9.2 MULTIVARJATE ANALYSIS 234 with t = I, II-= A , f(t) = 'l'(1) and /l replaced hy /I - I, we see that V = 2A 2, and d = dob = {O-I/J)/tr 1: I ai, -1il/tT 1: for i = 1. , .. , k, for i = k + J , ... , p. Therefore o"
1553,unknown,"Therefore oj, is asymptotically normal with mean [(Al = I/J, and w ith variance ,,2 (n -l~(tr 1;)2 {(I -I/J)'(A ~ ...... HD+ 1/J2(A;+ I + ... H~)} 2 Ir 1:2 2 (n-l)(tr1:)z (1/J -2 .. 0/1+""), (8.4.1) wbere .. =(A~ + ... +A;J/(Ai+ ... +A;). Note that .. may be interpreted as the proportion of variance explained by the first k principal compo­ nents of a variable whose covariance matrix is 1;2. Result"
1554,unknown,"above have also been obtained by Sugiyama and Tong (1976), and Kshirsagar (1972, p. 454). Notice that since we are only interested in ratios of eigenvalues, it makes no difference whether we work with the eigenvalues of S or of Su' Example 8,4.1 Equation (8.4.0 may be used to derive approxima te confidence intervals for 0/1. This may be illustrated using the open/closed book data. Let k = 1 so tha"
1555,unknown,"book data. Let k = 1 so that we seek a confidence interval for the proportion of variance explained by the first principal component. Here /I = 88 and we have, trSu=l, + ... + I. = 1109.6, tr S~ =Ii+ ... +I~ = 5.3176 x 10'. oj, = 687 .0/tr So = 0.619 . If we estimate .. and T2 by a and .p, using the sample eigenValues I"" ... , I., then we find a = 0.888, 1"" = 0.148/87 = 0.0017. Thus, the standard "
1556,unknown,"95% confidence interval is given bv 0.619 ± 1.96(0.041) = (0.539,0.699). In other words. although the point estimate seemed to indicate that the first principal component explains 61% of the variation, the 95% confi­ dence interval suggests that the true value for the population lies between 54% and 70%. 235 PRJ NC IPAL CO~iPONENT A.a'lAL YSIS 8.4.3 The bypotbesis tbat (p - k) eigenvalues of 1: ar"
1557,unknown,"It should be noted that any non-zero ei~envalue, however small, is ""significantly"" different from zero, because il A. = 0 then the scatter of points lies in a lower-dimensional hyperplane and so I. = 0 with probabil­ ity one. H ence a test of the hypothesis A. = 0 has no meaning . However. a test of whether A. = A._, is meaningful. This would be testing the hypothesis that the scatter is partially"
1558,unknown,"hypothesis that the scatter is partially isotropic in a two-dimensional subspace. W e consider the more general bypothesis that A. = Ap _ , = ... = Ak + ,. The isotropy test is usefuJ in determining the num ber of principal compo­ nents to use to describe the data. Suppose we have decided to include at least k compone nts, and we wish to decide whether or not to include any more . The acceptance o"
1559,unknown,"more . The acceptance of the null hypothesis implies that if we are to include more than k principal components we should include aU p of them because each of the remaining components contains the same amount of information. Often one cond ucts a sequence of isotropy tests, starting with k = 0 and increasing k until the null hypothesis is accepted. The likelihood ratio for this hypothesis can be f"
1560,unknown,"manner as that used for the one-sample hypotheses on 1: which were discussed in Chapter 5. As Theorem 5.3.2 indicated, the LRT is given by -210gA = I/p(a - 1 - log g), where a and g are the arithmetic and geometric means of the eigenvalues of i-'s, where i is the m.l.e. of 1: under the null hypothesis. Now in our case, by virtue of Theorem 8.3.2, t and S have the same eigenvectors, and if the eige"
1561,unknown,"and if the eigenvalues of S are (ILo ...• Ip), then the eigenvalues of i are (I, •... ,Ik' ao, ... , ao), wbere ao = (I~ "" + ...... Ip)l(p - k) denotes the arithmetic mean of the sample estimates of tbe repeated eigenvalue. (In our problem, we assume that the repeated eigenvalues come last, al­ though there is no need for this to be so.) It foUows that the eigenvalues --, of 1: S are (1, ... , I, "
1562,unknown,"(go/ao)(p-k)l., where 80 =(lh' X ... X 1,.),,··-0, is the geometric mean of the sample estimates of the repeated eigenvalue. inserting these values in the above equation we deduce that -2 log A = I/(p-k) log (ao/go). (8.4.2) Now the number of degrees of freedom in the asymptotic distribution of -2 log A might appear to be (p - k -1), the difference in the number of eigenvalues. However, this is no"
1563,unknown,"eigenvalues. However, this is not so because HI) also affects the number of orthogonality conditions whicb 1: must satisfy. In fact (see Exercise MULTrvARlATE ANALYSIS 236 8.4.1) the relevant number of degrees of freedom is 1(p - k T 2J(p - k -I). Also, a better chi-squared approxim ation is obtained if "" in (8.4.2) is replaced by n' ="" -(2p + 11)16 to give B artlett's approxima tion, ( 2P+ll) (ao"
1564,unknown,"( 2P+ll) (ao) 2 /I 6 (p-k) log g;; -X (p-k'2)<p-k-'>l2, (15.4.3) asymp totically. Note that the ratio ao/go is the same whether we work with the eigenvalues of S or Suo ElUlJIIple 8.4.2 The three-dimensional data [rom Jolicoeur and Mosimann (1960) relating to /I = 24 male turtles given in Example 5.3.4 may be used to test the hypothesis tbat A2 = A ,• Using S, given in Examp le 5.3.4, it is found "
1565,unknown,"of a"" and go in (8.4.2) are ao =~(3. 54 + 1.05) = 2.29. g"" = (3.54 X 1.05)""2 = 1.93. Also, ,,'=24-17/6=21.17. Thus, the test statistic is given by -2 log A = (21.17)(2) log (2.2911.93)=7.24, which under H o has a chi-squared distrihution with i(p-k +2)x (p - k -1) = 2 degrees of freedom . The observed value lies between tbe 95th percentile (=5.99) and the 99 th percentile (=9.21). 8,4.4 Hypotheses"
1566,unknown,"8,4.4 Hypotheses concerning correlation matrices When the variables have been standarized. the sample correlation matrix R ma y be regarded as an estimate of the population correlation matrix P . The hypothesis that all eigenvalues of P are equal is equivalent 10 the hypothesis that P = I. In (5.3.17) we described a test of this hypothesis due to Box (1949). This uses the statistic -/l'logIR I, wh"
1567,unknown,"It - (2p + 11)/6. This statistic has an asymptotic chi-squared distribution, with !p(p - 1) degrees of freedom. Considerable difficulties arise if a test is required of the hypothesis that the (p-k) smallest eigenvalues of P are equal, where O<k<p-1. Bartlett (1951) suggested a test statistic similar to (8.4.2), namely (/I - I)(p - k) log (a,Jgo), (8.4.4) where ao and go are the arithmetic and geo"
1568,unknown,"where ao and go are the arithmetic and geometric means of the (p-k) sm allest eigenvalues of R . (See also Section 5.3.2a.) Unfortunately, this statistic does not have an asymptotic chi-squared distribution. Neverthe­ less, if the first k components account for a fairly high proportion of the variance, or if we are prepared to accept a conservative test, we may treat 237 PRlNCIPAL COMPONENT ANALYS"
1569,unknown,"tbe above expression as a c1u-squared statistic with !(p - k + 2)(p-k -1) degrees of freedom (Dagnelie. 1975 . p. 181; Anderson , 1'163). Example 8.4.3 The correlation matrix of the openlclosed book data has eigenvalues 3.18, 0.74.0.44,0.39. and 0.25. Use of (8.4.4) leads to the following test statistics: for the hypothesis A~ = A, = A. = As, {or the hypothesis X~= 26. 11, xi= 7.30. The former is "
1570,unknown,"X~= 26. 11, xi= 7.30. The former is significant at tbe ! % level, but tbe latter is not even significant at the 5% level. Thus w e accept tbe hypothesis A, = A. = As. 8.S Correspondence Analysis Correspondence analysis is a way of interpreting contingency tables which ha~ ,everal affinitie with principal component analysis. We shall intro­ duce this technique in the context of a botanical problem "
1571,unknown,"""gradient analysis"". This concerns the quantification of the notion that certain species of flora prefer certain types of habitat, and that their presence in a particular location can be taken as an indicator of the local conditions. Thus one species of grass might prefer wet conditions, while another might prefer dry conditions. Other species may be indifferent. The classical approach to gradient"
1572,unknown,"The classical approach to gradient analysis involves giving each species a ""wet-preference scorc"", according to its known preferences. Thus a we t-loving grass may score to, and a dry-loving grass 1, with a fickle or ambivalent grass perhaps receiving a score of 5. The conditions in a given location may now be estimated by averaging the wet-preference scores of the species that are found there. To"
1573,unknown,"the species that are found there. To formalize this let X be the (rt x p) one -zero matrix which represents the occurrences of n species in p locations: that is, x;, = I if species i occurs in location j, and x;; = 0 otherwise. U r, is the wet-preference score aUocated to the ith species, then the average wet-preference score of the species found in location j is S. a:. L ~/l/x' i' where x'l = L ~"
1574,unknown,"I This is the estimate of wetness in location j produced by the classical method of gradient analysis. Olle urawback of the above method is that the r, may be highly subjective. However , they them selves could be estimated by playing the same procedure in reverse-if s, denotes the physical conditions in location j, then r, could be estimated as the average SCO re of the locations MULTIVARIATE ANA"
1575,unknown,"MULTIVARIATE ANALYSIS 238 in which Ihe ith species is found; thaI is The tecimiqlle of correspondellce analysis effectively rakes both the above relatiolls/lips Sin"",ltaneollsly. and uses them to deduce scorillg vectors r alld S which satisfy both the above eqllatiolls. The vectors rand s are gellerated i,tremally by the data, ,a,her Ihan beil1g extemally glVell. To see how correspondence analysis"
1576,unknown,"To see how correspondence analysis is related to principal com ponent analysis. let us wTite the above equations in matrix form. Letting A = diag (x,.), B =d iag (x-;l. the above two equations are r 0: A -IXs . If both these equations hold simultaneously then r o:: A - LXB - 1X 'r and (8.5.1) 18.5.2) Therefore r is an eigenvector of A -IXB -IX '. a ma trix which depends only on the elements of X. "
1577,unknown,on the elements of X. Similarly s is an eigenvector of B -'X 'A -IX . Of course one wou ld like the coefficients of proportionality in each of the above equations to be unity. SO that r and s can be interpreted as arithm etic means. This would lead one to choose r and 8 as the eigenvec­ tors corresponding to an eigenvalue of unity. Unfortunately however. although unity is indeed an eigenvalue. the
1578,unknown,"although unity is indeed an eigenvalue. the correspond ing eigenvectors are each 1. Hence this requirement would involve all scores being made identical, w hich is not a very sensible requiremen t. Therefore we are led to consider solutions of (8.5.1) and (8.5.2) which involve an eigenvalue p'"" 1. Th e correspondence analysis technique is to choose the largest eigenvalue less than I. One then uses"
1579,unknown,"ing (right) eigenvectors of A -1XB -IX ' and B -IX 'A -'X to order the wet-preferences of the species and locations, respectively. It is not difficult to show that if r is a standardized eigenvector of A -'XB -'X ' with eigenvalue p (p >O), then s= p-l/2B -IX 'r is a standar­ dized eigenvector of B -IX ' A -'X with the sam e eigenvalue. Further if p<1 then r'A1 =0 and s'B1 =0 . See Exercises 8.5.1"
1580,unknown,"For more details of the me thod of correspondence analysis see Hill {I 974). Example 8.5,1 In an archaeological study there are six graves containing five types of pottery between them . It is desired to order the graves and the pottery chronologically. Let X;, = 1 if the ith grave contains the jth 239 PR J 'ClPAL COl\ofPONENT ANALYSIS pottery and 0 otherwise. Suppose 1 2 3 4 5 A 0 0 1 1 0- B I 1 "
1581,unknown,"1 2 3 4 5 A 0 0 1 1 0- B I 1 0 0 1 C 0 1 1 L X = D 0 0 I I 0 E I 0 0 0 F 1 0 1 The largesl eigenvalue of A -IXB -IX' less than one is 0.54, giving standardized eigenvectors r' = (0.515, - 0.448, 0.068, 0.515. -0.513 , 0.009) and 8'=(- 0.545, - 0.326,0.476,0.476, -0.380), thus leading to chronological orders «A, 0), C , F, B , E) and «3,4),2,5,1). Re-ordering the columns of X. w e get 3 4 2 5 A L 0"
1582,unknown,"3 4 2 5 A L 0 0 0 D 0 0 0 C 1 1 0 F I I 0 1 B 0 0 1 1 1 E 0 0 0 1 1 Columns 3 and 4 are indistinguishable, as are rows A and D . Also, the direction of time cannot be determined by this technique. otice that the I s are clustered about the line between the upper left-hand corner of the matrix and the lower right-hand corner, agreeing with one's intuition that later graves are assodated with more r"
1583,unknown,"correspondence analysis seems reasonable. 8 .6 AUometry-the Measurement of Size and Sbape One probLem of great concern to botanists and zoologists is the question of how bodily meas urem ents are best summarized to give overall indica­ tions of size and shape. This is the problem of ""allometry"", w hich is described in considerable detail by H opkins (1966). Sprent (1969, p. 30. MULTIVARIATE ANALYS"
1584,unknown,"1972), and Mosimann (1970). We shall follow the approach of Rao (1971). Define a size faclOr to be a linear function s = a'x such that an increa e in s results ""on average"" in an increase in each element of x. Similarly, a sltape faclOr is defined as a linear function h = b'x for which an increase in It results ""on average"" in an increase in some of the elements of x and in a decrease in others. T"
1585,unknown,"a decrease in others. The phrase ""on average"" is to be interpreted in the sen e of a regression of x On s (and h). The covariance between s and X; is E(a'x>;) =a'a"" where a , is tbe ith row of the covariance matrix 1:= V(x). Therefore tbe regression coefficient of X; on s ,s C(x"" s) a'a , V(s) a'l',a . [f ~ is a pre-assigned vector of positive elements which specify the ratios of increases desired"
1586,unknown,"of increases desired in the individual measurements, then to determine a such that a'x represents size. we must solve the equation 1:a '"" ~. Tben !; is called a size veceor. Note the difference between the size veC/OT ~ and the size factor a'L in practice we often use standarized vectors x sO that 1: is replaced by the sample correlation matrix R . In these circumstances it is plausible to take!; "
1587,unknown,"these circumstances it is plausible to take!; to be a vector of ones. so tbat a ""'R -Il. Using similar argwnents R ao suggests Lbat a shape factor should be h = b'x. where the elements of b are given by R -' ..... where .... is a pre-assigned vector of plus and minus ones, with possibly some zero. Of course, it is possible to consider several different shape factors. Note that the interpretation o"
1588,unknown,"the interpretation of a sbape factor is given by looking at Lbe components of the sbape veceor .... , not the coefficients b of the shape jac/or b'x. U the variables are uncorrelated so that R = I, then the two vectors coincide. Since it is only the relative magnitude of the elements of Ii and .... which are of interest, both sand h may be re-scaled without affecting their interpretive value. If a"
1589,unknown,"interpretive value. If a and b are scaled so that sand h have unit variance (and if each column of the data X is scaled to have mean 0 and unit variance), then the correlation betwo!en s and II is given by A shape factor corrected for size can be obtained by considering 11'= Il- ps. 241 PRINClPAL COMPOl\TENT A.~AL YSIS The values of s and I,' are uncorrelated, and can be plotted on a two-dimension"
1590,unknown,"two-dimensional chart. An alternative approach to allometry based on principal components has been suggested by Jolicoeur and Mosimann (1960). If all tbe coeffi­ cients of the first principal component are positive, then it can be used to meas ure size. In this case tbe other principal components will have positive and negative coefficients, since they are orthogonal to the first principal compone"
1591,unknown,principal component. Therefore second and subsequent principal compo­ nents may be taken as measures of shape. (Note that if all the elements of the covariance matrix are positive. then the Perron-Frobenius theorem ensures that all the coefficients of the first principal component are positive-see Exercise 8.6.1.) One advantage of principal components is that the principal component loadings have 
1592,unknown,"loadings have a dual interpretation in Rao 's framework. First, if III 1\ is the first eigenvector of R , that is 1l<1l is the loading vector of tbe first principal compo nent, then ~ = g, 1\ can play the role of a size vector. Moreover, sInce a = R -1g,n '"" g,1). Il<n can also be interpreted as the coefficient vector of a size faclOr s = gI""x. The other eigenvectors can be used to measure ,bape, "
1593,unknown,",bape, and the sizes of the eigenvalues of R give some indication of the relative magnitude of the varjous size and sbape variations within the data. U nfortunately, the interpretation of principal components as size and shape vectors is not always straightforward. One 's notions of size and shape are usually given in terms of simply-defined ~ and TJ (usually vectors of ± Is and Os), hut the princ"
1594,unknown,"vectors of ± Is and Os), hut the principal component loadings can behave '00 arbitrarily to represent size and shape in a consistent w ay (Rao , 1964). Ex ample 8_6_1 [n an analysis nf 54 apple trees, Pearce (1965) considers the following three variables: total length of lateral shoots, circumference of the trunk, and height. Although the variables can all be measured in the same units, it is sens"
1595,unknown,"the same units, it is sensible in this case to use standardized variables because the variances are clearly of different orders of magnitude. The correlation matrix is 0.5792 0.2414] 1 0.5816 , I ~iv ing principal components Y ,= 0.554x, + 0.65\ x., + 0.520x,. y, = -0.657x,-0.042x, + O. 753x"" Y3 = -0.51Ix, +0.758x 2 - 0.404x, MULTIVARIATE ANALYSIS 242 with corresponding variances 1.94, 0.76, and 0"
1596,unknown,"MULTIVARIATE ANALYSIS 242 with corresponding variances 1.94, 0.76, and 0.33 (64, 25, and 11 % of the total, respectively). Note that the first principal component is effec­ tively the sum of the three variables, the second principal component represents a contrast between X, and x"" and the third is a contrast between x2 on the one band and x, and x, on the other. If we use a size vector ~=(1, 1, 1"
1597,unknown,"If we use a size vector ~=(1, 1, 1)"" then we get a size factor s = 0.739x , + 0. 142x~ + 0.739x"" which is similar to the first principal component y"" although the second coefficient is perhaps low. Sim ilarly a shape vector 11, = (1. 0, - 1), con­ trasting X, with -', leads to a shape factor which is similar to the second principal component . H owever , if we use a sbape vector .... 2 =(1, -1, 0)"
1598,unknown,"obtain a shape factor h, = 2.56x, - 3.21x 2+ 1.25.>:, which bears no resemblance to any of the principal comp onents. 8.7 Discarding of Variables In Section 6.7 we gave a method of discarding redundant variables based on regression analysis. We now give a method based on principal compo­ nent analysis. Suppose a principal component analysis is performed on the correlation matrix of all the p varia"
1599,unknown,"matrix of all the p variables. lnitially, let us assume tbat k variables are to be retained where k is known and (p- k) variables are to be discarded. Consider the eigenvector corresponding to the smallest eigenvalue, and reject the variable with the largest coefficient (absolute value). Then the next smallest eigenvalue is considered. This process continues until tbe (p -k) smaUest eigenvalues ha"
1600,unknown,"(p -k) smaUest eigenvalues have been considered. In general, we discard the variable (a) which has the largest coefficient (absolute value) in the compon ent, and (b) which bas not been previously discarded. This principle is consistent with the notion that w e regard a component with small eigenvalue as of less importance and , consequently, the vari­ able whiclt domin ates it should be of less i"
1601,unknown,"243 PRrNClPAL CO MPOf'lENT ANAL YSIS However, the choice of k can be made more realistic as follows. Let Ao be a thresltold value so that eigenvalues ~ .. ~ f' can be regarded as contribut­ ing too little 10 the data. The m etltod in essence is due to Beale el al. (1967). Jolliffe (1972 ) recommends Ao= 0.70 £rom various numer ical examples. Ex ampl e 8.7.1 Consider Jeffers' (1967) pine pit-prop d"
1602,unknown,"6.7.1. From the principal component analysis on the correlation matrix (see Example 8.2.6). it can be seen that the first six components account lor 87% of the variability. The other seven eigenvalues are below the threshold value of .0.0 =0.70, so seven variables are to be rejected. The variable with the largest coefficient 011:(13)' the eigenvector corresponding to the smaUest eigenvalue, is the"
1603,unknown,"to the smaUest eigenvalue, is the second and so x., is rejected. Similarly, the variabl~ with largest coefficients in K\1 2) • ... '~7) are X4~ X 7. XIO • X6 , X., X'2, respectively. aDd as these are aU distin.ct, these are successively rejected. An alternative method of discarding variables based on the multiple correlation coefficient was given in Example 6.7.2, and the two methods give differen"
1604,unknown,"give different sets of retained variables. For example, to retain six variables the above principal component method chooses variablC$ l, 3, 5, 8, J 1,13 whereas lrom Table 6.7.4. the multiple correlation me tbod chooses variables 3,4, In, ll, 12, 13. Variahles, 3, J I, and 13 arc retained by both methods . Remarks (1) Instead of using A o, k may be SCI equal to the number of components needed to "
1605,unknown,"components needed to account for more than ~ome proportion, ao. of total variation. Jolliffe found in his examples that A n appeared to be better than ao as a criterion lor deciding bow many variable to reject. He finds a,,=O.80 as the best value if it is to be used. Further, using either criterion. at least four variables sbould always be retained. (2) A variant of this method of discarding varia"
1606,unknown,"(2) A variant of this method of discarding variables is given by the following iterative procedure. At each stage of the procedure reject tlte variable associated with the smallest eigenvalue and re-perform the prinCipal component analysis o n the remaining varia hies. Carryon until all the eigenvalues are bigh. (3) T he relation between tbe ""smaUness' of !.I,x, and the rejection of one particular"
1607,unknown,"one particular variable is ratlter open to criticism. Howeve r, present studies lead us 10 the beliel that various different methods of rejecting variables in order to explain the variation within X produce in practice almost the same results (see Jolliffe, 1972, 1973). Of course. the method selected to deal with one's data depends upon tbe purpose or the study, as the next section makes clear. MU"
1608,unknown,"MULTIVARIATE AtlALYSrS 244 8.8 Principal Component Analysis in Regression In the multiple regression model of Chapter 6 it was noted that if the independent variables are higbly dependent, then the estimates of the regression coefficients will be very imprecise. In this situation it is advantageous to disregard some of the variables in order to increase the stability of the regression coefficients"
1609,unknown,"An alternative way to reduce dimensionality is to use principal compo­ nents (Massy, 1965). However, the choice of components in the regres­ sioo context is somewba t different from tbe choice in Section 8.2.5. In that section the principal components with the largest variances are used in order to explain as much of the total variation of X as possible. On the other hand, in the context of multip"
1610,unknown,"other hand, in the context of multiple regression it is sensible to take those components having the largesl correia lions with the dependent variable because the purpose in a regression is to explain the dependent variable. In a multivariale regression the correlations with each of the dependent variables must be examined . Fortunately, there is often a tendency in data for the components witb th"
1611,unknown,"explain the dependent variables. If the principal components have a natural intuitive meaning. it is perhaps best to leave the regression equation expressed in terms of the components. In other cases, it is more convenient to transform back to the original variables. More specifically. if II observations are made on a dependent variable and on p independent variables (y(n x 1) and X(II x p» and if"
1612,unknown,"vations are centred so that y = it = 0, i = 1, ... ,p. then the regression equation is y =xp+£, where £-N.(0.o-'8) and 8=I- n'I11'. (8.8.1) If W = XG deno tes tbe principal component transformation then tbe regression equation may be written y=Wa+£ , (8.8.2) where a= G'II. Since the column s of W are orthogonal, tbe least squares estimators 6; are unaltered if some of the columns of Ware deleted f"
1613,unknown,"tbe regression. Further, their distribution is also unaltered. The full vector of least squares estimators is given by ci=(W'W) 'IW'y and £=y-Wci , or J =1 •...• P. (8.8.31 245 PRINCIPAL COMPONENT ANALYSIS where ~ is the itb eigenvalue of the covariance matrix. n "" X 'X . Then 6, has expectation Q i and variance O"" /(nl,). The covariance between W ; and y. (8.8.4) can be used to test whether or no"
1614,unknown,"y. (8.8.4) can be used to test whether or not the contribution of Wi to the regression is significant. Under the null bypothesis, Q i = 0, aj(nU t12 w eoY (E'E/(n- p-l»IJ2 (i;nE'i/(ll-p-J)),"" (""'.'1' (8.8.5) regardless of the true values 01 the other Q j. Because the components are orthogonal to one another and orthogonal 10 the residual vector J we can write p y'y= I 6;Wij\W (O+ E'E • =- L II~Q i"
1615,unknown,"10 the residual vector J we can write p y'y= I 6;Wij\W (O+ E'E • =- L II~Q i+ E'E. (8.8.6) i=l Thus, n46f/y'y= corr' (y. w e.» represents the proportion 01 variance 01 y accounted lor by component i. Selecting those components for which the statistic in (8.8.5) is signific­ ant is a straigbtforward way to choose which components to retain. Procedures wbich are more theoretically justifiable have a"
1616,unknown,"veloped (see Hill et al., 1977 ). Example 8.8.1 Consider the Jeffers' (1967) pitprop data of Examples 6.7.1. 6.7.2, and 8.2.6. The proportions 01 the variance 01 y accounted for by each of the components and by the residual vector (times 100%) are given by 7.56. 33.27, 3.05. 0.26,13.70,6.13,3.24. 1.28,0.03,0.00,0.07, 4.56, 0.00. and 26.84; and ci is given by ci = (0.134"" . -0.374"",0.128*', -0.048."
1617,unknown,"-0.170'·.0.031.0.000, -0.115, - 1.054*', 0.002). Using the distributional result (8.8.5). we can check from Appendix C, Table C .2 whether eacb 6, is significantly different from zero, and' ("") indicates significance at the 5% (1%) level. By a procedure similar to Examp le 8.2.6, Jeffers decided to retain six components in his analysis. Of these components, 1.2,3.5. and 6 bad significant regressio"
1618,unknown,"tbese five components. However , from the above discussion a more valid analysis would be MULTlVI\RlATE ANALYSL~ 246 obtained by including components 7, 8, and 12 as well. Letting J = {1, 2, 3. 5. 6. 7.8, 12} denote the set of retained components. the reo stricted least squares estimator Ii for a is given by a, = af lor j E J and a;=O for jeJ. Then ~ =Gii . If a, =0 for jeJ, then the variance of e"
1619,unknown,"~, is given by je l otice that '12=0.04 is very small compared to the other retained eigenvalues. Thus, for several variables (namely, 1,2,3,4, and 5). V(~i ) is drastically increased by including component 12. (However. these high variances are partly balanced by the resulting large correlations between these ii,.) Hence. for this data. the regression is more easily interpreted in terms of the pr"
1620,unknown,"in terms of the principal components than in terms of the original variables. The problem of redundant information in a mUltiple regression can also be approached by discarding variables, and Example 6.7.1 describes this technique applied to the Jeffers' data. Note that the regression based on the eight most significant principal components explains 72.8% of the variance of y. which is slightly be"
1621,unknown,"variance of y. which is slightly better than the 72.5% obtained from Table 6.7.3. using the optimal choice for eight retained variables. However. for fewer than eight retained variables or components. the discarding vari· abies approach explains a higber percentage of the variance of y than the principal componen ts approach. [n general, it is perhaps prelerable to use the discarding variables app"
1622,unknown,"approach because it is conceptually simpler. Another tecbnique for dealing with redundant information in a regres· sion analysis is the method of ridge regression (HoerJ and Kennard, 1970). See Exercise 8.8.1. Exercises and Complements 8.1.1 For p>'O. show that the eigenvaJues are !(T~ + (Til ±t.1, where .1 = {«T~ -(Til' + 4(Ti(Tip2},11 with eigenvectors proportional to (2p<r,(T2, (Ti-(T~ +.1) and"
1623,unknown,"- 2p<r,(T2)' 247 PRINCIPAL CO~1PO:-""'B\rr ANALYS1S 8.1.2 Find the eigenvalues and eigenvectors of I = [(32;il I ~il ~]. (3 1 1+1l (Hint: 1:-ill=« (3. l. 1)'((3, I, 1) is of rank 1 with non·zero eigenvalue (3'+2 and eigenvector «(3. 1,1)'. Therefore I has eigenvalues (32+8+2. Il. Il. and the same eigenvectors as 1:-81.) 8.1.3 Suppose that x' = (x"" x,) has a bivariate multinomial distribution with ,"
1624,unknown,"with ,,= l. so that x ,= 1 with probability p. and x ,= 0 w ith probability (I = 1- p. and x, alway s satisfies X 2 = I - X ,. (a) Show that the covariance matrix of x is ( 1 -1) pq - 1 l' (b) Show that the eigenvalues of this matrix are 2pq and zero. and that the eigenvectors are (I. -1), and (1,1)"" respectively. 8.2.1 (DagneJie, 1975. p. 57) Berce and W ilbaux (1935) collected meas urements on f"
1625,unknown,"meas urements on five meterological variables over an I I-year period. Hence II = 11 and p =5 . The variables were x I = rainfall in ovember and Decem ber (in millimetres). x, = average July temperature (in degrees Celsiu ) x, = rainfall in July (in millimetres). x. = radiation in July (in m ilJilitres of alcohol). Xs = average harvest yield (in quintals per hectare). The raw data was as foUows: Y"
1626,unknown,"Year X, x, x, x. x, 192()-ZI H7.9 19.6 1.0 1661 28.37 192 1-22 89.9 15.2 90.1 968 23 .77 1922-23 153.0 19.7 56.6 1353 26.04 1923-24 132.1 17.0 91.0 1293 25.74 1924-25 88.S lR.3 93.7 1153 26.6g 1925-26 220.9 17.8 106.9 1286 24.29 1926-2 7 117.7 17.8 65.5 1104 28.00 1927-28 109.0 18.3 41.8 1574 28.37 1928-29 156.1 17.8 57.4 1222 24.96 1929-30 181.5 16.8 140.6 902 21.66 1930-31 181.4 17.0 74 .3 1150 "
1627,unknown,"1930-31 181.4 17.0 74 .3 1150 24.37 MULTlVARlA IE Al.'lAL YS JS 248 Show tbat this data leads to the me an vector. covariance matrix, and correlatioo m atrix given below: x = (138.0. 17.75,74.4,1242.25.66). 1794 -4.473 726.9 -2218 -52. OJ -0.087 1.488 - 26.62 197.5 1.577 R IS = 0.491 -0.624 1224 -6203 -56.44 -0.239 0.738 - 0.808 48104 328.9 -0.607 0.640 -0.798 0.742 4.087 (This matrix contains cor"
1628,unknown,"(This matrix contains correlations below the main diagonal. variances on the. main diagonal, and covarianoes above the m ain diagonal.) 8.2.2 (Dagne lie, 1975. p. 176) Using the correlation matrix for the meteorological variables x, to x. in Exercise 8.2.1, verify that the principal components Yo to y. explain 65,24,7. aDd 4%. respectively, of tbe total variance, and that the eigenvectors (x 1000)"
1629,unknown,"(291, -506, 577. -571), (871. 425,136,205), (-332,742.418. -405), (-214, -111. 688, 685). Following Section 8.2.5, show that the correlations between x, and Y, are given by the elements of the following matrix (+ 1000): [ 468 862 -177 -81j -815 420 397 -42 930 135 223 260 -919 202 -216 259 Hence calculate the proportion of varianoe explained by y, and Y. for each of XlJ X2, x3, and X4' 8.2.3 (Dagn"
1630,unknown,"each of XlJ X2, x3, and X4' 8.2.3 (Dagnelie. 1975. p. 178) Using the covariance matrix for the meteorological variables Xl to ;(. used in Exercise 8.2.2, show that eigen­ values are 49023, 1817.283, and 0.6, and that the eigenvectors (XIOOO) are, respectively, (-49,4, - 129, 990). (-296, -8,949.109), (954,3.288, 84), (-5,1000,8, -3). Interpret the principal comp one nts, and compare them with thos"
1631,unknown,"based on the correlation matrix found above. Which interpretation is more m eaningful? 249 PRINCIPAL COMPONEJ'Iff ANAL VSIS 8.2.4 Exercise 8.2.2 gives the first principal component in terms of tbe standardized variables u, = xJs,. Express this linear combination in terms of the original variables X;, and standardize the coefficients so tbeir squares sum to 1. Compare this new linear combination wi"
1632,unknown,"sum to 1. Compare this new linear combination with tbe first principal component using the covariance matrix (Exercise 8.2.3), to illustrate that principal component analysis is not scale invariant. 8.2.S U X is a sample data matrix, show that no SLC of the observed variables has a variance which is larger than that of the first principal component. Also prove resul t~ corresponding to the other p"
1633,unknown,"component. Also prove resul t~ corresponding to the other parts of Theorems 8.2.1-8.2.3. 8.2.6 (a) Extending Example 8.2.3, show that tbe first four students io Table 1.2.1 have lhe following SCOres on the first two principal compo­ nents. St.udenr I 2 3 4 Principal component 66.4 63.7 63.0 44 .6 2 6.5 -6.8 3.1 -5.6 (h) Show that tbe score on the first principal component is zero if aDd only if O."
1634,unknown,"only if O.5ix, +0.37 x. + 0.35x,+ 0.45x. + 0.53xs -99. 7 = 0 and the score On the second principal component is zero if and only if O. 75x, +0.21 x2 -0.08.<, -0.30x.-0.55xs + 1.5 =0. 8.2.7 Suppose F is a (p x k) matrix (/''''' p) with orthonormal columns and let A,:;' A, ... :;. Ap. Set "" h, = I f~, i =1 .. , .. PI .. , and set • • k d> (h) = I ,,,A, = I I f~A ,. i- I I- I i"""" 1 Show that p 0,,;; "
1635,unknown,"i- I I- I i"""" 1 Show that p 0,,;; h. """"1 and I h,=k. ,-1 MULTIVARIATE ANALYSlS 250 Hence deduce that c/>(b) is maximized when h, = ... = """" = 1, and h •. , = ... = lip =0; that is, when fhe columns ofF span the subspace of the first k rows of F. 8.2.8 In Example 8.2.5, the regression with both variables subject to error, let S denote the (2 x 2) covariance matrix of the data X . Using Exercise 8.1"
1636,unknown,"Exercise 8.1.1, show that the first principal component of the covariance matrix of (Ax"" x,) is given in terms of (xJ , x2 ) by the line with slope a = [522 - AS Il +{(S21- ASII)'+4 AS~2}'I2J/2s 12' Show that ISl2llsll<> lal<>s22/Is""I; that is. the slope lies between the slopes of the regression lines of X, on x, and of x, on x,. Also show that the line goes through the sample mean (x"" j.J if we t"
1637,unknown,"8.2.9 Verify formula (8.2.14) to show that the correlations between all the variables and a group G of components can be combined to give that part of the total variation accounted for by the components in G. *8.3.1 (Girshick, 1939) The p.d.f. of the eigenvalues of a W .(l, m) distribution is f(1 ..... , t,,)=c Ii Ijrn-.-I>n f1 (~-I,)exp( -1 t I;). ,- l i<, l ozo l Show that this may also be writt"
1638,unknown,"Show that this may also be written {(I ..... , 1,,) = cg·1m - .- O Il exp (-iap) n (4 -/j), i<, where a and g are the arithmetic and geometric means of the eigen­ values. Show that the eigenvectors of the Wishart matrix have the Haar invariant distribution on the space of orthogonal matrices. 8.3.2 Show that in the situatioo of E aml'ie 8.3.3, alternative confi­ dence intervals could in 'principle"
1639,unknown,"totically, (Hn - 1)'12(1, - AJ/.I.; - N(O , 1). Show that this leads to confidence intervals which can include negative values of Ai' and hence that the variance·stabilizing logarithmic transform used in Example 8.3.3 is preferable. 8.3.3 If p, and P2 are the two estimates defined in (8.3.2), show that p,=p,+{(p-trS)/(p -I)}. 251 P.RINCIPAL COMPONEt-.7 ANAL YSJS Hence the estimates are equal if an"
1640,unknown,"251 P.RINCIPAL COMPONEt-.7 ANAL YSJS Hence the estimates are equal if and only if tr S = p. Compare these estimators with the m.1.e. of p given in Exercise 4.2.12, where X is assumed to be of the form ~=<T'[( l-p)l +pll· ]. 8.4.1 Show that -2 log A given by (8.4.2) has an asymptotic chi-squared distribution with ~(p - k + 2)(p - k - 1) degrees of freedom. (Hint: Under Ho, X is determined by k dist"
1641,unknown,"Ho, X is determined by k distinct eigenvalues and one common eigen­ value, and the k eigenvectors each with p components which correspond to the distinct eigenvalues. This gives k + 1 + kp parameters. However , the eigenvectors are constrained by i,k(k + 1) orthogonality conditions, leaving k + 1 +kp-~k(k ... 1) free parameters. Under H , the Dumber of free parameters is obtained by putting Ie = p"
1642,unknown,"i,(p-l)p =~p(p+l). By subtraction and using Theorem 5.2.1, we find that the number of degrees of freedom is i(p-Ie + 2)(p - k -1).) 8.4.2 (Dagnelie. 1975, p. 183) An (86 )< 4) data matrix led to the covariance matrix s ~ ['""'''''' -0.008545 0.001 143 - 0.006 594] 0.003 318 0.000533 0.003 248 0.004 898 0.005231' 0.008463 Here the variables relate"" to the number of trees, height, surface area. and v"
1643,unknown,"volume of 86 parcels of land. The eigenvalUes of S are 1,=0 ,033687, 1,=0.01l163, 1,=0.000592 , and 1.=0.000241. Calculate the 'percentage of variance explained' by the various principal components and show that tr S = 0.045 683 and lSI = 0.536 41 x 10 '0. Show that if the hypothesis is A, = ,1.2, = A, = A"" then using (8.4.3) gives as a test statistic (86 -~) log 317.2 = 477 , which is highly sign"
1644,unknown,"gives as a test statistic (86 -~) log 317.2 = 477 , which is highly significant against X~· Show similarly that the hypothesis A, = A, = A. gives a X; statistic of 306, and the hypothesis ,1.,= 44 gives a X~ statistic of 16.2. Since all these are highly significant we conclude that the eigenvalues of X are all distinct. 8.4.3 (See Kendall, 1975. p. 79). Let D, be the determinant of S so that D,=I,"
1645,unknown,"D,=I,I, ... Ie' Let D 2 be the determ""inant of the m.l.e. of X tbat would MULTIVARIATE ANALYSIS 252 have arisen had the last (p - k) eigenvalues of l: been assumed equal. Show that D = II ,(""+' + ... +""')P-> , '2'''' p-k . Hence show that where ao and go are the arithmetic and geometric means of the last (p-k) eigenvalues of S. Hence show that"" log (D,/D,) = - 2 log A. where -Zlog A is given by (8"
1646,unknown,8.4.4 Consider tbe special case of (8.4.1) where k = I. Sbow that ill this case () = tb' tI' ~/tr 1;2. so that Confirm that this form ula leads to tile same numerical results as obtained in Example 8.4.1. 8.5.1 Show that the matrices A -lXB 'X' and B 'X'A 'X in (8.5.1) and (8.5.2) bave an eigenvalue of unity and that each of the corresponding eigenvectors is a vector of ones. 8.5.2 Show that each 
1647,unknown,"8.5.2 Show that each eigenvalue p of A -IXB -IX' lies between () and 1, and if p < 1 then tbe corresponding eigenvector r satisfies l' AJ = O. Prove a similar result for the matrix B -'X'A-'X. (Note that A ""21 is an eigenvector of the symmetric posltrve semi-dehnite matrix (A -I/'XB-1/2)(A-I/'XB-II')' and all other eigenvectors must be or­ thogonal to this one.) 8.5.3 If r is a standardized eigenv"
1648,unknown,"thogonal to this one.) 8.5.3 If r is a standardized eigenvector of A -IXB -IX' with eigenvalue p (p > 0), show that p-InB -1X'r is a standardized eigenvector of B -1 X' A -I X with the same eigenvalue. 8.6,1 (perron-Frobenius theorem) If A is a symmetric matrix with positive elements, then aU of the coefficients of the first eigenvector of A have the same sign. (Hint: Let I be the first eigenvecto"
1649,unknown,"by ,,~= IH Then m'Am;;.I'A1.) 8.7.1 The following principal components were obtained from the cor­ relation matrix of six bone measurements on 276 fowls (Wright. 1954). Show that by taking the threshold value A,,= 0 .70 we reject variables x •. 253 PRINCIPAL COMPONENT ANALYSTS X., Xl' and X, in that order. Component numbers . -Measurements 2 3 4 5 6 X,. skull length 0.35 0.53 0.76 -0.05 0.04 0.00 "
1650,unknown,"X,. skull length 0.35 0.53 0.76 -0.05 0.04 0.00 '"" skull breadth 0.33 0.70 -0.64 0.00 0.00 -0.04 X 3. wing humerus 0.44 -0.19 -0.05 0.53 0.19 0.59 x,.. wing ulna 0.44 -0.25 0.02 0.48 -0.15 -0.63 \: .. , leg 0.43 -0.28 -0.06 -0.51 0.67 -0.48 x,,, leg tibia 0.44 -0.22 -0.05 -0.48 -0.70 0.15 Eigenvalues 4.57 0.71 0.41 0.17 0.08 0.06 Show that the six components can be interpreted as overall size, hea"
1651,unknown,"size versus body size, head shape, wing size versus leg size, leg shape, and wing shape (although these last two interpretations are perhaps dubious). Verify thai the successive elimination of variables reduces the number of these features wb.ich can be discerned. 8.8.1 Hoerl and Kennard (1970) have proposed the method of ridge regression to improve tbe accuracy of tbe parameter estimates in the r"
1652,unknown,"regression model y = XIl+ '"" 1 + u, u - N. (0, ,,21). Suppose the columns of X have been standardized to have mean 0 and variance 1. Then the ridge estimate of ~ is defined by jl""= (X'X + kl)-'X'y, where for given X, k;;. 0 is a (small) fixed number. (a) Show that ~. reduces to the OLS estimate P = (X'Xl-'X'y wben k=O. (b) Let X'X = GLG' be a spectral decomposition of XX and let W = XG be tbe prin"
1653,unknown,"XG be tbe principal component transformation given in (8.1:1.2). If 0. = G'~ represents the parameter vector for the principal components, show tbat the ridge estimate Cl* of Cl can be simply related to the OLS estimate .. by j = 1 ~ ...• p. and hence jl*=GDG 'p , where D=diag(V(lj+k» . MULTTVARIATE ANALYSIS 254 (c) One measure of the accuracy of II"" IS given by the trace mea,. square error, "" 1>("
1654,unknown,"square error, "" 1>(k)=tr E[P*-II)(P*-II)']= L E(f3~-f3.)2 . I-I Show that we can write cb(k)='YI(k)+'Y,(k), where P • I 'Yl(k) = L V(f3~) = CT ' L '2 ._ 1 I-I (I,+k) represents the sum of the variances of f3~. and "" • 1 'Y2(k)= L [E(f3~-f3.)]2 =k2 L a ; 2 ._ , ;_I(I,+k) represents the urn of the squared biases of f3r (d) Show that the first derivatives of 'Y1(k) and 'Y2(k) at k = 0 are I 'Y'I(O)=-"
1655,unknown,"I 'Y'I(O)=- 2CT 2 L "" 1'2(0)= 0. I, Hence there exist values of k > O for which 1>(k)< q,(O), that is for w hich 1\* has smaller trace mean square error than ~. Note that the increase in accuracy is most pronounced when some of the eigenvalues I, are near O. that is. when the columns of X are nearly collinear. However, the optimal choice for k depends on the unknown value of II = Ga . 9 Factor Ana"
1656,unknown,choice for k depends on the unknown value of II = Ga . 9 Factor Analysis 9.1. Introduction Factor analysis is a mathematical model which attempts to explain the correlation between a large set of variables in terms of a small number of underlying faclOrs. A major assumption of factor analysis is that it is not possible to observe these factors direcUy; the variables depend upon the [actors but are
1657,unknown,"[actors but are also subject to random errors. Such aD assumption is particularly well-suited to subjects like psychology where it is not possible to m easure exacUy tbe concepts one is interested in (e.g. ""intelligence""). and in fact it is often ambiguous just how to define these concepts. Factor analysis was originally developed by psychologists. The subject was first put on a respectable statis"
1658,unknown,"was first put on a respectable statistical footing in the early 1940s by restricting attention to one particular form of factor analysis. that based on maximum likelihood estimation. In this chapter we shall concentrate on maximum likelihood factor analysis and also on a second method. principal factor analysis, which is closely related to the technique of principal component analysis. In order to"
1659,unknown,"In order to get a feel for the subject we first describe a simple example. Ex .am ple 9.1.1 (Spearman. 1904) An important early paper in factor analysis dealt with children's examination performance in Classics (x,). French (x2 ), and English (x,). It is found that the correlation matrix is given by 0.83 1 0.78) O.~7 . Although this matrix has fuJI rank its dimensionality can be effectively MULTIV"
1660,unknown,"MULTIVARiATE ANALYS tS 256 reduced from p = 3 to P = l by expressing the three variables as follows; X,=A,/+U,. (9.1.1l In these equations f is an underlying ""common factor"" and A,. )."" and A3 are know n as ""[actor loadings"". The terms "" I' "",. and '"" represent random disturbance terms. The common [actor may be interpreted as ""general ability"" and"", w ill have small variance if x, is closely relat"
1661,unknown,"general ability. The variation in u. consists of two parts which we shall not try to disentangle in practice. First. this variance represents the extent to which an individual's ability at C lassics. say, differs from his general ability, and second. it represents the fact that the examination is only an approximate measure of his ability In the subject. The mode l defined in (9.1.1) w ill now be "
1662,unknown,com mon factors. 9.2 The Factor Model 9.2.1 Definition Le t x(p x I) be a random vector with mean I'-and covariance matrix :t. Then w e say that tbe k-factor model holds for x if x can be written in the form s=Ar+ n + ..... (9.2.1) w here A (p x k) is a matrix of constants and I(k)( 1) and nIp x l) are random vectors. T he elements of f are called common factors and the elem ents of u specific or 
1663,unknown,"E(f) = 0. VIr) = I. E (n)= 0, C(u"" II,) = O. and C(I. u)= O. ; # i, (9.2.2) (9.2.3) (9.2.4) Deno te the covariance matrix of u by V(uJ = '"" = diag (1/1"" ..... 1/1,,,,). Thu s, all of the factors are uncorrelated with one another and further the comm on factors are each standardized to have variance 1. It i~ someti me~ convenient to suppose that f and u (and hence x) are multinormally distributed. "
1664,unknown,"distributed. No te that , x, = L A,J, + I', + iJ. .. i = l. .... p, ,-I 257 so that • U ii= L A ~+ tVii' i= 1 FACTO R ANAL YSlS Thus , the variance of x can be split into two parts. First, • hf= L A ~ ,-I is called the communaliry and represents the variance of X, which is shared with the other variables via tbe common factors. In particular A ~= C(x"" ti) represents the extent to which x.. depends"
1665,unknown,"A ~= C(x"" ti) represents the extent to which x.. depends on the jth com­ mon factor. On the other hand tjJ"" is called the specific or unique variance and is due to the unique factor 11;; it explains the variability in X; Dot shared witb tbe other variables. The validity of the k-factor m odel can be expressed In terms of a simple condition on :t. Using (9.2.1)-(9.2.4) w e get :t=AA '+'If. (9.2.5) "
1666,unknown,":t=AA '+'If. (9.2.5) The converse also holds. If 1: can be decompo sed into the form (9.2.5) tben the k-factor m odel holds for x. (See Exercise 9.2.2;) H ow ever. l and u are not uniquely determined by x. W e shall discuss this point further in Section 9.7 on factor scores. 9.2.2 Scale invariance Re-scaling tbe variables of x is equivalent to letting y = Cx , where C = diag(c,). If the k-factor m"
1667,unknown,"then x J:' y = CA ,I+Cu + CI'- and V (y) = C1:C = CA, A~C + C'If. C. Thus the k-factor model also holds for y with factor loading matrix A , = CA , and specific variances 'If v = Cqr. C = diag (ci'tjJ,,). Note that the factor loading m atrix for tbe scaled variables y is obtained by scaling the factor loading matrix of the original variables (multiply the ith row of A , by e,). A similar comment h"
1668,unknown,"specific variances. In other words, factor analysis (unlike principal comp o­ nent analysis) is unaffected by a re-scaling of the variables. 9.2.3. Non.uniqueness of factor loadings If the k-factor model (9.2.1) holds, then it also hold!> if the factors are rotated; that is, if G is a (k x k ) orthogonal matrix, then x can also be MULTIVARI A TE ANA LYS IS 258 written as ,,= (A G )(G 'f) + 0 + ..."
1669,unknown,"Since the random vector G 'I also satisfies the conditions (9.2.2) and (9.2.4). we see that the k -factor model is valid with new factors G 'f and new factor loadings AG. Thus . if (9.2.6) holds, we can also writel: as l:= (A G )(G 'N ) + IjI. In fact. for fixed 'It, this rotation is the on ly indeterminacy in the decomposi­ tion of l: in terms of A and '1': that is, if l: = Ai\' + 'It = A ' A "" +"
1670,unknown,"A = A *G for some orthogona l mat rix G (Exercise 9.2.3). This indeterminacy in the definition of factor loadings is usually resol­ ved by rotating the factor loadings to satisfy an arbitrary constraint such as A ''It- ' A diagonal. (9.2.7) or A 'O -' A is diagonal. 0 = diag (<1"" •.... <1"",,). (9.2.8) where in either case, the diagonal elements are written in decreasing order. say. B oth constrain"
1671,unknown,"order. say. B oth constraints are scale invariant and. except for possible changes 01 sign of the columns, A is then in general completely deter­ mined by either constraint. (See Exercise 9.2.4.) ote that when the number 01 factors k = 1. the constraint is irrelevant. Also. if some of the 0/1"" equa l O. then the constraint (9.2.7) cannot be used: an alternative is given in Exercise 9.2.8. n is of "
1672,unknown,given in Exercise 9.2.8. n is of interesl to compa re the number of parame ters in l: when ~ is unconstrained. with Ihe number of free parameters in the factor model. Let 5 denole the difference. At firsl sight A and 'I' contain pk + P free parameters. However (9.2.7) or (9.2.8 ) introduces 4k(k- l) constraints. Since the number of distinct elements of ~ is !p(p + I) we see that .t= !p(p + I )-{pk
1673,unknown,".t= !p(p + I )-{pk + p - 4k(k -I)} (9.2.9) Usually it will be the case that 5 > O. Then 5 will represent the extent to which the factor model offers a simpler interpretation for the behaviour of "" than the alternative assumption Ihat V(x) = 1:. If s :;' 0 and (9.2.6) holds and A and 'I' are known. then ! can be written in term of A and '1'. subject to the constraint (9.2.7) or (9.2.8) on A. 9,2,4 "
1674,unknown,"9,2,4 Estimation of tbe parameters in factor analysis fn practice. we observe a data matrix X whose information is summarized by the sample mean i and sample covariance matrix S . The location 259 FACTOR A N ALYSIS parameter is not of interest here, and we shall estimate it by ;.. = i. The interesting problem is how to estimate A and 'I' (and hence l: = Ai\' + '11) from S: that is, we wish to find"
1675,unknown,"constraint (9.2.7) or (9.2.8), for wbicb the equation (9.2.10) is satisfied. at least approximately. Given an estimate A, it is then natural to set "" ~ti = Sjl - I A ~, i = 1 ....• p, (9.2.11) ,-I so that the diagonal equations in (9.2.10) always hold exactly. We shall only consider estimates for which (9.2.11) is satisfied and ob :;. O. Setting f = AA' +-it. we get "" ~o that (9.2.11) is equivalen"
1676,unknown,"~o that (9.2.11) is equivalent to the condition i = 1. ... ,p. (9.2.12) Three cases call occur in (9.2.10) depending on tbe value of 5 in (9.2.9). If s < 0 then (9.2.10) contains more parameters than equations. Then, in general, we expect to find an infinity of exact solutions for A and 'It, and bence, the factor model is 1I0r well-defined. If 5 = 0 then (9.2.]0) can generally be solved for A and "
1677,unknown,"(subject to the constraint (9.2.7) or (9.2.8) on A ). The lactor model contains as many parameters as l: and hence offers no simplification of the original assumption that V(x )=1:. However. the change in viewpoint can sometimes be very helpful. (See Exercise 9.2.7.) If s> II, as will usually be the case, then there will be more equations tba~ para'lleters. Thus, il is not possible to solve (9.2.1"
1678,unknown,"tba~ para'lleters. Thus, il is not possible to solve (9.2.10) exactly in term s of A and '1', and we must look. for approximate solutions. [n this case the factor model offers a simpler explanation for the behaviour of x than the full covariance matrix. 9,2.5 Use of the correlation matrix R in estimation Because the factor model is scale invariant. we sball only consider estimates of A = A, and 'I"
1679,unknown,"estimates of A = A, and 'I' = 'It, which are scale invariant. It is then convenient 10 consider the scaling separately from the relationships between the variables. Let Y = HXD :s·12 where O s = diag (su ..... 5"",), MULTIVARIATE ANALYSIS 260 denote the standardized variables so that r= l 1 "" and - L y~= 1, n ,"""" I j= 1, .... p. Then Y will have estimated factor loading matrix A, = D oI/2A. and est"
1680,unknown,"estimated specific variances >It, = DoS '''' L' and (9.2.10) can be written in terms of the correlation matrix of x as R=A ,A;+>It,. (9.2.13) Note that (9.2.11) becomes i = 1 ..... p (9.2.14) so that""', is not a parameter of the model any more, but a function of A, . However. R contains p fewer free parameters than S so that '. the difference between the number of equations and the number of free "
1681,unknown,"parameters in (9.2.13). is still given by (9.2.9). The p equations for the eslimates of the scaling parameters are given by (9.2.12). Since in practice it is Ihe relationship between the variables which is of interest rather than their scaling. the data is often summarized by R rather tban S . The scaling estimates given in (9.2.12) are then not mentioned explicitly, and the estimated factor loadi"
1682,unknown,"iances are presented in terms of the standardized variables. Example 9.2.1 Let us return to the Spearman examination data wilb one factor. Since s=I(3-1)'-1(3+1)=0, there is an exact solution to (9.2.13), which becomes R= [ A~+tb"" (9.2.14) The unique solution (except for the sign of I At, .12, .13» is given by (9.2.15) - '2 tP"" = I-A ,. With Ihe value of R given in Example 9.1.1. we find thaI .1,="
1683,unknown,".1,=0.983, .1,=0.844. '\3= 0.794 tbll =0.034. $22 =0.287. .b,,=0.370. 261 FACTOR ANALYSIS Since tb;; = 1 - ,j ~. the model explains a higher proportion of tbe variance of x, than of x~ and X3' For general (3 x 3) correlation matrices R , it is possible for A I to exceed I (see Exercise 9.2.6). When this happens .bit is negative. However. this solution is unacceptable because tPll is a variance. He"
1684,unknown,"solution is unacceptable because tPll is a variance. Hence we adjust AT to be 1, and use an approximate solution for the other equations in (9.2.14). This situation is known as a ""Heywood case"". We shall describe two methods of estimating the parameters of the factor model when s > O. The first is principal {actor a.w/ysis and uses some ideas from principal component analysis. The second method . "
1685,unknown,"maximum likelihood factor analysis, is applicable when the data is as­ sumed to be normally distributed. and enables a significance test to be made about the validity of the k-factor model. For further techniques see Lawley and Maxwell (1971)_ 9.3 Principal Factor Analysis Principal factor analysis is one method of estimating the parameters of the k-factor model when s given by (9.2.9) is positive"
1686,unknown,"the k-factor model when s given by (9.2.9) is positive. Suppose the data is summarized by the correlation matrix R so that an estimate of A and'"" is sought for tbe standardized variables. (We are implicitly assuming that tbe variances for the original variables are estimated by u;, = s;, in (9.2.12).) As first step. preliminary estimates 11; of the communalities Ii:. i = 1 .... ,p, are made. Two c"
1687,unknown,"1 .... ,p, are made. Two common estimates of the itb communality are (a) the square of the multiple cnrrelation coefficient of the Hh variable witb all tbe other variables, or (b) the largest correlation coefficient between tbe ith variable and one of tbe otber variables; that is. maxi ~ ' Ir,.I. Note that the estimated communality fir is bigher when Xi is highly correlated with the other variable"
1688,unknown,"correlated with the other variables. as we would expect. The matrix R - -It is called the reduced cOffelarion matrix because the 1s on the diagonal have been replaced by the estimated communalities ii~ = 1-,fJ,;. By the spectral decomposition thenrem (Theorem A .6.4), it c<ln be written as (9.3.1) where a J;>< . • • ;>< a. are eigenvalues of R - >It with ortbonomlal eigenvec­ tors Y Olo' .. ,Y tp,"
1689,unknown,"MULTIVARiATE ANA L YSIS 262 Then the ith column of A is estimated by ... _ 1/2 At"" - a I 'Vhh i= l. ... ,k; (9.3.2) that is, ~h) is propOrtional 10 the ith eigenvector of the reduced correla­ tion matrix. In matrix form. /\ = rI A~ /2 . where r J = ('Yw •... , 'Y, ,,) and A I = diag (a I' . .. , a.). Since the eigenvec­ tors are orthogonal, we see that A'A is diagonal, so the constraint (9.2.8) is"
1690,unknown,"is satisfied. (Recall that we are working with the standardized variables here, each of whose estimated true variance is 1.) Finally, revised estimates of the specific variances are given in terms of A by k "" , ""2 0/11 = 1-L Aij, i= l, .... p. (9.3.3) ,=1 Then the principal factor solution is pennissible if all the .j,,, are non­ negative. As a motivation for the principal factor method, let us co"
1691,unknown,"happens when the communalities are known exactly and R equals the true correlation matrix. Then R - 'IT = AN exactly. The constraint (9.2.8) implies that the columns of A are eigenvectors of AN , and hence A is given by (9.3.2). Further the last (p-k) eigenvalues of R - 'IT vanish, ak ~1 = ... = up = 0. Th is property can be helpful in practice when R and 'IT are not exactly equal 10 their theoret"
1692,unknown,"'IT are not exactly equal 10 their theoretical values and k is nol known in advance. H opefuUy, lor some value of k. a ...... a. will be ""large"" and positive. and the remaining eigenvalues, U._J , •••• ap. will be near zero (some possibly negative). Of course, k must always be small enough that s»U in (9.2.9). Example 9.3.1 Consider Ihe open /closed book data of Table 1.2.1 with correlation matrix"
1693,unknown,"correlation matrix 0.553 0.547 0.410 0.389 1 I 0.0 to 0.485 0.437 I 0.7110.665 . 1 0.607 I If k > 2 then s < 0 and the faclor model is nol well defined. The principal factor solutions for k = 1 and k = 2, where we estimate the ith communal­ ity h~ by max, lr""l. are given in Table 9.3.1. The eigenvalues of Ihe reduced correlalion matrix are 2.84. O.3l!. 0.08. U.02. and - 0.05, sug­ gesting thai lhe"
1694,unknown,"gesting thai lhe two-factor solulion fits the data well. 263 FAcroR ANALYSIS Table 9.3.1 Principal factor solutions for the open/closed book data with k = I and k = 2 factors k=1 k = 2 Variable 11 ; ).,,, II? ).,1\ )."", I 0.417 0.646 0.543 0.046 0.354 2 0.506 0.711 0.597 O.711 0.303 3 0.746 0.864 0.749 0.864 -0.051 4 0.618 0.786 0.680 0.786 -0.249 5 0.551 0.742 0.627 0.742 -0.276 The first factor "
1695,unknown,"The first factor represents overaU performance and for k = 2. the second factor, which is much less important (a2=0.38« 2.84=a l). rep­ resents a contrast across the range of examinations. Note that even for the two-factor solution h~« 1 for aU i, and therefore a fair propOrtion of the variance of each variable is left unexplained by the common factors. 9.4 Maximum Likelihood Factor Analysis When "
1696,unknown,"When the data under analysis X is assumed La be normally distribuled. then estimates of A and 'IT can be found by maximizing Ihe likelihood. If 11 is replaced by its m.l.e. ji.=i. then. from (4.1.9). the log likelihood function becomes (9.4.1) Reg arding l:= AA'+'IT as a function of A and 'IT. we can maximize (9.4.1) with respecl to A and 'IT. Let A and'"" denote the resulting m.l.e.s subject to th"
1697,unknown,subject to the constraint (9.2.7) on A. Note that in writing down the likelihood in (9.4.1) we have summarized the data using the covariance mat rix S rather than the correlation matrix R . However J it can he shown that the maximum likelihood estimates are scale invariant (Exercise 9.4.1) and that the m.l.e.s of the scale parame­ ters are given by u ;; = s;; (see Theorem 9.4.2 below). Thus . (9.2
1698,unknown,satisfied. and the m.l.e.s for the parameters of the standardized variables can be found by replacing S by R in (9.4.1). However. it is more convenient in our theoretical discussion to work with S rather than R. Consider the function (9.4.2) where l:=AA '+'IT. MULTIVA.RIATE ANALYSIS 264 This is a linear function of the log likelihood I and a maximum in r corresponds to a minimum in F. Note that F 
1699,unknown,"corresponds to a minimum in F. Note that F can be expressed as F=p(a-Iogg-l). (9.4.3) where a and g are the aritbm etic and geometric means of the eigenvalues of ,rIS. The minimization of tbis function F(A . 'IT) can be facilitated by pro­ ceeding in two stages. First. we min imize F(A. 'IT) over A for fixed 'IT. and second, we minimize over 'IT. This approach has the advantage that the first mini"
1700,unknown,"minim ization can be carried out analytically although the minimization over 'IT must be do ne numerically. It was successfully developed by Joreskog (1967). Theorem 9.4.1 Le r 'IT>O be fixed and rer S"" = 'IT- I""S'IT- IJ'. Using tlte spect.,al decomposition theorem. write s*= r8r'. Then rI,e value of A satisfying rhe consrrainr (9.2.7). which mrnrnuzes F(A , 'IT) occurs when rI,e ith column of A """
1701,unknown,"c,'Ym, where C. = [max (8, -1. 0)]'12 for i = I. .... k. • Proof Since A *A "" (p xp) has rank at most k it may be written as A *A *'=GBG '. where B =diag(b"" ...• b,) contains (non-negative) eigenvalues and G = (&til' ... ,1:<,,) are standardized eigenvectors. Let M = GT . so that G may be written in terms of r as G = rM '. W e shall m inimize F(A. 'IT) over possible choices for M and B . in that o"
1702,unknown,"Since G 'G = 1 •. we see that MM ' = I,. From (9.2.5). = rM 'BMr' + 1 = reM 'BM + 1)1"". Using (A.2.4.!). we can write :£*-1 = r(1- M 'B (I + B )-I M )r'. A lso • 1:£*1 = n (h, + I). '-J 265 Thus FACTOR A..""JAL VSIS F(A. 'IT) = C+ I log(b, + 1)-trM'B (I+B )-IM6 I -:' -:' blJ = C + 4.. log (b, + I) - 4.. --'--b' mr;. I I.J 1+ i where C depends only on S and 'IT. If the eigenvalues {b,} and {e,l are "
1703,unknown,"written in decreasing order. then. for fixed {b,} and {e,l this quantity is minimized over M when 1tI;, = 1 for i = I, ...• k, and m"" = 0 for ij j. Thus M = (I"", 0) and so G = I'M' = ('Y(I)' ...• 'Y(k»' Then F(A. 'IT)=C+ I [Iog(b, + I)~]. b, +1 The minimum of this function over b, .. 0 occurs when b, = max (0, - 1. 0), (9.4.4) (although tbe value 0 in fact occurs rarely in practice). and so the mi"
1704,unknown,"minimizing value of A * satisfies • *A *' - -:' b ' .a - L. I 'Y(i )""""1(1)' (9.4.5) If the constraint (9.2.7) is satisfied then the columns of A ' are the eigenvectors of A*A ·'. and the theorem follows . • When 1""'1 = 0, the constraint of Exercise 9.2.8 can be used and special care must be taken in the minimization of F(A. 'lit). As in Example 9.2 .1, We sball term a situation in which 1""'1 = 0, "
1705,unknown,"We sball term a situation in which 1""'1 = 0, a Heywood case. Usually. the estimate '"" will be positive definite, although Heywood cases are by nO means uncommon (Joreskog, 1967). An important property of the m.l.e.s A and'"" is thal tbe estimate of tbe variance of the i lh variable. k • _ -:'""2 • U II- L A,,+,pll' equals the sample variance SII for i = I ..... p (Joreskog, 1967). We shall prove tbi"
1706,unknown,"prove tbis result in the special case wben the solution is proper. tbat is wben '"" > 0, though it is true in general. Theo rem 9.4.2 If '"" > O. rhen Diag (A.A' + '"" - S) = o. • Proof Writing aF/iJiJr as a diagonal matrix and using Section A.9 and the chain rule. it is straightforward to show that iJF(A, 'lit) Diag (! -'(}: - S)!-'), (9.4.6) MUL TtV ARlATE ANALYSIS 266 which equals 0 at (A , ""') wh"
1707,unknown,"which equals 0 at (A , ""') when'"" is proper. From (9.4.4) it is clear that at the m.L solution, b,""'(l + b,) = b:120, (or all i = J, ... ,p. Hence . S ';:' - b'''S· * -b""'e ""-b'12(I+b }y"" I\. (I) - I 'Y(i) - i .'Yeo - ; i (I) j= 1, ... ,po Since B = A ·'A· and i* =A * A"" + I, we can write this in matrix form as S*A* =A*(I+A""A*) = 0+ A*A*')A' =i'A*. Thu s (t· - S*)A' = O. Using this result together"
1708,unknown,"Thu s (t· - S*)A' = O. Using this result together with the expansion (A.2 .4f) (or i *-', we get (i *-s*)i*-I = (i'-S*l(I-A *(1 + A"" A*)-I A *'j = .i*-S·. Then pre-and post-multiplying by W In and "",-""', respectively. gives (i-S)i-' = (i - S)""'-'. Using this formula in (9.4.6). we get aF(A. ""'1 Diag(i-'Cl:-s)i -') a'"" =-0, and hence Diag (i- S)=o as required . • Example 9,4.1 Consider the open/clo"
1709,unknown,"Example 9,4.1 Consider the open/closed book data of Example 9.3.1. A m aximum likelihood lactor analysis for k = 1 and k = 2 factors is pre­ sented in Table 9.4.1 The. value and interpretations of the factor loadings Table 9.4. J Maximum likelihood (aclOr solutioos for the open/dosed boole data with k. = 1 and k = 2 f3ctors k = I k =2 k = 2(rolated) V ariable h~ ).11' h; 1<, .. 1<,,, 1\,,, 8('21 I"
1710,unknown,"I 0.359 0.600 0.533 0.628 0.372 0.270 0.678 2 0.446 0.667 0.582 0.696 0.313 0.360 0.673 3 0.841 0.917 0.811 0.899 -0.050 0.743 0.510 4 0.596 0.772 0.648 0.779 -0.201 0.740 0.317 5 0.524 0.724 0.569 0.728 -0.200 0.698 0.286 267 FACTOR ANALYSIS are broadly similar to the results of the principal factor analysis. How­ ever, caution must be exercised when comparing the two se t~ of factor loadings wbe"
1711,unknown,"loadings wben k = 2 because they have been rotated to satisfy diflerent constraints «9 .2.7) and (9.2.8), respectively). Note that. unlike principal factor analysis, the factor loadings when k = 1 do not exactly equal the first column of factor loadings when k = 2. 9.S Goodness of Fit Test One of tbe main advantages of the maximum likelihood technique is that it provides a test of the hypothesis H"
1712,unknown,"it provides a test of the hypothesis H . that k common factors are sufficient to describe the data against the alternative tbat 1: has no constraints. We showed in (5.3.8) that the likelihood ratio statistic A is given by - 2 log'"" = np(a - log 8 - 1). (9.5.1) where a and g are the arithmetic and geometric means of the eigenvalues of i -'S. Comparing this with (9.4.3) we see thaI - 2 log A = ""F(A,"
1713,unknown,"Tbe statistic -210g A has an asymptotic x; distribution under H •. where s is given by (9.2.9). In the trivial case where k = 0, H "" is testing the indep endence of the observed variables. The m .Le. of 1: in this case is i = Diag (S), and tbe eigenvalues of i-'S are the same as those of i-'i2Si-1/2 = R. In this case (9.5.1) becomes -It log IRI, a statistic whicb bas already been discussed in (5.3"
1714,unknown,"in (5.3.17) and Section 8.4.4. B artlet1 (1954) showed that the chi-squared approximation of (9.5.1) is improved if n is replaced by ,,' = n -l-Wp +5 ) - ~k. (9.5.3) The Chi-squared approximation to which this leads can probably be trusted if It """" P + 50. Hence [or any specified k we shall test H . with the statistic (9.5.4) where It' is given by (9.5.3), F is given by (9.4.2), and A and '"" are m"
1715,unknown,"maximum likelihood estimates. When H . is true this statistic has an MULTIVARIATE ANALYSIS asymptotic ch,-squared dIstribution WIth s=4(p-k),-!(p+k) degrees of freedom. 26M (9.5.5) Of cou~e. only rarely is k, the number of common factors. ,pecified in advance. In practice the problem is usually to decide how many common factors it is worth fitting to the data. To cope with this. a sequential proce"
1716,unknown,procedure for estimating k is needed. The usual method is to start with a small value of k. say k =0 or k = I. and increas~ the number of commOIl factors one by one until Hk is not rejected. How ever. this procedure I~ open to criticism because the critical values of the test l-Titerion have DOl been adjusted to allow for the fact that a set of hypotheses is being tested In sequence. FOLsome data 
1717,unknown,"In sequence. FOLsome data it may happen that the k-factor model is rejected for elll value;, of k for which s;;.O in (9.5.5). In such cases we conclude thaI there IS no factor model which will fit the data. Of course. if there exists a k for whicb s = 0, then it is usual1y possible to fit the data exacrly using ti,e factor model. But no reduction in parameters takes place in this case, so no hypot"
1718,unknown,"no hypothesis testing can be carried oul. and it is up to the u,er to decide whether this reparameterization is helpful. Example 9.5.1 Consider the open/closed book data for which we found the maximum likelihood factor solutions in Example 9.4.1. Here n = 88. For k = 1 factor. the goodness of fit statistic in (9.5.4) is !(iven by U =8.65 -x~ Since xL,o> = 11.1. we accept the one-factor solution as"
1719,unknown,"data. 9.6 Rotation of Factors 9.6.1 Interpretation of factors The constraint (9.2.7) or (9.2.8) on !he factor loading.< i.< a mathematical convenience to make the factor loadings unique; however , it can compli­ cate the problem of interpretation. The interpretation o( the factor loadings is the most stralghlforward if each variable is loaded highly on at most one factor. and if all the factor loa"
1720,unknown,"most one factor. and if all the factor loadings are either large and positive or near zero, with few intermediate values. The variables are then splil into disjoint sets, eacb of wbich is associated with one factor. and perhaps 269 FACTOR ANALYSIS some variables are left over. A factor j can then be interpreted as an average quality over those variables i for which ""if is large. The factors f in t"
1721,unknown,"The factors f in the factor model (9.2.1) are mathematical abstractions and do not necessarily have any intuitive meaning . In particular, the facto~ may be rotated using (9.2.6) without affecting the validity of the model and we are free to choose such a rotation to make the factors as intuitively meaningful as possible. It is considered a disadvantage to choose a rotation subjectively be­ cause "
1722,unknown,cause the factor analyst may try to force the factor loadings to fit bis own preconceived pattern. A convenienl analytical choice of rotation is given by the varimax method described below. 9.6.2 Varimu rotation The varimax method of orthogonal rotation was proposed by Kaiser (1958). Its rationale is to provide axes with a few large loadings and as many near-zero loadings as possible. This is acco
1723,unknown,"maximization of a quadratic function of the loadings. Let I\. be the (p x k) matrix of unrotated loadings, and let G be a (k x k) ortbogonal matrix. Tbe matrix of rotated loadings is 4 = I\.G; (9.6.1) that is, Il"" represents the loadings of the ith variable on the jtb factor. The function <b that the varimax criterion maximizes is the sum of the variances of the squared loadings withiIl each culmn"
1724,unknown,"matrix, where each row of loadings is normalized by its communality: that is, where d"" = ili' and h, (9.6.2) , ... t i-I • - _ -\, 2 d,-p i... d"". ,·1 The varimax criterion <b is a function of G, and the iterative algorithm proposed by Kaiser finds the orthogonal matrix G which maximizes <b. See also Horst (1965. Chap. 18). and Lawley and MaxweU (I97J. p. 72). In the case where k = 2. the calculat"
1725,unknown,"by G = ( cos 8 sin 8 ] -sin 8 cos 8 and represents a rotation of the coordinate axes clockwise by an angle 8. MULTrvARJATE NALYSlS 270 Thus, d"" ={A;, cos 6-A ;2Sin 6}/I~ , d""={A ,, sin 11+10., oos III/h,. Let Substituting in (9.6.2) and using 4(cos"" 6+sin 4 6)=3 + cos411, sin 211 = 2 sin 0 cos 0, cOs 20 = cos' /I-sin2 0, it can be shown that where and 44> = (A 2+ B ,),n cos (46 - a)+ C. (9.6.3) A "
1726,unknown,"A =(G o .. + G •. o- 6G2.2- G~., -Gt,, + 2G O •2 G 2 .0 +4G ;',), (9.6.4) B = 4(G 1.3 - G ,.,- G, .,G "".2+Gl.l G,.o), (9.6.5) C = p(3[G 2.0+ Go.,J' -[3G~.~ +3G to+ 2G O,;z.G 2.0 +4G; .,D . (A '+ B 2)1I'oosa = A , (A 2+ B2)'12 sin a = B . (9.6.6) In (9.6.3), the maximum value of 4> is obtained when 40 = a. The value of ()( is obtained from (9.6,6) using tan a = B/A (9.6.7) and a is uniquely determi"
1727,unknown,"tan a = B/A (9.6.7) and a is uniquely determined from a consideration of the signs of A and B. Note that 0 can take four possible values. In the case of k > 2 factors, an iterative solution for the rotation is used. The first and second factors are rotated by an angle determined by the above method. The new first (actor is then rotated with tbe original third factor. and so on, until all the tk(k "
1728,unknown,"third factor. and so on, until all the tk(k -1) pairs of factors have been rotated. This sequence of rotations is called a cycle. These cycles are then repeated until one is completed in which all the angles have achieved some predetermined convergence criterion. Since the effect of each rotation is to increase the value of 4> and the values of 4> are bounded above. the iteration will converge (se"
1729,unknown,"Example 9,6,1 Consider the two factor maximum likelihood solutions for the open/closed book data given in Example 9.4.1. Using (9.6.3)- 9.6.7) it is found that 6=37.6° for the varimax rotation and the rotated 27 1 FACTOR AN A LYStS (actors are given in Table 9.4.1. The rotation can be represented graphi­ cally (Figure 9.4.1) by plotting the factor loadings as p = 5 points in k = 2 dimensions. The "
1730,unknown,"dimensions. The first factor now represents an ""open-book "" effect ami the econd factor a ""closed-book"" effect. although both factors influence the other exams as well. In this sinlple example the overall interpretation is the same whether we use the rotated or the original factors. However. in more complicated situations, the benefits of rotation are much more noticeable. Exam ple 9.6.2 (Kendall,"
1731,unknown,"Exam ple 9.6.2 (Kendall, 1975). In a job interview, 48 applicants were each judged on 15 variables. The variables were ( I) Form of leiter of application (2) Appearance (3) Academic ability (4) Likeability (5) Self-confidence (6) Lucidity (7) Honesty (8) Salesmanship ~ I , I (9) (10) (11) (12) (13) (14) ( 15) / I / "" I "" / ., Experience Drive A.mbition Grasp Potential Keenness to join Suitability "
1732,unknown,"Suitability , 1 · 2 , / , I "" / ---------'-li(-/--.-------------Ol i gt~l 0,., I / "" / "" / / I I / "" , I 8 = 39.8 0 Figure 9.4.1 PlOl of open/d osed book {aClOrS before alld a/rer varimax rorariml. MULTIVARiATE ANALYSIS 272 Table 9.6.1 Correlation matrix for the applicant data (K endall. 1975) :2 3 4 5 6 1 8 to 11 12 D 14 15 I I.IXll1.24 0.04 0.31 0.09 0.23 -0.11 0.27 0.55 0.35 0.28 0.34 0.31 0.47"
1733,unknown,2 1.00 0.12 0.3R 0.43 0.37 0.35 0.48 0.14 0.34 0.55 0.51 0.51 0.28 0.38 3 LOU 0.00 0.00 0.08 -0.03 0.05 0.27 0.09 0.04 0.20 0.29 -0.32 0.14 4 LOO 0.30 0.48 0.65 0.35 0.14 0.39 0.35 0.50 0.61 0.69 0.33 5 1.00 0.81 0.41 0.82 0.02 0.70 0.84 0.72 0.67 0.48 0.25 b 1.00 0.36 0.83 0.15 0.10 0.16 0.88 0.78 0.53 0.42 1 1.00 0.23 -0.16 0.28 0.21 0.39 0.42 0.45 0.00 R LOO 0.23 0.81 0.86 0.71 0.73 0.55 0.55 9
1734,unknown,"9 1.00 0.34 0.20 0.30 0.35 0.21 0.69 10 1.00 O.1~ 0.7\ 0.79 0.61 0.62 11 1.00 n.78 0.77 0.55 0.43 12 1.00 0.88 0.55 0.53 13 1.00 0.54 0.57 14 1.00 0.40 15 1.00 and the correlation matrix between Ihem is given in Table 9.6.1. Because many of Ihe correlations were quite high, it was felt that the judge might be confusing some of the variables in his own mind . Therefore a factor analysis was perform"
1735,unknown,"analysis was performed to discover the underlying types of variation in the data. A maximum likelihood factor analysis was carried out and, using the Table 9.6.2 Maximum likelihood [actor solution of applicant data with k ~7 factors. unrotated Factor loadings V ariable 2 3 4 5 f> 7 t 0.090 - 0.134 -0.338 0.4011 0.411 -O.oot 0.277 2 -0.466 0.171 O.OJ7 -0.002 0.517 -0.194 0.167 3 - 0.131 0.466 0.153"
1736,unknown,3 - 0.131 0.466 0.153 0.143 -0.031 0.330 0.316 4 0.004 -0.023 - 0.318 -0.362 n.657 0.070 0.307 5 - 0.093 0.017 0.434 -0.092 0.784 0.019 -0.213 6 0.281 0.212 0.33n -n.n37 0.875 O.OI)! 0.000 7 -0.133 0.234 -0.1 I -0.807 11.494 lUll) I -0.000 8 -O.oJ8 0.055 0.258 0.207 0.853 0.019 -0.180 9 -0.043 0.173 -0.345 0.522 0.296 0.085 0.185 10 -0.079 -0.012 0.058 0.241 0.817 0.417 -0.221 II -0.265 -0.131 0.4
1737,unknown,II -0.265 -0.131 0.411 0.201 0.839 -0.000 -0.001 12 0.037 0.202 0.188 0.025 f).B75 0.077 0.200 13 -0.112 0.188 0.109 0.061 0.844 0.324 0.277 14 0.098 - 0.462 -0.336 - 0.116 0.807 -0.001 0.000 15 -0.056 0.293 -0.441 0.577 0.619 0.001 -0.000 hvpothesis test described in Section 9.5. it wa s found that seven raclOrs . ere needed to explain the data adequately. The original factor loadings -e given in
1738,unknown,"-e given in Table 9.6.2 and the loadings after the varimax rotation are 'Iven in Table 9.6.3. It is very difficult to interpret the un rotated loadings. However. a • ·ltlern is clear from the rotated loadings. The firsl faclor is loaded """"avily on variable 5,6.8. 10, 11. 12. and 13 and represents perhaps an <ltward and salesmanlike personality. Factor 2. w eighting variables 4 and represents likea"
1739,unknown,"represents likeabilit)l and factor 3. weighting variables 1. 9, and 15, opresents experience. Factors 4 and 5 each represent one variable, . ~ademic ability (3). and appearance (2). respectively. The last two factors lve little importance and variable 14 (keenness) seemed to be associated lith several of the faclOrs. Thus . the judge seem 10 have meas ured rather fewer qualities in his lndidates t"
1740,unknown,"lndidates than he thought. 'l.7 Factor Scores )0 far our study of the factor model has been concerned with the way in Nhich the observed variables are (unctions of the (unknown) factors. For :xample . in Spearman 's examination data we can describe the way in w hich a child's test scores will depend upon his overall intelligence. MULTIVARlATE ANALYSJS 274 However , it is also of interest to ask th"
1741,unknown,"MULTIVARlATE ANALYSJS 274 However , it is also of interest to ask the converse question. Given a particular child's test scores, can we make any statement abo ut his overall intelligence.? For the general model we want to know how the factors depend upon the observed variables. One way to approach the problem is to treat the unknown com mOIl {aClor scores as parameters to be estimated. Suppose x i"
1742,unknown,"random vector from the factor model (9.2.1) and suppose that A and '"" and j1 = 0 are known. Given the vector f(p x 1) of commo n (actor scores. x is distributed as Np(AI, ""'). Thus. the log likelihood of x is given by I(x: I} = -4(x - AI),,,,-I(x - AI} -! log 12'IT""'1· (9.7. t) Setting the derivative with respect to f equal to 0 gives ~ = A ''''-'(X- Al) =0. af so (9.7.2) The estimate in (9.7.2) i"
1743,unknown,"so (9.7.2) The estimate in (9.7.2) is known as Barllell's faclor score. The specific factor scores can then be estimated by ii = x - M. Note that (9.7.1) is the logarithm of the conditional density of"" giveili. H ow ever, under the factor model, f can be considered as an N,(O, I) random vector, thus giving 1 a prior distribution. Using this B ayesian approach, the posterior density of 1 is proport"
1744,unknown,"approach, the posterior density of 1 is proportional to exp [-!(x- AI),'lrl(x- AI)-!f'f]. (9.7.3) wh ich i a multinomIal density whose mean I' = (I + ""-,,,,- I A)-' ""-,,,,-IX (9.7.4) is the B ayesian estimate of I. The estimate in (9.7.4) is known as Thompsoll's facror score. Each of these two factor scores ha some favourable properties and there has been a long controversy as to which is better. "
1745,unknown,"there has been a long controversy as to which is better. For example E(ilf)= f. (9.7.5) so that Bartlett's factor score is an unbiased estimate of I, whereas Thompson's score is biased. However, the average prediction errors are given by E «i - f)(f -I)') = (A''''-' A )-', E(I* - f)(f*-f)') = (I + A"" Jrl A )-I. (9.7.6) (9.7.7) so that Thompson 's score is more accurate. [f the columns of A satisfy"
1746,unknown,"so that Thompson 's score is more accurate. [f the columns of A satisfy the 275 F'ACfO R ANALYSIS constraint (9.2.7). then. for either factor score, the components of the factor score are uncorrelated with one another. ote that if the eigen­ values of A'Y I A are all large, then the prediction errors will be small. and also the two factor scores will be similar to one another. Of course in practic"
1747,unknown,"Of course in practice A . ""'. and j1 are not known in advance bot are estimated from the same data for w hich we wish to know the factor scores. Tt would be theoretically attractive to estimate the factor scores. (actor load ings, and specific variances all at the same time from the data. using maximum likelihood. However . there are too many parameters for this to be possible. There are many valu"
1748,unknown,"likelihood becomes infinite. See Exercise 9.7.2. 9.8 Re lationships Between Factor Analysis and Principal Component Analysis Factor analysis. like principal comp one nt analysis, is an attempt to explain a sct of data in a sm aller number of dimensions than one starts w ith. Because the overall goals are quite similar, it is worth looking at the differences between these twO approaches. First. pri"
1749,unknown,First. principal component analysis is merely a transformation of the da ta. 0 assumptions are made about the form of the covariance matrix from which the data CO m es. On the other hand . factor analy i supposes that the data comes from the w ell-defined model (9.2.J1. where the underlying factors satisfy the assumptions (9.2.21-(9.2.4). If these as­ sumptions are not met. then factor analysis ma
1750,unknown,"sumptions are not met. then factor analysis may give spurious results. Second , in principal component analysis the emphasis is on a transfor· mation from the observed variables to the principal componen ts (y = r 'x), w hereas in factor analysis the emphasis is on a transform ation from the underlying factors /0 the observed variables. Of coorse, tbe principal component transformatio n is inverti"
1751,unknown,"component transformatio n is invertihle ("",= ry), and if we have decided to retain the fir t k componcnts, then x can be approxim ated by these component s. x= ry= rlYI + r, y,~r' Y I ' However , this point of view is less natural than in factor analysis where x can be approximated in terms of the common factors and the neglected specific factors are explicitly assumed to be ""noise"". Note that in "
1752,unknown,"Note that in Section 9.3, when the specific variances are assum ed 10 be O. principal factor analysis is equivalent to principal component analysis. MULTJVARlATE ANALYSIS 276 Thus , if the factor model holds and if the specific variances are small. w e expect principal component analysis and factor analysis to give similar results. However, if the specific variances are large, tbey will be absorbe"
1753,unknown,"into all the principal components. both retained and rejected, whereas factor analysis makes special provision for them . 9.9 Analysis of Covariance Structures T he factor model can be generalized in the following way (Joreskog, 1970). Let X (nXp) be a random data matrix with mean E(X)=AEP , (9.9.1) where A (n x g) and P (h x p) are known matrices and E (g x h) is a matrix of param eters. Suppose "
1754,unknown,"tbe same covariance m atrix }; = B (A<I>N + ""')B + 8 . (9.9.2) Here B (P x q ). A(q x k) the symmetric matrix <1> ;;.0, and the diagonal matrices""';;, 0 and e ;;. 0 are all parameters. In applications some of the parameters may be fixed or constrained. For exam ple in factor analysis all the rows of X have the same mean and we take B = I, <I> = I, and 8 = O. The general model represented by (9.9.1"
1755,unknown,"The general model represented by (9.9.1) and (9.9.2) also includes other m odels where the means and covariances are structured in terms of parameters to be estimated, for instance multivariate regression. MANOY A . path analysis. and growth curve analysis. For details see Joreskog (1970). The practical importance of the general model lies in the fact that it can he successfully programmed . For a"
1756,unknown,"further details, see Joreskog and Sorbom (1977). Exercises and Complements 9.2.1 In the factor model (9.2.1) prove that l:=AN .... "",. 9.2.2 (a) If x is a random vector whose covariance ma trix can be written in the form 1: = AA ' + ""', show that there exist factors I. u such that the k-factor model holds for x. Hint: tet y -N.(O, h 'A'"",-IA ) independently of "" and define (u) _ (I. A)-l (""-11) I "
1757,unknown,"independently of "" and define (u) _ (I. A)-l (""-11) I - -A''If-l Ik Y ' 277 FAcroR ANALYSIS (~) 11 x is norm ally distributed. show that (f. u) ma y be assumed to be normally distributed. (c) Show that f and u are not uniquely determined by x. 9.2.3 If AN = d4' (A and d are (p x k ) matrices). show that d = AG for some (k x k) orthogonal matrix G . (Hint: use T heorem A.6.S) 9.2.4 If AN =~, and A'"
1758,unknown,"9.2.4 If AN =~, and A''If-1A and d '"",-14 are both diagonal matrices wjth distinct elements written in decreasing order. show that A,,,= ± om. i = I. .... k. (Hint: Write AN = ""'""2[ t a, 'Yn''Y:fI]W l !2, , ~ l where 'Y,,, ;s the ith standardized eigenvector of "",-·12AN"", -'12 with eigenvalue ad and show that ""'-""'A,,) = c%Q !12'Y,il' Now do the same thing for d .) 9.2.5 11 S(3 X 3) is a sam ple c"
1759,unknown,"for d .) 9.2.5 11 S(3 X 3) is a sam ple covariance matrix, solve S = AA' + '"" for A and W . Compare the answer witb (9.2.15) to show that the solution is scale invariant. 9 .2.6 Le t R be a (3 x 3) correlation matrix w ith '12= r"" =J. 'Z3 =iI"",. C heck that R is positive definite. Show that At > 1 in 19.2.1.5) and hence that the exact factor solution is unacceptable. 9.2.7 (Structural relationship"
1760,unknown,"9.2.7 (Structural relationships) Supp ose a scientist has p instruments with which to mea sure a physical quantity f;. He knows that each instrument meas ures a linear function of f; but all the measurements are subject to (independent) errors. The scientist wishes to calibrate bis instrume nts (that is, discover what the linear relationship between the different instruments would be if there were"
1761,unknown,"accuracy of each instrument. 1f the scientist takes a sam ple of meas urem ents for whicb the gs can be regarded as realizations of a random variable. then show that bis problem can be approached using the 1-factor model (9.2. I}. In particular it p = 2, >how that without m ore information abo ut the error variances. he does not have enough information to solve his problem . (See Examp le 8.2.5.) "
1762,unknown,"9.2.8 Suppo se the factor m odel (9.2.1) is valid with some of the 0/1"" equal to O. Then of course the constraint (9.2.7) is not appropriate and we consider an alternative constraint. For simplicity of notation suppose 0/1,,= ... =.v"",,=O and 0/1.+1.._"" ... ,0/1,,,,>0. and partition and A = rA Il LA"" Al!]. A '2 MULTIVARIATE AN A LYSIS 278 We shall explain the variation in the first g variables exa"
1763,unknown,"We shall explain the variation in the first g variables exactly in terms of tbe first g faclors and explain the part of the variation of tbe last (p - g) variables not explained by the first g variables (l:,2., =l:,2-l:,,~,' l:I2) in terms of the remaining k - g factors. Let l:"" = reT' be a spectral decom ­ position of I, l(g x g), where a is a diagonal matrix of eigenvalues and tbe columns of r a"
1764,unknown,"tbe columns of r are standardized eigenvectors. Suppose 1\.* satisfies Afl=re1l2, Af2 = O. I\.t =l:""ra-'J2, 1\.~2 ""'2' 1\.~2 is diagonal, A~2 Ai~ + 'It 2 -= ~2. 1' where ""'2(P - g)X (p- g) is a diagonal matrix with elements ojJ"", i> g. Show that (a) if 1\.* satisfies these conditions then I = 1\.11.' + '"" = I\. • I\. •• + ""'; (b) there always exists an orthogonal matrix G(k X k) sucb that I\. = I\"
1765,unknown,"I\. = I\. *G; (c) except for tbe signs and order of the columns , A · is uniquely determined by tbe above conditions. 9-3_1 Suppose in a principal factor analysis that all the preliminary estimates of the specific variances are given by the same number iii;; = c. (a) If c = 0, show that the columns of A are the first k principal component loading vectors of the correlation matrix R. scaled so that"
1766,unknown,"A,;j'm equals the variance of the ith principal component. (b) If c > 0 show that tbe columns of A are still given by the principal component loading vectors of R . but with a different scaling. (c) If k > l and R=(l-p)l+ pll', O<p<l , is the equicorrelation matrix, show that the largest value of c which gives an acceptable principal faclOr sol urian is c = 1-p. 9.4_1 Let F(A , >Jt; S) be given by"
1767,unknown,"9.4_1 Let F(A , >Jt; S) be given by (9.4.2) and let A ,.Jr denote the minimizing values of A and ""'. If C is a fixed diagonal matrix with positive elements. show that I\. = CA and '"" = C.JrC minimize the func­ tion F(I\.. ""'; CSC). Hence deduce that the maximum likelihood es­ timators are scale invariant. 9 .4_2 Explain why the hypothesis test described in Section 9.5 can be constructed with only "
1768,unknown,"being needed). 279 FA CTO R ANALYSIS 9_6_1 (Maxwell. 1961; Joreskog, 1967) A sel of p = 10 P ycbological variables were measured on /I = 810 normal children. with correlations given as follows: Tests 2 3 .j 5 6 ~ Q III I 0.345 !J.594 0.404 0.579 - 0,280 -0.449 - 0.188 -0.303 - 0.200 2 0,477 U.338 U .230 - 0,15 ' -0.205 - 0.120 - 0.168 -0.145 3 00498 0.505 - 0:251 - 0.377 - 0.186 - 0.273 - 0,154 4 "
1769,unknown,"4 0.389 - 0.168 - 0.249 - 0.173 - 0.195 -0.055 5 - 0,151 - 0 2 85 - 0.129 - 0.159 - U.079 6 0.363 0.359 0.227 0.260 7 11.448 0.439 11.511 ~ 0.429 0.3 1. 9 0.30 1 A maximum likelihood factor analysis was carried out with k = 4 factors yielding the following estimate A of the factor loadings (rotated by varimax): -0.Q3 0.59 - 0.31 0.41 -0.04 0.09 - 0.12 0.59 - 0.06 0.42 -0,20 0.69 - 0.11 0.33 - 0.08"
1770,unknown,"- 0.11 0.33 - 0.08 0.48 - 0.06 0.76 - 0.07 0.24 0.23 - 0.11 0.36 - 0.17 0.15 - 0.24 0.78 - 0.16 0.93 -0.04 0.37 -0.06 0.27 -0.11 0.44 - 0.18 0.09 - 0.00 0,63 - 0.04 (a) Give interpretations of the four factors using the factor loadings. (b) The m .1. factor loadings give a value of F(A , .Jr) = 0.0228 in (9.4.2). Using (9.5.4). carry out a test of the hypothesis that four factors are adequate to d"
1771,unknown,"9_7.1 Verify formulae (9.7.SH9.7.7) for the factor scores. (Hint: in (9.7.7) use (A.2.4f) to show that E(f*f*') = E«(*f') = \1 + 11.'''' -. 1\.)- • 11.''''-' A = 1- (1->-11.''''-' 1\.)-' ,) MULTIVARIATE ANALYS IS 280 9.7.2 The following example shows that the specific variances and factor scores cannot be estimated together. For simp licity let k = 1 and suppose that A = X(px 1) anu ,,""= 0 are kno"
1772,unknown,"that A = X(px 1) anu ,,""= 0 are known. Let F = f(/I x I) denote the un­ known factor scores. The n, extending (9.7.1), the likelihood of the data X can be written L = 12."".'l't,n exp [-~. f (x,,, - A,f)'(x,i) - A,f)/o/Iii]' ,-, (Note that f(n x 1) here represents the scores of /I individuals on one factor whereas in (9.7. I) f(k x 1) represented the scores on k factors of one individuaL) Suppose A"
1773,unknown,"one individuaL) Suppose A, ~ O. Show that for any values of 0/122, ... , oJIpp if f = A,' I) and 0/1,,->0, then the likelihood becom es infinite. H ence m.l.e.s do not exist for this problem 10.1 10 Canonical Correlation Analysis Introduction Canonical correlation analysis involves partitioning a coll~ction of vari­ ables into two sets, an x-set and a y-set. The Object is then to find linear combi"
1774,unknown,"combinations Tj = a'x and c/> =b'y such that Tj and c/> have the largest possible correlation. Such linear combinations can give insigbt into the relationships berween Ihe two sets of variables, Canon ical correlation analysis has certain maximal properties similar to those of principal component analysis. However, whereas principal com­ ponen l analysis considers interrelationships witlli .. a se"
1775,unknown,ponen l analysis considers interrelationships witlli .. a set of variables. the focus of canon ical correlation is on the relationship between rwo groups of variables. One way to view canon ical correlation analysis is as an extension of mul tiple regression. Recall that in multiple regression analysis the vari­ ables are par1itioned into an x-set containing q variables and a v-.et containing p = 
1776,unknown,"containing p = I variable. The regression solution involves finding the linear combination a'"" which is most highly correlated with y, In canonical correlation analysis the y-set contains p'"" I variables and we luok for vectors a and b for wbich the correlation bcrwen a'x and b'y is max imized. If x is interpreted as ""causing"" y. then a'x may be called the ""titst predictor"" and b'y the ""mos\.predi"
1777,unknown,"""titst predictor"" and b'y the ""mos\.predictable criterion"". However , there is no assumption of causal asymmetry in the mathematics of canonical correlation analysis; x and y are treated symmetr ically. Ex ample 10.1.1 Consider the open/closed book data of Table 1.2.1 anti split the five variables into rwo sets-the closed-book exams (x"" x2 ) and the open-boo k exalllS (y"" Y2, y,). One possible qua"
1778,unknown,"the open-boo k exalllS (y"" Y2, y,). One possible quantity of interest here is how highly a student's ability on closed-book exam is correlated with his ability on open-book exams . MULTIVARIATE ANALYSIS 282 Alternatively, one might try to use the open-book exam results to predict the closed-book results (or vice versa). This data is explored furtber in Example 10.2.5. 10.2 Mathematical Development"
1779,unknown,10.2 Mathematical Development 10.2.1 Population canonical correlation analysis Suppose tbat x is a q-dimensional random vector and that y is a v-dime nsional random vector. Suppose further that x and y have means ... and v. and that E{(y-v)(y- v)'j <E 22. Now consider the two linear comb inations 'J) = .'x and <I> = b'y. The correlation between Tj and <I> is pta. b) (J 0.2.1) We use tbe notation p
1780,unknown,"pta. b) (J 0.2.1) We use tbe notation pta, b) to emphasize the fact that this correlation varies with different values of a and b. Now we may ask what values of 8 and b maximize pta, b). Equivalently we can solve the problem maX8 'X , 2b subject to a'X lla= b'X 22b = I, ... because (10.2.1) does Dot depend on the scaling of a and b. ( 10.2.2) The solution to this problem is given below in Theorem "
1781,unknown,"The solution to this problem is given below in Theorem 10.2.1. First, we need some notation. Suppose that X II and x'z are non-singular and let Then set and NI=KK' , M J = ~111 /2N I 'I : {2 = l:l11I.1 2""I.2~ ~1 ' M 2 = l:2~ 12 2:I ~= I.2d'I21 l:-;-tIlz_ (10.2.3) (10.2.4) (10.2.5) 283 C ONlCAL CORRELA TrONAL ANALYSIS Note that NI and M I are (qxq) matrices, and N2 and M2 are (pxp) matrices. From T"
1782,unknown,"matrices. From Theorem A.6.2, MI ,N I, M 2 , and N2 all have the same noD -zero eigenvalues. Further, since NI =KK' is p.s.d., all the non-zero eigenvalues are positive. Let k denote the number of non-zero eigen­ values. Then , by (A.4.2e) aod (A.4.2f), k = rank(K) = rank(l:I2)' For simplicity suppose that the eigenvalues are aU distinct, A I> ... > A. > O. By the singular value decomposition theo"
1783,unknown,"be written ;n the form K=(fth .•• 'ft.)O(llI.··· ,P.l'. (10.2.6) where fti and PI are the standardized eigenvectors of N I and N 2, respect­ ively, for Ai, and O =diag(A:12, ... , A!I2). SlDce the Ai are distinct, the eigenvectors are unique up to sign. (The signs of the eigenvectors are chosen so that the square roolS in 0 are positive.) Also, since N, and N 2 are symmetric the eigenvectors are o"
1784,unknown,"N 2 are symmetric the eigenvectors are orthogonal. Thus (10.2.7) wbere Ili; is the Kronecker delta, equal to 1 for j = i and U otherwise. Definition Usillg Ihe above nolalio"" leI i = 1 ..... k. (10.2.8) Then (a) the vectors ai and hi are called Ihe jth canonical correlation vectors for x and y, respeclively; (b) the randoll! variables 'Ii = aix and <1>, = bly are called Ihe ith can­ onical correla"
1785,unknown,"(c) Pi = Ail12 is called the ith canonical correlation coefficient . Note that C(lli, llj)=a~l: 1l 8i =aiaJ =011' C(<I>,. <1>;) = blX,2h; =PiP, = Il;;. (10.2.9) Thus the ilh canonical correlation variables for"" are uncorrelated and are standardized to have variance 1; similarly for the ith canonical correlation variables for y. The main properties of canonical correlation analysis are given by the"
1786,unknown,"given by the foUowing theorem . Theorem 10.2.1 Using Ihe above '1OIatiol~ fix T, 1,;;; ,,;;; k, and leI subiecl to a'l:lla= 1, b'X 22b= 1, f, = max a'Il2h ... i = 1, ... , r-1. (10.2.10) MULTIVARIATE A N ALYSIS 284 Tltell tlte maxillllllll is given by f,=p, alld is arramed whell &=s"" b=b,. Proof Before giving the proof, note that (10.2.10) is the largest correla­ tion between linear combinations a"
1787,unknown,"tion between linear combinations a'"" and b'y subject to the restriction that a'"" is uncorrelated with the first r - 1 canonical correlation variables for x. Further, the case r= 1 gives the required maximum in (10.2.2). ote that the sign of s'l:12b is irrelevant because it can be changed by replacing & by - a. Then it is convenient to solve (10.2.10) for f. instead of f,. The proof splits naturall"
1788,unknown,"Step I Fix a and maximize over b ; that is, solve max (s':t'2b)2 = max b'l:.""aa'l:'2b subject to b'l:.,2b = 1. b b By Theorem A.9 .2 this maximum is given by the largest (and only non-zero) eigenvalue of the matrix 16l:.""aa'l:"". By Corollary A .6.2.1 this eigenvalue equals (10.2.11) Step 2 Now max.m.ze (10.2.11) Over a SUbject to the constraints in (10.2.10). Setting o = :t:;'&. the problem become"
1789,unknown,"(10.2.10). Setting o = :t:;'&. the problem becomes i = 1.. ,r-I, (10.2.12) where ai = :t-""2"""" is the ith canonical correlation vector. ote that the ""', are the eigenvectors on N , corresponding to the (r - 1) largest eigen­ values of N,. Thus . as in Theorem 8.2.3. the maximum in (10.2.12) is attained by setting'"" equal to the eigenvector corresponding to the largest eigenvalue not forbidden to us"
1790,unknown,"eigenvalue not forbidden to us; that is, take '"" = ""'~ or equivalently a = a,. Then Step 3 Lastly we show that this maximum is attained when a = lI, and b = b,. From the decomposition (10.2.6) note that KIl. = p..cxr Thus As standardized eigenvectors of N, and N"" respectively. ""'. and 13, are uniquely defined up to sign. Their signs are usually chosen to make the correlation a;:t.2b, > 0, although"
1791,unknown,"think of a negative correlation. 285 C A H ONIC A L CORRELATIONAL A AL YS1S The above theorem is sometimes stated in a weaker but more sym ­ metric form as follows: Theorem 10.2.2 Let suhject to a,'1:,,8 = bll:.2b = O. i=1. ... ,r-1. Then g. = p, and tl.e maximum is arrailled when a = 8, and b = b,. Prool Since 8 , and b, satisfy the constraints and since these constraints are more restrictive tha"
1792,unknown,"are more restrictive than the constraints in Theorem 10.2.1. the result follows immediately from Theorem 10.2.1. • The correlations between the canonical correlation variables are sum ­ marized in the following theorem . Theorem 10.2.3 Let '111 alld 4>. be the ith canollical correlation variables. i= I. ... , k. alld let '1]=(1)"" .... '11.)' and <1> =(4), .... ,<1>.)'. Then V (;) =(\~ ' 2 A:') (10"
1793,unknown,"V (;) =(\~ ' 2 A:') (10.2.13) Prool From (10.2.9) we see that V(1» = V(<I» = I. From Theorem 10.2.1, C(1) .. 4>,)= A.""'. Thus. the proof will be complete if we show that C(TJ"" <1>,)= 0 for i'"" j. This result follows from the decomposition (10.2.6) since C( 'Ii' d>,) = a~:t'2b, = a~KIli = A;"""""",; = O. • ote that (10.2.13) is also the correlation matrix for '1] and <1> . The next theorem proves an i"
1794,unknown,"The next theorem proves an important invariance property. Theorem 10.2.4 If x*= U'x + o and y·= V'y + v, wllere U (qxq) alld V(p x p) are nOli-singular marrices and o(q X I), v(p x 1) are fixed vectors. then (a) tl.e callonical correlations between "",* alld y* are the same as those between "" alld y ; (b) the canonical correlation vectors for ,,* atll) y* are givell by a~ = U-Iaj and bt = V - tbi1 "
1795,unknown,"cal correlation vectors for"" a""d y. MULTIVARIATE ANALYSIS 286 Proof The matrix for x* and y* corresponding to M, is Mf = (U'l:"" U)- 'U'~' 2 V(V'l:,2V)-'V'l:""U = U -'M, U . Then Mt has the same eigenvalues as M"" and the relationship between the eigenvectors is given by Theorem A.6.2. ote that the. standardiz­ ation arJ(Url:l I U) a~ =a~~llai = I, remains unaltered. Working with Mf gives the eigenve"
1796,unknown,"If we put U = (diag'l:,,)-I/' and V = (diag'l:,,)-tn in 'Theorem 10.2.2 then we obtain M*=P"" 'P '2P,iP"" , wbere P is the correlation matrix of x and y. Hence we deduce that canonical correlation analysis using Pleads to essentially the same results as canonjcal correlation analysis applied to l'.. Recall that principal component analysis did not bave this convenient invariance property. Example 10"
1797,unknown,"invariance property. Example 10.2.1 Working with the correlation matrix P , consider the situation q=p=2 with all the elements of P '2. equal; that is, let PII=(~ 7]. where 1= 11'. Then ~ ] = /31/(1 + a), if la\< J. Similarly, (or Iyl< I, P,iP "" =/31/(1 + y). Noting that "" = 2J, we have P ,,'P 12P2iPz, = {2/3'/(l +0)(1 + 1')}1. Now the eigenvalues of 11' are 2 and zero. Therefore the non-zero eige"
1798,unknown,"eigenvalue of the above matrix is ~,=4/32/(1 +0)(\ +1'). Hence , the first canonical correlation coefficient is p, = 2{3/{(1 +a)(1 + y)}'/2. ote that la I, \1'\ < I and therefore p, > {3. Example 10.2.2 In general, if P"" has rank one then we may write 287 C ANONl C AL C ORRE LA TIONAL ANALYSIS P I2= ab'. Therefore PJ~ PJ 2P2d pi l = p~,l ab'P2i ba'. The non-zerO eigenvalue of this matrix is A , = "
1799,unknown,"The non-zerO eigenvalue of this matrix is A , = (a'P,,'a)(b'p zib). Note that this holds whatever the values of q and p. If a oc 1, b C< 1, then for q = p = 2 we get the result already sbown in Example 10.2.1. In general when a c< 1, b '"" 1, A , is proportional 10 the product o[ the sum of the elements of P ,,' with the sum of the elements of pzi. Note that in general M , and M , will have k non-z"
1800,unknown,"Note that in general M , and M , will have k non-zero eigenvalues. where k is the rank of :1:'2' Usually k = min (p, q). Hence . in regression, where p = I, there is just one non-trivial canonical correlation vector, and thjs is indeed the (standardized) least squares regression vector. If the matrices :1:"" and l:n are not of full rank, then similar results can be developed using generalized inver"
1801,unknown,"10.2.2 Sample canonical correlation analyst. T he above development for a population may be followed through [or the aoalysis of sample data. All that is needed is that S"" should be substituted for :1:"" wherever it occurs (i. j = 1. 2); and I, for .I."" T, for Po. etc. [f the data is normal. then S is rhe maximum likelihood estimator ofl:. sO the sample canonical correlation values are maximu m lik"
1802,unknown,"canonical correlation values are maximu m likelihood estimators of the corresponding population values, except when there are repeated popula­ tion eigenvalues. Example 10.2.3 For the. head-length data of Tahle 5.1.1, we have the correlation matrix [ 1 0.7346J R "" = 0.7346 'I . R _ [1 0.8392] ,,-0.8392 I ' [0.7108 0.7040] R \2= R 2,= 0.6932 0.7086 . (See Example 5.1.1.) a te that all the e[ements "
1803,unknown,"(See Example 5.1.1.) a te that all the e[ements of R "" are about 0.7. so that this matrix is nearly of rank one, This means that the sec­ ond canon ical correlation will be near zero. and the situation approximates that of Example 10.2.1. (See Exercise] 0.2.12.) In fact the eigenvalues of M , = R ,-,'R "" R ,iR "" are 0.62]8 and 0.0029. Therefore the canonical correlation coefficients are ,,= 0.78 6"
1804,unknown,"canonical correlation coefficients are ,,= 0.78 6 and '2.=0 .0539. As expected, "" is close to zero. since R 12 is almost of rank one. Note that "" MULTIVARIATE ANALYSIS 2 8 exceeds any of the individual correlations between a variable of the first set and a variable of the second set (in particular 0.7886>0.7108). T he canooical correlation vectors for the standa rdized variahles are obtained from "
1805,unknown,"from the eigenvectors of M , and M ,. W e have [0.552] ., = 0.522 . [0.505] b , = 0.538 and [ 1.367] 8,= -1.378 . ,=[ 1.767] b_ - 1.757' H ence the first canon ical correlation variables are 'I, =O.552x, +0.52 2x, and 41, =0.505 ',+O.538yz. Th ese are app roxima tely the sum of length and breadth of the head size of each brother. and may be interpreted as a m easure of ""girth"". Th ese variables ar"
1806,unknown,"m easure of ""girth"". Th ese variables are highly correlated between brothers. The second « lOonical correlation variables 'Ih and 41, seem to be m easuring the difJerellce between length and breadth. Th is me asure of ""shape"" would distinguish for instance between long thin heads and short fat heads. The head shape of first and econd brothers appears therefore to have linle correlation. (See also "
1807,unknown,"to have linle correlation. (See also Section SA) 10.2.3 Sampling properties and tests Th e sampl ing distributions a ocia'ect wilh canonical correlation analysis are very complicated. and we shall not go into them in detail here. T he interested reader is referred to Kshirsagar (1972 , pp. 26 1-277). W e shall m erely describe briefly an associated significance test. First. consider the hypo thesi"
1808,unknown,"First. consider the hypo thesis 1:'2=0. which means the two sets of variables are uncorrelated with one anoLher. From (' .3.ll) (with q = p,. p = Q2), we see that und er normality, tile likelihood ratio <latisLic for testing H o is given by • J.. 2Jn = II- S,iS,'S""'SI2I = n (1-,;), ,-, which has a W ilks' A(p."" - l -q. q) d;sLribution. H ere. "" ..... '. are the sam ple canonical correlation coelli"
1809,unknown,"len's approxim ation (3.7 .l\), we sec that • -{II -~p ... q +3)) log n (l-,~)- x;... ,- I asympt otically for large n. 289 CANO leAL CORRELATIONAL A ALYS IS B artlett (1939) proposed a similar statistic to tesL the hypothesis thaL on ly"" of the population cano nical correlation coellicients are non-zero. T his test is based on the statistic • -{II-i(p+q+3)} log n (1- '~)- Xfr-' •• -.\. (10.2.14) "
1810,unknown,"1=,,-1 asymp toLically. An alternative large sample test was proposed by Marrion (1952). Examp le 10.2.4 Consider the head data of Exa m ple 10.2.3. There are "" = 25 observations and q = p = 2. First. let u tesL w hether or not the head meas urements of O ne brother are independent of those of the other. that is. the hypothesis p , = 1'2=0. The LR test for this hypo thesi was given in Example 5.3."
1811,unknown,"given in Example 5.3.3 and the null hypothesis was strongly rejected. Second , to test whether P2 =0, we use (10.2.14) with s = 1. This gives the statistic - (25 - D log (1-0.0539 2) = 0.063, which , when tested against the X~ distribution, is clearly non-significant. H ence we accept the hypo thesis P2 = 0 for this data. 10.2.4 Scoring and prediction Let (X . Y ) be a data mat rix o[ II individua"
1812,unknown,"Let (X . Y ) be a data mat rix o[ II individuals on (q +p) variables and let a . b, deno te the ith canon ical correlation vectors. Then fhe II-vectors X~. and Yb , deno te the score"" of the II individuals on the iLh canonicai correlation variables for x and y. In terms of the values of the variables for a particular individual. these scores take the form TJI = a~ x , (10.2.\5) Since correlations "
1813,unknown,"TJI = a~ x , (10.2.\5) Since correlations are unaltered by linear transforma tions, it i' some­ times convenient to replace these scores by new scores q,t =c2!y.y + d2 • (10.2.16) wher e c,. c,>O and d,. d2 are real numbers . The se scores are most imp ortant for the first canonical correlation vectors and they can be calculated for each of the"" individuals. Let r, denote the ith canonical correla"
1814,unknown,"Let r, denote the ith canonical correlation coellicient. If the x and y variables are interpreted as the ""predictor"" and ""predicted"" variables respecLively, then the '1, score can be used to predict a value of the q,~ score using least squares regression. Since Xa , and Yb , each have sa mpl~ MULTIVARIATE. A 'ALYS IS variance 1. the predicted value of <1>, given 7J, is cf,! = 'i (TIl - ar;n ~ b:j."
1815,unknown,"or equivalently ""'. C ... "" * -) d b'­<P r =---(1'), -d,-c,ai"" + 2+~ ,y. e, 290 (10.2.17) (JO.2.1S) Note that r: represents the proportion of the variance of <1>. which is ""explained"" by the regression On X. Of course. prediction is most important for the first canonical correlation variable. Example 10.2.S Consider again the open/closed book data discussed in Example 5.1.1. The covariance matrix "
1816,unknown,"Example 5.1.1. The covariance matrix for this data is given in Example 8.2.3. The canonical correlations are r, = 0.6630 and r2 =0.0412 and the first canonical correlation vectors are 7J, = 0.0260x, + O.0518x 2 • <1>, = 0.0824)', + 0.U081 y~ + 0.0035h Thus tbe highe t correlation between the open· and closed-book exams occurs between an avcorage of x, and x, weighted on X"" and an average of y,. y,"
1817,unknown,"y,. y,. and Y3 heavily weighted on y,. The m eans of the exam results arC 38 .9545, 50.5909. 50.6023, 46.6818, 42.306K. If we use these linear combinations to predict open-book results from the closed-book results we get the predictor <f"" = O.OI72x, + 0.0343x,+2.2905. ote that this function essentially predicts the value of y, from the values of x, and '~2 ' 10.3 Qualitative Data and Dummy Variabl"
1818,unknown,"Canon ical correlation analysis can also be applied to qualitative data. Consider a two-way contingency table N(r xc) in which individuals are classified according to each of two characteristics. The attributes on each characterlSIlC are represented by the , row and the c column categories. respectively. and n ip denotes the number or individuals. with the ith row and jth column allributes. We wis"
1819,unknown,"the ith row and jth column allributes. We wish to explore the relationship between the two characteristics. For example. T able 10.3.1 gives a (5 x 5) 291 CA 0 leAL CO RRELATI ONAL ANALYSIS Table 10.3.1 Social mobility contingency table (Glass. 1954 ; see also Goodman . 1972) Father'S status I 2 3 4 "" 50 28 1 1 14 o Subject's status 2 345 45 174 78 150 42 8 84 ttO 185 72 18 154 223 714 320 8 55 96"
1820,unknown,"2 345 45 174 78 150 42 8 84 ttO 185 72 18 154 223 714 320 8 55 96 447 411 con tingency table of n = 3497 individuals comparing their social status with the social status of their fathers. Now a contingency table N is not a data matrix since it does not have the property that rows correspond to individuals while columns represent variables. However, it is possible to represent the data in an (n x ("
1821,unknown,"data matrix Z =(X . V ), where the column s of X and Y are dummy zero-one variables for the row and column categories. respectively; that is, let x •• = {~ and if the kth individual belongs to the itb row category. otherwise. if the kth indiVIdual belongs to the jth column category. otherwise. for k = 1. .... II, and i = I •... , r; i = I. .... c. No te that x(i\Y(1) = II;;. Also. the columns of X"
1822,unknown,"the columns of X and the columns of Y each sum to 1"". The purpose of canonical correlation analysis is to find vectors aIr x 1) and b(c x J) sucb that the variables '1 = a'x and t/> = b'y are maxima lly correlated. Since :r has only one non-zero component . TJ = al! a l,'"" I or a,. Similarly <I> = b,. b2 •••• , or be. Thus. an individual in the ith row category and jth column category can be assoc"
1823,unknown,"category and jth column category can be associated w ith his ""score"" (a"" b;). These score may be plotted on a scattergram- there will be II"" points at (a;, bj)-and the correlation represented by the e points can be evaluated. Of course the correlation depends on the values of a and b, and if these vectors maximize this correlation, then they give the vector of firsl canonical correlation loadings."
1824,unknown,"firsl canonical correlation loadings. Such scores are known as ""canonical scores"". Of course. having calculated the first set of canonical scores, one could ask for a second set which maximizes the correlation among all sets which MULnVARIATE ANALYSIS 292 are uncorrelated with the first. This would correspond to the second canonical correlation, Let Z = (X, Y) be the data matrix corresponding to a"
1825,unknown,"canonical correlation, Let Z = (X, Y) be the data matrix corresponding to a contingency table N. Let t, = I; II"" and gj = Ii 11,;, so that ( and g denote the marginal ro\y and column sums, respectively, for the table. For simplicity, suppose t, > 0, g, > 0 for all i and ;. Then a little calculation sbows that IIS=Z'HZ= Z'Z -IIZZ [""SII IIS.2]=[diag(l)-:,-'ff' N-N ], = - '..1 (10.3.1) ""SlI IIS2 2 N "
1826,unknown,"[""SII IIS.2]=[diag(l)-:,-'ff' N-N ], = - '..1 (10.3.1) ""SlI IIS2 2 N '-N' diag (gj-n- gJ; where N = fg' is the estimated value of N under the assumption that the row and column categories are independent. Because the columns of X and Y each sum to 1, S;: and S:;i do not exist. (See Exercise J 0.3.!.) One way out of this difficulty is 10 drop a column of each of X and y , say the first column. Let "
1827,unknown,"component of the covariance matrix obtained by deleting the first row and first column from S "" for i,; = 1,2. Similarly, let f' and g* denote the vectors obtained by deleting the first components of ( and g. Then it is easy to check that (lisT.) • = [diag (f*)] ITt. '11', (lIsr2) '=[diag(g*)] '+g, lU ', (10.3.2) so tbat Mf = Sf. ' S~2St 'sr, and ~ are straightforward to calculate for tbe reduced "
1828,unknown,"ote that the score associated with an individual who lies in the first row category of N is 0; and similarly for the first column category of N . Example 10,3.1 A canonical correlation analysis on Glass's social mo­ bility data in Table 10.3.1 yields the following scores (coefficients of the first canonical correlation vectors) for the various social classes: Father's status: 1 2 3 4 5 0 3.15 4.12"
1829,unknown,Father's status: 1 2 3 4 5 0 3.15 4.12 4.55 4.96 Son's status: I 2 3 4 5 0 3.34 4.49 4.87 5.26 The first canonical correlation is r. = 0.504. Note that for both father's and son's social class. the scores appear in their natural order. Thus the social class of the father appears to be correlated with the social class of the son. Note that social classes 1 and 2 seem to be more distinct from one an
1830,unknown,one another than the other ~djacent social classes. both for the son's and the father's status. 293 CANONICAL CORRELA110NA.L ANALYSts Hypothesis testing is difficult for canonical scores because the data matrix Z is clearly non-normal. lOA Qualitative and Quantitative Data The ideas of the last section call be used when the data is a mixture of qualitative and quantitative characteristics. Each qu
1831,unknown,"qualitative and quantitative characteristics. Each quantitative characteris­ tic can of course be represented on One variahle. and each qualitative characteristic, with say g attributes, can be represented by dummy zer()-{)ne values on g - I variables. We shall illustrate tltis procedure for an example on academic predic­ tion of Barnett and Lewis (1963) and Lewis (1970). Lewis (1970) gives data c"
1832,unknown,"data collected from 382 university students on the number of GCE A-levels taken (a secondary school examination required for university entrance), and the student's average grade (scored I to 5, with 5 denoting the best score). These represented the ""explanatory variables"". The dependent variables concerned the student's final degree result at univer­ sity which was classified as First, Upper Seco"
1833,unknown,"sity which was classified as First, Upper Second , Lower Second. Third, and Pass. Alternatively. some students were classified as 3(4) if they took four years over a three-year course (the final degree results were not available for these students). or as -> if they left without completing the course. This information may be represented in terms of the following vari­ ables: x, = average A-level g"
1834,unknown,"ables: x, = average A-level grade; x, = 1 if two A-levels taken, 0 otherwise: x~ = 1 if four A-levels taken. 0 othenvise; Y. = 1 if degree class lI(i) obtained, 0 otherwise: y,= 1 if degree class JI(ii) obtained, 0 otherwise: y, = 1 if degree class III obtained. 0 otherwise; y. = I if Pass obtained, 0 otherwise; y,= 1 if 3(4) obtained. 0 oLhenvise; Yo = 1 if -> obtained. 0 otberwise. No te that a "
1835,unknown,"No te that a student who obtains a First Class (I) degree, would score zero on all the y-variables. The data is summarized in Table 10.4.1. Note that most of these variables are ""dummies"" in the sense that they take either the value 0 or I. Hence the assumptions of normality would be com­ pletely unwarranted in this example. Since the y-variables are in some sense dependent on the x-variables, we "
1836,unknown,"we may ask wbat scoring system maximizes the correlation between the ys and the xs. Lewis (1970) showed that the first canonical correlation MULTIVARIATE A NAL YSIS ;. "" Lt '"" ~ I ci "" C "" C "" S .;- ;. "" q Lt ~o ,:; M'q '(' 0 S .;- 0' "" q -u. c .; ~ ~ ~ "" """" 0 • S .;- 0' u 8 Lt 0 "" = c: "" c ... 8 .;- "" ~ ""- ~ f""I _ C \0( o N o "" , 0 ~~ '< u '"" _ MMIrl("".l ___ ...... f"""") <:2'_ ooc cc:::-:::.c---- "
1837,unknown,"ooc cc:::-:::.c---- Vl...... ...... r""> M N'>CJC""'OMOMCO ..... OV'l ~MnNNN-= ....:.~f""""iM N ..... ,....-<:::t - M - f"""")I""""'"". -_-- ---coc c oooocc cccc::::::::::::coo --- V'lVl f'""""t'f) f' f'"" ~r:""1C:'I:;r::::~~~C:""""f q MN(""lvMMM('IN - M-(""lN ----000000000000=00000 co:=:coocoococoooc ------ Vltr.V) f'""""t""l r-f""I .-C""'I f'""""M ~N~O~~=C~OG~O~~O ~ O ~ c ~ c ~~~ ~~~~~~~~~~ ~ ~ ~ ~~ ~~~~ V""; r--M t--("""
1838,unknown,"V""; r--M t--(""""; r-M r-­ C~""'~Ma~MOG~CG~=~ ~ = MN~~~~MM~MNN """":'~~ ~NN _ __ ooooo::::oooooooO'o ooc o ooooooooocooco -------- on ..,., or.r-f""'! r-M r-M r-M NN ...... ~~O~MO~MO~MC~OV)OV)OVl ~~~~~~~~~~~~~ ~ ~~~~~~~~ - --NO-~ N~.M -------- -- :::::cococccooc~:::::::::: ----- V} r-M t-M r-M r-C""'I CMO~MO~MO~MO~MOV'lC~C ~~ ~ ~~~~~~ ~~~ ~~ ~~~~~ ccococooc - ""'r-f""l M r- ""-, N...oMOM O\oCl 0 0 -iM-i-:ro"
1839,unknown,"""-, N...oMOM O\oCl 0 0 -iM-i-:ro::iM~N """": ""; 294 295 CANO leA L CO RRELATtO AL AN ALYSLc; vectors are given by a,=(-1.096, 1.313,0.661), and b, =(0.488,1.877, 2.401,2.971,2.527.3,310). Thus the scores corresponding to each or the degree results are I Jl(i) IIO;) ill Pass 3(4) ---> o 0.488 1.877 2.401 2.971 2.527 3.310 Th e first canonical correlation coefficient is ,,= 0.400, whkh means that with"
1840,unknown,"with the given scores on ly '; = 0.160 of the variation in the first canonical correlation variable ror y is explained by variation in A-level scores, H oweve r, any other scoring system (such as a ""natural"" One. ly,+2y,+ ... +6y.) would explain less than 0.16 of the variance. The above scores may be interpreted as follows. The scores for I, II(i), IlOi), Ill. and Pass come out in the ""natural"" or"
1841,unknown,"equally spaced, Moreover the 3(4) group com es between m and Pass, while ---> scores higher than Pass. ote the large gap between Il(i) and lI(ii). The canon ical correlation vector on the x-variables indicates that the higber one's A -level average. the better the degree One is likely to get. Also. those who take two or rour A-levels are likely to get poorer degree results than those w ho take thr"
1842,unknown,"results than those w ho take three A-levels. In raet, taking two A-levels instead of three is equivalent to a drop of about 0.5 in average grade. The fact tbat three A -levels is better than rour A-levels is somewha t surpris­ ing. However , because tbe value of ,~ is not high, perhaps one should not read a great deal into these conclusions. (The test or Section JO .2.3 is not valid here because t"
1843,unknown,"valid here because the population is clearly non -norma L) Th e me ans for the nine variables are (j', Y) = (3.138, 0.173, 0.055, 0.175,0.306,0.134. 0.194,0.058.0.068). Thu s we can use the A -level perrormance to predict degree results using <f, = - 0.439x, ,-0.525x, + 0.264x3 + 3.199. Fo r a student taking three A -levels with an average grade of 3.75, <f, = 1.55, so w e w ould predict a degree "
1844,unknown,"<f, = 1.55, so w e w ould predict a degree result slightly better than ll(ii). B artlett (J 965) gives various other examples on the use of dummy variables. Exercises and Complements 10.2,1 Show that ror fixed b (a'I 12b)' m~x (a'I "" a)(b'l:,2b) b'l:"" l:~,' l: ' 2b b'l:,2b MULTIVARIATE ANALYS tS 296 and that the maximum of tbis quantity over b is given by the largest eigenvalue of 1:': 1: '2 ~i I:"
1845,unknown,"10.2.2 Show that M "" M 2 • .. and N , of Section 10.2.1 aU have the same non-zero eigenvalues and that these eigenvalues are real and positjve. 10.2.3 Show that M ,=lp -L.,'1:"" .2' wher e 1:"" ,=1: II- 1: '2~:! l:o . Hen ce deduce that the (i,(St canonical correlation vect r f , x, )!ll'tn b} the eigenvector corresponding to the smallest eiger v~llIc of ~"" 10.2.4 Show that the squared canonical cor"
1846,unknown,"equation 1 1: 12~:! l:o ,-A1: 1I1 =0 and that the canonical correlation vectors for x sati<fy l: 12~:!l:na, = A,1:""a;. 10.2.S (a) Show that the canonical correlation vectors a, and b, are eigenvectors of M , and M 2 , respectively. (b) Write M ,=L ,L"" M 2 = L2 L .. where L ,=L.J'1:12 and L, = ~:! I:"" . Using Theorem A.6.2 show that the canonical correlation vectors can be expressed in terms of one"
1847,unknown,"10.2.6 Let x(q x 1) and y(p X J) be random vectors such that for all i, k=l, .... q and j.l= I, .... p, i,ek and i"" I. V (x;l=I, V (y,)= I, C(x"" x.J = p, C(y"" y,)= p', C(x,. y,)= 7, where p, p' and 7 lie between canonical correlation variables are {q[(q- I)p + 1m-In t x, , . 1 o and l. Show that the only and {p[(p-I)p'+ lv,r"" 2 f Y"" ,-, 10,2.7 When p = I show that the value of p~ in The orem 10.2."
1848,unknown,"squared multiple correlation I:, ,1:,,'1: ""/""22' 10.2.8 (a) The residual variance in predicting x, by its linear regression on b'y is given by f>f= V(x,)- C(x"" b'y)2/V(b'y). 297 CA ONI CA [~ CO RRELATIONAL ANALYS IS Show that the vector b which m inimizes L:-, f>r is the eigenvector corresponding to the largest eigenvalue of ~:!l:"" I' 2' (b) Show further that the best k linear functions of y"" ..• "
1849,unknown,"predicting x, ..... x"" in the sense of minimizing the sum of residual variances correspond to the first k eigenvectors of ~:! I:""I 12' 10.2.9 When q = p = 2 the squared canonical correlations can be com­ puted explicitly. tf then show that the eigenvalUes of M l = III' I""~i I:, , are given by A = {B ± (B2 - 4C)'fl}/{2(1 - ~)( I - If)}. where B = a'+b'+c'+ d'+2( ad + bc)a(3-2(ac +- bd)a - 2(ab +-cd"
1850,unknown,"and C = (ad - be)'(I +a 2 (3'-a' - /3'). 10.2.10 (H otelling. 1936) Four examinations in readingspeed, reading power. arithm etic speed . and arithmetic power were given to II = 148 children. The question of interest is whether reading ahility is correlated with arithmetic ability. The correlations are given by _ (1.0 0.6328) =(1.0 0.4248) R =( 0.2412 0.(586) R I1 - 1.0 ' R n 1.0' 12 -0.0553 0.065"
1851,unknown,"Us ing Exercise 10.2.9, verify that the canonical correlations are given by P , = 0.3945, p, = 0.0688. 10.2.11 Us ing (10.2.14) test whether p, = p,=O for H otelling's exami­ nation data in Exercise 10.2.10. Show that o ne gets the tesl tatistie 25.13 - X~. Since X~ . "" 01 = 13.3, we strongly reject the hypothesis that reading abililY and arithmetic ability are independent for this data. 10.2.12 I"
1852,unknown,"10.2.12 If in Ex am ple 10.2.1. (3 =0.7, a = 0.74 , and y =0.84. show that P I =0.782. thus approximating the situation in Example 10.2.3 wher e ',= 0.7886. MULTIVARIATE ANALYS IS 298 10.2.13 (a) U ing tbe data matrix for the open/closed book data in Example 10 2.5 and Table 1.2.l. show that the scores of the first eight individuals on the first canonical correlation variables are as follows: Subj"
1853,unknown,"Subject 6.25 6.35 2 5.68 7.44 3 5.73 6 .67 4 5.16 6.00 5 4.90 6.14 6 4.54 0.71 (b) Plot the above eight points on a scattergram . 7 4.80 0.12 5.16 6.30 (e) Repeat the procedure for the second canonical correlation variable and analyze the difference in the correlations. (The second canonical correlation is '2 = 0.041 and the corresponding loading vectors are given by ""2 = (- 0.064,0.076). b!,= (-0"
1854,unknown,"by ""2 = (- 0.064,0.076). b!,= (-0.091. 0.099. -0.014).) 10.2.14 The technique of ridge regression (H oerl and Kennard . 1970) has been extended to canonical correlation analysis by Vinod (1976) giving wh at he calls the ""canonical ridge"" technique. This technique involves replacing the samp le correlation matrix R b [R ,,+k.l R .2 ]. R 2I R ,,+ kol where k. and k2 are sma ll non-negative numbers. "
1855,unknown,"R 2I R ,,+ kol where k. and k2 are sma ll non-negative numbers. and carrying out a canonic~ t correlation analysis on this new correlation matrix. For data which is nearly coUinear (that is. R •• and/or R 22 have eigenvalues near 0), show that small but non·zero values of k •. k2 lead to hetter estimates of the true canonka l correlations and canonical correlation vectors than the usual analysis o"
1856,unknown,"10.2.15 (Lawley, 1959) If S is based on a large number , II, of observa­ tions, then the following asymptotic results hold for the k non-zero canonical correlations. provided p~ and p~- P~ are not 100 close to zero, for all;, j=I, .... k. ;'I'j: 2p,E(r,-p;)=_I_(J-p;){P+Q-2-p;+ Z(I-PT) ± 2~ 2}T O (n'2), (II- I) .. ,I', 1', ~ ~ j l ')' O ( .2) V(r, )=--(1-p. -~ "" , (n -1) ( )_Z p,p,(1-p;)(I-p;)+0('1"
1857,unknown,"corr f"" f, - ( 1)( 2 2)2 . IJ - P i -P I 299 CAr-;O;-·JlCAL COR R E LATIONAL ANALYSIS 10.3.1 Show that S1I1= 0 and s,21 = 0 in (10.3.1). Hence S,.' and S2i do nOl exist. 10.3,2 The following example illu tratcs that it does not matter which row and column are deleted from a contingency table when constructing canonical scores, Let N=G ~l (a) Show that deleting the first row and first column [rom N"
1858,unknown,"(J 0.3.2\ to M* =...!.. [2 1>9] J 15 3 and hence the canonical scores for the row categories are proportional to (0,2,3). (b) Similarly, show that deleting the second row and first column from N in (10.3.2) leads to 1 [ 8 -6] M f= 15 - 4 3 and hence to canonical sCOres proportional to (-2. O. 1). Since these scores are related to the scores in (a) by an additive constant, they are equivalent. (c) "
1859,unknown,"equivalent. (c) Show that the canon ical correlation between the row and column categories equals (11/15)112. (d) ote that because there are only two column categories, all scoring functions for column categories are equivalent (as long as they give distinct values to each of the two categories). 11 Discriminant Analysis 11.1 Introduction Consider g populations or groups n, , .... n., g"""" 2. Suppo"
1860,unknown,"sociated with eacb population nO' there is a probability density [,(x) on R P, so that if an individual comes (rom population n"" he has p.dJ. [,(x). Then the object of discriminant analysis is to allocate an individual to one of these g groups on the basis of his measurements x. Of course, it is desirable to make as few ""mistakes"" as possible in a sense to be made precise later. For examp le. the "
1861,unknown,precise later. For examp le. the populations might consist of different di eases and x migh t meas ure the symptoms of a patient. Thus one is trying to diagnose a patient's disease o n the basis of his symptoms. As another example. consider the samples from three species of iris given in Table 1.2.2. The object is then to allocate a new iris to one of these species on the basis of its measurements
1862,unknown,"its measurements. A disc,iminallr rule d corresponds to a div; ion of R P into disjoint regions R "" ... , R . (U R , = R ""). The rule d is defined by allocate I to n, if X E R "" for j = 1, ... , g. Discrimination will be more accurate if n, bas most of its probability concentrated in R , for each j. Us uaU y, w e have no prior information about which population an individual is likely to come from"
1863,unknown,"individual is likely to come from. However, if such information is availa­ ble, it can be incorporated into a Bayesian approach. T he situation where the p.d.f.s [,(x) are known exactly is the simplest to analyse theoretically, although it is tbe least realistic in practice. We examine this case in Section 11.2. BIBLIO. ~::;A • - "":'R.U,· UFLA I ~OI DISCRI~I ANT ANALYSIS A variant of this situatio"
1864,unknown,"~OI DISCRI~I ANT ANALYSIS A variant of this situation occurs when the form of the p.dJ. for each population is known , but there are parameters which must be estimated. The estimation is based on a sample data matrix X (II x p) whose rows are pa rtitioned into g groups. The (II, x p) matrix X , represents a ample of II, individuals from the population n,. ote that in this chapter it is tbe illdioi"
1865,unknown,"which arc grouped into categories. whereas in the last chapter it was the variables (columns) which were grouped. Finally, there is an empirical approach to discriminant analysis where we do not assume any particular form for the populations n,. but merely look for a ""sensible"" rule wbich will enable us to discriminate between them. One such ruJe is based on Fisher's linear discriminant function a"
1866,unknown,"i. described in Section 115 . 11.2 Discrimination when the Populations are Known 11.2,1 The maximum likelihood discriminant rule Consider tbe situation wbere the exact distrihutions of the populations II, ..... fl. are known. Of course this is extremely rare. although it may be possible to estimate the distributions fairly accurately provided the samples are large enouj!;h. In any case an examinat"
1867,unknown,"samples are large enouj!;h. In any case an examination of the situation where distributions are known serves as a useful framework against which other situations can be compared . The starting point for our analysis is the intuitively plausible maximum likelihood rule. We ~h311 write the p.d.f. of the jth population as {,(x) = L,(x) to emphasize that we are thinking of the likeLihood of the data p"
1868,unknown,"thinking of the likeLihood of the data point x as a function of the ""parameter"" j. Definition n .2.1 Tile maximum likeliltood discriminant rule {or a/locat­ ing all obseroalioll x ro o""e of rile populario""s n , ..... n"" is 10 allocale x to rite pop,t/aliotl which Kives Ille largesr likelihood 10 x. That is. the maximum likelihood rule says one should allocate"" to n"" "" here L ,(x)= max L,(x). (11.2"
1869,unknown,""" here L ,(x)= max L,(x). (11.2.]) MULTIVARJATE ANALYSIS 302 If several likelihoods take the same maximum value, then anyone of these may be chosen. This point will not always be repeated in what follows. Fur1her, in the examples we consider, it will usually be the case that P(L,(x) = 4 (x) for some if k I IT;) = O. for all j = 1, ... , g, so that the form of the allocation rule in the case of tie"
1870,unknown,"has no practical importance. Example 11.2.1 If x is a 0-1 scalar random variable, and if TT , is the population with probabilities (~, 4) and TT 2 is the population with prob­ abilities G, ~). then the maximum likelihood discriminant rule allocates x to TT, when x = 0, and allocates x to TT , when x = 1. This is because L. (O)=~> L,(O)=t and L,(I) =~> L, (!)=~. Example 11.2.2 Suppose that"" is a mu"
1871,unknown,"Example 11.2.2 Suppose that"" is a multinomial random vector, which comes eitller from n"" with multinomial probabilities a ...... ak or from n2 , with multinomial probabilities f3 "" ... , 13k, where La, = L13i = 1 and Lx. = II. fixed. 1f x comes from J]"" its likelihood is tI! ---:-:.:.:.---: a x. a "" I "" ..., . XI' ... xrc . If x comes from TT z the likelihood is 01.2.2) If A is the ratio of these "
1872,unknown,"log A = L x, log :: = L XiS"" (11.2.3) where s, = log (aJf3i)' The maximum likelihood discriminant rule allocates x to IT, if A > 1, i.e. if log A > O. We shall meet this rule again in the con text of a linguistic seriation problem in Example 11.3.4. Example lL2.3 Suppose that IT, is the N (IL"" O"" }) distribution, and TT2 is the N(I'-""2.O""D distribution. This situation is illustrated in Figure 11.2"
1873,unknown,"for the case where IL2 > IL, and 0"", > 0"",. The likelihood L, (i = 1. 2) is L,(x) = (2 1T0""~)-'n exp {-t("" :ILi)'} • Note L.(x) exceeds L,(x) if 0"" {[(X-IL)' (X-""""')']} U: exp -! ~ - U 2 - > 1. 303 DIsc.RD.1 ANT ANALYSIS Figure 11.2.l Normal likelillOods witl1 unequal means and unequal variances (from Eulmp/. 11.2.4). On taking logarithms and rearranging, this inequality becomes If 0"".> 0""2 as in "
1874,unknown,"If 0"".> 0""2 as in Figure 11.2.1, then the coefficient of x2 is negative. Therefore tbe set of xs for which this inequality is satisfied falls in two distinct regions. one having low values of x and other having higb values of x (see Exercise 11.2.8 and Anderson and B ahadur, 1962). Example 11.2.4 An important special case of the previous example occllrs when 0"", = 0""2. for lhen l.,(x) exceeds 4(x)"
1875,unknown,"occllrs when 0"", = 0""2. for lhen l.,(x) exceeds 4(x) when Ix -"""",I > Ix -IL.I- In other words . if 1'-.,> IL. then the maximum likelihood discriminant rule allocates x to n2 if x> HIL. + IL,), and allocates x to IT, otherwise. II IL. > IL""2 the position is reversed. The multivariate generalization of this result is fundamental to this chapter and is given as a theorem . Theorem 11.2.1 (a) If ITi i"
1876,unknown,"Theorem 11.2.1 (a) If ITi is Ille N p (IL!,1:) population, i = 1, ... , g, and 1: > 0, tile"" the maximum IikelillOoo discriminant rule allocates x to ITI, II'herej E (I .... , g) is thaI value o/i which minimizes the square o/the MallO' lanobis distance (x - 11;)'l;-'(X -11;). (b) When g=2. tile rule allocates x to TT. if """"(>:- 11»0, wltere ""' = ~'(11 '- 11') and l1=tb.l,+I12), and to IT, otherwi"
1877,unknown,"Proof From (2.5.1), the itb likelihood is L ,(>:) = 12 7Tl:j- ln exp (4(X-l1i)'~'("" - .... n. (11.2.4) MULTIVARIATE ANALYSIS 304 This is maximized when the expOnent is minimized , which proves part (a) of tbe theorem. For part (b) note that L,(x» L ,(x) if and on ly if (x - 11. Y.k'(>: - 11 ,) < (:O:- 112),1:- '(x-I12). Cancelling and simplifying leads to the condition stated in the theorem. (See "
1878,unknown,"(See Exercise I 1.2.1.) • Example 11.2.S (Singular 1:; Rao and Mitra, 1971. p. 204) Consider the situation of the above theorem when 1: is singUlar. Then the ML rule must be be modified. Note that n, is concentrated on the hyperplane N '(X- I1,) = O. where the column s of N span the null space of '1. and that D2 is concentrated on the hyperplane N'(. -112) = O. (See Section 2.5.4.) rr N'(I1,- 112)"
1879,unknown,"N'(I1,- 112)"" 0, then these two hyperplanes are distinct. and discrimina­ tion can be carried out with perfect accuracy; namely if N'(X-I1I)=O allocate to D, and if N '(x-1121 = 0 allocate to D 1 . The more interesting case occurs when IL, -112 is o rthogonal tu the null space of I. If '(11,-110)= 0, then the ML allocation rule is given by allocating"" to n, if a 'x > ia'(I1,-I12), where a = 1: {IJ"
1880,unknown,"chapter we shall assume '1> 0. Note that when there are just g = 2 groups. the ML discriminant rule is defined in terms of the discriminant {lIl1ctioll h(>:)= log L,(x)-Iog L,(x). and the ML rule takes the form allocate x to fl, if 1.(.»0. allocate x to f12 if ""(.)<0. (11.2.5) ( 11.2.6) to particular. Dote that the discriminant function given in (I 1.2.4) for two multinormal populations with the s"
1881,unknown,"multinormal populations with the same covariance matrix is lillear. Thus the boundary between the allocation regions in this case is a hyperplane passing through the midpoint of the line segment connecting tbe two group means, although the hyperplalle is not necessarily perpendicular to this line segment. See Figure 11.3.1 for a picture in the sample case. 11.2.2 The Bayes discriminant rule fn cer"
1882,unknown,"fn certain situations it makes sense to Suppo Se that the variou.. populatioos bave prior probabilities. For instance. in medical diagnosis we 305 DISCRIMINANT AN.~LY51S may regard flu as intrinsically more likely than polio. This information can be incorporated into the analysis using a Bayesian discriminant rule. For simplicity we shall suppose all prior probabilities 7T, are strictly positive j"
1883,unknown,"positive j = 1, .... g. Definition 11.2.2 If popu/atiolls n, ... ., n. Itave prior probabilities (7T"" ... ,7T.)=1T'. tlte"" the B ayes discriminant rule (wilh respect to n ) al/ocates all observatioll • to the populatioll for whirh (11.2.7) is maximized . The function in (11.2.7) can be regarded as proportional to the posterior likelihood of fl, given the data., ote that the ML rule is a special ca"
1884,unknown,"of the Bayes rule when all the prior probabilities are equal. In the case of d iscrimination between g = 2 populations. the effect of introducing prior probabititic. is simply to shift the critical value of the discriminant function by an amoun t log (""',}1T ,). The rule (11.2.6) be­ cOmes allocate"" to n, if It(x) > log (1T,!7T,) and to O 2 otherwise. In particular for the ca e of two multinormal "
1885,unknown,pop ulations with the same covariance matrix. the boundary hyperplane is moved closer to the less likely popldation. but remains paraltel to the nou nd ary hyperplane of the ML altocation regions. 11.2.3 Optimal properties T he Bayes discriminant rules described above (including the ML rule) have certain optim al properties. First notice that the above rules are deterministic in the sense that if 
1886,unknown,"deterministic in the sense that if x, = x, then "" and X 2 will always be allocated to the same population. H owever. for mathematical purposes it is convenient to define a wider etass of discriminant rules. (In this wider class. it is possible to average two discriminant rules. and hence the set of all discriminant rules forms a convex seLl Definition 11 .2.3 A randomized discriminant rule d illvo"
1887,unknown,"Definition 11 .2.3 A randomized discriminant rule d illvolves al/ocarilll( all observation x to a populatioll i "",itl. probability 0,(X). "",IIere <1>, ..... <1>. are IIol1-IIegative tWletiolls earh defilled on R "". "",hiclt sarisfy L <1> , (x) = 1 {or all x. it is clear that a deterministic allocation rule is a special case of a randomized allocation rule obtained by putting 'b;(x) = 1 for XE R, an"
1888,unknown,"0 ,(.)=0 elsewhere. For example. the Baye rule with respect to prior MULTIVARIATE ANALYSIS 306 probahilities 71"", ••••• 71"". i defined by if 7I"",L ,(x) = max """",4(""). otherwise. 01.2.8) except for tbose "" where the maximum is attained by more than one popUlation. (Since we are supposing that the set of such xs has zero probability. whatever the true population. this ambiguity in </>,(x) is irrelev"
1889,unknown,"irrelevant.) The probability of allocating an individual to population fl,. when in fact he comes from n,) is given by POI = J </>,(x)L,(x) dx. (11.2.9) In particular. if an individual is in fact from flO' the probability of correctly allocating him is p"" and the probability of misallocating him is 1 - POO"" The performance of the discriminant rule can be summarized by the numbers p"" ..... P ... Th"
1890,unknown,"the set of discrimin ation rule~ . Definition 11.2.4 Sa)' rhar 0'"" discriminanr rule d wirla probabilirres of correcr allocatioll {P .. ) is as good as allorlaer rule d' wir/. probabiliries I,,;.} if (or all i = I ..... g. Say rlaat d is beller rhall d' if ar leasr olle of rite illequaliries is srricl. If d is a rule for wlaiell rhere is 110 berter nde. say rhar d is admissible. 'otice that it may"
1891,unknown,"'otice that it may not always be possible to compare twO allocation rules using this criterion. for example if p"" :> p~, but P22 < P~"" However. we can prove the following optimal proper!)' of Bayes discriminant rules. which can be considered as a generalization of the Neyman-Pearson lemm a (see R ao. 1973. p. 448). Theorem 11.2.2 All Bayes discrimillanr rules (illcluding rhe ML "",'el are admissibl"
1892,unknown,"admissible. Proof Let d· denote the Bayes rule with respect to prior probabilities 1T . Suppo se there exists a rule d which is beller than this rule. Let {p,,} and {p:) denote the probabilities of correct cla ,ification for this rule and the Bayes rule. respectively. Then because d i better than d* and since 307 OISCRf .M1NANl · ANALYSIS """",:> 0 for all j, I 7T,P .. > L 7T,p~. However. using (11."
1893,unknown,"L 71"",1' .. = L J 4>, (X)7I"",L, (x) dx "",~ J </>,(x)m~x7I"",L,(x)dx= J (L</>,(x)} max 7I"",4(x)dx = J max 7I"",L,(x) dx = J L <I>~(xJ7I"";L, (x) dx = L 1Tjp!. which contradicts the above statement. Hence lhe theorem is proved. • o te that in the above tbeorem. discriminant rules are judged on their g probahilities of correct allocation. p"" .... , p ... However . if prior prob­ abilities exist. then a "
1894,unknown,"abilities exist. then a discriminant rule can also he judged on the basis of a single number-the posterior probability of correct allocation. r """",P..­ Using this criterion, lilly two discriminant rules can be compared and we have the following result, first given in the case of g = 2 groups hy Welch (1939). Theorem 11.2.3 If popu/arions n, ..... n. have prior probabilit"""" 71"", ..... 71"" •• Illell"
1895,unknown,"71"", ..... 71"" •• Illell /10 discrimillanr "",Ie has a larger poslerior probabiliry of correcr allocarion rhall ale Bayes rule wirh respecr to rhis prior. Proof Let d' denote the Bayes rule with respect to the prior prob­ abilities 7T, ..... 7T. with probabilities of correct allocation {pt). Then. using the same argument as in Theorem 11 .2.2. it is easily seen that for any other rule d with probah"
1896,unknown,"other rule d with probahilities of correct classification {p;;). I 1T,P .... L 7I"",p~: that is. the posterior probability of correct allocation is at least as large for the Bayes Iule as for the other rule. 11.2.4 Decision theory and unequal costs The discrimination problem described language of decision theory. Let K(i, il= {O. ell' above can be phrased ill the i = j. i i-j. (11.2.10) be a loss {"
1897,unknown,"i i-j. (11.2.10) be a loss {unction repre~enting the cost or loss incurrect wben an observa­ lion is allocated to n, when in fact it comes frolll n,. For thi!, to be a sensible definition, suppose coo:> (I for all i1' j. If d is a discriminant rule with allocation functions <1>, (X) given in (11.2.8). then the risk f""""criol! is MULTIVARl.-\TE AI""oI'ALVSIS 308 defined by R(d. j) = E(K(d (x), j) I f"
1898,unknown,"defined by R(d. j) = E(K(d (x), j) I fl,) = I. K (i. j) f <1>. (x)L, (x) dx . (11.2.11) and represents the expected loss given that the. observation comes from fl,. In particular. if c;, = I for if j, then R(d.j)= I-p"" represents the misallocation prohabilitie,. U prior probabilities exist then tbe Bo)'e., risk can be defined by r(d.1T) = I. ""',R(d. j) and represents the posterior expected loss. A"
1899,unknown,"As in Section 11 .2.3. say that a discrimination rule d is admissible if there exists no other rule d' such that R(d', i> """" R (tI, JJ for all J. with at least one strict inequality. Define the Bayes rille in tbis ""tu3tioll with re peet 10 prior probabilities ""'·1 ..... ""'. as follows: allocate x to fl, if I. e,,'IT,Ldl<)= mrn I. c,,'IT,l,(x). (11.2.12\ It-. I 1.. ... The following results can be "
1900,unknown,"and Theorem 11.2.3 (See also Exercise 11 .2,4.) Theorem11.2.4 All Bayes lliscril11irralioll rules are admissible for IIle risk tWlclioll R. • Theorem 11.2,5 If lire populaliolls fl , ..... n, Iwve prior probabililies 'IT ...... 'IT"" rhell rIO discrimillallt "",Ie has smaller Bayes risk for Ihe risk function R chan the Ba~les rule with respect ro 1T . • The advantage of the decision theory approach "
1901,unknown,"The advantage of the decision theory approach is that it allows us to attach varying levels of importance to different sorts of error. For example, in medical diagnosis it might be regarded as more harmful to a patient's survival for polio to be misdiagnosed as flu than for flu to be misdiagnosed as polio. [n the remainder of Ihis chapter we shall for simplicit}' place most of the emphasi on the M"
1902,unknown,"with equal prior probabilities). 309 DJSCRIMI'-:ANT ANALYSIS 11.3 Discrimination under Estimation 11.3.1 The sample discriminant rule The sample ML discriminallt rule is useful when the forms of the distributions 01 fl "" .... fl. are known, bul their parameters must be estimated from a data matrix X(II X p). We suppose that the rows of X are partitioned into g groups, X' = (X , .... , X~) and that"
1903,unknown,"partitioned into g groups, X' = (X , .... , X~) and that X , contains T!; obser­ vations from fl;. For example, suppose the groups are assumed to be samples from the multinormal distribution with different means and the same covarjaDce matrix. Let i, and S, denote the sample mean and covariance matrix of the ith group. Then unbiased estimates of .. , ........ and 1: are Xl .... ' i, and S"" = 2: II"
1904,unknown,"S"" = 2: II.S,/(T! - g). The sample ML discriminant rule is then obtained by inserting these estimate:. ill Theorem 11.2.1. In particular when g = 2 the sample ML discriminant rule allocates x to n, if and only if (11.3.1) where a =S~ '(XI - i,). Example 11.3.1 Consider the ,t 1= 112 = 50 observations on two species of iris. I. selOsa and I. versicolour. given in Table 1.2.2. For simplicity of expo"
1905,unknown,"exposition. we shall d iscriminate between them only on the basis of the first Iwo variables. sepal length and sepal width. Then the sample means a fld variances rOI each gro up are given by Thus. x, = (5'()06. 3.428)', S ,=(0.1218 0.0972) 0.1408 . x, = (5.936.2.770)', S, = (0.2611 0.0835) 0.0965 . B = [(50S, + 50S2)198J-I(>:,-i2) = (0.1953 n.0922)-1 (-0.930) = (-11.436) 0.0922 n.1211 0.658 14.143"
1906,unknown,"0.0922 n.1211 0.658 14.143 ' and the discriminant rule is given by allocating to fl, if il(x) = (-11.436. 14. I 43) (X, -~(S.006 + 5.936») x2 - ,(3.428 + 2.770) =-1I,436xl + 14. L43x2 + 18.739> 0 and 10 fl2 otherwise. A picture of the allocation regions is given in Figure MVLTlVARJATE A NALYSIS 310 4.5 I,.,s s~/osa •. 0 J.5 J.O 2 .5 2.0 ,. 51Lo--:-':C---::.L--=--=---::'-:--~;;---::-'o--~-:::--~7.'"
1907,unknown,"Fif{I4Tf! I' . .1_' ni~t'Ti mjll{J.liml herweel1 two species of ins: {I plot of fire dalO poinl$ in tWQ dimen ""Slofls (x,= ... epal lengrh. x1.-=sepal width) rngrl/ler Wi lli ellipses of con ce""traliol1 for probal,ili,ies 0.5. 0 .9. and 0.Y9 . and tile mumda ry Ij'lt defined by Ihp discriminalll IUlu'lion (Da gm:'iie. 1975. p.313 ). 11.3.1. The estimated ellipses containing 50 . 90. and 99% of the"
1908,unknown,"bility mass within each group have been drawn to give a visual impression of the accuracy of the discrimination. See also Example 11.6. L Note thal the sample means themselves have scores Jr(i,) ~ 9.97 and h(i,j=-9.97. so that tbe boun dary line passes through the midpoint of the line segment connecting the two means . Consider a new iris with measurement, x = (5.296. 3.213)'. Since lI(x)= 3.615>0"
1909,unknown,"3.615>0 we allocale x to n,. Example 11.3.2 Ex tend the last example now to incude the II, = 50 observations on tbe third species of iris. 1. virginica. in Tab le 1.2.2. which has sample m ean and variance _. _ (6.51\8) X 3 - 2.974 . In this case 1: is estimated by S, = (0.3963 0.0919). 0.1019 ( 0.2650 So = (50S, - 50S2 + 50S,)/147 = 0.0927 0.0927) 0.1154 • 311 DISeR ! IINAI""IlT ANALYS IS and disc"
1910,unknown,"and discrimination is based on the three functions II ,,(x) = (x, -X,I'S: '{x - i(i, + i,») =-7.657 x, + 11.856x, + 5.153 . "" '3(x) =-] 0.220,'(, + 12.147x,+20.362. It,,(,,) =-2.562x , +O.29 1x,+ 15.208. Notice that I,,,(x) is not identical to the discriminant function It(x) of Example 11.3.1, because we are using a slightly different e timate of 1:. Then the allocation regions are defined by Writ"
1911,unknown,"Writing G' if Ii ,,(>< ) > 0 and I1\3(x»O. allocate x to n, if h u(x)<O and ""2'(x»0. , If /t,b)<O and Itn (x)<O. () (- - )'S-' '-'S-'- + ,-S~ ' -Jl.jj x = X j - X j u X - 2:X j u x/ 2X , U X l for i1 j. it is easy to see tbat (he discriminant functions are linearly related by It ,,(x) + h,,(x) = It ,,(x). (See Exercise 11.3.2.) Thu s the boundary consists of thTce litles meeting at the point wh er"
1912,unknown,"at the point wh ere It ,,(x) = ""23(x) = It ,,(x) = O. A picture oi the allocation regions for the iris data is given in Figure 11.3.2. Notice that it is m Ore difficult to discriminate accurately between "" .,0 .'"" 4 .0 0 .' J.5 0 ,' 3.0 2.5 2.0 "" 3.5 4.0 4.5 5,0 5.5 6.0 6.5 7.0 7.5 B.O Figure 11.3.2 Discriminatioll b~fween rhree specie ... of iris uslltg fWD variables (XI = sepal lengrh, X2 = selX"
1913,unknown,"(XI = sepal lengrh, X2 = selXd w idth); a plol of tlte sample m e a'I$ logetller with the ellipses of con cenrranon for probabiliries 0.5 and 0.9. and 'lte howtdaries between lhe allocariQn regions. Here I -= I. serosa, 2 = T. versicolou"" 3 = I. virginica (Da g ­ lIelie, 1975. p. 322). MULTIVARIATE ANALYSIS 312 T. versicolour and 1. virginkll than it is to discriminate between either of these spec"
1914,unknown,E xample 11.3.3 Sm ith (1947) collected information On normal individu­ als and psychotic individuals. The data on two of the variables is illus­ trated in Figure 11.3.3. The diagonal line in this figure represents the boundary given by the samp le discriminant rule defined in (1\.3.1). which assumes equal covariance ma trices for the populations. H owever. from the figure it is clear thaI Ihe var
1915,unknown,the figure it is clear thaI Ihe variances of the variables lor the normal individuals are smaller than for Ihe psychotics. A lso. Ihe correlation between the variables within each population appears low. For this reason Smi th tried a sample ML discriminanl rule assuming bivariate normal pop Ulations with ullequal covariance mat rices bill w ith zero correlalion between the variables in each popul
1916,unknown,"between the variables in each population. These assumptions lead to a quadratic allocation rule w ilh an elliptical boundary (Exercise 11.2.1\). (In Figure 11.3.3 the axes have heen scaled to make the boundary circular.) 60 '"" '"" '"" t:> '"" '"" ""'"", '"" '"" 40 '"" '"" '"" '"" '"" '"" '0 "",0 '"" '"" 8 0 0 Oro 0 '"" 00 0 0 ~ 0 o~oo '"" 0 '0 20 30 Figure 11.3.3 Discri-miflation berween normal individuals (0) (lnd "
1917,unknown,"'"" 0 '0 20 30 Figure 11.3.3 Discri-miflation berween normal individuals (0) (lnd pSyclIOIICS(.6) on Ihe basis of fWQ variable.s x and y: a plot of the data points plus a linear bou.""dary and a circ«lar bOlmdar}, herweell the gro""p~ (Smilll, J947; Barrlett, 1965, p. 208). 313 DISCRIMIr..A1\T ANALYSIS For the given sample Ihis rule appears 10 give a ~I ighl improvemen t Ovcr Ihe use of a linear boun"
1918,unknown,"Discriminant analysis can also be used as an aid to seriation (putting objects into chronological order), as seen by the following example . Exam ple 11.3.4 Cox and Brandwood (1959) used di criminant analysis to place in chronological order seven works of Plato-Republic, Laws. Critias. Philebus. Politicus. Sophist. and Timaeus, where it is known that Republic was written hefore Laws and that the o"
1919,unknown,Republic was written hefore Laws and that the other five works were wrillen in between. However. Ihe order of these five works is not known. The stylistic properly on which the statistical analysis is based is the type of sentence ending. The last five syllables of a sentence are each either long or short. leading to 2 ' =32 types of sentence ending. For each work the-percentage of sentences havin
1920,unknown,"the-percentage of sentences having each type of ending is given in Table 11.3.1. 1t is assumed Ihat sentence style varied systematically over time, and a function measuring this change of slyle is sought in order to order Ihe works chronologically. ( ole that tbi$ problem is not really discriminant analysis as defined in Section 11.1. hecause we do not want to allocate these intermediate works to "
1921,unknown,"Suppose each 01 the works has a multinomial distribulion and in particular suppose Ihe parameters for Repu/)/ic and Laws are 0, .... , 0 "" , and (3, ..... [l"", respectively. Then. from Example 11.2.2, the ML discri­ minant function betwee.n R epuhlir and Law< (slandardized by the nllmher of """"en1encf""~) is. givf'n hy h(~)= ""i. x.s/""i. x,. where s, = log (OJ(3i) and X, ...... '"" are Ihe number of s"
1922,unknown,"where s, = log (OJ(3i) and X, ...... '"" are Ihe number of sentences with each Iype of ending in a particular work. We do nOI know the parameters 0 , and (3"" i =1.. ... 32. However. since the number of sentences in Republk and Laws is much larger than in the other works, we shall replace 0, and (3. by their sample estimates from Table 1 I .3.1. The scores of each work on this discriminant function "
1923,unknown,"Table 11.3.2. The table also gives their standard errors; the formulae for these are given in Exercise 11.3.3. From these scores it appears that the. most likely order is Republic, Timaeus. Sophist, Critias. Politicus. Philebus, Laws. This order is not in accord with the view held by the majority of classical scholars, although there is a minority group who reached a similar ordering by apparently"
1924,unknown,"similar ordering by apparently independent arguments. For a discussion of questions related to the tatistieal significance of this ordering, see Exercise 11.3.3. and Cox and Brandwood (1959). MULTIVARIATE ANALYSIS 314 315 OISCRIMl ANT ANALYSIS Table 11.3.1 Percentage distribulion of sentence endings in seven w orks of Plato (Cox and Brandwood. 1959 ) ~ >C ~ "" ..... Type of n"" n, ..J '"" ending R ep"
1925,unknown,"0: UUUUU 1.1 2.4 3.3 2.5 1.7 2.8 2.4 ""' '"" N '"" ~. -uuuu 1.6 3. 2.0 2.8 2.5 3.6 3.9 .,; ~ >C '"" N U - UUU 1.7 1.9 2.0 2.1 3.1 3.4 6.11 0 a 0 uu-uu 1.9 2.10 1.3 2.6 2 .6 2.6 I.R ~ I uuu-u 2 .1 3 .Q 6.1 4.0 3 .3 2.4 3.4 '0 00 "" "" 2.0 3.8 4.0 4 .8 2.9 2.5 3.5 UUUU- - '"" co 0 ..... a- --UUU 2.1 2.7 3.3 4.3 3.3 3.3 3.4 ."" ..... 0 """" "" ~ 0 ~ -U-UU 2.2 1.8 2.0 1.5 2 .3 4.0 3.4 "" 0 -uu-u 2.8 0.6 1.3 0.7 0"
1926,unknown,"0 -uu-u 2.8 0.6 1.3 0.7 0.4 2.1 1.7 x a c 0 I -uuu- 4.6 8.8 6.n 6.5 4 .0 2.3 3.3 ~ a- u--uu 3.3 3.4 2.7 6.7 5 .3 3.3 3.4 S U-U - U 2.10 1.0 2 .7 0.6 0.9 1.6 2.2 ~ ..... 0:: ..... '"" '"" u-uu- 4 .6 I. J 2.0 0 .7 1.0 3.0 2.7 0 0 '"" '5 Q. "" 0 N U U - -U 2.6 1.5 2.7 3.1 3.t 3.0 3.n "" 0 c c ~ '"" 0 => 0 UU - U- 4.4 3.n 3.3 1.9 3.0 3.0 2.2 ~ I uuu- - 2 .5 5.7 6.7 5.4 4A 5 .1 3.9 0 ~ M --- uu 2.9 4 .2 2.7 "
1927,unknown,"M --- uu 2.9 4 .2 2.7 5.5 0.9 5.1 3.0 ~ ..... - -u-u 3.0 1.4 2.0 11.7 2.7 2 .6 3.3 > '"" M "" M '"" c- --uu - 3.4 1.0 0.7 0.4 0.7 2.3 3.3 v. ~ 0 c a. E M C -U-- U 2 .0 2.3 2.0 1.2 3.4 3.7 3.3 g 0 0 =5 -U-U - 6.4 2.4 1.3 2.8 1.8 2.1 3.0 0 -U U-- 4.2 0.6 4.7 0 .7 U.R 3.U 2.H t: g N uU-- - 2.8 2.9 1.3 2.6 4.6 3.4 3.0 "" 4.2 1.2 2.7 1.3 1.0 1.3 3.3 "" M U-U-- '"" M M '0 :;: c- o <Xl u--u- 4.8 8.2 5.3 5.3 4."
1928,unknown,"u--u- 4.8 8.2 5.3 5.3 4.5 4.10 3.0 -"" '"" '"" 0 U---U 2.4 1.9 3.3 3.3 2.5 2.5 2.2 '0 0 C 0 '"" 0 """" U---- 3.5 4. J 2.0 3.3 3.8 2.9 2.4 ;;; -u- -- 4 .n 3 .7 4.7 3.3 4.9 3. - 3.0 - '"" .<; '"" --U-- 4.t 2.1 6.0 2.3 2 .t 4 , J 6.4 -5 '"" ..... '"" - --U- 4.1 R.8 2.0 9.0 6.8 4 ,7 3.8 . ~ ... M '0 M 0 '"" 2 .0 2.9 2.6 2.2 c 0 0 c - ---U J.G J.3 2.9 "" C C c ~ I 4.2 5.2 4.0 4.9 7.3 3.4 1.8 "" ---- - 8 ~ Number of"
1929,unknown,"~ Number of c sentences 3778 3783 150 958 770 919 762 '"" -"" 0 ::;: -:; "" '"" ""'<:I ~v'O ...; 8 ~ g u ; tJ') «:~~'O ~ . § ' ~ . § ~ "" It) (;i > Vi .,. :<; ~ui p.j ~ MULTIVARLATE A N ALYSlS 316 11.3.2 The likelihood ratio discrimioant rule An alternative rule to the sample ML allocatio n ruk uses the likelihood ratio erilerioll, due to Ander son (1958. p, 141). This rule involves calculat­ ing the li"
1930,unknown,"ing the likelihoods of the hypo theses H i : x and the rows of X , are from n,. and the rows of X i are from n"" j oF i. for, = 1.. .. , g. Then x is allocated to the group for which the hypOthesis H , has the largest likelihood. For example , con ider the di criminatio n problem between g = 2 mul­ tinormal populations with the same covariance matrix. The m .l.e.s of 11"" ...... and 1: under H , are"
1931,unknown,"The m .l.e.s of 11"" ...... and 1: under H , are ("" ,ii I + x)/(Il, ~ I), ii"" and 1,= "", I { "" , - - ,} . I w +-I--(x- x,)(x- x,) . n2 T + HI where W = Il,S,-+-'12S,. Similarly. under H , the m.l.e.s are i,. ("",X, ... ,)/ (n,+ I), and - I { II, _ _ ,} 1:,= W~--(X - X1J( X - X ') . - 11 ,+ ,t2 + 1 1 + tl2 ... The likelihood ratio statistic. A: is the ~ (II , + "" 2 + I )Ih power 0 1 11,1 1+ (11,/(1 +"
1932,unknown,"11,1 1+ (11,/(1 + n,»)(x- i,YW -'(x- x,) li,l 1+(11 ,/(1 + 'I,»)(X - X, )'W '(x- ill We accept H ,. and allocate x to n, if and o nly if this expression exceeds one. This OCcurS if and only if II, ( -)' W -'( - '"" ( - )'W-'( -) --X - X l X-Xl»-- x- x, X - X I· 1 + 11 , 1 + 11 , In the special case where II, =11, this likelihood ratio criterion is equival­ ent to the sample ML discriminant rule def"
1933,unknown,"ent to the sample ML discriminant rule defined above. The procedures are also asymptotically equivalent if "", and "" 2 are both large. However . when ""l oF II, this likelihood ratio criterion has a slight tendency to allocate ~ to the population which has the larger sample size. 11.3.3 A Bayesian approach A problem wilh the sample ML rule. is that it does no' alluw for sampling variation in the est"
1934,unknown,"variation in the estimates of the parameters. This problem m ay not be serious for large II but can give misleading results if '/ is not milch bigger than p. 317 DISCRTMrNANT ANALYSIS A Bayesian method of dealing with this problem is to put a prior density n(6) on the parameters 6 (see, for example, Aitchison er al .. 1977). Then the likelihood of an observation x given the data X , on the assumpt"
1935,unknown,"assumption that x comes (rom the jth population. is given by averaging the p.d.f. of x given 6 with respect to the posterior density of 9 given X; that is L , (x I X ) = J f,ex 19)f(91 X ) d9, where [(91 X ) oc r(X I 6)"",(9). For example . if we are comparing two multi normal populations with the same covariance matrix, let 9 = (11,,112.1:) and let ."".(9) be the vague prior of Example 4.3.1. Then "
1936,unknown,"L,(x I X) "" J 1l:1-'·'''·'I)/lexp {-4 tr r '["" IS, + "",s, + "" ,til - Il,)(i, - 11)' + "",(i,-I1,)(i, -112)' + (x - 11,)(1 - 11,),] } dl1, dl12 d1:. (11.3.2) Using Ihe identity. "",(ii, - 11, )(i, - 11,)' +(x - 11,)(,. - 11,), n'Xl+X)'+ 11 , ( - )(- J' --XJ-X X I -X 11 1+ 1 111 + 1 and integrating (11.3.2) over dl1, and dl1, first, we get '"" III S +_""_l_ (x- i,)(x - x, )'1""'"" ""'2 til + 1 oc (I - ("" -"
1937,unknown,"(11 .3.3) x (x- i,) }-tn - ,- "" "",·p }l2. Thus. using Exercise 2.6.5, x has a multivariate Student's t distribution with parameters n - p - L. x,. and {n(1+ I/II,J/(n-l-p)}S. Here 11 = II, + II, and nS = II,S, + 112S,. Similarly. [,(x I X) is a multivariate Student's r distribution with parameters 11 - P - I, x2, and {1I(l ~ 1/11.,)/(11 - 1- p)}S. As with the sample ML rule, we allocate x to the p"
1938,unknown,"larger likelihood. Note that if 11, = ""., the twO rules are the same . This Bayesian approach can also be used w ith unequal covariance matrices. See Exercise 11.3.5. MULTIVARIATE ANALYSIS 318 11.4 Is Discrimination Worthwhile? Consider the situation where we are discriminating between g multinor­ mal populations with the same covariance matrix. and the parameters are estimated from the data X = ("
1939,unknown,"estimated from the data X = (X ; ..... X~)'. If all the true means are equal. IL, = ... = IL •• then of course it is pointless to try to discriminate between the groups on the basis of these variables. However. even if the true means are equal. the samp le means i, ..... i. will be different from one ano ther, so that an apparently plausible discriminant analysis can be carried out. Thus to check "
1940,unknown,"discriminant analysis can be carried out. Thus to check whether or not the discriminant analysis i, worthwhile. it is interesting to test the hypothesis IL, = ... = .... given X, = ... =1: •. This hypothesis is exactly the one-way multivariate analysis of variance described in Section 5.3.3a. Recall that two possible tests of this hypothesis are obtained by partitioning the ""total"" sum of squares "
1941,unknown,"X'HX as T = W + B . where W and B are the ""within-groups"" and ""between-groups"" SSP matrices. repectively. Then the Wilks' Ii test and the greatest rool test are given by fUDctions of the eigenvalues of W -'B . In particular. if g = 2. then W -'B has only one non-zero eigenvalue and the two tests are the same and are equival­ ent to the two-sample Hotelling's T' test. Then under the null hypothesis"
1942,unknown,"{"",II,(1I -2)} II d'W 'd - T 2(p.,.-2)={(,.-2)p/(/I-p-l)}F"",._._, and the null hypothesis i' rejected for large values of this stalistic. 11.5 Fisher's Linear Discriminant Function Another approach to the discrimination problem based on a data matrix X can be made by not assuming any particular parametric form lor the distribubon of the populations fl, ..... fl., but by merely looking for· a ""sens"
1943,unknown,"""sensible"" rule to discriminate between them. Fisher's suggestion was to look for the lillear {""""Clioll a'x which maximized the ratio of the between-groups sum of squares to the within-groups sum of squares; that is, let 319 D1SCRlMINANT ANAL VSIS be a linear combination of the columns of X. Then y has total sum of squares y'Hy = a'X'HXa = a'Ta , (11.5.1) which can be partitioned as a sum of the w"
1944,unknown,"L y:H.y,= La 'X ;H ,X ,a = a'Wa. (J 1.5.2) pill.' the between-groups sum of squares. L II, (Yi - 912 = L II. {a'Ii. - il}' = a'Ba . (11.5.3) where y, is the mean of the ith sub-vector y. of y, and H, is the (II. x II.) centring matrix. Fisher's criterion is intuitively attractive because it is easier to tell the groups apart if the between-groups sum of squares for y is large relative to the withi"
1945,unknown,to the within-groups sum of squares. The ratio is given by a'Ba/a'Ws . (11.5.4) If a is the vector whiCh maximizes (11.5.4) we shall caU the linear function a'x. Fisher's linear discrimillalll [ulIclion or the firsl canollical variate. otice that the vector s can be re-scaled without affecting the ratio (11.5.4). Theorem 11.5_1 The vector a ill Fisher's lillear discrirnillalll [ullclion is Ihe eig
1946,unknown,"Ihe eigenvector o[ W -'B corresponding 10 Ihe largesl eigenvalue. Proof Tbis rC1;ult follows by an app lication of Theorem A.9.2. • Once Ihe linear discriminant function has been calculated, an obser­ vation x can be allocated to one of the g populations on the basis of its ""discriminant score"" S 'lt. The sample means ii have scores Bli. = )'1' Then x is allocated to that population whose mean sco"
1947,unknown,"allocate x to fl; if I' ,-I I' ,-I a I-a X i < a X-3X j for all if j. (11.5.5) Fisher's discriminant function is most important in tbe special case of g = 2 groups. Then B has rank one and can be written as B = (""~'2)dd"" where d = X, - x,. Thus.W-'B h'"" only one non-zero eigenvalue which can be found explicitly. This eigenvalue equals tr W -'B = (n~n,) d'W- 'd MULTIVARIATE ANALYSIS and the corresp"
1948,unknown,"tr W -'B = (n~n,) d'W- 'd MULTIVARIATE ANALYSIS and the corresponding eigenvector is a = W -'d. (See Exercise 11.5.1.) Then the discriminant rule becom e allocate x to a, if d'W -'{x-i(x,+x,)}>O and to a, otherwise. 320 (J 1.5.6) (11.5.7) Note that the allocation rule given by (11.5.7) is exactly the same as lhe sample ML rule for two groups (rom tbe multinormal distribution with the same covarian"
1949,unknown,"the same covariance ma trix. given in (11.3. I). However. the ju.,tifications for this rule are quite differenl in the twO cases. In 01.3.1) there is an explicit assumption of norm ality. whereas in (11.5.7) we have mere ly sought a ""sensible"" rule based on a linear function of x. Thus . we might hope that thi~ rule will be appropriate for populations whe re the hypothesis of norma lity is not exa"
1950,unknown,"For g'"" 3 groups the allocation rule based on Fisher's linear discrimin­ ant function and the sample ML rule for Olultinormal populations with the same covariance matri-x will not be the same unless the sample means are collinear (although the two rules will be sim ilar if the m eam; are nearly collinear). See Exercise 11.5.3. In general, w -'n has min (p. g- I) non-zero eigenvalues. The corres­ p"
1951,unknown,"ponding eigenvectors define the second, third. and subsequent ""canonical variates"". (Canonical variates have an important connection with tbe canonical correlations of the la I cbapler. see Exercise 11.5.4.). The first k canonical variates. k"",;; min (p. g -]). are useful when it is hoped to summarize the difference between the group' in k dimensions. See Section 12.5. 11.6 Probabilities of Miscla"
1952,unknown,"11.6 Probabilities of MisclassificatioD 11.6.1 Prohabilities when the parameters are estimated Formally, the probabilities of mi classification P,. are given in (11.2.9). If the parameters of the underlying distributions are estimated frol)1 the data. then we get estimated probabilities P, .. For exam ple consider the case of two normal popu lations N,(/l,.:1:) aDd N e(/l,. :1:). If "",=4(/l, + 112"
1953,unknown,"N e(/l,. :1:). If "",=4(/l, + 112). then when x comes from a., a '(x- /l)­ N (lal/l. - """"l.a'l:a). Since the discriminant function is given by h(x)= a~x - /l) with a = :1:-'(/l, - 112), we see that if x comes [rom fl •. h(x)­ N(~112 , Ll'), where 321 DISCRIMINANT A..""iAL YSIS is the squared Mahalanobis distance between the populations. Similarly, if x comes from a"" lI(x)-N ( ~L1 ', Ll'). Thus , the"
1954,unknown,"Thus , the misclassification probabiJities are given by P., = P(IJ (,,;) > 0 I U ,) = <P(- E (IJ)/ V(II»= <1>(-4Ll). (11.6.1) where <1> is the standard normal distribution function. Similarly, P"" = <1>(~Ll) al·o. If the parame ters are estimated from the data then a natural estimate of <1 ' is 0 '= (x,-x,),S:'(x,- i,), and the estimated probabiJitie of misclassificatioo are p"" = P2I = <P (-10 j. U"
1955,unknown,<P (-10 j. Unfortunately this approach tends to be optimistic: that is. it tends to underestimate the true misclassification probabilities whe n II is small. Aitchison el al. (1977) have argued that probabilities based on the Bayes rule of Section 11.3.3 are more realistic. Example J l.6.1 In discriminating between T. setosa and I. versicolour in Example J 1.3.1 we find D ' = 19.94. Therefore the 
1956,unknown,"of misclassification i <p (-10)= 0.013 or 1.3%. Th., value is confirm ed visually by looking at the confidence ellipsoids in Figure 11.3. L. 11.6.2 Resubstitution method Suppose that the discrimination is based on a data matrix X of w hicb II, individuals come from pop ulation j. If the discriminant rule is defined by allocation regions R,. let 11,; be the num ber of individuals from a, which lie "
1957,unknown,"lie in R , fso L. /I;, = /I,). Then p"" = 11./11, is an estimate of r..-Unfortu­ nately. this method also tends to be optimistic abo ut m isallocation prob­ abilities. Example 11.6.2 Consider the I. setosa (a ,) and 1. versicolour (a z) data again. From Figure 11.3.J it is dear that one observation from a, will be allocated to n, aDd no observations from U , will be allocated to U,. Thus P.z=o. Pz,"
1958,unknown,"P.z=o. Pz, = 1/50. II we assume that P,z = II"". thus we get the single estimate PIZ = p"" = 1/100, which is in general agreemen t with Example 11.6.1. H owever. MULTIVARIATE A ALYS IS 322 note that for such a sma ll probability of misclassification. these samp le sizes are too small to get accurate estimates. 11.6.3 The U-method of jack-kni6ng The problem with the resubstitution method is Lhat the "
1959,unknown,"are used to define the discriminant rule and to judge its accuracy. One wa y to avoid this problem is the U-mcthod of jack-knifing. defined as follows. Let X,. T = 1 ..... "" ,. be the first II, rows of the X matrix representing the individuals from n ,. Fo r eacb T. let R \"", ... , R~' deno te the allocation regions for sO me discriminant rule based 0 11 the (II - l) X P m atrix ob­ tained by dele"
1960,unknown,"tained by deleting the rtb row from X . Then "", can be judged o n the basis of thi., rule, which is not derived using X ,. If we let II;'; deno te the number of the first II, individuals for which Xr E R;r'. then P~I = tid/ill is an c~ timat c or the mi~allocation rates. B y repeating this procedure for each of the other populations j = 2 ..... g, we ge t estimates p~. For two muitillormal pap ula"
1961,unknown,"ge t estimates p~. For two muitillormal pap ulatio ns with the same covariance matrix. this approach leads to more reliable estimates of the misallocation prob ­ abilities than either of the above (WII method~ . For further details and more methods of estimatinll-the misallocation probabilities. see Kshir­ sagar (1972, Pl'. 218-22S) . 11.7 Discardin~ of Variables Consider the discriminatio n probl"
1962,unknown,"11.7 Discardin~ of Variables Consider the discriminatio n problem between two mulrinorm al popula­ tions with means f.1h J12 and comm on covariance matrix I.. The coeffi­ cients of the theoretical ML discriminant function a'x arc given by (11.7.1) In practice of course the parame ters are estimated by x,. X,. and S"" = "",-'(n ,S,+ lIzSz}= m 'w . where 1II=II,+ n,-2. Letting d= i,-x,. the coefficien"
1963,unknown,"coefficients of the sample ML rule are given by a =m W 'd . Partition """" = (a ',. a Z) and 6' = (i.,. 62), where a , and I), have k compo ­ nents. and suppo e a 2 = 0 ; that is suppose the variables x. _, ..... .tp have no d iscriminating powe r o nce the other variables have been take n into account. and hence may be afely discarded. Note that the hypothesis ""'z= O is eq uivalent to 62 ,=0. where"
1964,unknown,"323 DISCIUMh A1'cT ANAL YSIS 11. 7 .1.) It is also equivalent to d ! = d~. where that is. the Mahalanobis distance between the pop ulations is the same whether based on the first k comp onents or On all p components . A test of the hypothesis H o: a 2 = 0 using the sam ple M ahalanobis distances D ;= m d'W- 1 d and D~ = mdi WI,' d , has been proposed by R ao (1973, p. 568). This test uses the stat"
1965,unknown,"(em - p + O/(p - k)}c'(D~ - D n/(m + c' D~) . (11.7.2) where C Z = n, n,/"". Under the null hypothesis. (It. 7.2) has tbe F._k•m _ . _ , d istribution and we reject H "" for large values of this statistic. See Theorem 3.6.2. The most important application of this test occurs with k = p - I, when we are testing the importance of one particular variable once a/l the other variables have been taken int"
1966,unknown,"variables have been taken into account. I n this case the statistic in (J 1.7.2) can Ix simplified using the inverse of the total SSP m atrix T '= (r""l. Co nsider the hypothesis H ll:a. = 0, where i is fixed. Then ( 11 .7 .2), with D k now representing the Mahalanobis distance based On all the variables except the ith. equals ( 11.7.3) and has the F , .... _ p , distribution wh en a , = O. (See Ex"
1967,unknown,"( 11.7.3) and has the F , .... _ p , distribution wh en a , = O. (See Exercise I 1.7.2.) Of course the statistic (11.7.3) is strictly valid o nly if the variable i is <elected ahead of time. H owe ver, it is often convenient to look at the value of this stalistic on all the variables to see which ones are important. Example n.7.1 Cons ider the rwo species of iris of Example 11.3.1, usillg the firs"
1968,unknown,"usillg the first two variables. sepal length and sepal width. Here n, = ""2 = 50, /II = 100-2 = 98, and the discriminant function has coefficients a = l- 11.436. 14.143)'. It is easily show n that T -' = [0.025 62 0.007 07] 0.04602 and that D~ = 19.94. Thus the two F statistics testing the imp ortance of one variable given tbe other from (11.7.3) are given by 211.8 and 180.3. respectively. Since F "
1969,unknown,"211.8 and 180.3. respectively. Since F , .• 7.o.o, = 7.0, w e conclude that both variables are highly useful in the discriminatio n. MULTIVARIATE ANALYSIS Table. 11.7.1 Discriminant co­ efficients for iris data with F s tatis.tics. a. 3.053 18.023 - 21.766 - 30 .844 F statistic 0.710 25 .7 24.6 10.7 324 H owever . a different conclusion {ollows if we discriminate On the basis of all four variables"
1970,unknown,"of all four variables. sepal length and width and petal length and width. Then the discriminant coefficients and corresponding F statistics are given by Table I l. 7. L Since F, ""'.11'"" = 7 .0. the F statistic for sepal length is not significant. so we conclude that sepal length is redundant if all the other variables are present. No tice that the F statistics in Table 11.7.1 are all smaller than "
1971,unknown,smaller than the F statistics based on two variables alone. This feature reRccts the empi rical fact that as the number of variables increases. the information carried by anyone variable. not carried by tbe other vari­ ables. tends to decrease. 1.1.8 When Does Correlation Improve Discrimination? It might be thought that a linear combination of two variables would provide a better discriminator if 
1972,unknown,"provide a better discriminator if they were correlated tban if they were uncorrelated. However. this is not necessarily so. as is shown in the following example . Example 11.8.1 (Cochran . 1962 ) Let n, and II, be two bivariate normal populations. Suppose tbal II, is N ,(O. I ) and n, is ,(11. I ). where 11 = (1'-,.1-',)' and I = [~ p] 1 . Now the Mahalanobis d istance between II, and n, is If the"
1973,unknown,"If the variables are uncorrelated then 325 DISCRI!-UNA~ ANALYSIS ow the correlation will improve discrimination (i.e. reduce the probabil­ ity of misclassification given by (11.6.1) if and only if /12 > /1~. This happens if and only if p{(1 + {,)p-2f}> 0. where [ = 1-'2/1',· In other words . discrimination is improved unless p lies between zero and 2[/(1 + I'). but a small value of p can actually "
1974,unknown,"2[/(1 + I'). but a small value of p can actually harm discrimination. Note that if ILl = J.L2. then any positive correlation reduces the power of discrimi­ nation Exercises and Complements 1l.2.1 [f n. is the .( ..... l:) population for I = l. 2.1:> O. show that the ML discriminant rule is given by allocate x lO n I if o'{x -~(p. , + """"')} > O. where 0 = 1: '(11, - 11,)· 1l.2.2 Consider three biva"
1975,unknown,"covariance matrix. given by 11, = [~l (al Draw the ML allocation regions and show that the three boundary lines meet at the point (-'t. i). (b) Suppose the three populations have prior probabilities 4. t and ~. Draw the Bayes allocation regions and find the point where the three boundary lines meet. ll.2.3 Show that the boundary hyperplane for the ML allocation rule between N.(I1,. I) and N .(112 "
1976,unknown,"only if S is an eigenvector of I. 11.2.4 Show that the Bayes allocation rule with unequal costs given in (11.2.J2) reduces to the rule given in (11.2.8) when the costs are all equal. Prove Theorems 11.2.4 and 11.2.5. 11.2.5 (See Bartlett. 1965) (a) Consider populations ,,(11 , .l:) • • (I1,.l:) and let S = 11,- 112' [n the case of biological data it is sometimes found thaI the correlation between "
1977,unknown,"found thaI the correlation between each pair of variables is approxim ately the same. so that scaling each variable to have unit variance. we can write I = E. where E=(I- p)H'pll' is the equicorrelation matrix. Using MVLTIVARJATE ANALYSI5 326 (A.3.2b) show that the ML discriminant function is proportional to p • h(x)= L 6,x,-ppO ..,(p-\)p}-'.5L x,+consl., where .5= p-' I6,. Calculate the correspon"
1978,unknown,"where .5= p-' I6,. Calculate the corresponding allocation regions for the ML rule. Discuss the cases p small. p near I. (This discriminant function is very useful in practice. even when the correlations are not exactly equai. because it can be easily calculated without the use of a computer.) (b) Write It (x) = It ,(x)+ It,(x)+co nsl., where h,(x) = p.5{p-' -p[1+ (p-l)pr'} Lx, is proportional to t"
1979,unknown,"is proportional to the first principal component. and h2 (x) = L (5, -.5)x, lies in the isotropic (p -I)-dimensional subspace of principal components corresponding to the smaller eigenvalue of :E. Interpret It,(x) and 112(x) as size and sllape factors, in the sense of Section 8.6. 11.2.6 (See Bartlett. 1965) T he following problem involves two mu! ­ tinorma! populations with the same means but dif"
1980,unknown,"tinorma! populations with the same means but differellt covariance mat­ rices. In discriminating between monozygotic and dizygotic. twins of like sex on the basis of simp le physical measurements such as weight, height. etc., the observations recorded are the differences x, .... , xp between corresponding measurements on each set o[ twins. As either twin might bave been measured first, the expecte"
1981,unknown,"bave been measured first, the expected mean differences are automati­ caUy zero. Let the covariance matrices for the two types of twins be denoted :E, and 1:2 , and aSSume for simplicity that t, =u;H [-p)101' I)'}. t2 = criH L - p)J + pll'}. Under the assumption of multivariate normality. show that the ML discriminant function is proportional to where ""', = x~ + ... + x~ and ""'2 = (x, + ... + x.f."
1982,unknown,"where ""', = x~ + ... + x~ and ""'2 = (x, + ... + x.f. How would the bounda ry between the allocation regions be determined so that the two types of misclassification have equal probability? 11.2.7 There is a prior probability. 1f,. that an observation x =(x"" x2 )' come from population n,. where i = 1 or 2 and 1T, +""""2 = I. Within fl"" x 327 DISCRIMINANT ANALYSIS has a bivariate Cauchy distribution. "
1983,unknown,"327 DISCRIMINANT ANALYSIS has a bivariate Cauchy distribution. with density function j = 1. 2. The cost of wrongly deciding that x comes from n, is cj (i = 1.2) and there are nO other costs. Find the decision rule which minimizes the expected cost 01 misallocating x. 11.2.8 (a) In Example 11.2.3 with "", > cr2 calculate the set of xs for wbich we would allocate to n, and compare with Figure 11.2. L"
1984,unknown,"(b) Co nsider two bivariate normal populations 2(11,1:,). Nzlll, 1:2), wbere we supp ose the correlation between the two variables is 0 in each population; that is :E, = diag (0'~. (J'~). :E, = diag (.,.f . .,.~). If a~ >,r, and .,.;>.,.i, show that the boundary of the ML allocation rule is an ell ipse and find its equation. 11.3.1 Why i the di~criminant function I,(x) used in Exampl e 11.3.1 to d"
1985,unknown,"discriminate between two species of iris different from the discriminant function h,,(,,) which is used in Example 11.3.2 to help di,crimillate between tbree species of iris. 11.3.2 Let h,,(x), h 13(x). and 1'23("") be the three sample discriminant functions used to discriminate between three multinormal popu lations with the same covariance matrix. as in Example [1.3.2. Show that unless X"" il2 • a"
1986,unknown,"h d x)=O, h,,(x)= 0, hn(x)=O, have a point in common. Draw a picture describing what happens if i"" X2. and x) are collinear. 11.3.3 in Example 11.3.4 suppose that Ihe multinomial parameters a, .... , an and (3, .... , (3"" for Plato's works , R epublic (flo) and Laws (n ,). are known exactly. Suppo se the distribution of sentence ending for each of the other works also follows a multinomial distrib"
1987,unknown,"of the other works also follows a multinomial distribution, with parame­ ters where A is a parame ter which is different for each or the five intermediate works. MULTIVARIATE ANALYSIS 328 (a) Show that this family of multinomial distributions varies from IT,,(A =0) to n ,(A = I) and hence can be used to represent a gradual change in populations from II"" to 11 ,. (b) Show that q,(A) = L 'Y.,s,. whe"
1988,unknown,"(b) Show that q,(A) = L 'Y.,s,. where s, = log (aJ(3,) is a monotone func­ tion of A, 0 ... A ... I. Thus q,(A) can be used instead of A to parametrize this fam ily of distributions. (c) Suppo se that for a particular work containing N sentences. there are x, endings of type i. Show that the m.l.e. of q,(A) is given by which is the same as the discriminant score of Example I 1.3.4. Thus the discri"
1989,unknown,"discriminant SCore estimates a function which gives a measure of the location of the work between nu and n,. (d) Show that the variance of s equals V(s) = and that an unbiased estimate of VIS) is V,(s)=[N(N- I)] '{Lx,sZ-Ns2 }, (e) For Iwo works of sizes 'and "". leI the corresponding mean score, and estimated variances be s', 5"" and V,(s'). VAs""). Using the fact that for large N' and N"", s' - ,,' w"
1990,unknown,"that for large N' and N"", s' - ,,' will be approximately normally distributed with mean q,(A')- q,(A"") and variance V,(s')-;-V,(S""). show that an ap­ proximate significance test of chronological ordering of these two works is given by the statistic 0/1 = (S' - S"")f{ V.(S') + V,(n}""2. If I tit I is significantly large for an N(O, 1) variate then the observed o rdering of the works is significant. ("
1991,unknown,"(I) For the two works Crinas and Timaells we have S' = -0.0346 s""= -0.1170, Vets') = .003 799, V,(s"")= O.OOO 721 8. Test the hypothesis A' = A' and hence assess the significa'lce of the ordering given by the discriminant scores. 11.3.4 The genus Chaelocnema (a ~eI1US of flea-beetles) contains two allied species. CI., cOllc;,ma and CI .. ',eikerlingeri. that were long confused with one another. Lub"
1992,unknown,"with one another. Lubischew (1962) gave the fullowing measurements of 329 D1SCRJMINANT ANALYSIS characters for samples of males of the two specics: Ch . Ch. concimw 11eike.rtil1ge.r; ,----""--., ,----""--., x, x, x, x, 191 131 186 107 185 134 211 122 200 131 201 114 173 127 242 131 171 118 1M \08 160 118 211 11M 188 134 217 122 186 129 223 127 174 131 208 125 163 115 199 124 190 143 211 129 174 131 "
1993,unknown,"174 131 218 126 201 130 203 122 190 133 192 116 18~ 130 195 123 184 131 21 I 122 177 127 187 123 178 126 192 109 Here. x, is the sum of the widths (in "",icrometres) of the fi~t joints of the first two ta~i r'fee!'·); X2 is the corresponding measurement for lhe second joints. Find the sample ML discriminant function for the two species. Suppose that we have the pair of observations (x,. x,), for a "
1994,unknown,"pecimen. but do not know to which of the two species the specimen belongs. Calculate the equation of the line (in the (x,. x,) plane) such that, if (x,. x,)' lies on one side, the specimen seems more likely to belong to CI"" cOllci""na, whereas if (x"" X2)' lies on the other side. the specimen seems more likely to belong to Ch. heikertingeri. Allocate a new observa­ tion (190,125) to o ne of these sp"
1995,unknown,"together with the line and comment. 11.3.5 Using the Bayesian approach of Section 1 I .3.3, for two mul­ tinormal populations with unequal covariance m8lrices, show that f,ex I X ) is a multivariate Student's t density with pa rameters '1 1 - p. Xll and HIII+I)/(II,-p»)S,. Find {,(x iX ). I, MULTIVARIATE ANALYSlS 330 11.5.1 In Section l1.5 when g = 2. how that the matrix W -'B has only one non·zer"
1996,unknown,"one non·zero eigenvalue, which equals {rr,n,!II)d'Wd, and find the corres­ ponding eigenvector. 11.5.2 Show that the following eigenvectors are equivalent (as uming W has rank p): (a) the eigenvector corresponding to the largest eigenvalue of W -'B ; (b) the eigenvector corresponding to the largest eigenvalue of W -1T : (c) the eigenvector corresponding to the smallest eigenvalue of Y -'W . 11.5.3"
1997,unknown,"11.5.3 (a) When the number of groups g = 3. show that the allocalion rule based on Fisher's linear discriminant function is different from the sample ML allocation rule, unless the sample means are collinear. (How . ever. if the means are nearly collinear the two rules will be very similar.) (b) Calculate Fisher's discriminant funclion for the first IwO variables of the Ihree species of iris, and "
1998,unknown,"of the Ihree species of iris, and compare the allocation regions with the three given in Example 11.3.2. (The largest eigenvalue of W -'B is 4.17. with eigenvector (I, -1.29)'. 11.5.4 Let X(II x p) be a data matrix partitioned into g groups. Define a new dummy zer~ne dala matrix Y (II x (g-1) by y;, = {~ if Xi is in the jth group, otherwise. for;= 1.. .. , g-l; i =1.. .. , n. LetS denote Ihe covar"
1999,unknown,"Show that nS ,,=T = ""total"" SSP matrix and nS'2S>iS"" = B = ""between -groups"" SSP matrix. Hence , carry oot a canonical correlation analysis between X and Y , and deduce that the canonical correlation variables for X equal the canonical variates of Section 11.5. (Hint: Ihe canonical correlation variables aix of X are given by T -'Ba, = ~i a "" or equivalently by W -'Ba, = f~J(J -A,}la,.) 11.6.1 If x"
2000,unknown,"11.6.1 If x- N.("",,, 1:), show lhat .. '( . -"",) -N.(-~.1 2,.1 '}, where IL = ~(IL ' + fll), « = r' ("""" -fll), A2.= (IL, -IL2.l'1:-' (p., - fll). 11.7.1 let 62 , = 6,-1:,,1:,,'6 , and let a = 1;-'6. Show that li, ,= 0 if and only if a = 0 if and only if6'l: '6 = 6',1;,,'6 ,. (Hint: Using (A.2.4g) and (A.2.4f), show that 6'1;-'6 = 6 ,1;;-,'6, +lIl,1:226' J') 331 DISCRL\1.1N'AN'T ANALYSIS n .7.2 Sho"
2001,unknown,"n .7.2 Show Ihal when k = p - I. (11.7.2) can be expressed in the form (11.7.3) (with j =p). (Hinl: Partition W -' so that W 22=(W 22-W2 tW I1IW ,.:!)-1 = W ""IJ, Using (A.2.4b) and (A.2.3m ) show that IW OIln +c'D ;/m ) IW I(1+c'O;/m) Using (A.2.4g) and (A.2.4f), show that d'W ' d -d~W;,' d ,= d' Vd . where v = w""'(-~)(-l\"" I) and P = W ;,'W "" . Finally no Ie Ihal G p = m(W 2Id , + W ""'''d2}= mw '"
2002,unknown,"G p = m(W 2Id , + W ""'''d2}= mw '""(d,-W 2 ,W ,:d ,), where d, = <1"", and hence (l'=m 'w ·'d W "" d = mw· ·(D '-D '» p 2.1 2.1 p"" ' 11.7.3 Verify thaI formulae (1 1.7.2) and (11.7.3) give the same values for the F statistics based on the two variables sepal length and sepal width in Example 1l.7.t. 11.7.4 A random sample of 49 old men participating in a study of aging we re classified by psychiatric"
2003,unknown,"we re classified by psychiatric examination into one of two categories: senile or non-senile. Morrison (1976, pp. 138-139 ). An independently admi nistered adult intelligence test revealed large differences between the Iwo groups in certain standard subsets of the lest. The results for these sections of the test are given below. The group means are as follows: Sub.es. XI lnfonnation -'=2 Similarit"
2004,unknown,"x, Arithm etic x .. Picture completion Senile (N, =37) 12.57 9.57 11.49 7.97 NOn-senile (N 2,.= 12 ) H.75 5.35 R.50 4.75 MULTIVARJATE ANALYs ts 332 The ""within-group"" covananCe m 31rix SIt and its inverse are given by s"" = [11.2553 S-'= "" [ 0.2591 9.4042 13.5318 -0.1358 0.1865 7.1489 7.3830 11 .5744 3.3830] 2.5532 2.6170 5.80l!5 -0.0588 -0.0647] -0.0383 -0.0144 0.1510 -0.0170 . 0.211 2 Calculate t"
2005,unknown,"0.1510 -0.0170 . 0.211 2 Calculate the linear di criminant function between the two groups based on the data and investigate the errors of misclassification. Do the sublesb ""inrormation"" and ""arithmetic"" provide add itional di~crimjJ)a ­ tion o nce the other two subtests are taken into account? 12 Multivariate Variance 12.1 Introduction Analysis of When there is more than one variable measured per"
2006,unknown,"Analysis of When there is more than one variable measured per plot in the design of all experiment. the design is analysed by ,!!ultivariate analysis Qf ,,!!riance techniques (MA OVA techniques in short). Thus we have the direct multivariate extension of every univariate design, but we will mnline our main allention to the multivariate one-way classification which is an extension of the univariate"
2007,unknown,"12.2 Formulation of Multivariate One-wa y Classification First consider a formulation resulting from agricultural experiments. Let there be k treatments Wllicb are assigned in a completely random order to some agricultural land. Suppose there are "", plots receiving the jth treatment, j = I, ... , k. Le t us assume that """" is the (p x 1) yield-vector of the ith plot receiving the jth treatment. In "
2008,unknown,"In general terminOlogy, the plots are experimental designs, the treat­ ments are condirions, and the yield is a response or outcome. We assume that the "";, are generated from the model i = 1, ...• n"" j :;:::: 1. .... k. (12.2.1) where £ ;; = independent N .(O.I ), ... = overall effect on the yield-vector, and T j = effect due to the jth treatment. This design can be viewed as a multi-sample proble"
2009,unknown,"This design can be viewed as a multi-sample problem, i.e. we can regard x;;, i = 1, ... , ,~, as a random sample from N .(I-';, I ), j = I, ... , k, w here j= 1, ... , k. (12.2.2) MUL.TIVARIA TE ANAL Y51S We usually wish to tcst the hypothesis HO: P.l = ... = f.Lk, 334 (12.2.3) which is equivalent to testing that there is no difference between the treatments T "" .••• T k• This problem has already "
2010,unknown,"5.3.3a and Example 6.4.1. We now give some further details. 1 2.3 The L ikelihood Ratio Principle Tile Lest To lest H o against H,: 1'-,;' 1'-"" for SOme i;' j, we have from Section 5.3.3. that the likelihood ratio criterion i II = IW I/trl. (12.3.1/ where , "". W = L L (I"" - i,)(:I'J - i,)' (12.3.2) I_I '_I and . "". T = L L (I""-sHI,,-X),, (12.3.3) ,=) 4=- 1 with k .. I Ix"" i=J- ll"",,1 II Recall tha"
2011,unknown,"I Ix"" i=J- ll"",,1 II Recall that W and T are respectively the ""within-samples"" and ·'total'· sum of square, and products (SSP) matrices, respectively. We can further show thaI T = W + B , (12.3.4) where , B = L 'I/(i, - i)(i, -il' ,-I is the ""between-samples"" SSP matrix. The identity (12.3.4) is the MAN OVA identity. Under H o. it was shown in Section 5.3.3a that w - W p(.I, /I - k). B - Wp(.I. k "
2012,unknown,"w - W p(.I, /I - k). B - Wp(.I. k - I), (12.3.5) 335 l\'\ULTfVAR Jr\TE ANALYSIS OF VARIANCE where Wand B are independent. Further, if II;' P + k, II = IWI/IW+ BI-A(p, n -k. k-I). whe re j\ is a W ilks' lambda variable. W e reject H o for sma ll values of II. Ho can be tested by forming the MANOV A table as set OUI in Table 12.3.1. For calculations of A, the following result is helpful: , A = TI O "
2013,unknown,"12.3.1. For calculations of A, the following result is helpful: , A = TI O A,t'. (12.3.6) ;-, where A , •...• A, are the eigenvalues of W ·IB . Th is result follows on noting that if A, ..... A"" are the eigenvalues of W · 'B . then (A,+I), i= I ....• p. are the eigenvalues of W ·'(B + W ). More details are given in Section 3.7. In particular, we shall need (3.7.9), namely , (m - p.,.I)l!- A(p, m ,"
2014,unknown,"(m - p.,.I)l!- A(p, m ,2)}_ F A ( 2) 'p.""m _ po U, p p, m. (12.3.7) E xam ple 12.3.1 Consider measurements published by Reeve (\941) on tbe skulls of /1 = 13 ant·eaters belonging to the subspecies clwpadensis. deposited io the Briti~h Museum from k = 3 different localities. On each skull. p = 3 measurements were taken: X ,. the basal length excluding the premaxilla, x"" the occipito-nasal length. x"
2015,unknown,"x"" the occipito-nasal length. x"" the maximum nasal length. Table 12.3.2 ~hows the common logarithms of the original measurements Table 12.3.1 Multivariate one-way cJaSSlficalion Source d.L SSP manix W ilks' criterion Between .-± .,0, -""', -or) samples k·l IW I/IW + B, ,-I -:1(1'. n -k, k-I, Within samples n -k W = T - B t , "", Total PI-1 T = L L (x., -i)(x"" - x)' I~ I to-I t Ditt'ereuce MULTIVARIA"
2016,unknown,", "", Total PI-1 T = L L (x., -i)(x"" - x)' I~ I to-I t Ditt'ereuce MULTIVARIATE ANALYSIS 336 Tabl. 12.3.2 Logarithms of multiple measurements on ant-eater skulls at three localities (Ree ve, 1941) Minas Graes, Brazil Matto Grosso, Brazil Santa Cruz, Bolivia , , • , • , x, X. x, x, x, x, x, X. X3 2.068 2.070 1.580 2.045 2.054 1.580 2.093 2,098 1.653 2.068 2.074 1.602 2.076 2.088 1.602 2.100 2.106 1."
2017,unknown,"2.090 2.090 1.613 2.090 2.093 1.643 2.104 2.101 1.653 2.097 2.093 1.613 2.111 2.114 1.643 2.117 2.125 1.663 - 2.140 2.146 1.681 Means 2.097 2.100 1.625 2.080 2.087 1.617 2.099 2.102 1.643 in millimetres and their means. We form the MANOVA table as given in Table 12,3,3. We find that A (3,10,2) = IW I//T! = 0.6014. We have from (12.3.7) that W -A ""')/A 112 = 0.772-F •. ,. which, from Appendix C, Ta"
2018,unknown,"which, from Appendix C, Table Co3 , is not significant. Therefore, we conclude there are no significant differences between localities. Mardia (1971) gives a modified test which can be used for moderately non-normal data. Tabl. 12.3.3 Matrices in MANOVA table for Reeve's data x 10-1 Source Between Within Total d.f. 2 1 12 a"" au Q l3 a"" 8060 6233 7498 4820 5859 11844 63423 62418 76 157 63528 76 127"
2019,unknown,"7148368651836556834881986121517 337 MULTIVAR IATE A ALYS IS OF VARJANCE 12.4 Testing Fixed Contrasts There are prob lems where the interest is not so much in testing the equality of the mean s but testing the significanoe of a fixed contrast, i.e. (12.4.1) where a,. a2' ...• a. are given cO nstants such that La, =0. Thus we wish to test H~ : a ,11, + ... + a."". = 0 and no other contrast null again"
2020,unknown,"against H , :11, ""'11,. i'"" j. W e show that the likelihood ratio leads to ihe criten on !W I/!W + C! - tI(p, 11- k, I). (12.4.2) where c=(t a,i,)(± a,i;)/(± a~). ,- 1 , _ 1 "" •. 1 PI, (12.4.3) The value of max log L under H , is the same as in Section 5.3.33. Under H ., let ~ be a vector of Lagrange multipliers. Then 1 "" .\ logL=~log l 1:I- - L L (""""- ""I)'1:- I( """" J- "",)+ ~·(a' I1 ,+· .. +a."" .)"
2021,unknown,"On differentiating with respect to 11"" it is found that I-l, = ij - 11 J I a,IX , which on using the constraint leads us to (12.4.4 ) (12.4.5) On substituting this result in the right-hand side of (12.4.5), we get til' Insetting the "" , in (12.4.4) and maximizing over 1: it is found using Theorem 4.2.1 Ibat nt= W + C . (12.4.6) MULTIVARIATE ANALYS IS 338 Hence , we obtain (12.4.2). ote that H f. i"
2022,unknown,"Hence , we obtain (12.4.2). ote that H f. implies I a;'T,: 0 because Ia;=O. We can describe C as the SSP malrix due to the contrast I ai,.;. If there is a set of mutually orthogonal contrasts, such matrices will be additive (see Exercise 12.4.1). 12.5 Canonical Variables and a Test of DimensionaUty 12.5.1 The problem W e can regard ,.. as the coordinates of a point in p-dimensional space for j: I,"
2023,unknown,"j: I, ... , k. When the null hypothesis H o of (12.2.3) is true, the vectors ,."" ... ,,., are identical. Thus if , is the dimens ion of the hyperplane spanned by ,."" ... ,,.., then H "" is equivalent to r: O. n can be seen that in any case ,,,;;min (p, Ie - I): ~ say. (12.5. L) 1f H,. is rejected, it is of im portance to determine the actual dimensional­ ity "" whe re,: O. I •.•. ,I. l! ,: I there i"
2024,unknown,"ity "" whe re,: O. I •.•. ,I. l! ,: I there is no restriction on the ,.s, and r< I occurs if and only if there are exactly s: /-, linearly independent relationships between the k mean vectors. ote that such a problem does not arise in the univariate case because for p = I we have I = I, so either, = 0 (i.e. H o) o r , = I (i.e. the ailernative). Thus we wish to test a new hypothesis H ,,: the,., li"
2025,unknown,"against H I : the ,., are unrestricted. (12.5.3) where i = I. .... k. 12.5.2 The LR test (l: known ) LeI us noW assume that i,. i: I, ...• k. are k sample means for samples of sizes "" l' ... ' ""k from N.(,.., I ). respectively. W e assume that l: is known. Since I, - p(,.,.l:/n,j, the log likelihood function is • 1c,.1 ..... I',)=c-i L n,(i,-Il,),l:-I(II-"" ')' (12.5.4) where c is a constant. Under"
2026,unknown,"where c is a constant. Under H I, ':""1 = Ii and we have max 1(,."" ... ,,.. i: c. H , ( 12.5.5) 339 MUL TJV AR IAT E. AN A LYS IS OF V ARIAK C E Under H o the max imum of I is given in the [ollowing theorem: Theorem 12.5.1 We have maxl(,.l···· ,,.,)=c-1('Y,., + ... T'Yp ), (12.5.6) Ii. whe,e 1'1'"" 1'2'"" ... ."" 1'"" a'e Ihe eigenvalues of (12.5.7) Proof (Rao, 1973. p. 559 ) We need to show , min L '1"
2027,unknown,", min L '1,(x,-,.,)'I ·I(i, - ,.,): 1',.1 + ... +1' •. H o 1"""" 1 (L2.5.8) Let (12.5.9) Then H o imphes that the v, are confined to an ,-dimensional hyperplane. We can determine an ,-dimensional hyperplane by a point Zo, and , orthonormal direction vectors Z I' ... , Z,. so that the points"", on this plane can be represented as j= 1 •...• k~ (12.5.10) where the d"" are constants. Hence (l2.5.8) is eq"
2028,unknown,"± 1I,(y,-zo-_t d,iZi)'(y,-z.,-t d'iZI) 1""""1 , a l I- I 02.5.11) subject to z;z, = I, ziz,=O, j""'-I, for i,l= I, 2, .... ,. On differentiating with respect to do) for fixed Zo, Z l' ... , Z"" the minimum of (12.5.11 ) is found to Occur when (12.5.12) Then (12.5.11) reduces to , . , L n,(y,-zo),(y.-zo)-L L n,[(Y ,-Zo)"",]2. (12.5.13) which is to be minimized with respect to Zo, z"" .... z,. Let y= I lI"
2029,unknown,"Then (12.5.13) cau be wrillen as , . , L n,(Y,-y)'{J,-y)-L L n,[{J,-y),z,J'+n(Y-Zo ),F('y-Zo), 1--1 1= 1 /""'""1 {I 2.5. 14) M UL TIVARIA TE A NAL Y SIS 340 where F=I- f Zj Z~ ~ i- I Since F is positive semi-definite, we see that (12.5.14) is minimized for fixed 21 ~ '"" .z, by Zo=j . (12.5.15) Also. minimizing (12.5.14) with respect to z, ..... z. is equivalent to maximizing c , , L L ll,[(y,-j),Z,l"
2030,unknown,"c , , L L ll,[(y,-j),Z,l'= L ziAz ,. (12.5.16) i- 1 J-1 .-1 wh ere A = L I~ (y , - y)(y,- j),. Using (12.5.9), it is seen that A = X - '12B1:- '''. Hence IA - yll = 0 if and only if IB - y1:1 = 0, so that A and r' B have the same eigenvalues. namely y,;;"" ... ;"" yp' given by (J 2.5.7). Let g, •... ,gp be the corresponding standardized eigenvectors of A . Then. following the same argument as in Sec"
2031,unknown,"that , max I z;Az , = )'1 + ... + 1'"" (12.5.17) ,-I and that this maximum is attained for z, = g~ j = 1. ... , ,. Further, L n,(y,-y),(Y,-y)= tTA = 'Y,+ ... + Yp' (12.5.18) Hence , {rum (I2.5.13H 12.5.1S) we obtain left-hand side of (12.5.8) = Y. _, + ... ""'1'p, and the proof of (12.5.8) is complete. • (\2.5.19) ow the log likelihood ratio criterion)"" is (12.5.6) minus (12.5.5) and thus (12.5.20) "
2032,unknown,"(12.5.20) is the tcsl criterion. The hypothesis H oi is rejected for large values of (12.5.20). 341 M U LTIVARIATE ANALYSIS OF V ARIAN CE 12.S.3 Asymptotic distribution of the likelihood ratio criterion For large values of II"" ...• n"" (12.5.20) is distributed as a chi-squared variable with f d.!. where f is the number of restrictions on the pk parameters f.l.. i = I, .... k. under H o. It can be s"
2033,unknown,"dimensional hyperplane is specified by ,+ I points, say f.l"" ...• "",_,. The rest of the points can be expressed as a,Jll + ... + Q ,+IJ,L,-h where a t + ... + a"" , = 1 say. Thus the number of unrestricted parameters is p(,+ I) +( k - ,- 1),. Consequenlly. asymptotically, where f= pk - p(,+ 1)-(k - ,- I), = (p- ,)(k -,-I). 12.5.4 The estimated plane Write (12.5.21) Z ' = (z, .... , z.). Y ' = (y"" ."
2034,unknown,"(Note that here a lower case letter v i used to represent a (k x p) matrix.) From (12.5.15), Zy = j = X -' -, so, using (12.5.10) and (12.5.12), we can write v' = Z 'Z(Y ' - jl')+ jI', (12.5.22) where v is the m .l.e. of v under H o. Letq/=X -""2Z; denote the jth eigenvector of 1:-'B = .I-,nAl:'12, so that qjl:q, = J, j'"" I, (12.5.23) and writc Q' = (q"" ... , q,). Then since Y ' = X-I12X , Q ' = X-"
2035,unknown,"we find from (12.5.22) that ,,' = XQ 'Q (X' - il')+ iI', (12.5.24 ) where ,,' = (jI."" ... ,,,.) is the m.l.e. of f.l under H o and X ' = (i"" ... ,id . o te that (12.5.24) is of the form "" , = Di, + (1- D )i for i= 1, .... k, where D is of rank r; that is, til is an estimator of fl.i in an r-dimensional plane passing through i. It is convenient to represent points s in this plane using the coordina"
2036,unknown,"plane using the coordinates 1:* = (Qjx, .... q~Ir, which are called clUlOllical coo,dinares. The linear function y, = qi"" is MULTJVARlATE ANALYSIS 342 called the jth canonical variable (or variate) and the vector q, is called the ;th canonical vecror. Using Q 1;Q ' = I, from (12.5.23), we observe from (12.5.24) that QV.'=Q X'. Thus, the canonical coordinates of ji., are given by ( ql~"" .. t q:~ )'"
2037,unknown,"Thus, the canonical coordinates of ji., are given by ( ql~"" .. t q:~ )' = jJ.'f ~ say, (12.5.25) for i = 1, ... , k, and are called canonical means. ote that lhe ji.~ afe r-dimensional vectors whereas the j.I., are p-dimensional vectors. Suppose for the moment that Q does not depend on i i - j.l.i' Since ii- N.U.i' n;'1;) and qjl;q;= 1. qjl;q, = 0 for jf' I. it can be seen tbat -,. N 1 •• * -'.I) "
2038,unknown,"-,. N 1 •• * -'.I) J.li ~ ,\Poi, Ti l , where J.L~ = (q~ """"it ... , q~ f.L IY is the canonical coordinate representation of the true mean j.I., (under H o or H,). Hence n, {"".~ - j.I.~)'<ii~ - j.I.~) - X;· {12.5.26} This formula can be used to construct confidence ellipsoids for the j.l.r Of course, since Q depends to some extent on i , -j.l.i' these confidence regions are only approximate. U.S.5 "
2039,unknown,"regions are only approximate. U.S.5 The LR test (unknown 1:) Consider the hypotheses H o versus H, in (12.5.2) and (12.5.3) where 1: is now unknown . Unfortunately. the LRT in this situation is quite compli­ cated. However, an alternative test can be constructed if we replace 1: by tbe unbiased estimate W /(n - k), and use the test of Section 12.5.2, treating I as known . It can be shown that for "
2040,unknown,"asymptotically equivalent to the LRT. (See Cox and Hinkley. 1974, p. 361.) Then the test given by (12.5.21) becomes in this context, asymp toti­ cally, {={p-r){k-r- I), (12.5.27) where A , •...• Ap are now the roots of IB-AW1=o . (12.5.28) Note that (n - k)Ar ,';"" y, of the last section because W /{n - k):, 1;. Esti­ mates of the k group means and their r-dimensional plane can be constructed as in"
2041,unknown,"constructed as in Section 12.5.4, and will be explored further in tbe next section. 343 M UL TIV ARIA T E A LY SIS OF VA RIA-.:CE Banletl (1947) suggested the alternative statistic D ;={II-L -~(p + k}1 flog (1 + ~,)- xf, (12.5.29) ,- r-+ I which improves the chi-squared approximation in (12.5.27 ) and therefo re (12.5.29) will be preferred. ote that for large 11. (12.5.29) reduces to (12.5.27). We"
2042,unknown,"(12.5.27). We can now perform the tests of dimensionality r = 0, 1,2 .... , t se­ quentially. First test H o:r = O. If D~ is significant then test H o: r = I. and !)o on . Tn general, if D~. D~, ... t D;_I are significant but D; is T'l0l significant then we may infer the dimensionality to be r. Although we have assumed p roots of (12 .5.28). there will be at most I non-zero roots. 12.5.6 Th e esti"
2043,unknown,"non-zero roots. 12.5.6 Th e estimated plane (unknown 1:) Assume that r i the dimension of the plane spanned by the true group means. As in Section 12. -.4 we can look more deeply into the separation of the groups in this plane (now assuming 1: is unknown ). Let .i be the eigenvector of W -'B corresponding to Ap normalized by 1;[W /(1t - k)]~ = I. (12.5.30) Theil the I ...... 1, are canonical veClO"
2044,unknown,"Theil the I ...... 1, are canonical veClOrs analogous 10 q , •...• q, of Section 12.5.4 (where we assumed 1; was known ). As in that section, these can be used to estimate the plane of the true group mean s and to represent points within this estimated plane. The projection of a point x onto the estimated plane can be rep­ resented in terms of the r-dimensional callOllical coordillates O,x •... ,I"
2045,unknown,"In particular the canonical means of the k groups, m j = O)iu ...• I ~.)', i = 1, ... , k, represent the projection of the group means onto this plane and can be used to study the differences between the groups. The vector .i is the canollical vectOr for the jth callo""iclIl vllriabl. Y, =I;x. ote that from Section 11.5. the canonical variables are optimal discriminant functions; that is, for the d"
2046,unknown,"discriminant functions; that is, for the data matrix X , the jth canonical variable is that linear function which maximizes the between group variance relative to within group variance, subject to the con traint that it is uncorrelated with the preceding canonical variables. Consequently, for any value r ~t, the canonical variables Yll ... , y, are those linear func­ tions which separate the k sam"
2047,unknown,"tions which separate the k sample means as muc h as possible. A graph of the canonical means in r = 1 or ,= 2 dimensions can give a useful picture of the data. ote that on such a grapb, Ibe estimated MULTrvARlATE ANALYSIS 344 variance matrix for the canonical coordinates of an observation coming from any of the k groups is the identity. (The canonical variables are uncorrelated with one another an"
2048,unknown,"uncorrelated with one another and their estimated varianc~ is normalized by (12.5.30) to equal 1.) Thus the units of each axis on the graph represent one standard deviation for each canonical coordinate of an observation, and the canonical coordinates are uncorrelated with one another. Hence , we can get some idea of the strength of separation between the groups. Also, we can estimate the accuracy"
2049,unknown,"between the groups. Also, we can estimate the accuracy of each of the canonical means on such a graph. From (12.5.26) a rough 100(l-a)% confidence region for the ith true canonical mean /1~ = (1\/1"" ... , I~,)' is given by the disk of radius n;-'12x, .• about the sample canonical mean m , = (I\i"" .... I:i,)', where X;:. is the upper a critical point of a x; variable. Note that (A, + ... + A,)/(tr "
2050,unknown,"(A, + ... + A,)/(tr W- 'B ) = (A, + ... + A, )/(A, + ... + A.) represents the proportion of the between groups variation which is explained by tbe first , canorucal variates. Canonical analysis is the analogue for grouped data of principal compo­ nent analysis lor ullgrouped data. Further, since an estimate of the inherent variability of the data is given by W, canonical coordinates are invariant "
2051,unknown,"invariant under changes of scaJe of the original variables. (This property is not shared by principal component analys;'.) Example U.S.1 Consider the iris data 01 Table 1.2.2 with p = 4. Let x, = sepal length, X 2 = sepal width, X J = petal length, and x. = petal width. Here, the three samples are the three species I. selosa, I. IJ.rsic%ur, and I. IJirgillica. Further, II, = II, = II, = SO. It is "
2052,unknown,"vectors are [ 5.006] 3.428 i, = 1.462 ' 0.246 [ 5'936] 2.770 i2 = 4.260 . 1.326 [ 6.588] 2.974 I,-5.552 . 2.026 The SSP matrix between species is -19.95 165.25 """"] 11.35 -57.24 -22.93 437.11 186.78 . 80.41 345 MULTIVARIATE ANALYSIS OF VARIANCE The SSP matrix within species is w-["""" 13.63 24.62 16.96 8.12 27.22 It is found thaI the eigenvalues of W "" B are 5.64] 4.81 6.27 6.16 A, =32.1877, A2 =0.28"
2053,unknown,"6.16 A, =32.1877, A2 =0.2853 and tbe corresponding eigenvectors normalized by (J 2.5.30) are 1'\ = (0.83, 1.54. - 2.20, -2.81), Ii = (-0.02, -2.17.0.92, -2.83). In this case, (12.5.29) becomes D~= 546.01-X~ wbich by Appendix C, Table C . I is highly significant. Thus the mean differeoces are significant. Now Df = 36.52-xi, which is again highly significant. Hence. since 1=2, both canonical variabl"
2054,unknown,"variables are necessary, and the dimension cannot be reduced. In lact, the canonical means for I. serosa are m; =(1;"""" 1,"",)=(5.50, -6.89). Similarly, for the 1. IJersic%ur and 1. IJirgillica varieties, we bave, respectively, m,=(-3.93, -5.94), m l=(-7.89, -7.18). figure 12.5.1 shows a plot together with the approximate 99% confi­ dence circles of radius (xi,o..,150) '12 = (9.21/50)'12= 0.429, for"
2055,unknown,"canonical means. It is quite clear from the figure that the three species are widely different but 1. versic%ur and I. virgi1lica are nearer than 1. selosa. Example U.S.2 (Delany and Healy, 1966) White-toothed shrews of the genus Crocidura occur in the Channel and Scilly Islands of the British Isles and the French mainland. From p = 10 measurements On each of 11 =399 skulls obtained from the k = \"
2056,unknown,"11 =399 skulls obtained from the k = \0 localities. TreseQ, Bryher, St Agnes, St Martin's, 5t Mary's, Sark, Jersey, A1derney, Guernsey. and Cap Gris ez, the between and within SSP matrices are as given in Exercise MUL T IVARIA TE A:-;,,\LYSIS 346 Second canonical variate - 4 I. versicolou r o o I. setosa I. virginica o - 8 L-____ ____ ______ ~ ______ ~~LL------~----~ - .9 - 5 - 3 5 7 Figure 12.5.1"
2057,unknown,"- .9 - 5 - 3 5 7 Figure 12.5.1 F irst canonical variate 12.5.1. The sample sizes for the data from the localities are. respectively. 144, 16, 12, 7, 90. 25, 6, 26, 53. 20. The first two canonical vectors are fo und to he I ~ = (- 1.48. 0.37, 2.00 . .59. - 3.93. 0.90. 8.10. 8.78. 0.00. 0.44) • • ,= (0.44,1.81.-0.5 .- 9.65.- 7.01 ,1.97.3.97.7.10.-7.61.0.34). The first two canonical variables account"
2058,unknown,"The first two canonical variables account for 93 .7% of the between­ samples variation (see Exercise 12.S.J). The sample canonical means . m : = (1\1'"" .,i,). j = I •...• II. (centred to have overall weighted mean O) and the 99% confidence circles for the true canonical means are shown in Figure 12.5.2. This suggests that the three populations of C. ,,,sstlia (Aid erney, Guernsey, and Cap G ris ez"
2059,unknown,"(Aid erney, Guernsey, and Cap G ris ez) show more differentiation than tbe five populations of C. suaueolells {T resco, Bryher, St Ag nes, St Ma ry'S, St Martin's} in the Scilly Isles. The shrews from Sark are distinct from those in the five Scilly Isles and lersey. Further discussion of this data is given in Exercise 12.5.1 and in Chapter 13 on Cluster Analysis. It should be noted that the presen"
2060,unknown,"It should be noted that the present analysis differs slighUy from the one given by Delany and H ealy (1966). In their analysis the SSP matrix was calculated without using the II, as weights. i.e . • B = L (I, - i)(I:, -jl'. t- l 347 l n V - -1 j MLJLl1 V ARI A rE ANALYSI S OF VARI ANCe G I\.fULTIVARiAlC ANALYStS 348 Consequently the canonical variables obtained differ from those given above, and t"
2061,unknown,"above, and their diagram corresponding to Figure 12.5.2 differs mainly in that the callollical m~ans for th~ Jersey sample become very close to those of Sark. However, the conclusions are broadly the same. 12.S.7 Profile analysis A related hypothesis to the above dimensionality hypothesis is the profile I,ypothesis. w hich assumes that the group means lie on a line and that tbe direction of this l"
2062,unknown,direction of this line is given by the vector 1; that is. the difference between each pair of group means is a vector whose components are all equal. This hypothesis i of interest when all of the variables measure similar quantities and it is expected that the difference between the groups will be reflected equally in all tbe variables. If the mode l is parametrized in terms of the k group means .
2063,unknown,"then the profile bypotbesis can be written in the form of Section 6.3 as C ,jIM ,= O. whe re ... =(.,., ........ ) and C,«k - l)x k) and M ,(p x(p- l») are given by - I 0 0 0 0 0 I - ) 0 - I 0 C ,= and M ,= 0 - I 0 0 0 0 -1 ° 0 - I This hypothesis is of interest because M , is nol equal to the identity; that is. a relationship between the variables is assumed as well as a relation­ ship between th"
2064,unknown,"ship between the groups. For further details see Morrison (1976. pp. 205-216). 12.6 The union intersection approach For the union intersection approach we concentrate on the one-way classification given in (12.2.1). We wish to test the hypothesis T , = ... = ""' •. Thi multivariate 349 MUL T IVARr A TE ANALYSIS OF V A RIAN C E hypothe is is true if and only if aU of the univariate hypotbeses • H oa"
2065,unknown,"hypothe is is true if and only if aU of the univariate hypotbeses • H oa.c.: L: c,aJ'Tj=U .-, are true for all p-vectors a and all k -vector contrasts c. Following Section 5.3.3a, it is found tbat a suitable test criterion is A"" the largest eigenvalue of W -'B . We reject H o for large values of A,. This approach has the advantage that it allows us to construct simul­ taneous confidence regions fo"
2066,unknown,"taneous confidence regions for contrasts between the T ;. Percentage points are given in Appe ndix C . Table C.4 of 6 = A ,t(l + A ,) for p = 2 and selected critical values. We may use these to form 100(1- a )% simul­ taneous confidence intervals which include intervals of the form where 6. denotes the upper 0: critical value of 6. In general we denote the random variable corresponding to the dist"
2067,unknown,"random variable corresponding to the distribution of the largest eigen­ value of IB - 6(8 + W )i = 0 by 8(p. V I> v,). wh ere B - W .(l:. v,) indepen­ dently of W - W .(l:. v,). Here v, is the error degrees of freedom and V, is the hypothesis degrees of freedom . More details are given in Section 3.7. This test is related to the LRT in as much as both are functions of the eigenvalues of W -'B . Ex"
2068,unknown,"eigenvalues of W -'B . Exam ple 12.6.1 In Reeve 's data given in Example 12.3.1 we find that the non-zero eigenvalues of W -'B are A, =0 .3553 . '\, = 0.2268. (Note rank (W -'B )=2 .) From Appendix C, Table C.4 the 1% critical value of 6(3.10.2) = 8(2. 9, 3) is 6 •. 01 = 0.8074. Since A ,/(1 + A ,) = 0.2622 < 60 .0 , we accept the hypothesis that tbere are no differences, just as in Example 12.3.1"
2069,unknown,"as in Example 12.3.1. Even if w e look at differences between x, for groups 2 and 3 which look piau 'ibly different. by forming a 99 % CI with a = (0, 0, 1)"" j =2, k = 3, w e get an interval T n -'I'3)E 1.617 _ J.643± {O .010978 00 , (.!+.!)}'/2 I - 60 .01 4 3 =-0.026± 0.164 which contsins O. Because the null hypothesis is accepted. it follows from the construction of the test that this and all ot"
2070,unknown,"significant. MULTIVARIATE ANAL VS1 S 350 Example 12.6.2 We now examine Example 12.5.1 by this method. The greatest eigenvalue of W-'B was A, =32.188 and so 8=A, /(I +'\,)= 0.9699 - 8(4. 147,2)= 8(2,145.4). From Appendix C , Table C.4. 800 , = 0.1098. so we strongly reject the hypothesis of equal mean vectors. Suppose we are interested in sepal width (X2) differences. Choosing a' = (0, 1,0,0). lhe "
2071,unknown,"{ 16.9680.0'(""':'+~)} 1/2= 0. 29 as 11,=11'=""3=50. 1 - 80 .QI 50 ,0 This leads to the following 99 % SCls for sepal width differences: I. uirginicQ and 1. uersicolo""r 1. setosa and 1. uer.ic%ur I. se/osa and 1. uirgiPlica 1"" 2-1UE 2.97 - 2.77 ±0.29 = 0.20±0.29; 1'n-1 22 E3.43 -2.77 ±0.29 = 0.66±O.29; 1 ,,-112 E 3.43- 2.97 ±0.29 = 0.46±0.29. Thus , we accept the hypothesis of equal sepal width mean"
2072,unknown,"Thus , we accept the hypothesis of equal sepal width means at lhe 1 % level for 1. uirgiPlica and 1. uersic%ur. It is easily verified that all other confidence intervals on all other variable differences do not contain zero. so apart from this single sepal width difference. the three species are differcnt on all four variables. ote that simultaneous confidence intevals are helpful because they pro"
2073,unknown,"provide detailed information about each variable and about the differ· ences in means pair-wise. However. lhere-is an advantage in cano nical analysis because it gives us a global picture of the group differences. 12.7 Two-w ay Classification The ANOVA has a direct generalization for vector variables leading to an analysis of a matrix of sums of squares and products, as in the single classificatio"
2074,unknown,"classification problem . ow consider a two-way layout. Let us suppose we have nrc independent observations generated by the model ][rrk = J.L+ CX . + '1', + ""li, + £ Iflt. i=1. ... ,r, j=l, .... c, k=l, ... ,Il, wbere Q ; is the ith row effect, T , is the jth column effect, 11.; is the interaction effect between the ith row and the jth column . and E'i< is the error term which is assumed to be ind"
2075,unknown,"error term which is assumed to be independent .(0.:1:) for all i, j, k. We require that the number of observations in each (i. j)·cell should be the same . so that the total sum of squares and products matrix can be suitably decomposed . We are interested in testing the null bypotheses 01 equality of tbe Q ;, equality of the T i, and equality 01 the 1111' We partition the total 351 MULTIVARIATE AN"
2076,unknown,"sum of squares and products matrix in an exactly analogous manner to univariate anaJysis. .Let 'r. R , C. and E be the total, rows, column s. and errors SSP matrices. respectively. As in the univariate case, we can show that the following MA OVA identity holds, i.e. where with and T = R + C+E. . R = cn L (i, .. -i. .. )(x, .. - i. .. )', , C =m L (i./ - i . .)(i,.-i. )'. ,-, • < • E = L L L ("""" /k"
2077,unknown,",-, • < • E = L L L ("""" /k-i,. - i.,.+i""')(""'""k-i,. - i.,.+i ... )'. 1'. 1,- 1 .... - 1 1 ' • ii. = - L LX., •. en ,-1 ""'''''1 1 • • i., =-L L ""'."" rU, _ ll_ 1 1 • c: "" i ... = - L L L """"/"" reI! i- 1 ,_ 1 l - I W e may further decompose E into E = I+ W , where . , I= n L L (i;, - i ... - i., + i ... )(i.j.-i; .. - i.,.+i ... ), .-I/- J and W = 1: t f (""'.,,, - iij')(""""i' -xu r, I- I j- 1 k - I wit"
2078,unknown,"I- I j- 1 k - I with (12.7.1) (12.7.2) MULTrvARlATE ANALVSIS 352 Here 1 is the SSP matrix due to interaction and W is the residual SSP matrix. Tests fo, interaclions Thus from (12.7.1) and (12.7.2) we may partition T inlO T = R + C +I+ W . (12.7.3) Clearly under the hypothesis H o of all 01,.7 "" and ."""" being zero. T m ust have the W p(l;, ,cn - I) distribution. Also. we can write say, and , wheth"
2079,unknown,"Thus , as in Section 12.3, W - W p(l;. ,c(n - I» . In the same spirit as in univariate analysis, whether or not the OIS and 'l'S vanish~ 1- Wp(I, (,-I)(c- I)) if the."""" are equal. 11 can be hown that the matrices R . C. I. and W are distributed independently of one another, and further we can show that the LR statistic for testing the equality of the interaction term is IW IIIW+1 1 = IW IlIE I-.A("
2080,unknown,"IW IIIW+1 1 = IW IlIE I-.A(p. v"" v.). where v,=,c(II-I), "", = (, - 1)(c- 1). (12.7.4) We reject the hypothesis of no interaction for low values of A . W e may alternatively look at the largest eigenvalue 0 of I(W + 1)-' as in Section 12.6. which is distributed as O(p, v"" v,). Note that if n = 1. i.e. there is one observation per cell. then W has zero d.l., sO we cannot make any test for the presen"
2081,unknown,"zero d.l., sO we cannot make any test for the presence of interaction. Tesrs for main effecls If the column effects vanish, then C can be shown to have the W p (1:. ,--1) distribution. The LR statistic to test the equality of the 'l', irrespective of the 01, and 11'1 can be shown to be IW II!W + Ci -A(p. ""L' V2)' 353 M U LTIVARIATE ANALYSIS OF VARJANCE where "",= ,c(II- I). 112 = c - l. (12.7 .5) E"
2082,unknown,"where "",= ,c(II- I). 112 = c - l. (12.7 .5) Equality is again rejected for low values of A . Alternatively we can look at the largest eigenvalue 0 of C{W+C r ' which is distributed as O(p. V"" v,), wh ere II, and v, are given by (12.7.5). Similarly. to test for the equality for the, row efIects 01, irrespective of the column effects 'l',. we replace C by R . and interchange, and C in the above . No"
2083,unknown,"above . Note that if significant interactions are present then it does not make much sense to test for row and column effects. One possibility in this situation is to make tests separately on eacb of tbe 'c row and column categories. W e m ight alternatively decide to ignore iDleraction effects compl etely, either because we have tested for tbem and shown them to be non­ significant, or because"" ="
2084,unknown,"significant, or because"" = 1, or because for various reasons we may have no 'l'j tenn in our model. In such cases we work on the error matrix E instead o f W and our test statistic for column effects is IE IIIE + C \-A(p. v"" V2) ' where IJ ,=rcn- r-c+ 1, v,= c-l. (12.7.6) We may alternatively look at the largest eigenvalue 0 of C(E + C)-' which is distributed as O(P. "",. "".) with the values of v, "
2085,unknown,"which is distributed as O(P. "",. "".) with the values of v, and v, given by (12.7.6). Example 12.7.1 (Morrison. 1976 , p. 190) W e wish to compa re the weight losses of m ale and female rats (, = 2 sexes) under c = 3 drugs where n = 4 rats of each sex are assigned at random 10 each drug. Weight losses are observed for the first and second wee ks (p = 2) and the data is given in Table l2.7.1. W e wi"
2086,unknown,"given in Table l2.7.1. W e wish to compare the effects of the drugs, the effect of sex, and wh ether there is any inleraction. We first test {or inleraction. We construct the MA OVA table (Table 12.7.2), using the totals in Tahle 12.7.1. From Ta ble 12.7.2, we find that iW I = 4920.75, IW + 11= 6281.42. Hence , A =IW IIIW + li=0.7834 - A(2 ,18.2). From \12.3.7), (17/2)(1 -./0.7834)1 0.7834= 1.10- "
2087,unknown,"From \12.3.7), (17/2)(1 -./0.7834)1 0.7834= 1.10- F •. 34 • MULTIVARIATE ANALYSIS 354 Table 12.7.1 W eight losses (in gram s) for the first and second weeks foc .. 1S of each sex under drugs A , B. and C (Morrison. 1976. p. 190) Sex Male Column sums Female Col umn sum s Trealm eOI sums Grand IOlal , A { (5.6) (5,4) (9,9) (7.6) (26.25) f 7 ,1O) (6.6) (9,7) ( ,10) (30.33) (56.58) Drug B C R o w sums"
2088,unknown,"( ,10) (30.33) (56.58) Drug B C R o w sums (7,6) ( 21 ,15) (33,27) (7,7) (14, II) (26,22) (9.12) (17.12) (35,33) (6.8) (12.10) (25.24) (29,33 ) (64.48) (119, 106) (10,13) (16,12) (33,35 ) (8,7) (14,9) (28.22) (7,6) (14.8) (30,21 ) (6,9) (l0,5) (24,24) (3 l. 35) (5:4. 34) (liS. 102) (60.68) (11 8.82) (234.208 ) This is clearly not significant so we conclude there are nO interactions aDd proceed to "
2089,unknown,"proceed to test for main effects. First, for drugs, iW + q = 29 180.83. T herefore A = 4920 .7S/(29 180.83) = 0.1686 - A (2, 18,2). From (12.3.7), (17/2)(1- 0.1686)/..10.1686 = 12.20-F4 •J •• Table 12.7.2 MA OVA table for the data in Table 12.7.1 SSP malrix A Source dJ, all a"" an Sex (R ) I 0.667 0.667 0.667 Drugs (C) 2 301.0 97 .S 36.333 Interaction (I) 2 14.333 21.333 32.333 R esidual (W ) 18 94"
2090,unknown,"R esidual (W ) 18 94.5 76 .5 114.0 TOia l (T) 23 410.5 196 .0 183.333 355 MULTIVARIATE ANALYSIS OF VARIANCE This is significant at 0.1 'Yo, so we conclude that tbere are very higbly significant differences between drugs, Finally, for sex, iW + R i=4957.75. Tbus A =4920.75/4957 .7S=O ,9925 - A(2, 18, 1). Again from (3.7.10), the observed value of F 2.34 is 0.06. This is not significant so we conclu"
2091,unknown,"significant so we conclude the.re are no differences in weight loss between the sexes. Simullalleous collfidenee regions We begin by requiring the interaction parameters Tt., in the two-way model to vanish, otherwise comparisons among row and column treat­ ments are meaningless. If so. tben, in a similar manner to Section 12.6, the 100(1-a )% simultaneous confidence intervals for linear compounds "
2092,unknown,"of the differences of the i,th and i2 th row effects are given by { 20 } III s'(a"" -a ,,) E 8'(i"" .. - i •. .) ± (a) a'Wa , ell 1 -o. where 8. is obtained from Appendix C. Table C.4, w ith V'2 = r -1, and a is any vector. Similarly, for the columns '( ) '(_ _) { 29. 'W }""2 a ""rh - Th e 8 I .'I--I 'h_ ± ( )8 a , rtI 1 - 6Q1 (12.7.7) "", = rC(1I -1), (12.7.8) wh ere O. is again obtained from tables w"
2093,unknown,"(12.7.8) wh ere O. is again obtained from tables with rand c interchanged in ""1 and ""2 above. ExlllDple 12.7.2 (Morrison, 1976 , p. 190) For the drug data from Table 12.7.2, W = [94.S, 76.S] 76.5, 114,0 . From Appendix C, Table C.4, it is found for p =2, "",=18, ""2=2 , and a = 0.0 1 that 6. = 0.S02. Taking 8 ' = (1, 0), for the first week, from (12.7.8), the 99% simultaneous confidence intervals fo"
2094,unknown,"(12.7.8), the 99% simultaneous confidence intervals for Ihe B-A , C -B, and C-A differences are -4.36"" TB l- T"" ,""S.36, 2.39 "" TC l -TB,"": 12.11 2.89""TC,-TA , "":12.61, where we have used the ahove value of Wand drug means (totals) from Table 12,7.1. Hence at the 1% significance level, the drugs A and Bare not different with respect to their effects on weight during the first week MULTIVARIATE A AL"
2095,unknown,"of the trial. Howeve r, the effect of drug C is different from drugs A and B. Extension 10 i1igiler designs By following the same principle of partitioning the total SSP matrix. we are able by analogy with univariate work to analyse many mOre complex and higher order designs. We refer to Bock (1975) for further work . Exercises and Complements 12.3.1 (a) Prove the multivariate analysis of variance"
2096,unknown,"after wri tin g X i, - i= (xv - i,) + (i,-i). (b) Show from Section 5.3.3a, that under H o, W - W.(l;, II-k), Further Wand B are independent. B - W .(l;, k - I). 12.3.2 (See, for example. Kshirsagar, 1972, p. 345) Under H , :11,""'11, for some i'"" i, show that (a) E(W) =(It-k)I . (b) E(B)=(k - l)I +~ where ~ ;; = ~ n,I1,. ~ = L... "",(l1j-ji.)(j1.,-jj.)'. r L... ,-1 ,_I n 12.3.3 The data considered "
2097,unknown,",-1 ,_I n 12.3.3 The data considered in Example 12.3.1 is a subset of R eeve's data (1941). A fuller data leads to the following information on skulls at six localities: Locality Subspecies Sample size Mean vector. x: Sta. Maw, Colombia instabilis 2 1 (2.054, 2 .(\~~. 1.621) Minas Geraes . B razil chapadensis 6 (2.097.2.100,1.625) Mauo Grosso. Brazil chapadensis 9 (2.091,2.095, 1.624) Sta. Cruz. B"
2098,unknown,"Brazil chapadensis 9 (2.091,2.095, 1.624) Sta. Cruz. Bolivia chapadensis 3 (2.099,2.102, 1.643) Panama c/,iriqutmis 4 (2.092, 2.1 J 0, 1.703) Mexico mexicana 5 (2.099.2.107.1.671) Total 48 (2.077, 2.086, 1.636) 357 MULTIVARIATE ANAL VSIS OF VARIA.\fCE Show that tbe ""between -groups"" SSP matrix is [ 0.0200211, 0.017 444 8, 0.013 081 1~ B = 0.0158517, 0.0150665. 0.0306818 Assuming the ""within-groups"
2099,unknown,"[ 0.013 630 9, 0.012769 I, 0.016437 9J W = 0.0129227, 0.0171355 0.0361519 show that A , =2.4001, A2 =0 .9050, A3 =0.0515 . Hence show that there are differences between the mean vectors of the six groups. 12.3.4 Measurements are laken On the head lengths u"" head breadths u2 , and weights U 3, of 140 schoolboys of almost the same age belonging to six different schools in an Indian city. The between"
2100,unknown,"SSP matrices were and B = [52.0 214 .2 151.3 521.3~ 401.2 1612.7 _ [13 561.3 1217.9 3192_5~ T - 1650.9 4524.8 . 22622.3 Show that trl =213 629 309 S44. Obtain W = T - B . Hence or otherwise, show that IW I = 176 005 396 253. Consequently IW I/IT I = 0.823 88, which is distributed as It (3, 134.5). Show that Bartlett's approximation (3.7.11) gives 26.06 - X~s. H ence conclude at the 5% level of sig"
2101,unknown,"there are differences between schools. 12.4.1 Let A = (ai,) be a known (k x r) matrix such that A'O -' A = In where O =diag(n"" ... , II,). Suppose we wish to test H o:A 'j1.= O, .,:= (.,. ..... ,14,) against H ,: 14, '"" 14"" i"" j. Following the method of Section 12.4 show. under H o, that MULTIVARIATE ANALYSIS 358 where • :EA, = L a/,i/. ,-, Hence prove that and derive the likelihood ratio te I. U."
2102,unknown,"U.S.1 For the shrew data of Delany and Healy (1966) referred to in Example 12.5.2 we have. from a total sample of 399 , 208.02 88.56 1 0 1.39 41.16 10.06 98.35 57.46 3 3.0 1 55.12 63.99 39.25 42.14 17.20 4.16 41.90 24.35 14.16 22.66 27.32 50.26 20.23 4 .98 47.75 28.04 16.04 27.54 3 1.14 8 . 7~ 2.14 19.33 11.66 6.41 1 1.03 12.~g B ~ 0.78 4.73 2.73 1.45 3.05 3.13 47.47 27.97 1 5.76 25.92 3 0.141 17A"
2103,unknown,47.47 27.97 1 5.76 25.92 3 0.141 17A R 9.47 15.11 l7.6g 5.44 8.58 10.06 15.59 16.88 19.87 57.06 21.68 21.29 4.29 2.68 22.31 2 .87 3.14 11.78 14.54 20.76 6.08 2.65 1.7R 8.63 0 .73 1.06 3.66 6.9 1 13.75 0.98 0.41 7.16 1.\4 1.25 6.20 5.35 1.59 0.68 2.06 0.28 0.21 0.62 1.49 W = 1.50 1.53 0.36 0 .13 0.22 0.94 14.34 1.38 1.21 4.39 5.92 2.13 0.27 0.72 0.77 0.93 0.73 0.74 4.97 3.16 5.79 The eigenvalues of
2104,unknown,"5.79 The eigenvalues of W' B are A, = 15.98, ,1.,= 0.99. >'3= 0.48. ,1..= 0.36. >'5= 0.15. '\6 = 0.10, ,1.7= 0.05. ,1..=0.01 , ,1..=0.0006 . ,1.1(1 =0 and the first two canonical variables are as given in Example 12.5.2. 359 M UL TIVAR IATE ANAL YSI S OF VA RIANCE Show that D~ = 1751.2, D f = 652.4. D~ = 385 .6, D i = 233 .5. D~ = 114.2, D~=60.0, D~ =23. 0. D~ =4.1 , D ;=0.2. Hence show that the d"
2105,unknown,"dimension of the data should be sill if we assume the approximation (12.5.29) for the distribution of D?, and use a test size a = 0.01. Com­ ment why tbis approximation could be inadequate. 12.7.1 For the drug data of Example 12.7.1, obtain the 99% simultane­ ous confidence intervals for the drug differences B-A , C-B, and C-A for the second week . Show that the pair-wise differences between Ihe d"
2106,unknown,for the second week are not significant at the 1 % level. Comment on whelher the significance of the original MA OVA could be due to the effect of drug C in the first week . 13 Cluster Analysis 13.1 Introduction 13.1.1 Single-sample case IT instead of the categorized iris data of Table 1.2.2. we were presented with the 150 observations in an unclassified ma nner. then the aim migh t have been to g
2107,unknown,"have been to group the data into homogeoeous classes or clusters. Such would have been the goal before the species of iris were established. For illustrative purposes, Table 13.2.1 gives a sma ll mixed sub-sample from the iris data and the aim is to divide it into groups. This kind of problem appears in any new investigation where ooe wishes to establish not o nly the identity but the affinity of "
2108,unknown,"Before D arwin, the eVOl utionary tree was such an unsolved problem. fn general we can summarize the problem of cluster analysis as follows. Single-sample problem Let """" ... ,X. be measurements of p variables on each of 0 Objects w hich are believed to be heterogeneous. Then the aim of cluster analysis is to group these objects into g homogene ous classes where g is also unknown (but usually assum"
2109,unknown,"than 0). Some points should be noted. We call a group ""homogeneous"" if its members are close to each other but the members of that group differ considerably from those of another. This leads to the idea of setting up a metric between the points to quantify the notion of ""nearness"". For various possible choices of dis­ tance, see Section 13.4. The techniques are usuaUy applied in two rather differe"
2110,unknown,"one case, the purpose of the analysis is purely descriptive. There are no 361 C LUSTER ANALYS IS assumptions about the form of the underlying population and the cluster­ ing is simply a useful condensation of the data. In other cases, there is an underlying mod el where each observation in the sample may arise from -anyone of a small number of different distributions (see Section 13.2.1). We will "
2111,unknown,"We will pursue both aspects. The term ""clustering"" is taken to be synonym ous to ""numerical tax­ onomy "", ""classification"". Other terms used in this context are O-anal)'sis, typology, pattern recognition, and clumping. 13.L2 Multi-sampfe case Another context where the problem of cluster analysis arises is when we are given a collection of samples, and the aim is to group the samples into homogeneo"
2112,unknown,homogeneous groups. The problem can be better understood through a specific situation in zoology already descrihed in Example 12.5.2. It is known that white-toothed shrews of the known genus Oocidura occur in the Channe l and Scill)' Islands of the British fsles and the French mainland. A large number of observations from 10 localities in the Channel and Scilly Islands were obtained to examine the
2113,unknown,"there m ay be two species of CrQciduril. The localities w ere geographi­ cally dose, but it is assumed that only one sub-species waS present in any one place. Thus the problem here is to group ""samples"" rather than ""ob jects,"" as in the single-sample case. In general, the problem can be summarized as follows. Multi-sumple problem Let X II' i= I, ... , """" be the observations in the jth (random ) sa"
2114,unknown,"jth (random ) sample, i = 1,2 ..... 111. The aim of cluster analysis is to group the m samples into g homogeneous classes where g is unkn own , g""m. 13.2 A Probabilistic Fonnulation 13.2.1 Single-sample case General case Let us assume that x"" ... ,s. are independent. Further, each may arise from anyone of g possible sub-populations with p.dJ. f(x; e.), k = 1, ...• g, where we assume g is known. Th"
2115,unknown,"1, ...• g, where we assume g is known. These assumptions would be the same as for the standard discrimination problem (see Chapter II) if it were known from which sub-population each X I came. Let y = ('Y, ..... 'Y.Y be a set of identifying labels so that y, = k =?~I come from kth sub-population. i= 1, ... , n ; k = I, ... , g. Suppose that MULTlVARIATE ANALYSIS 362 C , = the set of J< , assigned "
2116,unknown,"C , = the set of J< , assigned to the kth group by 'Y, k = L .... g. The likelihood function is L('Y;D, ..... 0.)= TI [(x:D,,""· TI [(x: 8,). (\3.2.1) s EC 1 I:CC . We can ,how that the maximum likelihood me thod possesses an important a\location propertv. Let .y. 9, ..... 0. be the m .l.e.s of 'Y and the Os respectively. Let C"" .... C, be the partition under .y. Since moving a ample point from C, "
2117,unknown,"moving a ample point from C, to C, will reduce the likelihood, we have L(.y; 9, ..... 9, ){(,,: 9, )j((x: 9,,)""';; L(.y: 9"" ... , 9,). Thus It-k. 1 = I. .... g. This is qu ite a familiar allocation rule per se (see Chapter II). The norlllal case ( 13.2.2) Let us assume that {(x; 8,) denotes the p.d.L of .("".,,1:.), k = I, .... g. Then the log likelihood function is 1('Y : 8)=const - ~ t L (X ,- "","
2118,unknown,"(13.2.3) w here there aIe n ~ oh ... ervations in C~. Hence. for a given 'VI the likelihood is maximized by the ordinary m.l.e. of ,. and 1:. i.e. (13.2.4) where i, is the mean and S, is the covariance matrix o( the "". observa­ tions in C •. On substituting (\3.2.4) in (13.2.3), we ohtain 1(,,(: 9(""())= const-t L II, log Is""l· H ence the m.l.e. o( 'Y is the grouping that minimizes (J 3.2.5) To avo"
2119,unknown,"(J 3.2.5) To avoid the degenerate case of infinite likelihood, we assume that there are at least p T J observations assigned to each group so that Il, ~ P + \ and l1 :;.g(p+l). If, however, we assume that the group have identical covarianc"" matrices. 1:, = . ,. = 1:, = 1: (unknown) , the same method leads to the 363 CLU~R A AL -VSIS grouping that minimlzes IW I, (13.2.6) where W = f L (x, - i, )(x"
2120,unknown,"grouping that minimlzes IW I, (13.2.6) where W = f L (x, - i, )(x. - i,Y. ft-I c. is tbe pooled within-groups sums of squares and products (SSP) matrix. and the second summation is taken over X; E C,. Example 13.2.1 In the iris data, suppose that we we re given the table unclassified according to varieties. For our illustrative purpose, let us aSSume (rom Table J .2.2 that a sub-sample was given a"
2121,unknown,"aSSume (rom Table J .2.2 that a sub-sample was given as in Table 13.2.1. Tn Table \3.2.1, the first three observations are the first three readings lrom 1. oersicolour, whereas the last three observations are the first three r""adings from T. virsillica, both on the first two variables only. To satisfy 11 :;. 38. we shall take g = 2. with three ob 'ervations in each group. For all possible partitio"
2122,unknown,"possible partitions. Table 13.2.2 gives the values of IS ,I, 15,1, and IW I = is, +S 21· Both ML methods lead to the clusters (1,3,6) and (2, 4, 5). ote that under tbe correct grouping (I, 2, 3) and (4, 5, 6), the values of ,1=0 .0012. 21= 0.1323 are disparate. From a scatter diagram of these points given in Figure 13.2 .1 we can understand the ML grouping better. T""bl. 13.2.1 A sub-sample of iris"
2123,unknown,"T""bl. 13.2.1 A sub-sample of iris data 2 3 4 5 6 Sepal length 7.0 6.4 6.9 6.3 5.8 7 .1 Sepal width 3.2 3 .2 3.1 3.3 2 .7 3.0 Tabl. 13.2.2 Clusters (or the data in Table 13.2.1 Group 1 Group 2 IS,I IS,-+S,I 123 456 0.001200 0.132 300 0.1588 0.180 124 256 O.OOt 200 0.073 633 0.0884 0.127 125 346 0.030000 0.000133 0.0040 0.198 126 345 0.004800 0.070533 0.3386 0.167 134 256 0.002133 0.073633 0.1571 0."
2124,unknown,"135 246 0.001633 0.000833 0.00 14 0.188 136 245 0.000 300 0.004033 0.0012 0.017 145 236 0.073633 0.000300 0.0221 0.181 J46 235 0.005633 0.032033 0.1805 0.170 1 5 6 234 0.Q28033 0.000533 0.0150 0.114 MULTIVARIATE ANALYSIS 364 5.8 6.0 6.2 6 •• 6.6 6.8 1.0 7.2 ~.~ •• ~.2 . 2 ., 3.1 . 3 ~. o . 6 2.9 2.B ~r Figure 13.2.1 Scallu diagram for th. rris data in Tabl. 13.2.1. It is known for this data (see E"
2125,unknown,"It is known for this data (see Example 11.3.2) that there is considerable overlap between the varieties I. uersicolollr and I. virginica, but these are clearly separated from 1. setosa. A lest for a single cluster Before starting clustering, we may wish to look into wh ether the data is unstructured, that is, whether there is just one cluster. Hen ce, the hypothesis of interest is H o:y,= ... =1'."
2126,unknown,"H o:y,= ... =1'. against the alternative that not all the ""I, are equal. Assuming normality with equal covariance matrices. we get - 2 log A = n log {max (ITj/jW I)}, (13.2.7) , where • T = L (Ii - x)(x, - x)' i* 1 is the IOtal sum s of squares and products matrix. Thus if the number of groups g is unknown , we need to minimize IW I over all permissible partitions of 'Y (those partitions for wh ic"
2127,unknown,"partitions of 'Y (those partitions for wh ich g s;;n - p, so that rank (W) =p ). U nfortunately, even for large II, the distribution of (13.2.7) is 00\ known. 365 CLUSTER ANAL YSlS (H owever, some progress has been made in the case p = 1; see H artigan, 1978.) A more tractable test is given below in (13.2.9). It may also be worthwhile to check that the data is not spherically sym m etric. One meth"
2128,unknown,"sym m etric. One method would be to apply the test of uniformity on lhe directions y.ljIY,1i given in Chapter 15, where Y. = S-'I2(X, -x) is the Mahalanobis transformation of Section 1.5.2. The criterion of minimizing jW j was first put forward on an intuitive basis by Friedman and Ru bin (1967), whereas the maximum likelihood justification was given by S CO II and Sym ons (1971). An alternative p"
2129,unknown,"procedure based on minirnizing tr(W) was developed intuitively by Edwards and Cavalli-Sforza (J 965), and this idea can also be given a maxImum likelihood interpretation (see Exercise 13.2.1). Remarks (I) Let Yj = Ax l + b, wh ere A is a non-singular matrix. Then (13.2.5) and (13.2.6) can be written as • jA j-2. IlIS •. ,I ..... , '-1 respectively, where S •. , and W y denote S. and W for the tran"
2130,unknown,"variables } "" Hence. in particular, as far as m inimization of these criteria is concerned, the scale of the variables is immaterial. (2) The task of m inimizing (13.2.5) or (13.2.6) is formidable even for a com puter. For example , if g = 2, we need to look at 2""- ' combinations in general. [n practice, to circumv ent this problem, a relative minimum is used which has tbe property that any reassi"
2131,unknown,"used which has tbe property that any reassignment of One or two observations results in a larger value. although it may not be an absolute minimum. it is w orthwhile to exami ne individually the partitions in lhe neighbourhood of the (estimated) true split. (3) Estimalion of g W e bave assum ed g to be preassigned. In practice, if g is allowed to vary, tbe maximum likelihood methods will always pa"
2132,unknown,"partition the data into the maximum numb er of partitions allowed. Thus, g must be chosen by som e other method. For equal covariance matrices, Marriott (1971) bas suggested taking the correct number of groups to be the value of g for which g2 jWl (13.2.8) is mlrumum. Everitt (1974, pp. 59-60. 92) from his study of various criteria finds (13.2.8) to be the most useful. Th e m ost suitable algorith"
2133,unknown,"for implementing the o ptimization of the criterion (13.2.8) are those of Friedman and Rubin (1967) and McRa e (1971 ). (See also Hartigan, 1975.) A rule of thumb is to use g - (/l/2) 1/2. (4) Mixtures Th e problem of cluster analysis can also be studied using MULTIVARIATE ANALYStS 366 a mixture of p.d.f.s. II we assume that each "", has probability Pk of com ing from the klh population k = L ... ,"
2134,unknown,"frum • [(x) = L pJ(x; 1'-,. ~.), .-, where ft· ; I'- , ~) is the p.d.f. of Np(p., ~), L Po = l. For tbe maximum likelihood estimates of p., Il,. alld 1:, when k = 2, see Exercise 13.2.3. Once these estimates are known, we can regard each distribution as indicating a separate group, and individuals are then assigned by the Bayes allocation rule of Section 11.2.2, i.e. assign X ; to the kth distribu"
2135,unknown,"the kth distribution when M(x,: ii"" i ,)"", pd(,,;; ii"" l;.) for all 1# k. For this m odel it is possible to carry out asymptotic hypothesis tests. Let ,\ = L,IL .. where ,\ is the ratio of the iikelihood of g groups against that of g' groups (g < g'). From a Monte Carlo investigation, Wolfe (1971) recommends tbe approximation 2 ( , ') 2 - - n-l - p-~g 10gA-X I' n f = 2p(g' - g), (J 3.2.9) for the "
2136,unknown,"for the distribution of ,\ under tbe null hypothesis that there are g groups. No te that this mixture model is equivalent to the first model of this section with the additional assumption that'Y is an (unobservable) random va.riable whose components are the outcome.~ of n independen t multino­ mial trials. Scott and Symons (1971) give a Bayesian approach in which all the parameters are random vari"
2137,unknown,"(5) Criticisms These teChniques usually require large amounts of comp uter time, and cannot be recommended for use with large data sets. In man y applications the number of parameters to be estimated increases indefinitely with the sample size and therefore the e timate' are not ""consistent"" (Marriott. 1971). Further, even if g is fixed and the data consists of a sample from a mixture of unimodal "
2138,unknown,"consists of a sample from a mixture of unimodal distributions, the groups will be tbe truncated centres of these distributions. mixed with tails of other distributions. Thus even for 1:, = '"" = 1:. =~. W will not be a consistent estimate of .1:. Further. in this case the criterion (13.2.6) has a tendency to partition the sample into groups of about the same size eveD when the true clusters are of "
2139,unknown,"when the true clusters are of unequal sizes (Scott and Symons , 1971). When tbe mode s are near together and the distributions overlap consider­ ably, separation may be impossible even for very large sample sizes. For our discussion we have assumed x, ....• Y, to be independent random variables, but this may not be true in some cases, e.g. when 367 CLUSTER ANA LYSIS .x, .... ,x. are measurements o"
2140,unknown,"particular creature. 13.2.2 Multi-sample case For the multi-sample problem described in Section 13.1.2, we shall limit Our discussion to normal samples with equal covariance matrices. Each .-ample can be summarized by its mean i, and SSP matrix W ;. Then i. - N t.. n - '~) independently for i = I, .. _ , m , and a consistent esti-r p ...... u. • D1Jl te of 1: is given by l;= W /(n - m), ( 13.2.10)"
2141,unknown,"l;= W /(n - m), ( 13.2.10) where, in the notation of Chapter 12, W = W, + -.. + W m is the within­ samples SSP matrix and 1/ = "" "" ... + "" ... Assuming n to be large. we shall now suppose I is known . Supp ose these,"" samp les have com e from only g pOpu lations (g'"" m) , i.e. J.LI E (v "" ... ,v g ), g ~ m, Thus . there are only g distinct means out of 1'-"" ... , I'-m' Let fI be tbe set of identif"
2142,unknown,"of identifying labels, and let C, be tbe set of ii a~signed to the klh group hy fI, k = 1. .... g. Now. maximizing the likelihood implies minimizing • L L /I;(i,-v,rl:""'(i,-v,). (13.2.11 ) 10: - 1 ~eCL LeI so lhat X, is the weiahted mean vectur of the means in the kth group. From (13.2.11). vk '; x .. so that the m.l.e. of P is the grouping tbat minimizes (13.2.12) It is equivalent to maximizing ,"
2143,unknown,"(13.2.12) It is equivalent to maximizing , b!= L N.(x.-i)'1:-'(x.-i) (13.2.13) . ~ , since the sum 01 (13.2.12) and (13.2.13) is fixed. Note that.i is the mean vector of the pooled sample. Thu s we arrive at (13.2.13) as our criterion. MULTrvARIATE ANALYSIS 368 For computa tional purposes, the following forms of (13.2.12) and (13.2.13) are useful. Let D ' (- - )""'-'(- - ) i,= I ,-I , .. I i-I; , s"
2144,unknown,"so that D,} is tbe Mahalanob is distance between Xi and xr It can be seen that (13.2.12) and (13.2.13) can be written, respectively, as (see Exercise 13.2.4) (J 3.2.1 5) and b:= t N ;' L tljrt,B ., (13.2.16) Ie-I c.. wber e the second summa tion is extended over iu ii E c;.. If the matrix o[ Mahalanobis distances D'I is available, then (13.2.15) is computationally convenient. When 1; is unk now n,"
2145,unknown,"convenient. When 1; is unk now n, replace it by (13.2.10) in these criteria. Example 13.2.2 For the shrew data descrihed in Section 13.1.2. the Mahalanobis distances D ij are given in Table 13.2.3, w here 1; is estimated from (13.2.10). Here k = 10, and "",=144, "",= 16, ""3= 12, "".=7, "",=90. ""6= 25, 11,=6. "" 0=26, 11.=53, ""10=20. We assume a priori (say from Figure 12.5.2) that we are looking for tw"
2146,unknown,"two clusters C, and C, with 3 and 7 elements, respectively. We wish to Table 13.2.3 Mahalanobis distances D "" between I 0 island races of white­ toothed shrews (from De lany and Healy , 1966: Gower and Ross. 1969) Scilly Islands C hannel Islands France . 2 3 A ~ 6 7 8 9 10 r Tresco 0 2. 8ryher 1.61 0 5cilly 3. St Agnes t.97 2.02 0 Islands 4. St Martin's t.97 2.51 2.S8 0 5. 5t Mary's 1.40 t.70 1.35"
2147,unknown,5. 5t Mary's 1.40 t.70 1.352.21 0 r Snk 2.45 3.49 3.34 3.S3 3.19 0 Channel 7. Jersey 2.S3 3.94 3.64 2.89 3.01 3.00 0 Islands 8. Alderney 9.58 9.5910.05 S.78 9.30 9.74 9.23 0 9. Guernsey 7.79 7.82 8.43 7.0S 7.76 7.86 7.76 2.64 0 French {10. cap Gn. Nez 7.86 7.92 8.367.44 7.79 7.90 8.26 3.38 2.56 0 mainland 369 Ta ble 13.2.4 Va lue of clustering criterion w! in ascending order for the shrew data and
2148,unknown,"the shrew data and C. , C 2 is the comp lement of C 1 Partition C 1 (8,9, to) (7,8,9) (4,8,9) (3,8,9) (1. 5, 6) (1,3,9) (1,8. 10) w! 643 1997 2014 2457 2499 5404 5446 CLUSTE R ANALYS1S minimize w~ over 120 partitions. Using values of D "" in T able 13.2.3 with the I1j in (I3.2.15) on the computer (we give a brief summary in Table 13.2.4), it is seen that the clusters are (8)-(10) and (1)-(7). ote t"
2149,unknown,"13.2.4), it is seen that the clusters are (8)-(10) and (1)-(7). ote that the difference between Ihe two sm allest values of \V ~ is large. Thi. fact is seen in figure 12.5.2, wh ich is a projection onto two dime nsions, and Ihe analysis confirms this finding. We defer further discussion until Example 13.3.1. 13.3 Hierarchical Methods Th e clustering methods in Section 13.2 can be described as opti"
2150,unknown,"partitioltiltg tedl1liqlws since the clusters are formed by optimizing a clustering criterion. W e now consider hierarchical m etilOds in which the clusterings into g and g + 1 groups have the property that (i) they have g -1 identical groups and (ii) the rem aining single group in the g groups is divided into two in the g + 1 groups. By the very nature of these lechniques, once an Object is alloc"
2151,unknown,"group, it cannol be reallocated as g decreases, unlike the optimization techniques of Section 13.2. The end product of these teChniques is a tree diagram (""dendrogram"") (see Section 13.3.1). These techniques operate on a matrix 0 = (d'i) of distances between the points XI> ' .. ,x. rather than the points themselves. Possible choices for the distance matrix w ill be discussed in Section 13.4. We sh"
2152,unknown,"We shall only consider two im portant hierarchical methods to give the MULTIVARIATE A ALYSIS 370 flavour of tbe area. It should be empbasized lhat the methods are basically descriptive. 13.3.1 Nearest neigbbour single linkage cluster analysis Th is m ethod groups tbe po ints which are nearest to one another in the following way . First, order the 4n (n - 1) interpoint distances into ascend­ ing or"
2153,unknown,"ing order. (a) Let C"" ... , C"" be the starring clusters each con taining o ne point, namely :I, E C i' (b) Without any loss of generality, let d"""" = min"" so that x"" and x"" are nearest. Th en these two points are grouped into a cluster, so we have n -1 clusters, where Crt + 4 , is a new cluster. (c) Let d, ~, be the next smallest distance. If neither r, nor s, equals r2 or S2l the new n - 2 d uster"
2154,unknown,"or S2l the new n - 2 d uster are Crt + CS1 ' C,~ + C.s): plus the remain­ ing old clusters. If '2 = '1 and $1 -I-S2 the new n - 2 clusters are C,' + C,' + C ,"" plus the rem aining old clusters. (d) The process continues as in (c) through all !n(1I - \) distances. At the jth stage let d """" denote the ith smallest distance. Then the cluster containing r, is joined with the cluster containing Si' (No"
2155,unknown,"that if r, and Si are already in lhe same cluster. then 00 new groups are formed in this stage.) (e) The clustering process can be halted before all the clusters have been joined into one group by stopping when rhe inter-cluster distances are all greater thaD do, whe re do is an arbitrary threshold level. Let c:', ... , c: be the resulting clusters. These clusters have the property that if d~ (>d "
2156,unknown,"the property that if d~ (>d o) is a higher threshold. then two clusters C;, C . will be joined at the threshold d~ if at least one distance d"" (or a single link) exists between rand s with x, EC;, x, E C ., and do < d,s::S;; d ~ . This property has led the metho d to be called single linkage cluster analysis. Note that once links are established betw een objects, they cannot be broken. Exam ple 13"
2157,unknown,"cannot be broken. Exam ple 13.3.1 In the shrew data, let do= 3.0. We can order the distances as show n in column 2 ot Table 13.3.1. In the first step. (3,5) becomes a cluster. Note that at stages 4, 6, 7, and 8 no new clusters result. By the 16th step. we have clusters at the level d •. 7 = 3.00. (1,2, 3.4.5.6, ,), (8,9. 10). Of course, if we carryon further. we get a single cluster. 371 Table 13."
2158,unknown,"Table 13.3.1 Single linkage procedure Order Distances (ordered) 1 2 3 4 Sf 6t 7 8 9 !O 11 12 13 14 15 16 45 t 0 new clusters. d,,=1.35 d,,~1.40 d,,= 1.61 d"",= 1.70 d,,= 1.969 dlJ= 1.972 d23 =2 .02 d.,=2.21 d,.=2.45 d,. = 2.51 d •. ,o=2.S6 d .. ~2.64 d"" ~ 2.84 d,. = 2.88 d.,=2.89 d"" ~ 3 .00 d •• = 7.08 CLUSTER A ALY~tS Clusters (1), (2). (3. 5). (4). (6). (7), (8), (9). (10) (1,3,5). (2). (4). (6),"
2159,unknown,"(1), (2). (3. 5). (4). (6). (7), (8), (9). (10) (1,3,5). (2). (4). (6), (7). (8), (9). (10) (I. 2. 3. 5), (4). (6). (7), (8), (9), (10) (I. 2, 3, 5). (4). (6), (7). (8), (9), (J O)t (1. 2, 3. 4, 5). (6). (7), (8). (9), (10) (1,2,3,4.5), (6). (7). (8). (9), (IO)t (1.2.3,4.5), (6), (7), (8). (9), (10)t O. 2. 3. 4. 5). (6). (7). (8), (9). (10)t (1,2.3,4,5,6). (7). (8). (9), (10) (t. 2.3.4.5,6). (7). "
2160,unknown,"(1,2.3.4,5,6), (7), (8). (9,10) 0.2.3.4,5,6). (7), (8. 9, 10) (I. 2. 3. 4. 5,6,7). (8, 9,10) (I. 2, 3. 4. 5.6,7), (8, 9. 10)t (1.2.3.4,5.6.7), (8, 9. IO)t (1.2.3.4.5,6.7), (8, 9, IO)t (I. 2, 3. 4, 5,6,7,8,9,10) t M o re accurate \'alues of distancc.. .. to break the tie. At (/0 = 3.00, we infer that there are two species, one containing (l)-(7) and the otber (8HIO). In fact. the species containing"
2161,unknown,"Alderney (8), Guernsey (9), and Cap Gris e2. (J 0) is named Crocidura russcula and the other is named C. snaveolells. Some of these facts were only appreciated in 1958 (see Delany and H ealy, 1966 ). Algorirhm An alternative bu t equivalent description of the computa tion, involving only It - 1 steps, can be given as follows: (a) Assuming d,.,=mi n,., (di;)-.;do, let C;, ... , C,~ be the groups af"
2162,unknown,"joining the pair 1,2 to form ct. (b) D efine the new «n-I)X( n - l» distance matrix D * =( dm witb d~; ~ min (d'i' d2;) for j = 3, .... Il; dt = d'l for i, j = 3 ..... II. Find min (dt), i, j = 2, ... , II and then proceed as in (a). Note that this algorithm uses the recurrence relation (13.3.1) to define the distance between group k and the group (ij) formed by the fusion of groups i and j. MULTI"
2163,unknown,"fusion of groups i and j. MULTIVAJUATE ANALYSIS 372 in hierarchical methods, clustering is generally obtained through two types of algorithm: (a) agglomerarive-a successive pooling of subsets of the set of objects; (b) divisive-successive partitions of the set of objects. In the above discussion, we have used an agglom erative procedure, bur it should be noted that the single-linkage method can al"
2164,unknown,"by a divisive algorithm, and by an algorithm which helongs to neither category (Jardine, 1970). In general, a method should not he confused with its algorithm. Indeed. Lance and Williams (1967) have given a general agglome rative algorithm with which many of the common hierarchical methods can be described (see Exercise 13.3.1). The impor­ tance of a method lies in the fact that the generated clus"
2165,unknown,"tance of a method lies in the fact that the generated clusters have some desired property. Properties (1) Chaining The m .l.e. methods tend to lead to spherical or elliptical clusters each one around a nucleus. However , the above method leads to ""rod"" type elongated clusters partly because the links once made cannot be broken. Also. the clusters have no nuclei, thus leading to a chaining effect. "
2166,unknown,"leading to a chaining effect. (See Example 13.3.2.) Thus, it is expected that single linkage will not give satisfactory results if intermediates (as a result of random noise) are present between clusters. (2) MOl1orol1icity It can be seen tbat the single linkage me thod gives clustering of identical topology for any monotonic transform ation of d"". (3) Ties It is easily checked that it does not ma"
2167,unknown,"(3) Ties It is easily checked that it does not matter which choice is made if, at some stage in the algorithm. there is a tie for the smallest distance hetween two clusters. However, other hierarchical methods are not always well defined in tbis situation. (4) Other properties Jardine and Sibson (1971) show that this is the only method consistent with a particular set of axioms for hierarchical cl"
2168,unknown,"clustering. In particular, it optimizes ""connected sets of points"". Hartigan (1973) uses this method in an attempt to minimize the number of mutations in a reconstruction model of evolutionary history. Gower and Ro ss (1969) have shown the relation of this method to the minimum spanning tree, and a compu ter algorithm is given by R oss (1969). Dendrogram A dendrogram is a tree diagram in which the"
2169,unknown,"represents the ""objects"" while the lower y axis represents distances. The branching of the tree gives the order of the /I - 1 links; the first fork represents the first link. the second fork the second link. and so on until all join together at the trunk. We illustrate this definition by an example. EXlIlIIpie 13.3.2 For the data in Example 13.3.1. a dendrogram {rom 373 CLUSTER ANALYSIS Table 13.3"
2170,unknown,"Table 13.3.1 is given in Figure 13.3.1. The first link joins (3, 5), etc. The level of the horizontal lines shows the order in which the links were formed. We have arranged the order of the objects along the x axis as arising sequentially in clustering in Table 13.3.1 so that links do not ··overlap'·. The dendrogram indicates that the distances between animals from the SciUy Isles are very smaU , "
2171,unknown,"from the SciUy Isles are very smaU , suggesting origin from a common stock. The distances between shrews from Sark (6) and Jersey (7) is greater than between any pair of Scilly Island populations. The three populations of Crocidura russcula (8,9. 10) do not form as close a cluster as those of the Scilly Islands_ The value of the threshold (do) is arbitrary, and no probabilistic work bas been done "
2172,unknown,"bas been done here. For the complete dendrogram take do= oo. Obviously (or a given do, the clusters can be read off from a dendrogram. e.g. for do=3.50 we obtain from Figure 13.3.1 (1,2,3,4,5,6,7) and (8, 9. 10). I! we know g in advance. again the dendrogram gives the clusters. Broadly speaking, do and g are functionally related. 4 6 7 8 9 10 I 2 f-- ~ 31- - 5 6 7 Figure 13.3.1 Dendrogram for shre"
2173,unknown,"- 5 6 7 Figure 13.3.1 Dendrogram for shrew data (single linkage). MULTIVARIATE ANALYSIS 374 T he dendrogram can be used to construct a new distance matrix between the objects. For two objects i and j. let u'; be the smallest threshold do for which i and j lie in the same cluster. The horizontal lines in the dendrogram are called nodes. Then ',<, can be found from the dendrogram as the smallest nod"
2174,unknown,"dendrogram as the smallest node which is linked to both i and j. Fo r the single linkage dendrogram (although not in general for other dendro­ grams). the II"" satisfy the ulrramerric illcqllality fOr all i. j, k (13.3.2) (see Exercises 13.3.4 and 13.3.5). 13.3.2 Compl ete linkage (furthest neighbour) method The complete linkage method i similar to the single linkage method except that the distance"
2175,unknown,"except that the distance he tween two clusters is now defined as the largest distance between pairs of elements in each cluster. rather than tbe smallest. The method can be described by the following algorithm. (a) Stan with clusters C, ..... C . containing I "" ...• I ., respectively. (b) Assuming d.2 = min (d'l) over all i and j, let C: ....• C: be the groups after joining the pair 1.2 to form C;"
2176,unknown,"(c) Define a new «11 - 1) X (n - J)) distance matrix D* = (d~) with d!j= max(d ,/. d,,) for j= 3 ... "" n : d!= d"" for i,j=3 , .... n. Find min d~, i. j = 2, ... ,11; then proceed as in (b). Continue until all the distances between clusters are greater than d"", where do is an arbitrary threshold value. ote that when the clustering is completed we will have max I.I.e cI,,"""" do for each cluster C. Th"
2177,unknown,"method tends to produce compact clusters with no chaining effect. As in single-linkage. it can be seen that computatio n can be carried out using a recurrence relation for the distance between group k and a group (ij) formed by the fusion of groups i and j; namely, (13.3.3) Mo st of the comments made for the single linkage me thod apply to this method except that its optimization properties are no"
2178,unknown,"the final groups in complete linkage do possess the property that all within-group distance are less than the threshold value. but the converse is not true. i.e. the me thod is not guaranteed to find all such groups. Like 375 5 6 8 9 10 CLU STER ANALYSIS Figure 13.3.2 Dendrog ram for the shrew data (complete linkage). single linkage, the method has the invariance property under mon otone transform"
2179,unknown,"transformations of the d,,. However, the complete linkage method may not be well defined if at some stage in the algorithm there is a tie for the smallesl distance (unlike single linkage). E xample 13.3.3 For the shrew data, the smallest distance is dJ.5 = \.35, so that these are clustered, Although d 1.5 = 1.40 we cannot cluster these next because d1.3=1.98. The next cluster is (1,2) since d1.2 ="
2180,unknown,"Following this procedure, we find the dendrogram as given in Figure 13.3.2. ote that there is no cbaining as in the single linkage case (Figure 13 .3.1). At do = 3.38, we have groups (1,2,3,4, 5), (6,7), (8.9. 10) but at each stage the groups are tlo/ chained as seen in Figure 13.3.2. 13.4 Distances and Similarities 13A.1 Distances Definition L e/ P and Q be two poill/s wllere these may represent "
2181,unknown,"me ,lls X and y 011 two objects. A real-valued tUltc/iolt d(P, 0 ) is a distance MUl TIVARlATE ANAL YSlS function if ir has the fol/owillg properries: (1) symmetry. d (P, 0) = d (0, P): (II) non -negativity, d(P, Q);;.O; (III) identification ma rk, d(P, P )= 0, For many distance fUllctiolls the following properries also hold.: (IV) definiteness, d(P, Q) =0 if and only if P= Q: (V) triangle inequal"
2182,unknown,"If (I)-(V) hold d is called a metric, 376 For some purposes, it is sufficient to consider distance functions satisfy­ ing on ly (I}-(III), but we will on ly consider distances for which (l)-(V) are satisfied unless otherwise mentioned . Note that (I) need not always be true: in sociology perhaps, subject p's opinion of subject Q mea$ured by d(P, Q) may differ from subject Q 's opinion of subject P"
2183,unknown,"Further, (V) is not satisfied for some distances (see Exercise 13.4.6). On the other hand, the distance may satisfy some stronger condition than (V), such as the ultrametric inequality (13.3,2). One would expect d (P, Q) to increase as ""dissimilarity"" or ""di­ vergence"" between P and Q increases, Thus d(P. Q) is also described as a coefficient of dissimilarity even when it does not satisfy the metr"
2184,unknown,"properties (IV) and (V). Quantirarive data Euclidean disrance Let X be an (/I x p) data matrix with rows x;, ... , s~. Then the Euclidean distance between the points x, and X i is di"" where ~= f (x .. - X;,)2= I i -S, 112 , (13.4.1) k-' This distance function satisfies properties (I)-(V). It also satisfies the foUowing properties: (a) Positive semi-definite property Let A = (-1d~). Then HAD is p,s"
2185,unknown,"p,s,d., where B = I-n-'11' is the centring matrix. (For a proof of this result, see Section 14.2. In fact, Theorem 14.2,1 also gives the converse result.) This property will be of importance when we examine similarity coefficients. (b) di; is invariant under orthogonal transformations of the :IS. (c) We have the cosine law ( 13.4.2) 377 CLUSTER ANALYStS where b,; = (x, - i)'(s; -xl is tb. centred "
2186,unknown,"377 CLUSTER ANALYStS where b,; = (x, - i)'(s; -xl is tb. centred inner product between X i and X i' Another useful identity for calculation purposes is given by . "" . L L d~ =2n L biO, (13.4.3) 1-1,--1 I_ I Karl Pearson distance When tbe variables are not commensurable, it is desirable to standardize (13.4.1); that is. we can use d 2 _ t-(x,. -xr.f . 4,-~ ., k ... l Sk (13.4.4) where s~ is the var"
2187,unknown,"k ... l Sk (13.4.4) where s~ is the variance of the kth variable. We shall call such a standardized distance the ""Karl Pearson distance"" and denote it by K 2, This distance is then invariant under changes of scale. Another way to scale is to replace s. by the range R. = max I"""" -x"" I. (13.4,5) '"" For cluster analysis, one usually uses (13.4.4) except when the difference in scale between two variab"
2188,unknown,"in scale between two variables is intrinsic, when one uses Euclidean distance, Mallalanobis distance We define squared Mahalanobis distance between points X i and X j as (13.4.6) its variants and properties have already been discussed in Sections 13.2.2 and J ,6. Qualitative da ra (multinomial populations) Consider a classification of individuals into p categories. For each r = 1, ... , g, let (x,"
2189,unknown,"1, ... , g, let (x,j .. "", x.,,) = x; denote the observed proportions from a population of size II, lying in each of these categories. For examp le, x, might denote tbe proportions of people having blood group types AI' A"" B, and 0 in each of g countries. Of course, ,=I. ... ,g. We shall now consider several choices of a distance measure between these populations, Euclidean distance The points s. "
2190,unknown,"these populations, Euclidean distance The points s. Ue on the hyperplane x, + ... + xp = 1 in the positive orthant of RP, An obvious measure of distance between \tULTIVARJATE A . ALYSlS these points is Euclidean distance, given by d~= f (x,,- x,.)'. ,- I 378 This distance may be suitable if the proportions are merely measured quantitie for which nO model of stochastic ""ariation is being considered"
2191,unknown,"However. if the proportions are thought of as random vectors, then a Mahalanobis-like distance is more appropriate. A Mahalanobis-like distance Suppose that for each r = 1. .... g. x, represents the proportions based on a sample II, from a multinomial distribution with parame ter a =(a"" .... IIp)' (the same parameter for each r). Then x, has mean a and covariance matrix:1;, = II, 'X , where X = (<"
2192,unknown,"given by <T . = {"",( I - a,), "" - arilr• i=/. i"" j: (13.4.7) that is. 'l: = diag (a, - aa'. Since x, lies on a hyperplane. X is singular. However. it is casily checked that a g-inverse of X is given by I = diag(a,·, .... a.'). (13.4.8) Thus (see Example 11.2.5), we can define a (generalized) Mahalannhi. distance between x. and XJ as 1I.11 ~/( 1I. + uJ ) limes f (x"" -x""o)'_ (13.4.9) j .. l ar Remar"
2193,unknown,"j .. l ar Remarks (I) Unfortun ately, there are two problems with thIS approach. First. in practice we wish to compare mul tinomia l populations with diffe,em parameters (and hence different covariance matrices). Thus (13.4.9) can on ly be viewed as an ""approximate"" Mahalanobis distance and a must be thought o[ as an ""average"" parameter for the populations. To reduce the effect of the differences "
2194,unknown,"To reduce the effect of the differences between pop ulations on sample size, we shaU drop the factor 11,11,/(11, + II,} given before (t 3.4.9). (2) The second problem involves the estimate of the ""average"" parameter a. A common procedure is to estimate a pairwise using Ii, (r, s) = i(x"" + xu) (so a("" s) depends on r and .~) . Then (13.4.9) becomes the Mahalanobis­ like distance (13.4.10) 379 CLUST"
2195,unknown,"379 CLUSTER A AL YSfS This (orm was suggested by Mahalanobis (see Bhattacharyya, 1946) and rediscovered by Sanghvi (1953). (3) O ther possibilities for estimating II include a global average of all the proportions (ii, = g-'(x"" + .' . + x,;)) and/or weighting each propor­ tion x"" by its sample size n, (see Balakrishnan and Sang hvi, 1968). (4) The distance in (13.4.10) is obtained by using a poole"
2196,unknown,"the parameter a. An alternative procedure is to use a pooled estimate of X . based on the sample covariance matrices thus giving yet another Mahalanobis-like distance. See Exercise 13.4.1 and Balakrishnan and Sanghvi (1968). BlwlIacharyya dislallce Let v, = (x::', ... , X::'2)'. , = 1, ... , g, so that the vectors v, are points on the unit sphere in R ' witb centre at the origin. The cosine of the"
2197,unknown,"cos B r, = f L""n V,l =- t (X""xs1 )lf2 , (13.4.11 ) .- 1 I-I so that the angle B,s is ahe great Circle distance between "", and v ~. The Euclidean distance of the chord between v, and v, is given by D' = (!. (X'I2_X '''')' 2 :n- L n .,)""1· (13.4.12) We shall call D 2 •n the BharraciTaryya disrallce (although sometimes the angle Bn is given this name ). The two measures are connected by D t"" - 4 sin "
2198,unknown,"D t"" - 4 sin 2 ~B"".. Remarks (I) If .:, and x. cOme from multinomial populations with the same parameter a. then D~ and 4D~ are asymptotically the same for large n"" 11,. See Exercise 13.4.2. (2) Bhattacharyya distance can be interpreted as an asymptotic Mahalanobis distance. From Example 2.9.1, we see that v,­ N.(b,(4I1,)-'X ) (or large 11"" where b,=a,''', i= l, ... ,p, and X = J- bb'. Although X "
2199,unknown,"Although X is singular, it is easy to see that a g-inverse 01 X is given by X - = I. Thus il v, and v, are proportions from multinomial distributions with the same parameter. the asymptotic squared Mahalanobis distance between them is given hy 411,11,(11, + n,)D i."". (3) Note that with both D~ and D~, differences between very small proportions are given m ore weight than differences between interm"
2200,unknown,"Or large proportions. However. with E uclidean distance all such differ­ ences are weighted equally. MULTIVARIATE ANALYSIS 380 (4) Practical studies (Sangbvi and Balakrishnan, 1972) have shown that D~ and D i are very similar to one another and in practice it hardly maners which one is cbosen. Bhattacharyya distance is perhaps preferable because there is no need to estimate unknown parameters and "
2201,unknown,"has a simple geometric interpretation. Ex ample 13.4.1 Table 13.4.1 gives the relative gene frequencies for the blood-group systems with types A"" A z, S, and 0 for large samples from four human populations: (1) Eskimo , (2) Bantu , (3) English, and (4) Korean . The Object is to assess the affinities between the populations. The Bhanacharyya distance matrix is found to be Eskimo Bantu English KOrea"
2202,unknown,"Eskimo 0 23 .26 16.34 16.87 Bantu 0 9_85 20.43 Englisb 0 19.60 Korean 0 Use of Ihe complete linkage clustering method suggests the two clusters Bantu-English and Eskim<>-Korean. Cavalli-Sforza and Edwards (1967) came to this conclusion by a maximum likelihood method . However. the single linkage method , which one might think would be appropriate in this situation, does not support this conclusion"
2203,unknown,"this situation, does not support this conclusion. Multi-classification case H there are t c1assificatiuns instead of a single one, e.g. if there are gene frequencies for each of tbe five blood-group systems of classification. then we can sum the distances using Table 13.4.1 Relative frequencies of hlood groups AI> Az l R. 0 for four populations (Cavalli-Sforza and Edwards . 1967 ) Populations Sloo"
2204,unknown,"Populations Slood groups Eskimo Bantu English Korean A, 0.2914 0.1034 0.2090 0.2208 A, 0.0000 0.0866 0.0696 0.0000 S 0.0316 0.1200 0.0612 0.2069 0 0.6770 0.6900 0.6602 0.5723 381 CLUSTER ANALYSIS Table 13.4.2 Distances { • },a 1. Euclidean distance: I w.< ... - .... )' . .-, <al Unstandardized, w, ~ 1. (b) Standardized by s.d .. w. ~ 11s~ (Karl Pearson distancel. (c) Standardized by range w, ~ 11 "
2205,unknown,"(c) Standardized by range w, ~ 11 R ~. 2. Mahalanobis distance: HI, - I,)'l:-'(I, - I,n ,,, 'I = any transforming positive definite matrix. 3. City-block metric (Manhattan metric): f w, Ix.. -x.,I· ,-, Mean character difference W k = lJp 4. Minkowski metric: {t w .. lx.. _.xt:kI ... }IJ ~. ~ ~ L ,-, . f lx., -.... 1 5. C.anberra metnc: ~ ( ) . .,_1 x,. .. + x,. (ScaJing does nnl depend on whole ra"
2206,unknown,"6_ Shallacharyya distance (propo rtion,): {f (x:"" _ y:,,),}'f2 ,-, 7. Distances between groups: (a) Karl Pearson dissimilarity coefficient: {.!. f (X"" - x.,)' }'"" p ... 1 (s~n, )+(s~All n,) , where "", = size of jth sample j = '. s; i; .. , slit = rnean and variance of kth variable for the jtb sample, (b) ManaJonob is distance: Hi, - i.)'t -'(i. -x,)}'''- where D l, .. f = 1,2, is either the Mahala"
2207,unknown,"where D l, .. f = 1,2, is either the Mahalanobis distance or the Bhat­ tacharyya distance hetween two populations on the ktn classification. The process of summing up distances in this way is meaningful when the t types of classification are independent. A lis 1 of various distances is given in Table 13.4.2. 13.4.2 Similarity coefficients So far, we have concentrated on measures of distance or dis"
2208,unknown,"there are situations as in taxonomy where it is often common to use measures of similarity between points A and B. MULTIVARIATE ANALYS IS 382 De6nitioo A reasonable measure of similarity, ,(A , 8), sllOuld Itave tlte [ollowing properties: (i) s{A, 8 ) = siB, A ), (ii) s(A. 8 ) > 0, (iii) s{A, 8 ) increases as tlte similarity between A and 8 increases. Be cause greater similarity means less dissimi"
2209,unknown,"Be cause greater similarity means less dissimilarity, similarity coefficients can be used in any of the hierarchical techniques of Section 13 .3 simp ly by changing the signs of all the illequalities. W e now consider som e examples . Qu alitQtive variables Let the presence or absence of p attributes on two objects P and Q be denoted (x "" ... , xpl and (Y t> .... Yp ), wh ere X,= 1 or 0 depending "
2210,unknown,"wbether the ith attribute is present or absent for object P. Set II = LX,)'"" b = L (l-x,)y"" C = L x,{I-y,), d = L (I-x,)(l-y,): (13.4.13) that is, a, b, c, dare tbe frequencies of (x,. y,) = (1, 1), (0. 1), (1. 0). and (0,0), respectively. Th e simplest measur e o[ similarity between P and Q is a "",(P, Q) = - . p (13.4.14) Ar. alternative is the simple matching coefficielll (Sokal and Michener. 19"
2211,unknown,"1958 ) defined as which satisfies S2(P, P) = 1. s,{P, Q )= (a + d). p (13.4.15) It is not clear in practice whether to use 5 "" S2' or some other association coefficient. In S 2 all matched pairs of variables are equally weigbted, whereas in S, negative matches are excluded. In both s, and S2 every attribute is given equal w eight, but in some applications it might be preferable to use a differenti"
2212,unknown,"preferable to use a differential weighting of attributes. For a discussion of these problems, see Jardine and Sibson (1971. C hapter 4) and Eve ritt (1974. pp. 49-50). 383 C LUSfE R ANAL Y SIS Let X be the data matrix containing the presence/absence information on p attributes for n Objects. The m atrix X is called an incidence matrix since x"" = 1 or O. In view of (13.4.13), the matrix or similari"
2213,unknown,"(13.4.14) is simply (XX ') 8.=--, (13.4.16) p whereas. the ma trix ot similarities based on (13.4.15) is {XX'+(J- X )(J-xl1 8, . (13.4.17) P where J =1 1', s, =(s~:» . Note that the diagonal elemen ts of 82 are 1. Clearly, (13.4.16) and (13.4.17) are p.s.d. Example 13.4.2 Consider the (6 x 5) data matrix of Example 8.5.1 in which x"" = 1 if the ith grave contains tbe jth variety of pottery and 0 ot"
2214,unknown,"otherwise. The aim here is to see whicb graves have similar varieties of pottery to one another. It is found that 582 for A. ... , F is 5 0 3 5 3 0 5 2 0 4 2 3 2 5 3 3 5 0 3 5 1 3 1 4 1 1 5 3 3 2 3 3 3 5 We can now use any clustering method or Section 13.3 to see if there are any clusters. For exam ple, using single linkage. A and D are grouped together first (which is not surprising since they ar"
2215,unknown,"together first (which is not surprising since they are identical), and then B aDd E. The next link joins all the graves into a single group. Mixed variables U tbere are qualitative as well as quan titative variables, Gower (! 9710 ) has proposed the following similarity coefficient between ith and jth points: s?,' = 1 -.!. f w. Ix"" -xi.l, p 1-' (13.4.18) w here w. = 1 if k is qualitative, w, = l/R"
2216,unknown,"the range of the k th variable. MUL11VARlATE ANALYSIS 384 It can be shown that the matrix (s~) } is positive semi-definite, but if R .. is replaced by the sample standard deviation s"" this may not be so (see Exercise 13.4.3). In some applications the data consists of an (n x II) matrix D consisting of distances (or similarities) between the points, rather than an (n x p) data matrix X . In this si"
2217,unknown,"matrix X . In this situation, the cboice of distance has already been made and one can immediately apply any of the hierarchical techniques of Section l3.3. Examples of such data are quite common in the context of multidimensional scaling (see Chapter 14). 13.5 Otber Metbods and Comparative Approacb We have only considered a few approaches to cluster analysis in the previous sections. However, the"
2218,unknown,"previous sections. However, there is a wide range of different approaches none of which falls neatly into a single (ramework; for example, clumping techniques in wh ich the classes or clumps Cf .: overlap and mode analysis, in which one searches for natural groupings of data, hy assuming disjoint density surfaces in the sample distribution. For a good discussion of these methods, we rele, the read"
2219,unknown,"methods, we rele, the reader to Everitt (1974) and Jardine and Sibson (197 J). Other useful references are Cormack (I97l) and Sneath and Sokal (t973). For clustering algorithms, see Hartigan (1975). 13.S.1 Comparison of methods Optimization techniques usually require large amounts of computing time and consequently their use is limited to small data sets. For the various likelihood criteria mentio"
2220,unknown,"likelihood criteria mentioned, the underlying assumptions about the shape of the distributions are important (e.g. whether or not 1: is the same (or aU groups, and whether or not 1: is known). Also it i important to try several different starting configurations when carrying out the optimiza­ tion. Possible starting configurations include a partially optimized config­ uration arising from some sim"
2221,unknown,"based on a priori knowledge. Hierarchical techniques are more suitable for use in the analysis of biological or zoological data because for such data a hierarchical structure can safely be assumed to exist; e.g. in the shrew data where shrews can be gTouped ioto species and tbese species themselves can be grouped into genera, etc. These methods bave the considerable advantage in requiring far less"
2222,unknown,"far less comput ing time and therefore they can be used for larger data sets. The major difficulty with these techniques is making a choice of a 385 CLUSTE R ANALYSIS distance mea ure. Of various hierarchical techniques, single linkage is the only one to satisfy various analytical properties (see Jardine and Sibson, 1971 ; Fisher and van ess, 1971, 1973). As has been already mentioned. in practice"
2223,unknown,"As has been already mentioned. in practice single linkage may not provide useful solutions because of its sensitivity to the noise present between relatively distinct clusters and the subsequent chaining effect. On the other hand, the comp lete linkage method gives compact clusters, but it does not necessarily guarantee 10 find all groups where within-group distances are less than some value. T he"
2224,unknown,T he use of aDV hierarchical method entails a loss of information. For example the single linkage method effectively replaces the original dis­ tance matrix D by a new set of distances which satisfy the ultra-metric inequality. In general. hierarcnical techniques should be regarded as a useful descriptive method for an initial investigation of the data. For a fuller discussion. see Everitt (J974).
2225,unknown,"Spuriou., solurions All tbe cluster methods make implicit assumptions about the type of structure present. and when these assumptions are not satisfied spurious solutions are likely to be obtained; that is. each cluster­ ing method imposes a certain amount of structure on the data and it should be ensured that the conclusions we reach are not just an artefact of the method used. For example, most "
2226,unknown,"of the method used. For example, most of the optimization method are biased toward finding elliptical and spherical clusters. If the data contains clusters o( other shapes (say snake·like), these may not be found by such methods and consequentlY important information will have been lost, and in some cases a misleading solution will result. On the other nand, hierarchical methods impose hierarchy w"
2227,unknown,"Everitt (1974) considers examples to elucidate these points. 13.S.2 Comparative and Graphical Approach In general. several different techniques should be applied for clustering as the variety helps to prevent misleading solutions being accepted. These methods should be supported by graphical techniques; we have already used canonical variates for the shrew data for the multi-sample case and, simil"
2228,unknown,"similarly, principal coordinates could be used for the single samp le case. In some cases, when the only data is an inter-point distance matrix, multidimensional scaling techniques could be used, see Chapter 14. The harmonic curves given in Section 1.7.3 also give some idea of clu~tering. EIl1Dlple 13.5.1 Consider again the shrew data of Section 13.1.2. The differences between Ibe to races of shre"
2229,unknown,differences between Ibe to races of shrew are conveniently summarized MULTIVARIATE ANALYS IS 386 Table 13.5.1 Ca non ical means for shrew data C anonical va date Sample 2 3 4 5 6 7 8 9 I -2.33 -0.37 - 0.39 - 0.07 0.26 0.00 0.06 0.05 -0.0 1 2 -2.29 0.39 - 1.12 0.26 -0.49 -1.05 - 0.53 - 0.03 0.02 3 -2.77 1.06 0.34 0.46 - 1.01 0.79 - 0.54 0.28 -0.04 4 -1.37 0.53 - 0.85 - 1.37 0.59 0.85 - 0.75 - 0.52 
2230,unknown,"5 - 2.23 1.14 0.36 0.Q7 - 0.19 0.02 0.17 - 0.Q7 0.01 6 - 2.01 - 2.54 1.52 0.39 -0.31 - 0.16 -0.06 - 0.11 - 0.02 7 - 1.76 - 0.59 2.15 - 1.92 0.76 0.23 - 0.50 0.24 U.13 8 8.10 1.27 0.98 - 0.33 0.43 - 0.39 - 0.10 0.05 - 0.04 9 6.54 - 0.69 - 0.53 - 0.49 - 0.45 0.12 0.11 - 0.00 0 .01 10 5.95 - 0.05 - 0.19 2.01 0.52 0.30 - 0.14 - 0.03 0.03 in T able 13.5, I using the canonical m eans M of Example 12.5.2"
2231,unknown,"m ,; denotes the jth canoOlcal coordinate of the ith race of shrew. j = 1 .... , 9; i = 1, ... , 10. For convenience each canonical coordinate is centred to have weighted sample mean O. A plot 01 the 10 harm on ic curves is given in Figurc 13.5.1. Note that these curves are dominated by their lower Irequencies because the canonical means vary m ost in their first coordinate . Further. note that th"
2232,unknown,"the L 2 distance between the harm onic curves equals the Mahalsnobis distance between the corresponding races 01 shrew: hence the distances between the curves agree with T able 13.2.3. In particular, Ihe curves seem to form two ma in clusters, 1-7 and 8-10, in agreem ent with the single linkage cluster analysis 01 Ex ample 13.3.1. Exercises and Complements 13.2.1 (Scott and Sym ons, 197 1) Let us "
2233,unknown,"Exercises and Complements 13.2.1 (Scott and Sym ons, 197 1) Let us suppose I , . .... I "" to be a mixed sample from the N.{j.L.,:1:, l, k = 1, .... g, pop ulations w here the p , are unkn own . Ll 1:= ... = 1:, = 1: where :1: is known. show thaI the ML partition.y minimizes tr(W""l - '). Show that it is equivalent to m aximizing the we ighted between groups sum 01 squares < L n,(i, -i)'1:-'(x. - i)"
2234,unknown,"L n,(i, -i)'1:-'(x. - i). k-' where i. is the mea n for a cluster C o. 387 CLUSTE R ANAL VSIS Figure 13.5.1 Hannonic curves for tile shfew data. 13.2.2 Let us suppo se :t, . . ..• x"" to be a mixed samp le Irom the N.{j.L"" 1.,), k = 1. .... g, populations where p , and 1., are unkn ow n. Further, let y ••..... Y'rn. = I. .... g, be previous sampl es 01 indepen­ dent observations known to come Irom "
2235,unknown,"dent observations known to come Irom N.{j.L"" 1..), k = 1, .... g. respec­ tively. Show that the m .!.e.s of P k'l., and the partition y of the xs are given by (W k. +W "" + W • .,l. (m . + n.J MULTIVARLATE ANALYSIS and y minimizes where and W ."" = L (x, -i,Hx, - id ', e. W "" = L (y .. -j,)(y"" - j,)"" c. 13.2.3 (Day. 1969) Let x"" .... "". be drawn from the mixture {(x) = P.r,(S) + p,{,(,,). where {, i"
2236,unknown,"{(x) = P.r,(S) + p,{,(,,). where {, is the p.d.f. of p(p."" ::E), i = I. 2. and P, + p, = t. 388 (a) Show tbat the mean m and the covariance matrix V of the mixture are given by m = P,,,, + P2l'2, Show that the m.l.e.s of m and V are given by Let Show that m = i, V = S. i = V-'(ji., -ii2)/{ 1 - 11,11,(;;',-ji,)'V-'(ji..-"",J}. 6 = -ia'(ji., + JiJ+ log (P,/Pl)' (b) If P(k I ""i) is the probability tha"
2237,unknown,"component k then the m.l.e.s of P(k I Xi) are where P(I 1"",) = P, e,/{P1e"" + p,e,,}, P(21 "",) = 1 - P( I I "",), 389 CLUSTER A ALYSlS Hence show that the m .1. equations can be written as ~= f p(2lx4t P(1 ls), P I 1-1 ,-1 '"" = {~X;P(11 ""i) }/~ P(II "";)' .. , = { L x;f>(21 ""i)} / L P(21 ""i)' , , t = .!. L {(""i - ;"")(""; - ... )' p(11 ""j) + (""; -;',)(:1, - ... )' p(21 "",)}. It i Hence, deduce that Ph "
2238,unknown,"It i Hence, deduce that Ph .... and ... are SOme functions of the ""s and of Ii and 6, whereas V is a function of the ""s only. Hence conclude that (0) and ( .. ) form a set of equations of the form Ii = <1> .(8. 6; X, ' ...• "".), 6 = <1> ,(8,6; ""., .... "".) which can be solved numerically by iteration. 13.2.4 (a) Substituting i/-x,I( = ;1 L tI,(ij -ir) t,Ee.. in w; given by (13.2.12), and then, usi"
2239,unknown,"(ii -i,),X -I(i, -i,) = ~(Df, + D ;, - D~J. prove (13.2.15). j, r. SEC;. (b) Using i ,- ~ =N;' L 1I,(ii-i) i,£C, in (13.2.13), prove (13.2.16). For g = 2, use N,(x,- i)=-N,(x,-i) to show that 13.3.1 (Lance and Williams, 1967) Let D= (d,,) be a distance matrix. Show that the distance between group k and a group (ij) formed by the merger of groups i and j can be written d'(I,) = ad"" + ad., + y Id,,-"
2240,unknown,"d'(I,) = ad"" + ad., + y Id,,-d,)1 MULTIVARIATE ANALYSIS 390 391 CLUSTER A ALYSIS with observations into two groups and comment all the result. (i) a =1, Y = 4 for single linkage, (ii) a = t, y = t for complete linkage, (iii) a =i. y = 0 for a ""weighted average"". 13.3.2 Show that the squared Euclidean distance matrix (d~) for the iris data of Table 1.2.2 IS 0 0.36 0.02 0.50 1.69 0.05 0 0.26 0.02 0."
2241,unknown,"0 0.36 0.02 0.50 1.69 0.05 0 0.26 0.02 0.61 0.53 0 0.40 1.37 0.05 0 0.61 0.73 0 1.78 0 Draw the dendrograms obtained on applying single linkage and complete linkage clustering methods. Show that (i) at threshold 0.30, the single linkage method leads to clusters (1,3,6), (2,4), (5) and (ii) at threshold 0.75, the complete linkage method leads to clusters (1,2,3,4,6), (5). From the scatter diagram o"
2242,unknown,"From the scatter diagram of tbe points in Example 13.2.1, compare and contrast these results with the clusters 0.3.6) and (2,4,5) obtained in Example 13.2.1. 13.3.3 Suppose two bivariate normal populations AI> A, have means (~) and (~) and each .has dispersion matrix [1 OJ' o L' The following observations were drawn at random from A, and A 2 , the first six (rom A , and the last three from A 2 • ("
2243,unknown,"(1.14) LOI • ( 0.98) (0.49) -0.94' 1.02' (-0.31) (1.78) 0.76' 0.69' ( 0.40) -0.43 ' (2.42) (2.55) (2.97) 1.37 ' 1.06 ' 2.37' Using single linkage cluster analysis, and the Euclidean distance m atrix (unsquared distances) for these nine observations given below, divide the 2 3 4 5 6 7 8 9 1 1.96 0.65 1.47 0.72 1.62 1.33 1.41 2.28 2 2.02 2.13 1.82 0.77 2.72 2.54 3.86 3 0.84 1.33 1.45 1.96 2.06 2.82 "
2244,unknown,"4 2.09 1.39 2.80 2.88 3.65 5 1.78 0.93 0.85 2.06 6 2.71 2.62 3.80 7 0.34 1.14 8 1.38 13.3.4 (Ultrametric distances) If d is a distance which satisfies the u!trametric inequality diP, 0) """"max {d(P, R ), d(O. R )}, for all points P, 0 , and R, then d is called an ,,/rramerric distance. (a) For three points P, O. and R , let a=d(P, 0), b= d(P, R ), and c = d( 0, R ) and suppose a'"" b """" c. Show that"
2245,unknown,"c = d( 0, R ) and suppose a'"" b """" c. Show that b = c, i.e. the two larger distances are equal. (b) Using (a), show that d satisfies the triangle inequality. 13.3.5 Let D =(d;,) be a distance matrix and let U= (u;,) denote the distance matrix obtained {rom the corresponding single linkage dendro ­ gram ; that is, ""'/ is the smallest threshold do for which ob jects i and j lie in tbe same cluster. "
2246,unknown,"tbe same cluster. (a) Show that U is an ultrametric distance matrix; that is, for all i, j, k. (b) Show that D = U if and only if D is ultrametric. (e) Consider the matrix of distances between five objects: o 4 4 3 o 4 2 4 043 o 4 o Draw the dendrogram for the single linkage method. Verify that D = U and hence that D is ultra metric. ~fUl.nvARLATE ANALYSIS 392 13.4.1 (Balakrishnan and Sanghvi, 196"
2247,unknown,"tions based on II, observations for r = 1. ...• g. Let Define i = i, i*i· Why might (x, - x.)':t-'(x,-x.,) be a suitable m:asure of distance when ,t is suspected that the :I'd are not all equal. Is :I singular? 13.4.2 Suppose x and y are proportions based on samples of sizes II, and "" 2 from multinomial distributions with the same parameter B . Let /II = min (II"" 112)' Using (3.4.10) and (3.4.12),"
2248,unknown,"/II = min (II"" 112)' Using (3.4.10) and (3.4.12), show that the ratio between the Mahalanobis distance and the Bhaltacharyya distance is given by D! = 2 t (x:n+ y:n)' D, ,.J (x, + y,) Let b =(a:n •... , a!'2)'. Using the fact that x:12-b; and y:l2-b, have order 0.(m-1I2), show that Di/ D~-4 has order Op (m -'} and hence D~ is asymptotically equivalent to 4Di as 111 -+ """". (The notation, 0.0, order"
2249,unknown,"in probability. is defined in (2.9.1).) 13.4.3 (Gower. 1971 a) (a) For quantitative observations with values x"" ... , x., consider the similarity coefficient 5"" = 1- Itx, -.x/II R}, wbere R is the range. Show that we can assume R = I by rescaling, and I = x, """" X2 """" .,. """"Xn -] ,,""x. = 0 by permuting the rows of the data matrix and shifting the origin so that x. = O. Thus the similarity matrix SM"
2250,unknown,"1 l-(x,-x,) 1-(.x, -.x,) 1 1 -(.x2 - x,) I I-(x,-.x.) 1-(x2-x,,) I-(x,-x.) 1 By a series of elementary transformations, show that for I ... P'"" II, the principal P x P leading minor is p-' LI •• = 2P-'[1-4(.I,-.xp )] n (.I,-.x,.,) . . -, 393 CLUSTER ANAL VSlS Since x, -xp "" x, - x"" = 1 and x, - x,_] """" 0 show that LI .. """" 0 for all p. and therefore S. is a p.s.d. matrix. Extend this result to I q"
2251,unknown,"therefore S. is a p.s.d. matrix. Extend this result to I quantitative variates, noting that the average of I p.s.d. matrices is again p.s.d. Hence deduce that (5::') is p.s.d. (b) Suppose S! is defined by s~ = l-{lx, - x,I/T} . whe re T > R. Show that again LI"""" """" 0 and so S!, is p.s.d. (c) If R is replaced by tbe standard deviation s, then we may have S < R. S'll. -~(x ,- xp ) need not be positi"
2252,unknown,"S < R. S'll. -~(x ,- xp ) need not be positive and therefore LIp. need not be positive. Hence S"" obtained from s"" = 1 -lix, -x/lis} need not be p.s.d. 13.4.4 Obtain S , for the data in Example 13.4.2 and apply the single linkage method to obtain clusters. Why is it an unsatisfactory approach? 13.4.S (D. G. Kendall, 1971.) Let X be a (n x p) matrix of actual number of occurrence (or proportions) ca"
2253,unknown,"occurrence (or proportions) called an abundance matrix; that is, XiI.. is a non-negative integer representing the number of artefacts of type k found on individual i; or x,. is a fraction between 0 and 1 repre enting the fraction of artefacts of type k which are found on individual i (so It., x"" = 1). Define a similarity measure s"" = f min (li •. >j.). '_I If ~t _, .x •• = I, ,how that s,' = 1--2 "
2254,unknown,"If ~t _, .x •• = I, ,how that s,' = 1--2 1 fix,. -"",.1 .-, and that (5,;) need not be p.s.d. 13.4.6 Show tbat the measure of divergence between two p.d.f.s I, and '2 given by f (f, - '2) log (f.) d.x does not satisfy the triangular inequality. 13.4.7 Let s, - pu."" :I}, i = I, 2. Show that the squared Bhattacharyya distance between the points is a function of the squared Mahalanobis distance, D 2 ="
2255,unknown,"D 2 = (jJ.,-jJ.2P:-'(jJ., -jJ.z). 14 Multidimensional Scaling 14.1 Introduction M ultidimensional scaling (MDS) is concerned with the problem of COn­ structing a configuration of n points in Euclidean space using information abou t the distances between the n objects, The interpoint distances themselves may be subject to error. Note that this techniqlle differs from those described in earlier chap"
2256,unknown,"those described in earlier chapters in an important way, Previously. the data points were directly observable as n points in p-space, but here we can only observe a function of the data points, namely the ~II( II -1) distances. A simple test example is given in Table 14.1.1, where we are given the road distances (nOt the ""shortest distances"") between towns. and the aim is to construct a geographic"
2257,unknown,"is to construct a geographical map of England based on this information. Since these road distances equal the true distances sllbject to small perturbations, we expect that any sensible MDS method will produce a configuration which is ""close"" to the true map of these towns. H oweve r, the distances need not be based on Euclidean distances, and can represent many types of dissimilarities between ob"
2258,unknown,"can represent many types of dissimilarities between objects. Also in some cases, we start nol with dissim ilarities bu t with a set of similarities between objects. (We have already given a general account of different types of distances and similarities in Section 13.4.) An example of similarity between two Morse code signals couid be the percentage of peop le who think the Morse code sequences c"
2259,unknown,"peop le who think the Morse code sequences corresponding to the pair of characters are identical after hearing them in rapid succession. Table 14.1.2 gives such data for characters consisting of the 10 numbers in M orse code. Th ese ""similarities"" can then be used to plot the signals in two-dimensional space. 'Ihe purpose of this plot is to observe wh ich signals were ""like"", i.e. near. and which "
2260,unknown,"signals were ""like"", i.e. near. and which were ""unlike"", i.e. far from each other, and also to observe the general interrelationship between signals. 395 MULTIDIMENSIONAL SCALING Table 14.1.1 Road distances in m iles between 12 British towns t 1 2 3 4 5 6 7 8 9 10 II 12 2 3 244 21g 350 284 77 369 197 ]67 347 312 444 94 215 221 150 469 583 251 166 242 116 212 53 298 253 325 57 270 168 284 4 5 242 4"
2261,unknown,"270 168 284 4 5 242 463 441 236 279 598 598 257 269 72 170 340 359 164 277 6 245 l69 210 392 143 378 7 380 55 168 117 143 8 349 53] 264 514 9 190 91 173 La 273 lit 11 L2 256 t .I = Ahcrystwyth. 2 = Bn ghton, 3 = Carlisle, 4 = Do .... er. 5 = Exeter. 6 = Glasgow. 1 = Hull, 8= Inverness, 9=Lceds, 10 = London. 11 = Newcasde. 12 =No rwich. De finition An (11 X II) marrix D is called a disrallce marrix"
2262,unknown,"symmerric and drT=O, r7' s. Starting with a distance matrix D. the object of MDS is to find points P"" .... p. in k dimensions such that if d"" denotes the Euclidean distance between P, and Pp then 6 is ""similar"" in some sense to D. The points P, are unknown and usually the dimension k is also unknown . In practice Table 14.1.2 Percentage of times that the pair~ of Morse code signals for two numbers"
2263,unknown,"numbers were declared to be the same by 59S subjects (Rothkopf, 1957; the reference contains enlries for 26 leners 8S well) ] 2 3 4 5 6 7 8 9 o 84 62 16 6 12 12 20 37 57 52 2 89 59 23 8 14 25 25 28 18 3 86 38 27 33 l7 16 9 9 .j 89 56 34 24 13 7 7 5 90 30 18 10 5 5 6 86 65 22 8 18 7 85 65 31 15 8 88 58 39 9 91 79 a 94 MULTIVARIATE A AL YSIS 396"
2264,unknown,"86 65 22 8 18 7 85 65 31 15 8 88 58 39 9 91 79 a 94 MULTIVARIATE A AL YSIS 396 one usually limits the dimension to k =""),2, or 3 in order to facilitate the interpretation of the solution. Nature of the solution It is important to realize that the configuration produced by any MDS method is indeterminate w ith respect to transla­ tion, rotation, and reflection. 1n the two-dimensional case of road d"
2265,unknown,"tion, rotation, and reflection. 1n the two-dimensional case of road dis­ tances (Table 14.1.1), the whole configuration of points can be ""shifted"" from one place in the plane to another and the whole configuration can be UrotatedH or ""reflected"". In general, if P I!""') p"" with coordinates J: ~ = (-G"" .. "" x,p), i = t, ... , II, represents an MDS solution in p dimensions, then y, = Ax , +b, i= i, ."
2266,unknown,"is also a solution, where A is an orthogonal matrix and b is any vector. Types of solwion M ethods of solution using only tbe. rank order of the distances (14.1.1) where (r"" s,), ... , (rM' SM ) denotes all pairs of suhscripts of rand s, r< s, are termed 1I01l-metric methods of multidimensional scaling. The rank orders are invariant under mon otone increasing transforma­ tions f of the d~, i.e. d"""
2267,unknown,"tions f of the d~, i.e. d""., < d"""" < ... ~ f(d"".,) < f(d"",,) < .... Therefore the configurations whicb arise from non-metric scaling are indeterminate not only witb respect to translation, rotation, and reflec­ tion, but also with respect to uniform expansion or contraction. Solutions which try to obtain P, directly from the given distances are called metric methods. These methods derive P, such t"
2268,unknown,"called metric methods. These methods derive P, such that, in some sense, the new distances dn between points P, and P, are as close to the original dn as possible. In general the purpose of MDS is to provide a ""picture"" whieh can be used to give a meaningful interpretation of the data. Hopefully, the picrure will convey useful information about the relationships between the objects. Note that this"
2269,unknown,the objects. Note that this chapter differs from most of the earlier chapters in that no probabilistic framework is set up ; the technique is purely data-analytic. One important use of MDS is seriation. The aim here is to order a set of Objects chronologically on the basis of dissimilarities or similarities between them. Suppose the points in the MDS configuration in k = 2 dimensions lie nearly on
2270,unknown,"dimensions lie nearly on a smooth curve. This property then suggests that the differences in the data are in fact one-dimensional and the ordering of 397 MULTIDIMENSIONAL SCAUNG the points along this curve can be used to seriate the data. (See D . G. Kendall, 1971.) 14.2 Classical Solution 14.2.1 Some tbeoretical results Definition A distance matrix 0 is called Euclidean if there exisls a configur"
2271,unknown,"configuration of poims in some Eue/idean space whose il1lerpoint distallces are given by D ; that is, if for some p, there exists points lC ..... , It, E R P such t!tat d ~ = (i:, - x,)'(i:, - x.). (14.2.1) The following theorem enables us to tell whether 0 is Euclidean, and , it so, how to find a couesponding configuration of points. First we need som e notation. For any distance matrix 0 , let A"
2272,unknown,"som e notation. For any distance matrix 0 , let A =(a,,), (14.2.2) and set B = HAH. (14.2.3) where H=1 - 1I-'II' is the ("" X n) centring matrix. Theorem 14.2.1 Let 0 be a distance matrix and define B by (14.2.3). Then 0 is Eue/idean if and only if B is p.s.d. In particular, the following results hold: (a) If D is tl,e matrix of Eue/idea"" interpoint distances for a configura­ tiott Z = (Zit ... ,z"""
2273,unknown,"tiott Z = (Zit ... ,z""),, then bn = (z, -iHz, - z), T,s=l •.... n. (14.2.4) bt matrix form (14.2.4) becom es B =(HZ )(HZ ), so B :;;.O. Note that B COI l be interpreted as the ""centred i,mer product matrix"" for tire configuration Z . (b) Conversely, if B is p.s.d. of rank p thell a configuration corresponding ro B can be constructed as follows. Let A I> ... > Ap denote the positive eigeltvalues of"
2274,unknown,"positive eigeltvalues of B with corresponding eigenvectors X = (X(1"" ... ,lI:(¥,) normalized by i = I, .... p. (14.2.5) The"" the points P, in RP with coordinates lC, = (.<"" , ... ,."""",r (so s, is the rth rOw of X ) ha ve interpoin, distances given by D . Further, this MULTIVARIATE ANALYSIS 398 configuration has cencre of graviry i = 0, and B represents the inner product matrix for this configurari"
2275,unknown,"Proof We first prove (a). Suppose We cao write where J = 11'. Now 1 -AJ= n where lilt ..•. a .... d;, = -2a"" = (z, -z,),(z, - z,). I -JA = "" a.I_"" ii."" a.I .,. a."" 1 -JAJ= ,,' "" (14.2.6) (14.2.7) Q "" ••. ii. a . .. a. ) . ii,. = - L a,,, '1 j,--1 1 "" a.~ =-L a'1-' n ._1 a 2 L ""n' (J 4.2.8) II 1 • .11 = 1 Thus b'5 = an - a,. -a., +ii ... (14.2.9) After substituting for an from (14.2.6) and using (1"
2276,unknown,"simplifies to b~ = (z, - z),(z,-z). (14.2.10) (See Exercise 14.2.1 for further details.) Thus (a) is proved. Conversely, to prove (b) suppose B ;;. 0 and consider the configuration given in the thorem. Let A =diag (A"" ...• Ap) and let r= XA - If2, so that the columns of r, 'Ym = A ;-\12""'0 are standardized eigenvectors of B. Then by the spectra) decomposition tbeorem (Remark 4 after Theorem A.6.4)"
2277,unknown,"B = rAI"" = XX' ; 399 MULTIDIMENS IONAL SCAliNG thal is. b, .. = x~x.n so B represents the inner product matrix for tbis configuration. We must nOW show that D represents the matrix of interpoin! distances for this configuration. Using (14.2.9) to write B in terms of A , we get ( x~ - x)'(s. - 1:,) = :'::1, - 2J:~J: .. ...L. J:~~ = b.r - 2b'5 + bn = alT - 2a,.!"" + au =-2an = d~ because a"" = -4d;=O "
2278,unknown,"= b.r - 2b'5 + bn = alT - 2a,.!"" + au =-2an = d~ because a"" = -4d;=O and -2an=d~. (14.2.)]) Finally, note that Bl = HABl = O. so that 1 is an eigenvector of B corresponding to the eigenvalue O. Thus 1 is orthogonal to the columns of X , xji)1 = 0, i = 1, ...• p. Hence . Ilj= L ~ ,= X'l=(~:""I ..... "",,,1),=0 .-, so that tbe centre of gravity of this configuration lies at the origin. • Remarks (1) T"
2279,unknown,"Remarks (1) The matrix X can be visualized in the folJowingway in terms of the eigenvectors of B and the corresponding points: Eigenvalues ~ A, .1.2 A. P, XII X I2· · ·X1p P2 X 21 X22' •• X 2p Points p. Xnl X 012 •• • ~p Vector notation. ),, (1~2)'"" .I(p) Centre of gravity: i,=O, ~= 0, ... , ip= 0, Vector notation x; ~ , x"" ) i=-L "". =0. n In short. the rth row of X contains the coordinales of the"
2280,unknown,"~ , x"" ) i=-L "". =0. n In short. the rth row of X contains the coordinales of the rth point, whereas the ith column of X contains the eigenvector corresponding to MULTIVARIATE ANALVS[S 400 (2) Geometrically, if B is the centred inner product matrix for a configuration Z , then b::Z equals the distance between z, and Z , and b,J(b""b .. )ll2 equals the cosine of the angle subtended at z berween z, a"
2281,unknown,"z,. (3) Ole tbat 1 is an eigenvector of B whe ther D is Euclidean or not. (4) The theorem does not hold if B has negative eigenvalues. The reason can be found in (14.2.5) because it is impossible to norm alize a vector to have a negative squared norm . (5) H istory This result was first proved by Schoenberg (1935) and Y o ung and Householder (1938). Its use as a basis for multidimensional scaling "
2282,unknown,"scaling was put forward by Torgerson (1958) and the ideas were substan­ tially ampWied by Gower (1966 ). 14.2.2 A practical algorithm Suppose we are given a distance matrix D which w e hope can approxi­ mate ly represent the interpoint distances of a configuration in a Euclidean space of low dimen sion k (usually k = l, 2, or 3). The matrix D m ayor may not be Euclidean; howeve r, even if Dis Euct"
2283,unknown,"may not be Euclidean; howeve r, even if Dis Euctidean, the dimension of the spa"",""-in which it can be represented will usually be too large to be of practical interest. One possible choice of configuration in k dimensions is suggested by Theorem 14.2.1. Cltoose the configuration in R ' whose coordinates are determined by tlte first k eigenvectors of B . if the first k eigenvalues of B are ""large"" "
2284,unknown,"are ""large"" and positive and the other eigenvalues are near 0 (positive or negative), then hopefully, tbe interpoint distances of this configuration will closely approxima te D . This configuration is called the classical solurion /0 the MDS problem in k dimensions. It is a metric solution and its optimal properties are discussed in Section 14.4. For compu tational purposes we sball sum­ marize th"
2285,unknown,"marize the calculations involved: (a) From D construct the m atrix A = ( -!d~). (b) Obtain the matrix B with elem ents b"" = an - ii,. - ii., + ii ... (c) Find the k largest eigenvalues AI> ... > A. of B (k chosen ahead of time), with corresponding ~igenvectors X = ('/(1) • ... ,.x,.») which are normalized by ""<.,xH) = A"" i = I, ... , k. (We are supposing here that the first k eigenValues are all p"
2286,unknown,"(d) The required coordinates of the points P, are s, = (x,."" ... , x .. )"" r= 1. ... , k, the rows of X . Example 14.2.1 To illustrate the algorithm , consider a (7 x 1) distance 401 MULTIDIMENSIONAL SCAUNG matrix 0 I ../3 2 ./3 1 I 0 .J5 2 ..J5 1 0 1 .J3 2 D = 0 1 .J5 0 1 0 1 0 Constructing the m atrix A from (14.2.2), it is found that r= 1, ... t 6. ii7 .= - ~; iL= -~. Hen ce from (l4.2.9) the m"
2287,unknown,"2 - 1 -2 -I 1 0 2 I - I -2 -1 0 2 ] -1 - 2 0 I B = - 2 2 1 - 1 0 2 1 0 2 0 0 The column s of B are linearly dependent. It can be seen that b(3)= b(2) - b(!), b •• ) = - b(l), ""<,, = - b(2), b(., = ben - b(2). b(7) = O. (14.2.12) Hen ce the rank of m atrix B is at the mo st 2. From the leading (2 x 2) matrix it is clear that the rank is 2. Thu s, a configuration exactly fitting the distance matrix "
2288,unknown,"the distance matrix can be constructed in k = 2 dimensions. The eigenvalues of B are found to be. .1.,=3, .1.2 = 3, .1.,='"" =.1.,=0 . The configuration can be constructed using any two orthogonal vectors MUL TIV ARIA TE ANALYSIS 402 tor the eigenspace corresponding to A = 3, such as x(1)=(a,a,O,-a,-a,O,O)t a=tJ3, ""~21 = (b, -b, -2b, -b. b. 2b, 0). b = ~. T hen the coord inates of the seven points "
2289,unknown,"ABC D E F G (O,4) GJ3, -!) (0,-1) (-0. -1) (-O,4) (0,1) (0,0). The centre of gravity of these points is of course (0,0), and it can be verified that the distance ma trix for these points is D. in fact, A to Fare vertices of a hexagon with each side of length 1, and the line Fe is the y axis. Its centre is G . (Indeed, D was constructed witb the help of these points.) A similar configuration based "
2290,unknown,"described in Exercise 14.2.7. 14.2.3 Similarities in some situations we start not with distances between n objects, but with similarities. Recall that an (n x n) matrix C is called a similarity matTix if Cn = c,r and if c"" ~ C,r for all r, . (14.2.13) Examples of possible similarity matrices were given in Section 13.4. To use the techniques of tbe preceding sections, it is nr.cessary to transform "
2291,unknown,"transform the similarities to distances. A useful transformation is the following. Definition Tile standard transformation from a similariry malrix C ro a distance marrix D is defined by (14.2.14) Note that if (14.2.13) holds, tben the quantity under tbe square root in (14.2.14) must be non-negative, and that d"" = O. Hence D is a distance matrix. Many of tbe similarity mat rices discussed in Secti"
2292,unknown,"matrix. Many of tbe similarity mat rices discussed in Section 13.5 were p.s.d. This property is attracrive because the resulting distance matrix, using tbe standard transformation, is Euclidean. Theorem 14.2.2 If C ;;.O, then the disrance matrix D defined by rhe standard transformation (14.2.14) is E .. c/ideall, wirh centred inner producr marrix B = BCll. Proof First note that since C ;;. O. 403 "
2293,unknown,"marrix B = BCll. Proof First note that since C ;;. O. 403 MULTI DIM E N SION A L SCAUNG where"" is a vector with 1 in the rth place and - 1 in tbe sth place, for rl' s. Thus, the standard transformation is well defined and D is a distance matrix. Let A and B be defined by (14.2.2) and (14.2.3). Since HCD is also p.s.d., it is sufficient to prove that B = nCB in order to conclude that D is Euclidean"
2294,unknown,"D is Euclidean with centred inner product matrix HCn. Now B = HAH can be written e1ementwise using (14.2.9). Sub tituting for a~ = -!d~. using (14.2.14) gives I • I "" --L (cl/-2c,.+c,,)+ , L (c,,-2c'l+c,,) n ,_I tI I""J- I = -2tn: +2c, +2c .s -2c ... Hen ce b,.~ =C,,,-c,. -c ... +c . or, in matrix form , B = nCR . Thus the theorem is proved. • Example 14.2.2 W e now consider the Morse code data giv"
2295,unknown,"Example 14.2.2 W e now consider the Morse code data given in Table 14.1.2 and described in Section 14.1. Th e data is presented as a similarity matrix C = (c,,). Using the standard transformation from similarities to distances, take da =(c"" + c"" - 2~)U2. We obtain the eigenvectors and eigenvalues of HCB in act'Ordance with Theorem 14.2.2. It is found lbat A,=187.4. A2 = 121.0, A,=9S .4, .1..=55.4,"
2296,unknown,"A. = 31.5, A,=9.6, A.=4.S. A.= 0.0, ,1"" .=-4.1. Tbe first two eigenvectors appropriately normalized are (-4.2, -0.3, 3.7, 5.6, 5.4, 3.8, 0.9, -3.0, -6.2, -5.7). (- 3.2, -5.8, - 4.3, -0.6, 0.0, 4.0,5.5,3.6.0.6,0.2). However, the first two principal coordinates account for o nly 100(.1., + A:JflIA,1 percent = 56% of the total configuration. The points Pr are plotted in Figure 14.2.l. It can be seen "
2297,unknown,"plotted in Figure 14.2.l. It can be seen that the x, axis measures the increasing number of dots whereas the X , axis m easures the heterogeneity MULTIVARIATE ANALYSIS a. __ _ . 7 . --. 6 • 9. - 0--. '-----'-'·==--=--=----'-----+::---'----~-,i .. ,. :3 XI o ·4 .... _ i. ~ . Figure. 14.2.1 C lassical solurion for Morse code data in Tablt 14.1, I 404 of the signal. If we regard the OP,s as vectors. "
2298,unknown,"consecutive vector.; are about 45 0 except between the vectors from 0 and 9, and 4 and 5. Tbe small separation between these latter points might be expected because tbe cbange in just the last character may not make mucb impact on the untrained ear. Thus the configuration brings out the main features of the general interrelationship between these signalS. The points are roughly on a circle in the "
2299,unknown,"14.3 Duality Between &incipa] Coordinate Analysis aod Principal Component Analysis So far in this chapter we have treated tbe (/I X II) matrix 0 of distances between n objects as the starting point for our analysis. However, in many situations, we start with a data matrix X(IIX p) and must make "" a choice of distance function. 405 /IofULTIDIMENSIONAL SCALING Several possibilities for a distance fu"
2300,unknown,"Several possibilities for a distance function were discussed in Section 13.4. The simplest choice is, of course, Euclidean distance. In this case there is a close connecllon between the work in Section 14.2 and principal component analysis. Let X(n x p) be a data matrix and let A.;;;' ... ;;. Ap be the eigenvalues of nS = X'IIX, where S is the sample covariance matrix. For simplicity we shall supp"
2301,unknown,"shall suppose the eigenvalues are all non-zero and distinct. Then A"" .... Ap are also the non-zero eigenvalues of B = IIXX'H. Note that the rows of IIX are just the centred rows o( X, so that B represents the centred inner product matrix. b"" = (s, -i)'(s, -i). De1in.ition Let Viii be Ihe ill! eigenvector of B , normalized by .... ,V'"" = Ai' i = 1. ... , p. For fixed k U'"" k ""'p), the rOws of V 0 ="
2302,unknown,"V 0 = (vO )' . ..• v(o,) are called rhe principal coordinates of X in k dimen­ sions. Thus , from Theorem 14.2.1, if 0 is the Euclidean distance matrix beTWeen the TOWS of X . then the k-dimensional classical solution to the MDS problem is given by the principal coordinates of X in k dimensions. Principal coordinates are closely linked to principal components . as the foUowiog result show s. Theor"
2303,unknown,"foUowiog result show s. Theorem 14.3.1 TIle principal coordi.wles of X in k dimensions are give"" by tI.e cenlred scores of Iile "" objecls on Ille firsl k principal compo""e""rs. Proof Let 'Y(.) denote the ith principal component loading vector. stan­ dardized by ""«i)'Ym = 1, so that by the spectral decomposition theorem (Theorem A.6.4). X'IIX=rAr', where r = h(.) ... "" . 'YiP» and A= diag (A ••... ,"
2304,unknown,"X'IIX=rAr', where r = h(.) ... "" . 'YiP» and A= diag (A ••... ,Ap). By the singular value decomposition theorem (Theorem A.6 .5), we can choose the signs of 'Yen and v(')' so that IIX can be written in terms of these eigenvectors as IIX=W'. where V = V p = (v(I), "" ..• v(p,)' The scores of the"" rows of IIX on the ith principal component are given by the n elements of IIX'YC»- Th us. writing r k = "
2305,unknown,"scores on the first k principal components are given by M\Jl.. TIV ARIA TE ANALYSIS 406 since the columns of r are orthogonal to one another. Heoce the tbeOrem is proved . • Since the columns of r k are orthogonal to ooe another, r~r. = I., we see that V . = xr, represents a projection oC X onto a k-dimensiooal subspace of RP. The projection onto principal coordinates is optimal out of aU k-dimeos"
2306,unknown,of aU k-dimeosional projections because it is closest to tbe original p-dimensional configuration. (See Theorem 14.4.1.) Tbis result is dual to the result in principal component analysis that the sum of the variances of the first k principal components is larger than the sum of the variances of any other k uncorrelated linear combination of the columns of X. (See Exercise 8.2.5.) 14.4 Optimal Prop
2307,unknown,"14.4 Optimal Properties of tbe Oassical Solution and Goodness of Fit Given a distance matrix D, the object of MDS is to find a configuration X in a low-dimensional Euclidean space R ' wbose interpoin! distances, d~ = (x, - x.),(i, -i.) say, closely match D . The circumflex Or ""bat"" will be used in this section to indicate that the interpoinl distances D for the configuration X are ""fitted"" to the "
2308,unknown,"denote tbe fitted centred inner product matrix. Now let X be a configuration in R P and let L=(LbL,) be a (pxp) orthogonal matrix where L, is (p x k). Then XL represents a projection of the configuration X onto the subspace of R P spanned by the columns of Lt. We can think of i = XL as a ""fitted"" configuration in k dimensions. Since L is orthogonal, the distances between the rows of X are the same"
2309,unknown,"same as the distances between the row . of XL , d~,= t (x"" _.<,,)2= f (x;J(I)_X:~i)2. (14.4.1) ,-, ,-, If we denote the distances between tbe rows of XL , by b, tben k d~= L (s;J(;) -S~,» 2. (14.4.2) ,-I Tbus, d,."", d~ ; tbat is, l'rojecting a configuration reduces tbe interpoint distances. Hence, a measure of the discrepancy between the original configuration X and the projected configuration X i"
2310,unknown,"~= t (d~-a;.). (14.4.3) ,~ - I 407 MULTlDIME..'I.ISfONAL SCAUNG Then the classical solution to tbe MDS problem in k dimensions has the following optimal property: Theorem 14.4.1 Let D be a Euclidean distance matrix correspondillg 10 a configuratioll X in R P, and fix k (1"". k < pl. TIl en amo llgst all projections XL , of X onto k-dime/lSional subspaces of R P, the quantity (14.4.3) is minimized w"
2311,unknown,"minimized wilen X is projected 0/110 its principal coordinates ill k dimell­ siol1s. Proof Using (14.4.1) and (14.4.2) we see that ~ = f f (x;~, , - r,l(l,)2 = tr L;{ f (s, - x,)(s, -x.r}L, ,.,-- 1 = 2 •• ' IT L!,SL, since f (x,-"",)("",-,.,)'=2n f (s,-i)(x,-i),- 2 f (x,-i) f (x,-i), r~ - 1 =2n '8. Letting A,;;.···;;. Ap denote the eigenvalues of nS with standardized eigenvectors r = (""Y O) ! ..• 'Y"
2312,unknown,"eigenvectors r = (""Y O) ! ..• 'Y(pl)' we can write ~=2ntrF2AF2 ' where F, = r'L, is a column orthonormal matrix (F,F2 = Ip _,). Using Exer­ cise 8.2.7, we see that ~ is minimized when F, = (0, Ip _.)'; that is, when L 2= (yr.-I)' ... ,Yrp,)· Thus the columns of LJ span the space of the first k eigenvectors of liS and so XL, represents the principal coordinates of X in k dimensions. ote that for th"
2313,unknown,"in k dimensions. ote that for this principal coordinate projection, (14.4.4) When D is not necessarily Euclidean, it is mor e convenient to work with the matrix B = BAH. ff i is a fitted configuration with centred inner product matrix B, then a measure of the discrepancy between Band B is given (Mardi a, 1978) by . 1/1= L ( b,,- 6~)'=tr(B - B)2. (14.4.5) ,.,-1 For this measure also, we can prove t"
2314,unknown,"problem is optimal. MULTIVARIATE M 'ALYS IS 408 Theorem 14.4.2 If D is a dislance ma lrix (nol necessarily Euclidean), Ihen for fixed k, (14.4.5) is minimized over all configurations i: in k dimensions when i: is Ihe classical solulion to Ihe MDS problem. Proof Let A I '"" ... '"" A .. denote the eigenvalues of B , some of which might be negative, and let r denote the corresponding standardized eige"
2315,unknown,"eigenvectors. For simplicity, suppose A. > 0 (the situation A. < 0 is discus­ sed in E~ercise 14.4.2). Lei A,'"" ... ~ An '"" 0 denote the eigenvalues of B . By the spectral decom position theorem (Theorem A.6.4) we can write the symmetric matrix r'ilr as r'or=GAG' , where G is orthogonal. Then 1I=tr (B - B )2=tr r'(B - il)rr'(B - B)r= tr (A - GAG ')(A - GAG '). We see that for fixed A (see Exercise"
2316,unknown,"We see that for fixed A (see Exercise 14.4.2), '"" is minimized when G = I, so that i-I Since X lies in R ', B = uXX 'H will have at most k non-zero eigenvalues, which must be non-negative. Thus. it is easy to see that 11 is minimized when . {Ai' A = , O. i= 1 .... , k. i=k+ I ..... n, Hence B = r ,A ,P "" w here r, = ('YO)' ... ,'Y(.,) and A , = diag (A ..... , A.) so that X can be taken to equal r"
2317,unknown,"so that X can be taken to equal r ,A :"", the classical solution to the MDS problem in k dimensions. Note that tbe minimum value of '"" is given by (14.4.6) The above two theorems suggest possible agreemelll measures for the ""proportion of a distance matrix D explained"" by the k-dimensional classical MDS solution. Supposing A. > 0, these me asures a-re (Mardia, 1978) a ... = (t, A.j.t,IA.I) X 100%, "
2318,unknown,"a ... = (t, A.j.t,IA.I) X 100%, (14.4.7) and (14.4.8) We need to use absolute values in (14.4.7) because some of the smaller eigenvalues might be negative. 409 MVLllDIMENSJO N AL SCALING Example 14.4.1 We nOW consider the example of constructing a map of Britain from the road distances between 12 towns (Table 14.1.1). From this data, it is found that A, = 394 473 , '\2= 63634 , '\, = J3 544, A. = "
2319,unknown,"A, = 394 473 , '\2= 63634 , '\, = J3 544, A. = 10 245 , '\, = 2465. ,\.= 1450, ,1.,=501, A. = 0, '\0= -17 , '\10=- 214 , '\"" = - 1141. A '2=-7063 . We note that Ihe last four eigenvalues are negalive, but they are small in relation to A"" ... , A •. We know from Theorem 14.2.1 that some negative values are expected because the distance matrix is not Euclidean. The percentage variation explained by "
2320,unknown,"The percentage variation explained by the liTst two eigenvectors is <>' •• 2=92.6% or a2.2=99.8%. The first two eigenvectors, standardized so that x"" ,'x("" = Ai> are (45,203. -131\, 212, lilY. - 234 , - 8, - 382, -32, 153. - 120. 112) (140, -18, 31, -76,140,31, - 50.- 26. - 5. - 27, -34, -106). Since the MDS solution is invariant under rOlations and translations. the coordinates have been superimp"
2321,unknown,"coordinates have been superimposed on tbe true map in Figure 14.4.1 by Procrustes rotation with scating (see ElCample 14.7.1). We find that the two eigenvectors closely reproduce the true map . 14.5 Seriation 14.5.1 Description Multidimensional scaling can be used to pick out one-dimensional struc­ ture in a data set; that is, we expect the data to be parametrized by a single axis. The mos t commo"
2322,unknown,"single axis. The mos t common example is seriation, where we want to ascertain the chronological ordering of the data. Note that although MDS can be used to order the data in time, the direcrioll of time mus t be determined independently. Example 14.5.1 Consider the archeological problem of Example 8.5.1 where the similarity between graves is measured by the number of types of pottery they have in"
2323,unknown,"of pottery they have in common . Using the similarity matrix S2 of Example 13.4,2 and the standard transformation of Theorem 14.22 (see Exercise 14.2.6), it is found that "",,=1.75. '\2=0.59, ,1,3= 0.35, ,1..=0.05, '\5=,\.=0 . l\fULTIVARlATE ANALYSIS 900---1~~~~--~f-----4------r-----t------ Boo----~~~~----_+----_1+_----1_----_+------ 7oo----~~~~~--_+_,~--+_----~----_+------ roo--~~~~~----.------r~-"
2324,unknown,"roo--~~~~~----.------r~---r-----7------ ~---1------~~--~----~----~----_+------ • Leeos 6 0' 400----+------+ __ ~----~--_4~--~4_~ __ _1------- ~----L-----1_--~~------------1_----_+~~~ 2oo----~--~~~,~_+--~~------1_~--~------ 100 200 300 400 500 roo 410 Figure 14.4.1 MDS solunons fortile road data in Table 1.2.2 . • , original pointS; ~, classical solution; 0. Shepard-Kruskal solwio,L 411 MUL TIDIMEN"
2325,unknown,"411 MUL TIDIMENSIO . AL SCALING with coordinates in two dimens ions (-0.60,0.77, -0.19, -0.60, 0.64, -0.01), and (-0.15.0.20,0.60, - 0.15, -0.35, -0.14). See Figure 14.5.1. The coordinates in one dimension suggest the order (A, D), C, F, E , B , which is similar bu t not identical with the ordering given by correspondence analysis (Example 8.5.1). It is often a good idea to plot the data i:: more "
2326,unknown,"It is often a good idea to plot the data i:: more than one dimension to see if tbe data is in fact one -dimensional. For example , the artificial data in the above example does not particularly seem to lie in one dimension. However , even when the data is truly one-dimensional, it need not lie on the axis of the first dimension but can sometimes lie on a curve as the following section shows . 6 e "
2327,unknown,"6 e 5 Figure 14.5.1 Classical MDS solution lit two dimens ions using similarity matrix S, for grave dara ill Example 14.5.1. MULTrvARIATE ANALYSIS 412 14.5.2 H orsesboe efted In some situations we can measure accurately the distance between two objects wben they are close together but not when tbey are far apart. Distances which are ""moderate"" and those which are ""large"" appear to be the same. For"
2328,unknown,"be the same. For example , consider the archeology example described above. Graves which are close together in time will have some varieties of pottery in common, but those which are separated by more than a certain gap of time will have 110 varieties in common . This merg ing of all ""large"" distances tends to pull the farthest Objects closer together and has been labelled by D . G. Kendall \ 1971"
2329,unknown,"shoe effect"". This effect can be observed clearly in tbe following artificial example. Exam ple 14.5.2 (D. G. Kendall, 1971) Consider the (51 XS1) similarity matrix C defined by c,. = 9. { 8 if ~I r- 51""'3. c~ = j"" if 22""'I,-sl"",24, o if Ir-sl;;;'25. Using the standard transformation from similarities to distances leads to eight negative eigenvalues (varying from -0.09 to -2.07) and 43 non-negativ"
2330,unknown,"non-negative eigenvalues, 126.09,65.94,18.17,7.82,7.61.7.38,7.02.5.28,3.44, ... ,0.10. O. A plot of the configuration in two dimensions is given in Figure 14.S.2. "". .. .. Figure. 14.5.2 Two·dimensionai represenraliofl of Kendall's matrix. 413 MUL llDlMENSIONAL SCALING The furtbest points are pulled together so tbat the configuration looks roughly like part of the circumference of a circle. Note t"
2331,unknown,"ordering is clear from this figure, it is not ascertainable from tbe one­ dimensional classical solution. 14.6 Non -metric Methods Implicit in the preceding sections is the assumption that there is a ""true"" configuration in k dimensiOns with interpoint distances lin-We wish to reconstruct this configuration using an observed distance matrix D whose elements are of the form (14.6.1) Here the e"" rep"
2332,unknown,"(14.6.1) Here the e"" represent errors of measurement plus distortion errors arising because the distances do not exactly correspond to a configuration io R '. However , in some situations it is more realistic to hypotbesize a less rigid relationship between d"" and 1>"" ; namely. suppose (14.6.2) where f is an unkn own monotone increasing function. For this ""model"", the only information we can use t"
2333,unknown,"the only information we can use to reconstruct the S~ is the ,ank order of the d"". For example, for the road map data. we could try to reconstruct the map of England using the information the quickest journey is that from Brighton to London; the next quickest journey is tbat from Hull to Leeds; tbe longest journey is that from Dover to Tnverness. In this non-metric approach D is not tbought of as "
2334,unknown,"In this non-metric approach D is not tbought of as a ""distance"" matrix hut as a ""dissimilarity"" matrix. In fact the oon-metric approach is often most appropriate when the data is presented as a similarity matrix. For in this situation tbe transformation from similarities to distances is some­ what arbitrary and perhaps tbe strongest statement one should make is that greater similarity implies less"
2335,unknown,"An algorithm to construct a configuration based On the rank order information has been developed by Shepard (J962a,b) and Kruskal (1964). MULTIVARIATE ANALYSTS 414 Shepard-Kruskal algorirlmt (a) Given a dissimilarity matrix D , order the off-diagonal elements so that I In =2n(n - 1), (14.6.3) where (r"" 5,) •... ,(rm • s"",) denote all pairs of unequal subscripts, r, < 5,. Say that numbeIll d~ are m"
2336,unknown,"r, < 5,. Say that numbeIll d~ are monotonically related to the d~ (and write d~""!?n <1,,) if for all r < 5, "" < v. (14.6.4) (b) Let X(I! x k) be a configuration in R ' with interpoint distances a"". Define the (squared) stress of X by (14.6.5) where the m.inimum is taken Over d! such that d: m~n duo The d~ which minimizes 04.6.5) represent the least squares mOMOtone regression of a"" on d ... Thus ("
2337,unknown,"regression of a"" on d ... Thus (14.6.5) represents the extent to which the rank order of the aB disagrees with the rank order of the dB' If the rank orders match exactly (wb ich is very rare in practice). then S<X) = O. The presence of the denominator in (14.6.5) standardizes the stress and makes it invariant under transformations of tbe sort y, = ex,., r = 1, ... t n, c# O. The stress is also inv"
2338,unknown,"formatioDs of the form y, =Ax, + b when A is orthogonal. (e) For each dimension k, the configuration which has the smallest stress is called the be.1 filting configuration in k dimensions. Let s, = .min SeX) XL~l< t ) denote this minimal stress. (d) To cboose the correct dimension, calculate S,' S2, ... , until the value becomes low. Say, for example, S, is low for k = ko. Since S, is a decreasing"
2339,unknown,"is a decreasing function of k, k = ko is the ""right dimension"". A rule of thumb is provided hy Kruskal to judge the tolerability of SI<; S,;;o20%, poor; S,=10%, fair; S,,,,5%, good ; S,=O, perfect. Remarks (I) The ""best configuration"" starting from an arbitrary initial configuration can be obtained by using a computer routine developed by 415 MULTIDIMENSIONAL SCALING Kruskal (1964) which utilizes "
2340,unknown,"Kruskal (1964) which utilizes the method of steepest descent to find the local minimum . The initial configuration can be taken as the classical solution. Unfortunately, there is no way of distinguishing in practice between a local minimum and the global minimum. (2) The Shepard-Kruskal solution is invariant under rotation, transla­ tion, and uniform expansion or contraction of the best-fining con"
2341,unknown,"tion. (3) The Shepard-Kru skal solulion is non-metric since it utilizes only the rank orders (14.6.3). However , we still need a sufficiently Objective numerical measure· of distance to determine the rank order of the d"". (4) Similarities The non-metric method works just as well with similarities as with dissimilarities. One simply changes tbe direction of the inequalities. (5) Missing values The "
2342,unknown,"inequalities. (5) Missing values The Shepard-Kruskal method is easily ada pled to the situation where there are missing values. One simply omi ts the missing dissimilarities in the ordering (14.6.3) and deletes tbe correspond­ ing termS from the numerator and denominator of (14.6.5). As long as not too many values are missing, the method still seems 10 work well. (6) Treatment of ties The constrai"
2343,unknown,"(6) Treatment of ties The constraint given by (14.6.4) is called the primary treatment of lies (PTT). If d"" = d"""" then no constraint is made on d~ and d~"". An alternative constraint, called the secondary tremmeHt of lies (STT ) is given by which has the property that d"" = d., ~ d! = d~, .. However , one must be cautious when using STT on data with a large number of ties. The use of STT on data suc"
2344,unknown,"STT on data such as Example 14.5.2 leads to the horseshoe effect (see, D . G. Kendall. 1971). (7) Comparison of melhods The computation is simpler for the classi­ cal method than it is for the non-metric method. It is not known how robust the classical method is to mo notone transformations of the dis­ tance function; however , both methods seem to give similar answers when appbed to well-known ex"
2345,unknown,"appbed to well-known examples in the field. Figure 14.4.1 gives the two solutions for the road data. For the Shepard-Kruskal solution for the Morse code data, see Shepard (1963). (8) We have not commented on the non-metric method of Guttman (1968). For a mathematical and empirical analysis of multidimensional scaling algorithms of Kruskal's M-D-SCA 1 and Guttman-L ingoes' SAA-I, we refer to Lingoe"
2346,unknown,"SAA-I, we refer to Lingoes and Roskam (l973) which also contains certain recommendations for improvement of these algorithms. MULTIVARIATE ANALYSIS 416 14.7 Goodness of Fit Measure: Procrustes Rotation We now describe a goodtless orjir measure (Green. 1'152: Gower. 1971b ). used lO compare two configurations. Let X be the (n x p) matrix ot the coordinates of n points obtained from D by one techniq"
2347,unknown,"coordinates of n points obtained from D by one technique. Suppose that V is the (n x q) matrix of coordinates of another set of points obtained by another technique, or using another measure of distance. Let q"";;; p. By adding columns of zeros to V , we may also assume Y lO be ("" x pl. The measure of goodness of til adopted is obtained by moving the points y, relative to the points x, until the "" "
2348,unknown,"f (x, - y,)'(x, - y,) (14.7.1) ,.1 is minimal. We can move y, relalive to x, through rotation. refleclion, and translation. i.e. by A 'y,Tb, r= I ..... n. (\4.7.2) where A' is a (p x p) orthogonal matrix. Hence. we wish to solve "" R '=min L (x,-A'y,-b),(x,- A 'y,-b) A.b ,_ 1 (14.7.3) for A and b. Note that A and b are found by least squares. Their values are given in the following theorem. Theorem"
2349,unknown,"Theorem 14.7.1 Let X ( .. X p) alld V (n x p) be two conjig,uariollS of n poinrs, for co,wenience cenrred at the origin, so x=y=O . Ler Z= V 'X and using tile singular value decomposirion theorem (TIleorem A.6.5), wrire Z = YrU . (14.7.4) where V and U are orthogonal (px p) matrices and r is a diagonal matrix of non-negarive eleme,lIs. TIle"" lI,e mtllimizing values of A and b in (14 .7.3) are give"
2350,unknown,"(14 .7.3) are given by Ii = O. A = vu'. alld further R '= Ir XX '''-If VY ' -2 tr r. Proof On differentiating with respect 10 b, we have b =i-A'y (14.7.5) ( 1-1.7.6) (14.7.7) where y =""i y ,ltl. it. = ~ X,lll. Since hoth configurations are centred. ii = 0 . 417 MULTIDIMENSIONAL SCALING Then we can rewrite (\4.7.3) as R'= min u (X - YA )(X - YA )' =tr XX'+ trVY'- 2 max u X 'YA . A A (14.7.8) The co"
2351,unknown,"A A (14.7.8) The constraints on A are AA' = 1, i.e. aiai = 1, a~aJ = O. i:F j, where a~ is the ith row of A . Hence there are p(p + lll2 constraints. Let iA be a (p x p) symmetric matrix of Lagrange multipliers for these constraints. The aim is to maximize Ir {Z ' A - ~A( AA' - I)}, where Z ' =X 'Y. By duect differentiation it can be shown that ~tr(Z'A)= Z aA . ..!..U(AAA')=2AA . ilA (14.7.9) (14."
2352,unknown,"(14.7.9) (14.7.10) Hence on differentiating (14.7.9) and equating the derivatives to zero, we find that A mu t satisfy Z = AA. (14.7.11) Write Z using (14.7.4). Noting that A is symmetric and that A is to be orthogonal. we get. from 04.7.11), A '= ZA 'AZ= ZZ ' = (VrU )(UrV'). Thus we can take A = YrV'. Substituting this value of A in (14.7.11) we see that A = VU ' (14.7.12) is a solution of (I4.7."
2353,unknown,"in (14.7.8) gives (14.7.6). Finally. to verify that A maximizes (14.7.9) (and is not just a stationary point) we must differentiate (14.7.9) with respect to A a second time. For this purpose it is convenient to write A as a vector a =(8(1) •... , afp»)', Then (14.7.9) is a quadratic function of the elements of a and the second derivative of ([4.7.9) with respect to a can be expressed as the matrix"
2354,unknown,"-I. ® A. Since A = vrv', and the diagonal elemen ts of r are non­ negative, we see that the second derivative matrix is negative semi· definite. Hence A maximizes (14.7.9). • We have assumed that the column means of X and Y are zero. Then the ""best"" rotation of Y relative to X is Y A , where A is given by (14.7.12), and A is called the Procrustes rotarion of V relative to X. oting from (14.7.4) th"
2355,unknown,"oting from (14.7.4) that X'YY 'X= ur'u', we can rewrite (14.7.8) as R '= If XX'+ 1f YY '-2 tr (X 'VY 'X )II2. (I4.7.12) MULTTVARJATE ANALYSIS 418 It can be seen that (14.7.8) is zero if and only if the y, can be rotated to the x, exactly. Scale faclOr J[ the scales of two configurations are different, then the transformation (14.7.2) should be of the form where c > O. Following the above procedure"
2356,unknown,"where c > O. Following the above procedure, it can be seen that c= (Ir n /(lr YY ') (14.7.14) and the other estimates remain as before. This transformation is called the Procrusres rotarion with scaling of Y relative to X. Then Ihe new minimum residual sum of squares is given by R 2 = tr (XX ').,.,:> Ir (YY')-2C Ir (X 'YY 'X ) '12. (14.7.15) where c is given by (14.7.14). Note that this procedure "
2357,unknown,with respect to X and Y. Symmetry can be obtained by selecting scaling so that Ir (XX ') = Ir (YY '). For an excellent review of this topic. see Sibson (!978). Example 14.7.1 The actual Ordnance Survey coordinales of the 12 IOwns in Table 14.1.1 are I 2 3 4 5 6 7 8 9 to 1 I 12 E 257 529 339 629 292 259 508 265 433 533 420 627 279 104 554 142 90 665 433 842 438 183 563 308 Treating these quantities
2358,unknown,"Treating these quantities as planar coordinates X (12 X 2) (the curvature of the Earth has little effect). and the first two eigenvector from lhe classical MDS solution fOr the data given in Example 14.4.1 as Y , It is found that 'Y= ( 182119.068 -91647.926) U = (-0.347729 -0.937595) X -495 108.159 -25629 .185 ' 0.937595 -0.347729 ' f =(527597.29 0 ) o 94851.13 ' v = (-0.999 890 0.014858) 0.014 85"
2359,unknown,"0.014 858 0.999 890 . This leads \0 the transforming of the ys to match the xs by y~ =cA· y,-rb. 419 MUL TIDlMENSJONAL. SCALING where c = 1.358 740, A= VU' = ( 0.333760, -0.942658) b = (424.250). -0.942658, -0.333760 ' 383.417 The transformation has been used on y, to obtain y~, and these y~ are plotted in Figure 14.4.1 together with the. x,. We have tr XX' = 853 917, trYY'=458107. Hence from (14."
2360,unknown,"tr XX' = 853 917, trYY'=458107. Hence from (14.7.15), the residual is R'=8172 . The She.pard-Kruskal solution has a stress of 0.0404 . Using this solution as Y and the Ordnance Survey coordinates again as X leads to a residual of R 2 = 13 749. Hence the classical solution fits a bit better for this data. Of course Figure 14.4.1 sbows little difference between the two solutions. If we are given two"
2361,unknown,"solutions. If we are given two distance matrices D c and D2 but not the corres­ ponding points, (14.7.6) cannot be computed w ithout using some method 10 comp~le Ihe ""points"". The first two terms are expressible in terms of D , and D , but not tI f . *14.8 Multi-sample Problem and Canonica l Va.riates Consider the case of g p·variate populations with means 11"" r = I, ... , g, and commO n covarianc"
2362,unknown,"and commO n covariance matrix 1:. If w e are given a data matrix Y representing samples of size II, from the Ttb group, r= I, ... , g, let y, denote the ,th sample mean , and estimate the COmmon covariance matrix 1: by W /v, where W is the within-groups sum of squares and products (SSP ) matrix with I' degrees of freedom . Assume that the overall (un weighted) mean j is "" j= I j,=O. (14.8.1} ~, We"
2363,unknown,""" j= I j,=O. (14.8.1} ~, We shall work with the Mahalanobis distances d2 (j - l'W -'I.. - ) ,.= JI ,-Ys \3,-Ys ' ( 14.8.2) it is easily checked that if B is defined by (14.2.3), then B=v-VW- ' Y'~ where Y'= (j"" ... ,Y,). MUL TIV ARIA TE A,. .... ·AL VSIS 420 Thus B "", 0 and so D is Euclidean. Let X be the configuration for B defined in Theorem 14.2.1, and fix k, 1"", k '"" p. Then, the first k colum"
2364,unknown,"of X can be regarded as tbe coordinates of points representing the g means in k dimensions (k ""'p). This configuration has the optimal prop· erty that it is the ""best"" representation in k dimensions. ate that Y 'Y is the (unweighted) beiween·groups SSP matrix. Let I, denote the ith canonical vector of Section 12.5 using this unweighted between·groups SSP matrix; that is. define I, by i= I, ... , m"
2365,unknown,"i= I, ... , min (p, g) where'\, is the ith eigenvalue of vW "" Y'Y. which is the same as the ith eigenvalue of B . Then the scores 01 the g groups on the ith canonical coordinate are given by ¥I,. Since BY! , = A, VI, and I;YYI , = A"" we see from (14.2.5) that Yll = I {IP' so that VI, is also tbe itb eigenvector of B . Thus, the canonical means in k dimensions, that is, tbe scores of the first k ca"
2366,unknown,"dimensions, that is, tbe scores of the first k canonical variates on the g groups. are the same as the coordinates given by Theorem 14.2.1. E""ercises and Complements 14.2.1 Using (14.2.6). show that (14.2.9) can be wrinen in the form (14.2.10). 14.2.2 In the notation of Theorem 14.2.1 show that (a) b""=ii. -2ii, .. b.,=a""-ii, -ii,-a. ,""'s; (b) B = i: ""xl .. ; 14.2.3 (Gower. 1968) Let D= (d"") be an "
2367,unknown,"(b) B = i: ""xl .. ; 14.2.3 (Gower. 1968) Let D= (d"") be an (n X n) Euclidean distance matrix with configuration X = (x, ____ ,x.J' in p·dimensional principal coordinates. given by Theorem 14.2. L Suppose we wish to add an additional point to the configuration using distances d, .• "", r= 1, .... "" (which we know to be Euclidean), allowing for a (p T !lth dimension. If the first n points are represe"
2368,unknown,"421 MUL T1D[MENSIONAL SCALJNG the (n + I Jth point is gIven by where f =(f .. · .. , f.J', and Hence"" is uniquely determined but y is determined only in value, nOt in sign. Give the reason. (Hint: substitute I, =21t:I""'I-X~t-1X""+1 for I, in terms of x"" ... , x.<, to verify the formula for x.) 14.2.4 If C is p.s.d. then show that c,' + C,i - 2c,; > O. Show that the distance d"" defined by d~=c"" +c,,"
2369,unknown,"distance d"" defined by d~=c"" +c,,-2c., satisfie.s the triangle inequality. 14.2.S For the Bhattacharyya distance matrix D given in Example 13.4.1, the eigenvalues of Bare A, =318.97, A, = 167.72. '\3=11.11, ,1..=0. Hence , D is Euclidean and the two·dimensional representation accounts for a 1.2 = 98% of the variation. Show that the principal coordinates in two dimensions for Eskimo, Bantu, English"
2370,unknown,"Bantu, English, and Korean are, respectively, (9.69,7.29), (-J 1.39, -2.51). (-6.00,4.57), (7.70, -9.34). Plot these points and comment on the conclusions drawn in Example 13.4.1. 14.2.6 For the grave data (Example 13.4.2) using the similarity matrix Sz show that the distance matrix given by the standard transformation is 0 FlO 2 0 -./8 2 0 16 10 i2 .J6 D = 0 2 J8 2 0 .f8 2 0 2 0 14.2.7 Suppose th"
2371,unknown,0 .f8 2 0 2 0 14.2.7 Suppose that I. 2 ..... 7 are region (enclosed b) unbroken lines) in a country arranged as in Exercise Figure l. Let the distance matrix be MULTlVARlATE ANALYSIS 422 5 6 7 2 Exercise Figure 1 Seven regions in a country. constructed by counting the minimum number of houndaries crossed to pass from region i to region j. Show that the distance matrix is given by 0 1 2 2 2 1 0 1 2
2372,unknown,"Show that the distance matrix is given by 0 1 2 2 2 1 0 1 2 2 2 0 1 2 2 0 1 2 1 0 1 0 I 0 Show thar the distances constructed in this way obey the triangle inequal­ ity d"" """" d"" + d,,, but by showing that the eigenvalues of the matrix S are ..\1=A2=~J ).J=..\,,=4, A5=0, A6=-~' A7 =-1. deduce that this metric is non-Euclidean. Since A I = ""2, select any two orthogonal eigenvectors corresponding to "
2373,unknown,"A, and A2 and, by plotting the seven points so obtained, show that the original map is reconstructed. As in Example 14.2.1, the points are vertices of a hexagon with centre at the origin. 14.2.8 (Lingoes, 1971; Mardia, 1978) Let D be a distance matrix. Show that for some real number a, there exists a Euclidean configuration in P"""" /I - 2 dimensions with interpoin! distances d! satisfying d!=O . Th"
2374,unknown,"d!=O . Thus d~2 is a linear function of d~, so the configuration preserves the 423 MULTJOrMENSIONAL SCALL"".JG rank order of the distances. ll-[int: show that the matrix D * leads to A * and B * given by A *= (-4d!2) =A-a(I-J), S * =UA* B = 8 - a B o lf S has eigenvalues A, ....... A. > 0:.. A; ....... A~, then S * has eigen- values A, - a, r = I , .... u ; 0: and A; - G , r = I, .... v. Then the c"
2375,unknown,"a = A ~ makes S * p.s.d. of rank at most n -2.) 14.4.1 Let I"" ... ,I. be orthonormal vectors in R q(p""""q) and Jet z"" r= 1, ... , n, be points in R q. Let H, denote the foot of the perpendicular of Z , on the subspace spanned by I"" ... ,I •. Show that with respect to the new coordinate system with axes I"" ... , I., the coordinates of H, are (I;z, ..... 1;,z,)'. What modification must be mad e if th"
2376,unknown,"but not orthonormal' 14.4.2 Let A =diag (A, •...• Ar). where A,:"""" ... Ar are real numbers . and ler A = diag (A"" ... ,Ap). where A, """" ... :""A."" 0 are non· negative numbers. Sbow that mininlizing tr(A-GAG ')' over orthogonal matrices G is equivalent to maximizing tr (AGAG' ) = f AIA;gf, = f A;h, = c,b(h) say. i,J- 1 i-I where and Show that c,b(h) is maximized over such vectors h when h, = Aj for "
2377,unknown,"i = t, .... p; that is. when G = I. 15 Directional Data 15.1 Introduction We have so far considered data where we are interested in both the direction and the magnitude of the random vector I = (~ ••.... ~. )'. There are various statistical problems whicb arise when the observations them· selves are directions, or we are only interested in the direction of ~. We can regard directions as points on "
2378,unknown,"can regard directions as points on the circumference of a circle in two dimensions or on the surface of a sphere in three dimensions. In general. directions may be visualized as POlOts on the surface of a hypersphere. We will denote a random direction in p dimensions by I. where J'J = I. The unit vector I takes values on the surface of a p·dimensional hypersphere 5, of unit radius and baving its c"
2379,unknown,"hypersphere 5, of unit radius and baving its centre at the origin. ( ote that this notation differs from the notation in topology where 5. is usually wrinen 5.-,.) For p = 2. we can equi""alently consider the circular variable 0 defined hy (see Figure 15.1.la) I) = cos 8, 0""'0<27T. ( 15.Ll) whereas for p = 3 we have the spher;cal variable (0. tb) defined by (see Figure \'.l.lb» I, = cos O. 1,=5in 0"
2380,unknown,"I, = cos O. 1,=5in 0 cos </>. 1,= sin 0 sin </>. 0.;;0'"" 7T, O""'</> < 27T (15.1.2) In general, it is convenient to consider the density of I in terms of the 425 DtREcnO AL DATA ~ __ L-____ L-__ +-~ •• Figure '-.1. Ja RepresenraJion for circular variable. '. p II I I e/ I I 0 I ~ f "" 4> '-..J N "" Figure 15.1.1b Represelllariorr for sphencal variable. MUL TTV ARIA TE A. ... AL YSlS 426 spherical pol"
2381,unknown,"spherical polar coordinates 9'=(91, ••• ,0._1) with the help of the IIans­ formation 1= u(9), i = 1 ..... p-2. 0.s0._1 < 2n. (15.1.3) wh ere 1-1 ll, (9) = cos 8, fl sin 8"" i = 1 •...• p. sin Oo=cos 8. = 1. ,-0 15.2 Descriptive Measures Let I"" i= 1, ... , r ~ be a random sample On 5 •. For p=2, these points can be ploned on a circle. whereas for p = 3, they can be plotted 00 a globe. Let 1 n 1= - L"
2382,unknown,"Let 1 n 1= - L Ii n I-I denote the sample mean vector. The direction of the vector I is 10= I/R, (15.2.1) which can be regarded as the mean direction of the sample. The points I, regarded a..(; of unit m asst have centre of gravity 1 which has direction L, and distance from·the origin R (see Figure \5.2.1). The mean direction has the following /ocational prope rty similar to i. Under an orthogo na"
2383,unknown,"Under an orthogo nal transformation 1* = AI , i.e. under an arbitrary rotation or reflection, tbe new mean direction is seen 10 be that is, the transformation amounts to simply rotating 10 by A, Further, it hasthe following millirniza'rioll property, Consider 5(a), the arithmetic mean of the squared chordal distances between I, and a, where a'a = 1. We have 1 n 5(a)=- L 11,-al'=2 (1-I'8). n i- I ("
2384,unknown,"n i- I (\5.2.2) Using a Lagrange multiplier, it can be seen tbat (15.2.2) is minimized when a = 10. Further. min5 (a)=2(t-R). (15.2.3) 427 DfREcnONAL OATA • '-il ;I ~----------------~----~o· Figure 15.2.1 Basic circular SlallSlIcs R (distance from orr.gil1). 1{1 (mean drrection). (Da ta in Exercise 1:5.2.3.) Since 5(a) is non-negative and R is positive, we have O"""",R .sl. (15.2.4) In view of (15.2"
2385,unknown,"O"""",R .sl. (15.2.4) In view of (15.2.3) the quantity 2(1- R) may be called the spherical variance. However, it is usual to work with R in place of 2(1-R) in directional data. and it m easures clustering around the mean direction. Note that R ;, 0 when ttle points are uniformly distributed (Le. widely dispersed) and R '"" J wben the points are heavily concentrated near a point. Following the termino"
2386,unknown,"called the ""resultant length"", and the vector n r=ol= L I, (15.2.5) i-J is called the ""resultant vector"". Example IS.2.1 The declinations and dips of remnan t magnetization in nine specimens of Icelandic lava Hows of 1947--48 given in Fisher (1953 ) ~iULTlVAR1ATE ANALYS TS Table 15.2.1 ~reasuremenlS (Tom the mag­ netized lava flows in 1947-48 in 1celand (H osper d,l. cited in Fisher, 1953) I, I, I"
2387,unknown,"I, I, I, 0.2909 -0,0290 -0.9563 0.1225 -0.0624 -0.9905 0.2722 -0.2044 -0.9403 0.4488 -0,4334 -0.7815 0.2253 -0,2724 -0.9354 0.1705 -0.3207 -0.9317 0.3878 0,1171 -0,9143 0.5176 0.0217 -0,8554 0.1822 0.0032 -0.9833 lead to th~ values given 10 T able 15.2.I,W e find that 428 . L I""~ = 2.6178. n L 42 = -!.lS03. f 1.3 = -8.2887. , -I ,.1 ,-, Hen ce Consequently. R =0 .9747. which indicates heavy concen"
2388,unknown,", -I ,.1 ,-, Hen ce Consequently. R =0 .9747. which indicates heavy concentration around the m ean direction. Further. the mean direction from (15.2.1) is -given by 10= (0.2984. -0.1346. -0.9449). 15.3 Ba sic Distributions Follow ing Section 15.2. we define the population resulrartr lertgrh p by p = [J. {E(I,)}'),,2 = {E(I')E(J»)I'2, (15.3.1) and when p > O. the population mean direction is define"
2389,unknown,"and when p > O. the population mean direction is defined by Il=E(I)/p. (15.3.2) We will denote the prohability element (p.e.) of 1 on Sp by dSp• The Jacobian of the transformation from (r.9) to x is given by (2.4.7). On 429 DLRECTIOSAL DATA separating the parts for rand 0, in tbe Jacobian, it is seen that dSp = a,(O) dO, (15.3.5) where. ,-, ap(O) = n sinP - ' 9,_"" a,(9) = 1. ,-2. 15.3.1 The unifor"
2390,unknown,"where. ,-, ap(O) = n sinP - ' 9,_"" a,(9) = 1. ,-2. 15.3.1 The uniform distribution If a direction I is uniformly diSlribw ed on S"", then its probability element is cpdSp• Th us, on using (15.3.3), the p,d.!. of 9 is Cpa,. (9). (15.3.4) Integrating over 9, it is found that cp = r(~ p)/(21Tp/2) . (15.3.5) From (15.1.3) and (15.3.4), it can be seen that E(I)=O for tbis distribu· tion. Hence we have p"
2391,unknown,"tion. Hence we have p = O. Consequently, I' is not defined for this distribution. Similarly. E(I,I,) = 0 for i;i j, E(m= lip. 15.3.2 Th e von Mises distribution A unit random vector 1 is said to have a p.variate PO ll Mises-Fisher distribution if its p.e. is c (K)e·~' dS p P' (15.3.6) where K ;;' 0 and 1""1' = 1. Here K is called the concentration parame ter (for reasons given below) and <;'(K) is "
2392,unknown,"(for reasons given below) and <;'(K) is the normalizing constant. II a random vector I has its p.e. of Ihe form (15.3.6). we will say that it is distributed as M. (I', K ). For the cases of p = 2 and p = 3, the distributions are called von Mises and Fisher distributions after the names of the originators (see von Mises, 1918 ; Fisher. 1953), and these particular cases explain the common nomenclaru"
2393,unknown,"explain the common nomenclarure for p> 3. However, the distribution (15.3.6) was introduced by W atson and W illiam s (1956). TIle lIormalizirtg CO IISrartt We show that (15.3.7) wbere l.{K) denotes the modified Bessel function of the first kind and MULTIVARIATE ANALYSIS 430 order r. which can be defined by the integral formula (15.3.8) The proofs of the results assumed about Bessel functions in t"
2394,unknown,"can be found in G. _ Watson (1944). We have (15.3.9) where we have transformed I to I' by an orthogonal transformation so that If = j.t'1. Under this transformation d5. remains invariant. On using the polar transformation 1* = u(9). we find with the help of (15.3.3) that 1c,.(KW'= J ... J J e""~"" n sinP-101_ld9, ... dOp_I' u () u Using (15.3.K) in carrying out the integration on the right-hand side"
2395,unknown,"obtain (15.3.7). Mean directiotr We show that the mean direction of 1 is ,., and Je{K) (15.3.10) Let a = Kj.t. From (15.3.6), we have J e""' ds"" = 1I<:,(18'al'I2)· IEs,. Differentiating both sides with respect to a and using the recurrence relation (15.3.11) we find that (15.3.12) Hence (15.3.10) follows. On differentiating the logarithm of (15.3.6) twice with respect to j.t, we 431 DIRECTIONAL DAT"
2396,unknown,"431 DIRECTIONAL DATA note that the mode of the distribution is also j.t if K > 0. It has an antimode at 1=-..... Closure property Under the orthogonal transformation 1""= AI. 1* is again distributed as M.(Aj.t. K ). Particular cases Let "",' = (a, •... ,a._I) be tbe spherical polar coordinates of j.t. Using the polar transformation (15.1.3), we find that the p.d.!. of 9 is g(9; ""'. K) = cp("" )[exp {"
2397,unknown,"g(9; ""'. K) = cp("" )[exp {Ku'(a)o(9)}]a,,(9), O""'O,""'rr. i=1 ..... p-2. °"",0._1 < 2rr. (15.3.13) Thus the density of the vOn Mises distribution (p = 2) is g(O; a. K)= {2."".[.(KW'e""""""""·-o"" 0"", e <2 ."".. (15.3.14) whereas the density for the Fisher distribution (p = 3) is g(O, 4>; a, 13. K) = {KI(4r. sinb K)} exp [K{COS a cos 0 ·sin a sin 0 cos (d> - j3)}] sin 9. O:!!S;6~1T, 0""'d><21T. (1S.3.tS) whe"
2398,unknown,"where K > 0 and (see Exercise 15.3.2 for details) [,,.(,,)= (2 sinh K)I(2."".K)[12. [f a =0. we have. from (15.3.13). g(9; a. I() = Cp(K ){exp (K cos 0,)}1l.(9). 0""""9. """"7T. i = I. .... p -2. 0"""" 0._[ < 21T. (15.3.t6) (J 5.3.17) where 0, is the angle between 1 and (1. 0 •.... 0)'. Hence 0, and (8, •... ,8._,) are independently distributed. Further. from (15.3.4) (02 •.••• 8,_,) has a uniform distri"
2399,unknown,"(02 •.••• 8,_,) has a uniform distribution on 5._, and the p.d.f. of 0, js given by g(8,; K)={<:'(K)/Cp_,}e""""""'·' sinp-l Ol> Values of K For large K we have. approx.imately. 2K(1-1'"",)-X;_I' (15.3.19) To prove this result, suppose .... = (1,0, .... 0)' so that 1-1' .... = I -cos 0"" and note that. with large probability, 8, will be near O. Then substitute cos 9, ,;, I-!O;. MULTIVARIATE ANALYSIS 432"
2400,unknown,"MULTIVARIATE ANALYSIS 432 in (15.3.18) to conclude that, approximately K8 i-X;-I' which is eq uivalent to (15.3.19) since cos 9, ± I -19;. For K = O. the distribution reduces to the uniform distribution given by (15.3.4), where (see Exercise 15.3.1) lim Cp(K) = cp' .-0 Shape of the dislriblllion Using the foregoing results, we can now look into the behaviouT of the distribution. For K > 0. the dis"
2401,unknown,"distribution. For K > 0. the distribution has a mode at the me an direction, whereas when K = 0 the distribution is uniform. The larger the value of K , the greater is the clustering around the mean direction. This behaviour explains the reason for describing K as the concentration parameter. O.B - 180°-150 -120 -90 -60 -30 0 30 60 90 120 150 180 0 FIgure 15.3. Ja Linearr.p, ..... "",aru,n of rh. d"
2402,unknown,"433 DIREcrJONAL DATA ~~----__ t-________ -,~ __ ~OO Figure 15.3.1b CirCldar represenlalion of the densllY of M 'l«cos a,sin a)', 1). Figures I 5.3.1a and 15.3.1 b give the shapc of tbe distribution for varying values of K and IJ.. For p= 3 and Il= (1. O. 0)' the distribution is rotation­ ally symmetric about the x, axis, baving a maximum at the north pole 9 = 0 and a minimum at the south pole fI ="
2403,unknown,"9 = 0 and a minimum at the south pole fI = 7r. 15.3.3 The Bingham distribution Sometimes the observations are not directions but axes; that is. the unit vectors I and - I define the same axis. In this context it is appropriate to consider p.d.f.s for I whicb satisfy the antipodal symmetric property f(J) = f(-I). The observations in such cases can be regarded as being on a hyperhemisphere, or on a "
2404,unknown,"hyperhemisphere, or on a sphere with oppos ite points not distinguiShable. An important distribution for dealing with directional data with an­ tipodal symme try and for dealing with axial data is the Bingham distribu­ tion. The unit vector 1 is said to have a Bingham distribution, B .(A , te), if its p.e. is of the form d (te) exp {tr (J(A'II' A n dS., IE 5"" (15.3.20) where A now denotes an ortho"
2405,unknown,"where A now denotes an orthogonal matrix, K is a diagonal matrix of MULTIVARIATE ANALYSIS 434 constants K= ("""""""" "",)' and d(K) is the normalizing constant depending only on K and is given (see Exercise 15.3.6) by • I ~ - n r('j +4) , • -=2 L '"" L ,-I n ~ ( d(K) .,_1) •• -or(f ,,+E)j-I'j!' 15.3.21) j_ 1 2 Since tr(A1I'A )= 1, the sum of the parameters 1(, in (15.3.20) is arbitrary. and it is usual "
2406,unknown,"arbitrary. and it is usual to take ""p = O. For '"" = ... = ""., the distribution reduces to the uniform distribution. We have tr(KA'U'A)= f K,(I'8;,/, Using the polar transformation (15.1.3) as in Section 15.3.2 when ""2= ... = K,=O. K, = K. the joint p.d.f. of a becomes O~OI~'""' i=1 ..... p-2, O-=S;6p _.<2rr. (15.3.22) For"" > O. this rlistribution is symmetric with high concentration around the equa"
2407,unknown,"the equator (for p = 3) in the form of a '""girdle"", while for K < 0 the distribution is bimodal with poles at 0, = O. 7r and is Totationally symmet­ ric about the x, axis. For K =0, the random variables 0, .. ,0._, are uniformly distributed. In general. the larger the elements of K , the greater the clustering around the axes with direction cosines 8 m , j = l. .... p. Further, different values of"
2408,unknown,"Further, different values of K in (15.3.20) give the uniform distribution. symmetric and asymmetric girdle distributions (i.e. the probabiliTy mass is distributed in the form of an elliptic pattern like a girdle around a great circle) and bimodal distributions. Figure 15.3.2 shows COmmOn configura­ tions of sample points on a sphere. The sample points in Figure 15.3.2(a) could arise from a Fisher "
2409,unknown,"could arise from a Fisher distribution while the sample points in Figure lS.3.2(b) and (c) could have arisen from the Bingham distribution. For p = 2. the p.d.!. (15.3.20) reduces in polar coordinates to K >O, 0,,;0<27T. The distribution is of course bimodal with the two modes situated 180· apart, aDd is called the bimodal distribution of von Mises type. The transformation O' = 0 mod 7r represents"
2410,unknown,"transformation O' = 0 mod 7r represents an axis by an angle in (0, rr), and the distribution of 9* is called the von Mises-type axial distribution. Note that the distribution of 0' = 20* = 20 is M( (cos 2a, sin 2""),, K). Thus the process of ""doubling the angles' of 0 (or 0*) leads to the von Mises 435 DTREcnONAL DATA • 0 (0 ) t .) Ie) Figure 15.3.2 Configuration of sample points from (a) unimoda~ "
2411,unknown,"• 0 (0 ) t .) Ie) Figure 15.3.2 Configuration of sample points from (a) unimoda~ (b) bimodal, and (c) girdle distributions. An open circle denofes a poil11 ()n tile. nrhe.r side.. distribution. Unfortunately there is no simple analogue to this process for p,,;3 related to the distribution M.( .... ,<). The distribution for p = 3 was put forward by Bingham (1964 , 1974) who investigated its statist"
2412,unknown,"who investigated its statistical properties extensively. For an elaborate discussion of the distribution for p = 2 and for p = 3, we refer to Mardia ( I 972a). 15.4 Distribution Tbeol'Y Let ,,, ...• 1, be a random sample from Mp( .... K). For inference prob­ lems. the sample resultant length R and the sample mean direction vector ii (in polar coordinates) are of importance: that is. . ~ 1,=(""""'"" '"
2413,unknown,"ii (in polar coordinates) are of importance: that is. . ~ 1,=(""""'"" ',)'=r= Ru(ii)_ (15.4.1) .-, The statistics 'I, ... , '. are the components of the resultant vector r = D .. In most cases, the samp ling distribution of Rand ii from tbe von Mises-Fisher population can be derived with the help of the correspond­ ing distribution for the uniform case, which we consider first. 15.4.1 The characteris"
2414,unknown,"15.4.1 The characteristic function method and the isotropic ca..., Let </>(t) = E{exp (it'I)} be the characteristic function of I. Then from the inversion theorem. the p.d.f. of r for sample size rl is given by (15.4.2) MULTIVARIATE ANALYSIS 436 provided ""'""(I) is an absolutely integrable function. On using the polar transformations 1= pula ) and r = Ru(e) in (15.4.2), we find that the p.d.f. of ("
2415,unknown,"of (R, e) is .) S, (15.4.3) where .p(p, a l = ["",(tl]' ••• ,."" and the second integral is taken over 0 """" ""', """"?T. i = I, ... , p - 2; 0 """" <1>._ , < 27T. We now deduce the following results for the isotropic random walk with n equal steps. starting from the origin, in p dimensions; that is, I ..... ,I"" is a random sample from tbe uniform population with p.d.!' given by (15.3.4). In this case it "
2416,unknown,"(15.3.4). In this case it can be show n that (15.4.2) is valid for all n;;.2 at all continuity points of the p.d.!. of R , provided that when <1>""(1) is not absolutely integrable, the integral is interpreted as a limit over increasing discs centred at O. (See Watson, 1944 , pp. 419-421.) Since (15 .3.9) holds for complex variables (see Exercise 15.3.3). the c.f. of I is .p(p. a ) = c. feU') dJ = c"
2417,unknown,"of I is .p(p. a ) = c. feU') dJ = c,JC.{ip) = rGp)Gp) '- pl2 J,p_')""(p), S, (15.4.4) where J,(p) is the ordinary Bessel function of rth order related to the modified Bessel function by J,(p) = i-'J,(ip). (1S.4.S) On substituting (15.4.4) in (15.4.3), we deduce from (15.4.3), after integrating over a with the help of (15.3.9), that (i) R aDd ii are independently distributed; (ii) ii is uniformly di"
2418,unknown,"(ii) ii is uniformly distributed On S.; (iii) the p.d.!. of R is given by ~ h. (R ) = (27T)-'C;-' R '-' J pP -'{c.(ipR )c;(ip))-' dp o = 2""-·""Kft-"" r(ip)""-' R f p""-.'""-lIn o 0< R < n (15.4.6) 437 DlRECTIONAL DATA. Of course. R denotes the distance covered from the origin after n steps in the random walle 15.4.2 The von Mises-FlSber case Let us now assume that I., ...• 1"" is a random sample from Mp"
2419,unknown,"Let us now assume that I., ...• 1"" is a random sample from Mp(sL, K). On integrating the joint density over constant values of (R. e). we find that the p.d.!. of (R, e) is g(R, e; "". K) = {C;(K)/c;}e""R.,,,,).(iJgo(R,ii), where go is the p.d.!. of (R, 9) under uniformity. From Section 15.4.1, we get (15.4.7) wbere h.,(R) is the p.d.!. of R for the uniform case given by (15.4.5). On integrating (15."
2420,unknown,"marginal density aT R is (15.4.8) This result is due to Watson and Williams (1956). From (15.4.7) and (15.4.8) it follows (Mardia. 1975b) that 91 R - Mp(sL, KR). (I 5.4.9) otice that the distribution of 9 depends on Runle ss K = 0, wh ich is in contrast to the norma l case where the mean vector and the covariance matrix are independently distributed. T his fact influences inference on J.L. For oth"
2421,unknown,"For other distributional problems, see Exercises IS.4.1-15.4.3. 15.5 Maximum Likelihood Estimators for the von Mises-Fisber Distribution Let I"" i = 1, ...• n, be a random sample from MplJ.L, K). We assume K >0 so that the population is non-uniform. The logarithm of the likelihood function is (15.S.1) where A is a Lagrange multiplier. On differentiating with respect to J.L. we find that Kr=2A "" . M"
2422,unknown,"Kr=2A "" . MULTIVARIATE ANALYSrS 438 Using 11'11 = 1. we find that it= r/R = 10 , ( IS.5.2) Differentiating (15.5.1) with respect to K and substituting (15.5.2) gives the m.l.e. of K as the solution of (IS.5.3) where R = n-' R. On USing (15.3.7) and (15.3.11), we find that c~K)lcp(K)= - A (K), (15.5.4) where A (K) is defined by (15.3.10). Hen ce (15.5.3) becomes A(K)=R or K=A -'(R). (15.5.5) The fu"
2423,unknown,"The functions A(K) and A -I(K) are tabulated for p = 2 and p= 3 (see. fOT examp le. Mardia , 1972). Using for large K the asymptotic formula (15.5.6) we have (15.5.7) Hence , for large K, K~t(p-I)/(I-R). (lS.5.8) This approximation can be trusted for R ?-0.8 when p = 2 and R ?>0.9 when p=3. Since '.(K) can be written in a series as (15.5.9) it follow, that, for small K , (15.5.10) so that (IS.5.11"
2424,unknown,"so that (IS.5.11) This approximation can be used for R.""O .1 when p=2 and for R<O.05 when p= 3. 11 can be seen that tbe m.l.e. of a is 9. Further, differentiating the log likelihood, log L, twice and using the following result from (15.3.12), E(r) = IIA(K)I1= IIA(K)u(a), 439 OIRECTtO , Al DATA we obtain (Mantia. 1975a ) { iJ' log L} _ . E aK' - nA (K). i=l. ...• p-I. (15.5.12) E{ a'IOgL}= o. a""""aa"
2425,unknown,"E{ a'IOgL}= o. a""""aa, i>' j. { a' log L} ,-. E , = nKA (K) n sin' ""',. atli 1-0 i=l •...• p -1. (lS.5.13) Hence . for large n, we conclude from Section 4.2.1 that K, 6, ..... 6._,. are asymptotically independently normally distributed with means K. a l•·· .• 0 p _ lo and var (K)=[nA '(Klr'. (155.14) _ 1 { .-, }-. va! (8,) = - KA(K ) n sin' a, ' = 1 ..... P -1. (15.5.IS) "" ,-0 15.6 Test of Uniformi"
2426,unknown,""" ,-0 15.6 Test of Uniformity: The Rayleigh Test Let I,. i = I, ... , 11. be a random sample of M .(v., K). Consider H O: K =O against H ,:K>'O , where 11 is unknown . Let A be the LR statistic. Under H o, the likelihood function is L o= c;, (IS.6.1) whereas on using the m.l.e.s of K and 11 given (15.5.2) and (lS.5.5), we find that L , = {cp(K)}"" exp (m,R). (15.6.2) where (15.6.3) Le t A be the li"
2427,unknown,"Le t A be the likelihood ratio statistic for the problem . From (1S.6.1)­ (1S.6.3), we find that log A = - II{log (c.(R)lc.) + RA(K)}. Note that ,I, is a function of R alone. MULTIVARiATE ANALYSrS 440 We now show that A is a monotonically decreasing function of R. First think of log A as a function of R. Using the following result from (15.5.4), (15.6.4) we obtain d(lo~ A) _ /IRA '(0<). dK (15.6.5"
2428,unknown,"dK (15.6.5) Since R;;. 0, to show that (15.6.5) is negative it remains to establish (15.6.6) If u(9) is distributed as Mp«I,O ....• 0)'. K), we have, from (15.3.12), E(cos 8,)= A(K); (15.6.7) that is, from (JS.3.18), ~ A(K)= Cp(K) Jcos O,e""-"" sinP - ' 8, d8,. Cp _l "" On differentiating both sides with respect to K and using (15.6.4) and (15.6.7), we find that A '(K ) = V (cos 8,);;, O. (15.6.8) He"
2429,unknown,"Hence A is a monotonically decreasing function of K. Finally. on differen­ tiating (15.6.3) with respect to K and using (15.6.6), it is found that K is a monotonically increasing function of R. Therefore the critical region of the Rayleigh test reduces to R > K. (15.6.9) This test is wbat we should expect intuitively since, under the hypothesis of uniformity, the value of R will be mall. The p.d.f"
2430,unknown,"The p.d.f.s of R under H o and H, are given by (15.4.6) and (15.4.8), respectively. For large n, we show that asymptotically under H o. p1lR 2 - x~. (15.6.10) From Section 15.3.1, E(I)= O, V{I)= p-II. Hence r is distributed asymp­ totically as Np(O, (n/p)I). Consequently, the asymptotic distribution of R 2 = r'r can be obtained w hich leads to (15.6.10). The result for p = 3 was first proved by Ra"
2431,unknown,"The result for p = 3 was first proved by Rayleigh (1919). For approp­ Iiate tables and further discussion, see M ardia (1972a). This test is the uniformly most powerful invariant test (see Exercise 15.6.2) for testing uniformity against the alternative of a von Mise5-Fisher distribution. 441 DlREcrtONAL DATA Example 15.6.1 From the vanishing angles of 209 homing pigeons in a clock resetting experi"
2432,unknown,"clock resetting experiment (Schmidt-Koenig, 1965: Mardia, 1972a, p. 123). it is found that "" = -7.2098 , '2 = 67.3374, R=0 .3240. ft is expected that the pigeons have a preferred direction. We have 2nR' = 43.88. The upper 1% value of xi is 13.82. Hence we reject tbe hypothesis of uniformity. 15.7 Some Other Problems There are various inference problems which arise in directional data. 15.7.1 Test "
2433,unknown,"15.7.1 Test for the mean direction Consider the problem of testmg H Q :,,=(1.0.0 •... ,O)'=e, against H ,:"" ;'e,, (15.7.1) where K i~ known and is non-zero. Let A be the likelihood ratio test for the problem . U sing (15.5.2), we find that the critical region is - 2 log A =2 K(R - ,,» K. (15.7.2) When H o is true, w e ohlain from Wilks' theorem that, asymptotically as f1 -+ x, (15.7.3) For another"
2434,unknown,"f1 -+ x, (15.7.3) For another proof of (15.7.3) for large K . see Exercise 15.7.1. This approximation is valid for II;;' 20 and K ;;' 5 if P = 2, K ;;' 3 if p = 3. Better approximations for p = 2. 3 are given in Mardia (1972, p. 115 and p. 247). When K is unknown. we can again obtain the likelihood ratio test. For a discussion of thi and other exact tests see Mardi. (1972). However, for large K, t"
2435,unknown,"large K, the foUowing procedure of Watson and Williams (1956) is adequate. Since in polar coordinates, we have {rom (15.3.19) that approximately . 2K(n- ',)=2K I (1-cos 8.,)-x!(P-')' (\5.7.4) .-1 MULTIVARIATE ANALYSIS 442 Now 2K(n-'I) = 2K(R -,,)+2K(II-R ). (15.7.5) From an application of Cochran's theorem to (15.7.3) and (i5.7.4) it can be shown that (15.7.6) approximately for large K. Further th"
2436,unknown,"(15.7.6) approximately for large K. Further the two terms on the right-hand side of (15.7.5) are approximately independently distributed. Hence for testing (15.7.1) with large but unknown K we can use the statistic Fp _,.(._"",P_1l = (n - 1)(R - ,,)I(Il - R ), (15.7.7) where Ho is rejected for large values of F. For a general mean direction """". "" should be replaced by .'1';. in (15.7.7). Then this "
2437,unknown,"(15.7.7). Then this test for testing H o: .. ="""" against H , : .. '"" """" becomes Fp_ l.ln_ ""'p_"" = (,1 - 1)( R - r'"",,)I( .. - R) and H o is rejected for large values of F. This test can be used when r'"""""",tn if p=2 and when r'''''''~n if p= 3, for all sample sizes 11 "",2. Thus. under H o. P{(1I-1)(R-r'"",,)/(n - R) """" Fp _,.,.-"",v-II,.} = \-ClI, where Fp -U,,-l)(p-l).n. is the upper a percentage po"
2438,unknown,"where Fp -U,,-l)(p-l).n. is the upper a percentage point of the F p_l"" ... - U(P - Il distribution. W riting r'"""" = R cos 8. the above inequality can be rear­ THnged to give cos 8"", 1-(II - R )Fp _""(' _1)(p_II,.J[(n- 1 )R], which gives a 100(1-01)% confidence interval for Ii and hence for """". In three dimensions the vectors """" allowed by this value of Ii lie within a cone of semiangle 8 and axis t"
2439,unknown,"cone of semiangle 8 and axis through the sample mean I (see Figure 15.7.1). E xample 15.7.1 In an early Quaternary zone of lava flows from Western Iceland, it was found for 45 readings (Hospers' data cited in Fisher, 1953) that the sums of the observed directions we re ,,=-37.2193, '2= -116127, ,,=+0.6710. (15.7.8) The sample dipole field appropriate to the geographical latitude is found 10 have t"
2440,unknown,"10 have the direction 0.9724, 0.2334, o. (15.7.9) It is enquired whether the direction of magne tization differs from the reversed dipole field. 443 Isomo,. '"",u""o"" <~--- I ----> \ ..... - - I \ / \ \ \ \ \ \ \ \ I \ I o DIRECTIONAL DATA Figl~ re 15.7.1 Omfidence cone for rllt true mean direction. Under HOI. we may take the true mean direction as the reversed dipole field. i.e. from (15.7.9) the c"
2441,unknown,"1"", = -0.9724. 1""2 = -0.2334. 1-'3 = o. (15.7.10) We bave. R = 38.9946. Since if is large, we use the approximation (15.5.8) [or estimating K. It is found that K =7.3. Also. 1'-,', + 1'-2'2 + JL3', = 38.9025. Hence the observed value of (15.7.7) is 0.67. The 5% value of Fa8 is 3.10. Hence the null hypothesis is accepted al Ihe 5% level of signifi­ cance . For this data the confidence interval (or "
2442,unknown,"(n - I)R(I-cos 8)/(1l-R) """"3.10. This gives cos 1)""'0.9891, or 8""""8.4°. Thus the semiangle of the 95% confidence cone is 8.4°. 15.7.2 Principal compone nt analysis Let I;, i = 1, ... , n, be a random sample of unit vectors. We can sensibly define tbe fi'st p,illcipal di,ection as the direction a (a'a = 1) which MULTIVARIATE A<""IlALYSlS 444 minimizes the angles /I, between a and I"" i = 1, ... , n; "
2443,unknown,"Q = f cos' 8, = f (a'I;)' (15.7.11) .-1 i_I is maximum. The square of the angle ensures that we are talking of angles between two lines a and I, passing through the origin, so tbat for this purpose each I, is considered as an axis rather than a direction. Let "" T= \' 1·1' L, ,. (15.7.12) be the matrix of sums of squares and products. (Notice that the mean has not been subtracted off.) Suppose that"
2444,unknown,"not been subtracted off.) Suppose that A, .... , Ap are the eigenvalues of T (A, > ... > Ap) and 1'(11<'"" , Y (P) are the corresponding eigenvectors. In view of lili = 1, trT=n=A,+ ... +Ap . (15.7.13) Further maximizing (15.7.11) is equivalent to maximIzing Q=a'Ta subject to a'a = L Using the argument of principal compon~nt analysis (Chapter 8), we conclude that the maximizing value of a is a = 1'"
2445,unknown,"represeots tbe first principal direction. aod max Q = A"" which measures the concemration around this axis. We can Similarly talk of the second principal direction b, where b is perpendicular to a. Of course b = 1'(2) and so On. However. note tbal Ap is specified from (15.7.13) as SOon as A"" ... , Ap _' are known. If A,': ... ,: Ap , the angular distances will be almost the same for all a. Hence , "
2446,unknown,"Hence , in this case, the sample distribution is approximately uniform. We have not dealt with the distributional problems. but the maximum likelihood estImators of the parameters in the Bingham distribution neatly tie up with these eigenvalues and eigenvectors. (See Exercise 15.7.5) This leads to the solution of various other inference problems. For example. it is natural to use the following cri"
2447,unknown,example. it is natural to use the following criterion for testing uniformity for axial data. This has been studied by Bingham (1964) and Mardia (I 97Sd). There is a wealth of recent statistical thinking in this area and we refer to Mardia (1975d ) and Downs (1972) for a review of recent developments. 445 DIREcnONAL DATA Example 15.7.2 Mardia (1975d). Tyror (1957) analysed the distribution of tbe d
2448,unknown,"of tbe directions of perihelia of 448 long-period comets (with periods greater than 200 years up to and including comet 1952f). From the accretion theory of Lyttleton (1953, 1961), one expects the following behaviour of the distribution of these directions: the distribution should be non-uniform, and, in particular. the perihelion points should exhibit a preference for lying near the galactic plan"
2449,unknown,"The various statistics are as foUows. R =6 3.18, IQ = (-0.0541, -0.3316. 0.9419). [ 0.289988 -0.012393 0.030863] TIn = -0.012393 0.389926 -0.01l153 . 0.030863 -0.011 153 0.320086 where T denotes the matrix of sum o( squares and products. It may be noted that if we apply the Rayleigh test which assumes the alternative of a Fisher distribution. we have 3 PlR2-xi. The observed value 3 nR'= 26.3, wbic"
2450,unknown,"wbich is significant at the 0.1 % level. The values of the eigenvalues and the eigenvectors of TIn are as follows where A, = Aj,!: A, = 0.3947, A2 =0.3347 . A3 =0 .2705. Y (ll = (-0.1774, 0.9600, -0.2168)'. Y (2l = (0.4922, 0.2773. 0.8251)', y (J) = (0.8522,0.0397. -0.5217)'. Disparate values of A, indicate a lop·sided girdle distribution. Further, the normal to the preferred great circle (contain"
2451,unknown,"normal to the preferred great circle (containing the girdle) has the direction cosines 1=0.8522, In =0.0397 , II = -0.5217. (15.7.14) Now the direClion of the galactic pole is (0.8772, -0.0536. -0.4772) and the angle between this direction and the direction of the eigenvector (15.7.14) corresponding to A, is only 6.08"". Hence there is strong evidence thaI the Lyttleton theory is true. The large va"
2452,unknown,"The large value of R and the disparate values of A, suggest that this data comes from some distribution which combines the features of both the von Mises-Fisher and Bingham models. For further statistical analysis of this data, see Mardia (1975d). MULTIVARIATE ANALYSIS 446 Exercises and Complements 15.2.1 Show that R is invariant under orthogonal transformations I: = Ali' i = 1 ..... n. 15.2.2 Sho"
2453,unknown,"Ali' i = 1 ..... n. 15.2.2 Show that R = 0 for the points {cos (21Ti/n). sin (2'lTi/n)}. i = I, ...• n. (These points are called ""uniform scores"" on the circle.) IS.2.3 The following stopping positions were observed for nine spins of a roulette wheel marked 0°,1°, ... ,359°. Show that 1 -L cos 8,=0.4470. ,. ~ L sin 0, =0.5529 . n Hence. show that R= 0.71, 0=51°. Doe s the data indicate a preferred"
2454,unknown,"direction? 15.3.1 Using (15.3.7) and (15.5.9), show that lim._n Cp(K) = cp' IS.3.2 For p=3, show that (15.3.18) gives (with 9,= 9) 0"" integrating over (0. 7T), prove that the normalizing constant is C3(K)/C,= K/(2sinh K). Hence deduce (15.3.16). 15.3.3 (Mardia. 1975b) Denne an analytic function I!(z) = f (z/4)' . ._o[k! (v+ 1) ... (v+k)] (a) Using (15.5.9) show that 1!(z) is related to the modifie"
2455,unknown,"function by (b) Show tbat the normalization constant for the von Mises-Fisher distribution can also be written (c) Using analytic continuation on the normaLization constant, show 447 DlREcrtONAL DATA that the characteristic function of the von Mises-Fisher distribution is 0/1(1:"",. K)= J cp (K)exp(it'l+Kj1'I)dSp '£5, = J~2_ .(,,'-I't+ 2i""I'"",)/ 1:12_,(K2). (d) Deduce that E(I) = A(K)"", 15.3.4 Let"""
2456,unknown,"E(I) = A(K)"", 15.3.4 Let"" be distributed as N.c.., ,,- 'I) with .,.',,= J. Show tbat the conditional distribution of"" given ,,',,= 1 is M""c.., ,,). IS.3.S Let II be p(O,1;). Sh ow that the cond itional distribution of "" given ,,'x = I leads lO a density of Bingham form. 15.3.6 If I is distribllted as B p(A . K ), and G is an orthogonal matrix. show that 1* = GI is distributed as B.(GA, K). Hence t"
2457,unknown,"constant depends only on K . Further. using the polar transformation, sbow that [d(KW' = J .. , J J ,0, exp {K,u:(9)}up (0) dO. H 11 0 On expanding the exponentials and illlcrchangir:g the summation and integration. show that d(K) reduces to (15.3.21). 15.3.7 If I ,s uniformly distributed on Sp, show that I"" =Gl is also uniformly distributed where G is an orthogonal matrix. 15.3.8 (See, for exampl"
2458,unknown,"15.3.8 (See, for example, Mardia. 1972a ) (aJ Let 0 be uniformly distri­ buted on (0.2"".). Letting E(e''')=<I>. denote the Fourier coefficients of 8 for integer p, show that 4>. = 1 if P = O. and <p. = 0 otherwise. (b) Let two independent variables 8, and 9, be distributed on a circle with 9, uniform. Using the Fourier coefficients or otherwise, show that (6, + 82 ) mod 21T is again uniformly dist"
2459,unknown,"IS.3.9 (Ma rdia, 1975c, d) Maximum entropy dlGraclerizalions let t(l) be the p.d.!. of I. The entropy of the distribution is defined by E{-Iog t(l)}. Show that the m aximum entropy distribution (i) with no constraints on the moments of I. is the uniform distribution: (ii) with E(I) fixed, non-zero, is M.c.., K); (iii) with E(ll') fixed is B p(A, K). MULTIVARlATE ANALYSIS 448 15.3.10 (von Mises, 19"
2460,unknown,"MULTIVARlATE ANALYSIS 448 15.3.10 (von Mises, 1918; M. S. Bingham and Mardia , 1975) Maximum likelihood cha,acterizaljoltS Let f(l; /L) be a p .d.f. on Sp with mean direction /L and p > O. If (i) for all random samples with R > 0, 10 is a max imum likelihood estimator of /L, and (ii) f(l;/L) = g(1'/L) for all IES"", where g is a function of one variable and is lower semi·continuous from the left at"
2461,unknown,"and is lower semi·continuous from the left at 1, then J has a von Mises-Fisher distribution. 15.3.11 (Mardia. 1972a, p. 114; Kent, 1978) Show that as K-""', M.Cp, ,,) converges to a multinormal distribution with mean /L and covariance matrix ,,-' times the identity 00 the hyperplane tangent to the sphere at /L. (HIDt: using Exercise 15.3.3 aDd the asymptotic formula z~oo, t argz l<~1T-8, where 8 is"
2462,unknown,"as K ..... cc for each I, where 1: =}-....... J 15.4.1 Letl be uniformly distributed on Sp' Sh ow that the p.d.f. of r is cP',""(R)/RP -'. where h""(RJ is the p.d.!. of R = (r'r)'I> given by (15.4.6). 15.4.2 (Mard ia.1975b) If in the isotropic random walk, the successive steps are of different lengths f3"" ... ,f3"" then show that the c.t. of I, is c.Jc.(iPf3,) rather than (15.4.4). As in Section 15.4"
2463,unknown,"c.Jc.(iPf3,) rather than (15.4.4). As in Section 15.4.1, prove that (i) ij and R are independent; (ii) ij is uniformly distrihuted and the p.d.f. of R is fl. (R , Pl = (211')-·C;·1 W -1 f pP -'{cp(ipR ) fI cp (iPf3/W' dp. 1-' o Using (15.4.4)-(15.4.5), express the p.d.f. of R in terms of ordinary Bessel functions. 15.4.3 For /L = (1,0, ... ,0), show that 'I is a sufficient statistic for "". Hence d"
2464,unknown,"Hence deduce that the distribution of R I "" does not depend on K. (For the p.d.f. of R I,,, see Stephens (1962 ), Mardia (1975b ). 449 DIREcnO N AL DA'TA 15.5.1 For /L = (1, 0, ... ,0)', (i) show from the Rao--Blackwell theorem that , ,Ill is the best un· biased estimator of A(K), and (ii) show that the m.l.e. of K is A -'(,,). 15.5.2 Prove that A'(K) = [K - (p- l)p- Kp2]JK where p= A (K) and expr"
2465,unknown,"where p= A (K) and express (15.5.14) and (15.5.15) in terms of p and K. (Hence , for tabulated values of p= A(K ), the asymptotic variances of K and ij can be ob tained.) Using (15.5.7) and (15.5.10), examine the effect as K - 0 (uniformity) and K _ x (normality) on these variances. 15.5.3 Writing the log likelihood (15.5.1) as 10gL = co,lSIanl+nlogc(K) + I1K{T,/-LI + ... + 1.-,1',.-1 ± 1.0 - 1< i"
2466,unknown,"+ I1K{T,/-LI + ... + 1.-,1',.-1 ± 1.0 - 1< i -... -/-L;_I)'12} show that Efa> log L}= 0, OKOP., Hence. show that the distributions of K and (iLlo .... iLp-l) are asymptoti· cally independent normal but iL, •... , "";""'-1 are no longer independent. 15.6.1 Let I"" .... 1. be a random sample from M p(e"" K) where e, = (1,0, .... 0)'. For testing H o: K = 0 against H , : K~O, show that the eyman-Pearson "
2467,unknown,"show that the eyman-Pearson lemma leads to the best critical region of the form ',> K. Why is this critical region expected intuitively? What is the asymp totic distribution of "" under H o'! 15.6.2 (Beran, 1968 ; Mardia , 1975b ) From the eyman-Pearson lemma show that the uniformly most powerful test for uniformity in· variant under rotation with M pCp , K) as the alternative has a critical region"
2468,unknown,"region of the form [cp(K)]"" f ,1]. {exp (KI'l,)) dl> K ; .. MULTIVARIATE ANALYStS 450 that is, the test is defined by Using A(K);;'O, Cp(K);;'O. and c~(K)=-A(K)Cp(K), sbow that Cp(K) is a decreasing function of K. Hence show that this test is the same as the Rayleigh test. 15.7.1 Let 1"" ... , I. be a random sample from Mp(e"" K), where eJ = (1. 0, ... ,0)'. (a) Show that From (15.3.19), deduce that"
2469,unknown,"From (15.3.19), deduce that. for large K. 2K(R - 'I) is approximately a X;-l variable. (b) For large f1, show that, approximately R - N(p.A'(K)/n). 15.7.2 (Mardia, 1975b) For a sample of size fI from M pw., K) where K is known, show that a (1-a)100% confidence COne for I'"" can be con­ structed from (15.4.9) as follows. Since cos-' (1'1'"") I R is distributed as (15.3.l7) with K replaced by KR , let"
2470,unknown,"(15.3.l7) with K replaced by KR , let P(cos J (i'l1) >.s I R ) = a. Then show that for given K and R, the probability that the true mean direction lies within a cone with vertex at the origin, axis through j and semi-vertical angle .s is 1- a. 11 K is unknown , it can be replaced by K and the method provides an approximate confidence cone for 11. 15.7.3 (Watson , 1956) Let F. be the upper a critic"
2471,unknown,"F-ilistribution with degrees of freedom p - 1 and ("" - J)(p - 1). For large K , from (15.7.7) show that a 100(1-a )% confidence cone for 11 is a cone with vertex at O. axis through the sample mean direction and semi-vertical angle 5, where 5 = cos-' [I -{en - R )F./(II- l)R}]. 15.7.4 (Watson and Williams, 1956) Let R I • . .. , R., denote the resul­ tant lengths for q independent random samples of"
2472,unknown,"tant lengths for q independent random samples of size "", •... , n. drawn from M o(l1, K). Let ,,= L It;. Suppose that. R is the resultant of the combined sample. For large K, using tbe Watson-Williams approxima­ tions (15.7.4) and (15.7.6), sbow that. approximately. where these two random variables are independently distributed. Hence. for large K, construct a test of equality of the mean directio"
2473,unknown,"for large K, construct a test of equality of the mean directions. 451 DIRECTIONAL DATA 15.7.5 (For p=3see Bingham , 1972; also Mardia, 1972a ) Let I, .... , I. be distributed as Bp(A , K ). Suppose that K,'"" K,,,,;; ... .,. K p _ ' .,;: Kp and Kp = O. Show that the m.!.e. of s(i) equals 'Y(I)' Further, the m.l.e.s of K h • • , , Kp _J are the solutions of a log de,,) 1 -A j, dKj n i=l, ... ,p-l. w"
2474,unknown,"dKj n i=l, ... ,p-l. where A,;;' •.. ;;. Ap are the eigenvalues of T and 'Ym,· .. ,'Y(p) are the corresponding eigenvectors. Appendix A Matrix Algebra A.I Introduction This appendiJt gives (i) a summary of basic defimtions and results in matrix algebra with comments and (ii) details of those results and proofs which are used in this book but normally not treated in undergraduate Mathematics course"
2475,unknown,"Mathematics courses. It is designed as a convenient source of reference to be used in the rest of the book . A geometrical interpretation of some of the results is also given. If the reader is unfamiliar with any of the results not proved here he should consult a text such as Graybill (1969, espe­ cially pp . 4-52.163-196. and 222-235 ) or Rao (1973. pp. 1-78). For the computational aspects of mat"
2476,unknown,"computational aspects of matrix operations see for example Wilkinson ( 1965). Definition A matrix A is a rectallgular array of ""umbers. If A lJas "" rows and p columns we say it is of order "" X p. For example. n observations all p random variables are arrallged ilt this way . Notation 1 We write matrix. A of order n x p as a"" a12 a •• a2l 0,2 a,. A = =(a.,). (A. 1.1 ) a.. a., a.. where a., is the e"
2477,unknown,"a.. a., a.. where a., is the elemen t in row i and column j of the matrix A . i = 1, .... ,t; j = 1, ... ,p. Sometimes. we write (A )"" for a.;. 453 APPENDLX A MATRIX ALGEBRA We may write the matrix A as A(n x p) to emphasize the row and column o~der. In general, matrices are represented by boldface upper case leltcrs throughout this book, e.g. A. B , X , Y , Z . Their elements are represented by s"
2478,unknown,"represented by small letters with SUbscripts. Definition The transpose of a matrix A is formed by iltlerchanging the rows and columns : a u a21 aft 1 G I2 a22 Q .. 2 A' = Definition A matrix with column-order aile is called a column vector. TI.us Q , a = is a column vector with n component s. In general, boldface lower case leiters represent column vectors. Row vectors are written as column vector"
2479,unknown,"vectors are written as column vectors transposed, i.e. a'=(a lt··· , ~). Notation 2 We write the columns of the mairix A as 8;"", ""12., ...• a.., and the rows (if written as colum n vectors) as a:. aj, ... ,a,! so that where a' n (A .1.2) fl.1ULTIVAR 1ATE ANALYSIS 454 Definition A matrix w ritten In terms of its sub-matrices is called a partitioned matrix. Notation 3 Let A,,, A rl• A 21> and A n be"
2480,unknown,"partitioned matrix. Notation 3 Let A,,, A rl• A 21> and A n be submatrices such that AlI(,x s) has elements Il;j. i=l, ... ,r; j=l, ...• s and so on. Then we write A (n X p)=[ A II(rx s) A 2 ,«n-r)xs) A'2('X (P-S» ] A 2 ,(n-r)x(p-s» . Obviously. this notation can be extended to contain further partitions of A ll' A 12 • etc. A list of some important types of particular matrices is given in Table A"
2481,unknown,"A.I.1. Another list which depends on the next section appears in Table A .3.L Table A.l.l Particular matrices and types of matrix (List J). For List 2 see Table A,3.L ame Scalar 2a Co lumn vector 2b Un.it vector 3 Rectangular 4 Square 4a Diago nal 4b Identity 4c Symmetric 4d Unit matrix 4e Triangular-matrix (upper) Triangular matrix Oowe,) 5 Asymmetric 6 Null Defini(on p = "" = 1 p= l (1. .... 1)' "
2482,unknown,"p= l (1. .... 1)' p = n diag (1) G;j= a"" p = n. Q.;j= 1 a"" = 0 below the diagonal aij = 0 above the diagonal a.=O N otation a.b J or 1 .. A ("" x p ) A (p X p ) diag (0;,) 1 Or I, 1,=11 ' A ' o Trivial Example s (I) G) C) (~ ~ ~ 3 2 ; ) G ~) (0 0 0) 0 00 455 APPENDIX A ~fATRlX ALGEBRA As shown in Table A .Ll a square matrix A (p x p) is diagonal if a'j = 0 for all ito j. There are two convenient wa"
2483,unknown,"for all ito j. There are two convenient ways to construct diagonal matrices. U a = (a .....• a,,)' is any vector and 8 (p x p) is any square matrix then ( a, diag(a)=diag (a,)=diag(a, •...• a.)= ~ and ( bll Diag(B)= ~ each defines a diagonal matri'(. A.2 Matrix Operations T able A.2. L gives a summary of varIOus important matrix operations. We deal with some of these in detail .• assuming the defi"
2484,unknown,"Tablt. A .2.1 Basic matri.'{ operations Operation R estrictions Definitions Remarks I Addition A. B of the same order A + B ~(a;,+ b,,) 2 Subuaction A . B of the same order A-B ~(a;,- b,,) 3a Scalar multiplication cA = (CG ij) 3b Inner product •. b of the same order .1> ~ L a;b, 3c Muhiplkation Number of columns of A equals number of rows of B AB = (a:b"",) AB ;<JlA 4 Transpose ,..'=('a •. •2 . · ."
2485,unknown,"5 Trace A SQuare tT A. = [a, • Section A .2.2. 6 Determinant A square IA I Section A .2.3. 7 fm 'erse A square and IA I"" 0 AA -1= A -1A = J (A + 8 )-1 ¢ A -1+B - ', Section A .2.4 8 g -inve"""" (A -) A ("" X p) AA-A ~ A Section AS A.2.1 Transpose The transpose satisfies the simple properties (A')'=A . (A + B )'= A '+B ', (AB )'= B 'A'. (A .2.1) MULTIVARIATE ANALYSIS For partitioned A . A ,=[A"" A ZI]."
2486,unknown,"A ,=[A"" A ZI]. A ;l Ah If A is a symmetric matrix. aij = asn so that A '=A . A.2.2 True 456 The trace function. tr A = la"". satisfies the following properties for A (p X pl. B (p X p), C (p x n). D (II X pl. and scalar a : lfa = a. tr A±B = tr A ± tr B . traA=a irA (A.2.2a) tr CD = tr DC = L ci,d"", (A .2.2b) '., Lx :Ax ,=tr(A T), where T = L'X.x:, (A.2.2c) To prove this last property. note that si"
2487,unknown,"hand side of (A.2.2c) is tr L ""fAx, = L tr .:Ax, by tA.2.2a) = L tr Ax ,': by (A.2.2bl = tr A L "" x: by tA.2.2a). As a special case of (A.2 .2b) note that tr CC' = trC'C= L c~. A.2.3 Determinants and col actors (A.2.2d) Definition The delerminanl of a square malrix A is defilled as IA I = L (-I)""""a I.,,,· .. a""""pl' (A .2.3a) where Ihe summalioll is lakell over all permUlalions T of (I. 2, .... p)."
2488,unknown,"ITI equals + 1 Or -1. depending 011 whelher T call be wrillen as Ihe producl of all even or add number of Iransposilions. For p = 2_ (A.2.3b) Definition The cofactor of 0"" is defined by (-I)' -, limes lite minor of a"" . wltere tI.e minor of a"" is Ihe value of Ihe delerminanl oblailled afler delelillg the ilh raw alld the jlh colUllln of A . We denote the cofactor of a., by A.,. Thus for p = 3. a23"
2489,unknown,"a231· a.:n 0 22 1. 0 3 2 (A.2.3c) 457 APPElIffilX A MATRIX ALGEBRA Definition A square mal rix is non-singular if IA I ¥ 0; olherwise il i.r singular. We have the following results: • • (I) IA I = I a.,A"" = L ai,A. .. any i. j. ,-I but 0-1 (ll) If A is triangular or diagonal. (UI) leA l = cp lA I. (IV) IAB I = IA IIB I· IA I= n aU' i,. j. (V ) For square submatrices A (p x p) and B (q x q). (A.2.3"
2490,unknown,"(A.2.3d) (A.2.3e) (A .2.30 (A.2.3g) (A.2.3h) I ~ ~ 1 = IA IIB I. (A.2.3i) I All Aul -I - I = IA""IIAn - A 2IA "" A 121 = IAnIIA "" - AI2An A211. A21 An (A .2.3j) I:' ; I = IAI(b-a' A -I a). (Vi) (Vll) For B(P X n) and C(/I x p). and non-singular A(P X p). IA + Bq= I lip + A - IBq=lAllln + CA -IBI, IA + b'al= IAI (1 + b' A -, a). (A .2.3k) Remarks (1) Properties (I}-(llD foUow easily from the definiti"
2491,unknown,"(A.2.3a). As an application of (T), from (A.2.3b). (A .2.3c). and (A .2.3d), we have. for p = 3. IA I = 0,,(0220,,-a""a,,) - 0 \2(0 21 0 '3 - a2,a3l).,. an (a21 a32 -a31 0 22)' (2) To prove (V). note that the o nly permutations giving non-zero terms in the summati on (A .2.3a) are those taking {l, ...• p} to {l, ... , p} and{p+I. ... ,p+q} to {p+l •...• p+q}. (3) To prove (VI). simplify BAB ' and t"
2492,unknown,"(3) To prove (VI). simplify BAB ' and then take its determinant where B = [I -A,2A,i] o 1 . From (VI). we ded uce. after putting A "" = A . A 12= x'_ etc .. I~ : I = IA l{c -x' A -'x}. (A .2.31) MULTIVARIATE AI>ALYSIS 458 (4) To prove the second part of (VTI), simplify l ip - A -IB I C I. using (VI ). As special cases of (VII) we see that, for non -singular A . IA + bb'l = IA I (t + b'A -'bl. and t"
2493,unknown,and that. for B (p x rt) and C(rt x pl. lip + B C! = lin + CB I. (A .2.3m ) (A.2.3n) In practice. we can simplify determinant~ using the propeny that the value of a determinant is unaltered if a linear combination of some of the column s (rows) is added 10 another column (row). (5) Determinants are usually evaluated on computers as follows. A is decomposed into upper and lower triangular matrices 
2494,unknown,"decomposed into upper and lower triangular matrices A = LU. If A >O. then the C holesky decomposition is used (i.e. U = L ' so A = LL'). Other­ wise the Crout decompos ition is used where the diagonal elements of U are ones. A.2.4 Inverse Definition As already defined in Table A. t. l. the ,""verse of A IS the unique matrix A ' satisfyirtg AA - '=A -IA = I (A.2.4a) The inverse exists if and on ly i"
2495,unknown,"The inverse exists if and on ly if A is non ,singular. thar is, if and only if IA I1'O. We write the (i. j)th element of A - I by a'i. For partitioned A. we write The following properties hold: (I) A -, = I ~I (A ,,)'. (IT) (eA )-' =C'A -I (III) (AB )-'=B -'A -'. AIZ] A n' (IV) The unique solution of Ax = b is x = A -'b. (A .2.4b) (A.2Ac ) (A.2.4d) (A.2Ae ) (V) If all the necessary inverses exist."
2496,unknown,"C(rt x n). and D (n x pl. A (p x pl. B (p x n), (A+ OCD j-l=A -1-A -'B (C-1+ DA -'B r'DA - 1, (A + a b,)-I=A -1_ {(A -I a)(b' A -')(1 + b' A -I a)-I} (A.2.4f) 459 APPENDIX A MA1'RJX ALGEBRA (V I) Ii all the necessary inverses exist. then for partitioned A. the eleme nts of A -I are (VII) A "" = (A "" -AI2A2~A'l)-I. AI2=-AlIA I2A2~' A ""-(A A A -IA )-'} - 22- 21 11 12 • A '21 = -A;~ A2IA II. Alternati"
2497,unknown,"A '21 = -A;~ A2IA II. Alternatively, A ,2 and A 2' can be defined by (A.2.4g) A l 2=_A~ll A I2A U • A 21 =-A 22A 2IA ll'. For symm etrical matrices A and D . we have, if all necessary inverses exist ( A B )-'=(A - l 0) ( E) 0 ' D 0 0 + - , (D - B 'A-'O )-I(-E',I) where E=A -lB . Remarks (1) The result (I) follows on using (A.2.3dl. (A.2.3e). As a simple application. note that. for p = 2, we have A"
2498,unknown,"simple application. note that. for p = 2, we have A -' 1 ( a"" -al') altQ22-QllQ :2!1 -all all (2) Formulae (IIHVI) can be verified by checking that the product of the matrix and its inverse reduces to the identity m atrix, e.g. to verify (111), we proceed (AB) -'(AB ) = B -'A -'(AB ) = B -'IB = I. (3) We have assum ed A to be a square matrix with IA I""'O in defining A -I. For A (n x p), a generali"
2499,unknown,"A -I. For A (n x p), a generalized inver e is defined in Section A.S. (4) In compute r algorithms for evaluating A -'. the following methods are comm only used. If A is symme tric. the C holesky method is used. namely, decomposing A into the form LL ' where L is lower triangular and then using A -I = (L-I),L- I. Fo r non -symmetric m atrices. Crout's m ethod is used, which is a modification of Gau"
2500,unknown,"m ethod is used, which is a modification of Gauss ian elimination. A.2.5 Kronecker products Definition Let A =(a,;) be all (In Xn) matrix and B =(b .. } be a tpxq) matrix. Th eil the Kronecker product of A and B is defi/led as ""liB a 12B alnB U 21 B anB Q2n B Q,"" ,8 a"",2B arn""B "",/,iell is an (mp x IIq) matrix. Ir is d.rtoted by A ® B . MULTIVARIATE ANAL ¥SIS 460 Definition If X IS an (/1 X p) mat"
2501,unknown,"Definition If X IS an (/1 X p) matrix lei X V denote the np-vector obtained by ""vecwrizing"" X; that IS, by stacking the columns of X 011 lOp of one another so thaI From these definitions the elementary prupenies gIVen below eaSily follow: (1) a(A® B)=(aA)® B =.~® (a B) for all ,calar a. and bence can be written without ambiguity as aA ® B . (Tn A® (B®C )=(A®B)®C. Hence this can be wrinen a, A®O®C."
2502,unknown,"(Tn A® (B®C )=(A®B)®C. Hence this can be wrinen a, A®O®C. (rIl) (A®B )'=A '®B '. (IV) (A ® B )(F® G ) = (AF) ® (BG) . Here parentheses are necessary. (\I) (A ® 0 )-' = A -, ®B -' for non-singular A and B. (Vl) (A+ B l®C = A®C + B®C . (VII) A®(B + C )=A®B+A®C . (V1II) (AXB) v = (O '®A )X v. (LX) tr (A ® B) = (tr A) (tr B). A.3 Further Particular Matrices and Types of Matrix Table A.3.1 gives anothe"
2503,unknown,"consider a few in more detail. A.3.1 Orthogonal matriees A square matrix A(II x /I) is orthogonal if AA' = I. The followillg proper­ ties hold: (I) A -'=A'. (II) A 'A = 1. (ID) IA I=±1. (IV) a~.J = O. ; ~ j; aial =- 1. a:t )~"" = O. i r= j, a:1 )~n = I. (V) C = AB is orthogonal if A and 0 are orthogonal. Remarks (1) All of these properties follow easily from the definition A A ' = I. Result (IV) st"
2504,unknown,"461 APPENDlX A MA TRJX ALGEBRA Table A.3. J Particular types of matrices (List 2) Name Definition Examples Details jn Non-singular IA I""O [~ ~] Section A .2.3. Singular I AI~O [: ~] Section A .2.3. Orthogonal AA'=A'A=J [COS (J sin 8 -sin 6] cos 8 Section A.3.l. Equicorrelation E =(l-p)l+pJ [; ~] Section A .3.2. Idempotent A '=A 1 [ J 2 -1 -:J Centring matrix. RIO R IO = I .. -n- 1J .. Section A.3."
2505,unknown,"Positive definite (p.d.) xlAx>O for all ~y!O x;+x~ Section A.7 . Positive semi- definite (p.s.d.) x'Ax""O for all .,.0 (Xl - X,;)' Section A. 7. row (column) is unity wbereas the sum of the cross-products of the elements in any two rows (columns) is zero. (2) The H elmen matrix is a particular orthogonal matrix whose col­ umns are defined hy .(1)= (n-1/2 • • , •• n -1I2). ai"" = (d, .... , d;. - (i "
2506,unknown,"wbere d, = {iti -In-''', is repeated i- I times. (3) Orthogonal matrices can be used to represent a cbange of basis, or rotation. See Section A.S. A.3.2 Equicorrelation matrix Consider the (p x p) matrix defined by E = (l- p)l+pJ, (A.3.2a) where p is any real number. Then e;, = 1, e"" = p, for i,o i. For statistical purposes tbis matrix is most useful for -(p-lt' < P < 1, wben it is called the eqll"
2507,unknown,"the eqllicorreiarion matrix. Direct verification shows tbat, provided p,o 1, -(p -1)-', then E -' exists and is given by (A.3.2b) MULTIVARIATE ANALYSIS 462 Its determinant is given by iEi=(l-pr'{I +p(p-l)}. (A.3.2c) Thi, formula is most easily verified using the eigenvalues given in Rem ark Ii of Section A.6. A.3.3 Centring IIUItrix The (11 X 11) centring matrix is defined by EI = H "" = 1-,,-'1. w"
2508,unknown,"Then (1) H '=H. H'=H. (II) H1=0. HJ=JH=O. (lU) Hx=x-Xl. where i=,,-'Ix,. (IV) x Hx = ,,-I I (x, -x)'. Remark (1) Property (I) Slates that R is symmetric and idempotent. (2) Property (lIT) is most important in data analysis. The ith element of Ox is x, - x. Therefore. premulriplying a column vector by H has the effect of re-expressing the elements of the vector as deviatio""s from the "",ealt. Simila"
2509,unknown,""",ealt. Similarly. premultiplying a matrix by H re-expresses each element of the matrix as a deviation from ils colu"", .. mea ... i.e. ox has iL. ii, j)lh element Xii - Xj' where :t, is the mean of the jth column of X. This ""centring"" property explains the nomenclature for H . A.4 Vector Spaces, Rank, and Linear Equations A.4.1 Vector spaces The set of vectors in R "" satisfies the following proper"
2510,unknown,"and all A."". E R. (I) A(>:""'y)=h+Ay, (2) (A~""')X =Ax+!L X, (3) p."".)"" = A ("".xl. (4) lx = "", Thus R"" can be considered as a veclO' space over the real numbers R. Definition If W is a subset of R "" such Il,at fo, all "". YEW alld A E R A(X+Y)E W. then W is called a vector subspace of R O. Two simple examples of subspaces of R"" are {OJ and R"" itself. 463 APPE.""'lDIX A MATRIX ALGEBRA Definition VeclOr"
2511,unknown,"Definition VeclOrs x, ..... x. are called linearly dependent if there exist numbers ,1., •.•• ,A .. nOI all zero. sud, that A,x,+ ... +A""X.=O. Otherwise the k vectors a,e linearly independent. Definition Let W be a subspace of R"". Theil a basis of W is a maximal linearl~ independent set of vectors. The following properties hold for a basis of W: (I) Every basis of W contains the same (finite) numb"
2512,unknown,"Tbis number is called the dimension 01 Wand denoted dim W. In particular dim R"" = n. (II) II x, ..... x. is a basis for W tben every element lC in W can be expressed as a linearly combination of x, .... , x.: that is. x= A 1Xl + ... + Aft:!"" for some numbers )'1, ..• J Ak • Definition The inner (0' scalar 0' dot) product between two vee/ors x, y E R "" is defined by "" "". y=x'y= I x.y,. .-, The veCl"
2513,unknown,""" "". y=x'y= I x.y,. .-, The veClors x and yare called orthogonal if "" . y = o. Definition The norm of a vecto, xE R"" is given by 11.11 =(x' x),n = (I x~ )'''. Then the distance beTWeen two veCto's "" and y is given by Ilx-yli. Definition A basis x, .... , x, of a subspace W of R n is called orthonor­ mal if all Ihe elements have nom, 1 and are orlhogol/al 10 One anolher; that is, if , {l. x, xJ = O"
2514,unknown,"that is, if , {l. x, xJ = O. i = j, i'"" j. In particular, if A(n x n) is an orthogonal matrix then the columns of A form an orthonormal basis of RO . A.4.2 Rank Definition The rank of a malrix A(n xp) is defined as the maximum tlumbe, of linearly independetll rows (columns) iot A. MULTIVARIATE ANALYS1S 464 We denote it by rCA) or rank (Al. The following properties hold: (I) 0 <;; rCA) <;; min (n. "
2515,unknown,"(U) rCA) = rCA '). (A.4.2b) (TIn ,(A+B)<;;r{A)+,(B). (A.4.2c) (IV) ,(AB).;;min {rCA), ,(B)}. (A.4.2d) (V) ,(A' A) = ,(AA ') = rCA). (A.4.2e) (VI) If B (nxn) and C(pxp) are non-singular then r(BAC)=r(A). (A.4.2f) (V1l) If II = P then rCA) = p if and only if A is non-singular. (A.4.Zg) Table A.4.1 gives the ranks nf some particular matrices. Remarks (1) Another definition of rCA) is rCA) = the large"
2516,unknown,"those (square) submatrices which have non-vanishing determinants. (2) If we define M(A) a~ the vector subspace in R"" spanned by the columns of A. then ,(A)= dim M(A) and we may choose linearly inde­ pendent columns of A as a basis for M(A) . Note that for any p-vector s. Al< = x'~1) + ... + x""~,, is a linear combination of the columns of A and hence Ax lies in M(A) . (3) Define the null space of A"
2517,unknown,"hence Ax lies in M(A) . (3) Define the null space of A{n x p) by N(A)=b:e R' :Ax=O}. Then N(A) is a vector subspace of R ' of dimension k. say. Let e"" ...• e. be a basis of R' for which e"" ... , e. are a basis of N(A). Then Ae. - I' ... ,Ae. form a maximally linearly Independent set of vectors in M(A ), and hence are a basis for M(A). Thus, we get the important result dim N(A)+dinl M(A) = p. (A.4."
2518,unknown,"dim N(A)+dinl M(A) = p. (A.4.2h) (4) To prove (V) note tbat if Ax=O, then A'Ax=O; conversely if A'Ax=O then x'A'Ax= IiAx IF=O and so Ax=O. Thus N(A)=N(A'A). Since A and A'A each have p columns, we see from (A.4.2h) that dimM(A)=dim M(A'A) so that r(A)= r(A'A). Table A.4.1 Rank of some matrices Matrix Non-singular A(p ""p) diag (a,) "". Idempotent A CAB . non-singular B , C Rank p Number of non ·zero"
2519,unknown,"Rank p Number of non ·zero a, n-t irA ,(A) 465 APPENDIX A MATRIX ALGEBRA (5) If A is symmetric, its rank equals the number of non-zero eigen­ values of A . For general A(II x p), tbe rank is given by the number of nOD-zero eigenvalues of A'A. See Section A.6. A.4.3 Linear equations For the n linear equations "".-.,,+ ... +x,,-.,,= b (A.4.3a) or Ax=b (A.4.3b) with the coefficient matrix A(n x p), we"
2520,unknown,"or Ax=b (A.4.3b) with the coefficient matrix A(n x p), we note the following results: (I) If n = p and A is DOD -singular, the unique solution is (A.4.3c) (ID The equation is consislenl (i.e. admits at least one solution) if and only if rCA) = r[(A, b)J. (A.4.3d) (TIl) For b = 0, there exists a non-trivial solution (i.e. x'"" 0) if and only if r(A)<p. (IV) The equation A ' A = A'b is always consist"
2521,unknown,Remarks (1) To prol(e (II) note that the vector Ax is a linear combina­ tion of the columns of A. Thus the equation Ax = b bas a solutiol;l if and only if b can be expressed as a linear combination of the columns of A . (2) The proof of (m) is immediate from the definition of rank. (3) To prove (IV ) note tbat M(A'A) S;M(A ') because A 'A is a matrix wbose columns are linear combinations of the co
2522,unknown,"Remark 4 of Section A.4.2 we see that dimM(A'A)=dimM(A)= dimM(A') and hence M(A'A )=M(A') . Thus, A""beM(A'A ), and so r(A'A) = rCA' A, A'b). A.S Linear Transformations Definitions The trans!omlotioll from x(p X I) 10 Y( II X I) given by y = Al<+b, (A.S.I) where A is on (n x p) marrix is called a linear transformation. For n = p, MULTIVARIATE ANAL YSlS 466 Ihe transform arion is called non -singula"
2523,unknown,"case Ihe inverse transformalion is ,,= A -'(y- b). A"" orthngonal Iransfonnarion is defined by y = Ax , (A.S.2) where A is an orrhogonal marrix. Geomelrieally, all orrhogonal malrix represe,,'s a rOlalio"" of Ihe coordinale axes. See Secrion A.IO. A_6 Eigenvalues and Eigenvectors A.6.1 General resuUs If A(p x p) is any square matrix then q(A ) = lA - Ail (A.6 .1) is a plh order polynomial in A. The "
2524,unknown,"is a plh order polynomial in A. The p roots of q(A ), A , ..... A,. possihly complex numbers. are called eigenvalues of A . Some of the A, will be equal if q(A ) has mUltiple roots. For each i = I, .... P. - A, II = o. so A - AI I is singular. Hence , there exists a non-zero vector -y satisfying (A.6.2) Any vector satisfying (A.6.2) is called a (righl) eigenveclor of A for the eigenvalue A,. If A,"
2525,unknown,"eigenvalue A,. If A, is complex. then -y may have complex entries. An eigenvector -y w ith real entries is called slarrdardized if (A.6 .3) If x and yare eigenvectors for ,I., and a E R . then s + y and ax are also eigenvectors for .I.,. Thus. the et of all eigenvectors for ,I., forms a subspace which is called the eigenspace of A for A,. Since the coefficient of AP in q(A ) is (-1)"". we can write"
2526,unknown,"of its roots as P q(A)= n (A;-A) . Setting A = 0 in (A.6.1) and (A.6 .4) gives IA I=n A;; (A.6.4) (A .6.S) that is.IA I is the product of the eigenvalues of A . Similarly. matching the 467 APPENDLX A MATRIX ALGEBRA coefficient of A in (A.6.1) and (A.6.4) gives L a,,=trA= LA.; that is, tr A is the sum of the eigenvalues of A . Let C (p x p) be a non-singular matrix. Then IA -AlI=IClIA -A C -'CIIC -"
2527,unknown,"IA -AlI=IClIA -A C -'CIIC -'I=lcAC- '-AJ I. (A.6.6) (A.6.7) Thus A and CAC -' have the same eigenvalues. Further. if -y is an eigenvector of A for A;, then CAC -'(C-y)= A,C-y. so that "" =C-y is an eigenvector of CAC - ' for A,. Let aE R. Then IA+al-Al I=IA - (A-a)ll. so that A+al has eigen­ values A,+a. Further, if A-y =A,-y, then (A + a(}y=(A,+a}y, so that A and A + al have the same eigenvectors."
2528,unknown,"Bound s on the dimension of the eigenspace of A for A, are given by the following theorem. Theorem A.6.1 LeI A, denole any parlieular eigenvalue of A (p x pl. wuh eige""space H of dimension r. If k denOles Ihe m""llipliciry of A, in q(A). lhe"" I.;; r"" k. Proof Since A, is an eigenvalue, there is at least o ne non-trivial eigen­ vector so , ~1 . Let e, ..... e, be an ·orthonorm al basis of H and exte"
2529,unknown,"e •.... ~ e"" f., .. .,fp_f is an orthonormal basis of R P. Write E = (e, ....• e,), F = (I, .... ,1, .• ). Then (E . F ) is an orthogonal matrix so that I, =(E. F)(E, F)'= EE' + FF and I(E,F)I = 1. Also E 'AE = AIE 'E= ,1.,1 •• F'F= I •. ~ and F'AE=A ,F'E = O. Thus q(A) = IA - All = j(E , F)'IIA - A1 11(E , F)I = I(E . F)'[AEE ' + AFF -AEE'-AFF](E.F)I = I(A,-A )I, E 'AF 1 o F'AF - Ai._. =(A , - A "
2530,unknown,"= I(A,-A )I, E 'AF 1 o F'AF - Ai._. =(A , - A j' q,(A). say, using (A.2.3i). Thus the multiplicity of ,I., as a root of q(.I.) is at least r. Remarks (1) If A is symmetric then r = k ; see Section A .6.2. How ever, it A is not symmetric. it is possible that r < k. For example. A =(~ ~) MULTIVARIATE ANALYSJS 468 has eigenvalue 0 witb multiplicity 2; however , the corresponding eigen­ space which is"
2531,unknown,"space which is generated by (1,0), only has dimension 1. (2) If r = 1, then the eigenspace for A, has dimension 1 and the standardized eigenvector for A, is unique (up to sign). Now let A("" X p) and B (p x n) be any two matrices and suppose"";;' p. Then from (A.2.3j) I -H o -A 1= (-A)""-' iBA - H. i=iAB-H. i· B I. (A.6.8) Hence the n eigenvalues of AB equal the p eigenvalues of BA , plus the eigenva"
2532,unknown,"eigenvalue 0, n -p times. The following theorem describes the relation­ ship between the eigenvectors. Theorem A.6.2 For A (n X p) and B (p x n ). rhe non -zero eige""values of AB and BA are rhe same and have the same multiplicity. If "" is a nOn -trivial eigenvector of AD for an eigenvalue A;< O. Ihen y = Bx is a non -lrivial eigenveclor of BA . Proof The first part follows from (A.6.8). for the se"
2533,unknown,"ing y = Bx in the equation B (ABx) = AB"" gives BAy = Ay . The vector x is non -trivial if x"" O. Since Ay = ABx = Ax;< 0, it follows that y '"" 0 also .• Coronary A.6.2.1 For A{n x p), B (q X II), a(px 1). arId b(q x 1). the matrix A.b'B liaS rank at most 1. Th e nOn-zero eigenvalue. If present, equals b'BA. , WIth eigenvector A •. Proof The non -zero eigenvalue of Aab'B equals that of b'BAa, which "
2534,unknown,"is a scalar. and hence is its own eigenvalue. The fact that Aa is a corresponding eigenvector is easily checked . • A.6.2 Symmetric matrices If A is symmetric, it is possible to give more detailed information about its eigenvalues and eigenvectors. Tbeore'D A.6.3 All the eigenva/t.es of a symmetric matrix A(p x p) are real. Proof II possible. let "",(=1<+iy, A = a +ib, ""'(* 0. (A.6.9) From (A.6.2),"
2535,unknown,"From (A.6.2), after equating real and imaginary parts, we have Ax = ax-by, Ay=b1< + ay. 469 APPENDlX A. MATRIX ALGEBRA On premultiplyiog by l' and X , respectively, and subtracting, we obtain b = O. Hence from (A.6.9), A is real. • In the above discussion, we can choose y = 0 so we can assume ""'( to be real. Theorem A.6.4 (Spectral decomposition theorem. or Jordan decomposi ­ tion theorem) Any sym"
2536,unknown,"tion theorem) Any symmerric marrix A(p x p) CO"" be wrirre"" as (A.6.10) where A is a diagonal matrix of eige/lvalues of A. and r is an OrThogonal matrix whose columns are sta""dardized eigenvectors. Proof Suppose we can find orthonormal vectors ""'(0)' •••• ""'(Cp) such that A""'(Cil = Ai'Y(i) for some numbers Ai' Then Or in matrix form r'Ar=A. i = i. i;< i. (A.6.1l) Pre- and post-multiplying by rand r"
2537,unknown,"i = i. i;< i. (A.6.1l) Pre- and post-multiplying by rand r' gives (A.6.10). From (A.6.7), A and A have the same eigenvalues, so the elements of A are exactly the eigenvalues of A witb the same multiplicities. Thus we must find an orthonormal basis of eigenvectors. Note that if A i'"" Ai are distinct eigenvalues with eigenvectors x+y , respectively, then lI.,xy=x'Ay=y'Ax=II.,Yx.. so that y'x = O. He"
2538,unknown,"then lI.,xy=x'Ay=y'Ax=II.,Yx.. so that y'x = O. Hence for a symmetric matrix, eigenvectors corresponding to distincl eigenvah ... are orthogonal to one another. Suppose there are k distinct eigenvalues of A with corresponding eigenspaces H , .... ,H. of dimensions r"" ...• r •. Let • r= L 'i' i-I Since distinct eigenspaces are orthogonal, there exists an orthonormal set o[ vectors e"" ... ,e, such t"
2539,unknown,"o[ vectors e"" ... ,e, such that the vectors labelled . I . 't r,+ 1 .... , tr, i-I i-I form a basis for H,. From Theorem A.6 .1, ri is less than Or equal to the multiplicity of the corresponding eigenvalue. Hence by re-ordering the eigenvalues Ai if necessary, we may suppose Aei =A,e"" ;=1 .... ,r, MULTIVARIATE ANAl.ysrs 470 and r"" p. (If all p eigenvalues are distinct. then we know from Theorem A "
2540,unknown,"A .6.1 that r=p). If r = P. set ""Y(;) = e, and the proof follows. We shall show that the situation r < p leads to a contradiction. and therefore cannot arise. Without loss of generality we may suppose that all of the eigenvalues of A are strictly positive. (If not. we can replace A by A +a l for a suitable a. because A and A +a l have the same eigenvectors). Set B=A- t '\,eie~. ,-1 Then trB=trA-t "
2541,unknown,"B=A- t '\,eie~. ,-1 Then trB=trA-t )..(e:e,)= t ).,>0. • - 1 i- r.1 since r < p. Thus B has at least one non-zero eigenvalue. say e. Let x'"" 0 be a corresponding eigenvector. Then for 1 .. j .. r. ee;s=e;ax={A je;- t A,(e;e,)e(}x=o. ,_I so that x is orthogonal to e,. i = I ..... r. Therefore. 9x=B>:= (A -L ).,e,e:),,= Ax- L AJelx)e, = Ax so that x is an eigenvector of A also. Thus 0 = A, for some "
2542,unknown,"linear combination of some of the e,. which contradicts the orthogonality between x and the e,. • CoroUary A.6.4.1 If A is a lion-singular symmelric matrix. tlten for an)' integer n, A""=diag(A~) and A"" = TA""r'. (A.6.12) If all the eigenvalues of A are positive then we can define the raiional powers A m =rA""~r', where A 'it =diag(A.;""), (A.b.13) for integers s > 0 and r. If some of the eigenvalues "
2543,unknown,"(A.6.12) and (A.6.13) hold if the exponents are restricted to be. non­ negative. Proof Since A ' = (r AI""j2 = r AT 'r AT' = r A 'r ' and A -I=rA-1r'. A -I = diag (A;-I). 471 APPE,.""""lDIX A MATRIX ALGEBRA we see that (A.6.12) can be easily proved by induction. To check that rational powers make sense note that (A"" 'Y =rA"" 'r ' ... rA"" 'r' =rAT ' =A'. • Motivated by (A .6.13). we can define powers o"
2544,unknown,"exponents. Important special cases of (A.6.13) are A 1/2= rA 1/2r', A 112 = diag (). :12) (A.6.14) when )., ~ 0 for all i and A -H2 = rA -'12r'. (A.6.15) when A, > 0 for all i. The decomposition (A.6.14) is called the symmecric square root decompositioll of A . Corollary A.6.4.2 The rank of A equals the lIumber of IIOIl-zero eigen­ values. Proof By (A.4.2f). r(A )= rCA). whose rank is easily seen "
2545,unknown,number of non-zero diagonal elements. • Remarks (1) Theorem A .6.4 shows that a symmetric matrix A is uniquely determined by its eigenvalues and eigenvectors. or more specifi­ cally by its distinct eigenvalues and corresponding eigenspaces. (2) Since A 1/2 has the same eigenvectors as A and has eigenvalues which are given functions of the eigenvalues of A. we see that the symmetric square root is 
2546,unknown,"(3) If the A, are all distinct and written in decreasing order say, then r is uniquely determined, up to the signs of its column s. (4) If A'+I = ... = A. = 0 then (A.6.1O) caD be written more compactly as • A = r )Alr~ = L A' 'Ym'V~i), ,-I where AI =d iag(AI •...• A,) and r l=(""Y(lI.···.'Y(k». (5) A sym metric matrLx A has rank 1 if and only if A = xx' for some x. Then the only non-zero eigenvalu"
2547,unknown,"A = xx' for some x. Then the only non-zero eigenvalue of A is given by tr A = tr xx' = is and the corresponding eigenspace is generated by x. (6) Since J = 11' has rank 1 with eigenvalue p and corresponding eigenvector 1. we see that the equicorrelation matrix E = (l - p)1 + pJ has MULTIVARIATE A A L VS1S 472 eigenvalues A,=l +(p-l)p and A2 = ... =A,.=I-p, and the same eigenvectors as J. For tbe e"
2548,unknown,"eigenvectors as J. For tbe eigenvectors 'Y(2"" .... 'Y, ... we can select any standardized set of vectors orthogonal 10 1 and each other. A possible choice for r is the HeImerl matrix of Section A .3.t. Multiplying the eigenvalues together yields the formula for lEI given in (A .3.2c). (7) If A is symmetric and idempotent (that is. A = A ' and A 2= A ), then A; =0 or 1 for all i, because A=A 2 impl"
2549,unknown,"(8) If A is symmetric and idempotent then rCA) = tr A. This result follows easily from (A.6 .6) and Corollary A.6 .4.2. (9) As an example. consider A = (~ ~). The eigenvalues of A from (A .6.0 are tbe solutions of 1 1- A p 1=0 p I- A ' namely, A ,=l+ p and A2= 1- p. Thus , A=diag (l +p .l-p). (A.6.16) (A .6.17) For p;o' 0, the eigenvector corresponding to A I = 1+ p from (A.6.2) is which leads to "
2550,unknown,"which leads to x I = x"" therefore the first standardized eigenvector is (1/J2) 'Y,I) = IIJ2 . Similarly, the eigenvector corresponding to A,= I-p is Hence, ( 1/J2) 'Y"",= -l/J2 . r=(IIJ2 1/J2) IIJ2 -tlJ2' If p = 0 then A = 1 and any orthonormal basis will do. (A.6,18) (10) Formula (A .6.14) suggests a met bod for calculating the symmetric square roor of a matrix. For example, (or tbe matrix in (A.6"
2551,unknown,"473 APPENDIX A MA TRlX ALGEBRA p"" < 1. we find on using A and r from (A.6.1l) and (A.6.14) that A 1I>=rA II'r= (: !), where 2a =O+p )II2+(1_p )'12, 2b = (1 + P )'1>_ (1- p)""2. *(11) The following methods are commonly used to calculate eigen­ values and eigenvectors on computers. For symmetric matrices, the Housebolder reduction to tri-diagonal form (i.e. C1;j = 0, for i:;. j + 2 and i,,; j - 2) is"
2552,unknown,"i,,; j - 2) is used followed by the QL algorithm. For non-symmetric matrices, reduction to upper Hessenberg form (i.e. a;j=O for i:;. j+2) is used followed by the QR algorithm. (12) For general matrices A (n x p), we can use the spectral decomposi· tion theorem to derive the following result. Theorem A,6,S (Singular value decomposition theorem) If A is an (0 x p) matrix of rallk r. rhen A can be """
2553,unknown,"(0 x p) matrix of rallk r. rhen A can be "",rirrell as A=ULV' (A.6.19) where Urn X r) and V (p x r) are column orrhonormal matrices (U'U = V'V = I,) and L is a diagonal matrix wirh posirive elemenrs. Proof Since A ' A is a symmetric matrix which also has rank r, we can use the spectral decomposition theorem to write A'A=VAV' , (A.6.20) where V(p x r) is a column orthonormal matrix of eigenvectors o"
2554,unknown,"and J\ = diag (A"" ... , A,) contains the non-zero eigenvalues. Note that all the A; are positive because A; =v;,,A'Avm = IIA""m I12>O. Let i = 1 ..... r, and set L = diag (I"" ... ,~). Define U(n x r) by i= I, ...• 1. Tben Thus U is also a column orthonormal matrix. i = j, i '"" ;. (A,6.21) (A.6.22) Any p-vector "" can be written as )(=IQ;v<o +Y where y€oN (A) , tbe null space of A . Note that N(A)= N"
2555,unknown,"null space of A . Note that N(A)= N(A'A) is the eigenspace of A'A for the eigenvalue 0, so that y is orthogonal to tbe eigenvectors v (Il' Let e, MULTIVARlATE ANALYSIS denote the r-vector with 1 in the ith place and 0 elsewhere. Then ULV'x= La,ULe,+O = L a, ~u(j)+ O = L a,Av "",+Ay = As . Since this formula holds for all x it follows that ULV = A. • 474 Note that the columns of U are eigenvectors o"
2556,unknown,"474 Note that the columns of U are eigenvectors of AA' and the columns of V are eigenvectors of A'A . Also, from Theorem A.6.2. the eigenvalues of AA ' and A'A are tbe same. A.7 Quadratic Forms and Definiteness Definition A quadratic form in rhe vector,. is a function of rhe form p • Q(x)ErA:I= L L a""x,x"" (A.7.!) where A is a symmerric matrix; that is. Q(x)= Qllxi+ ... +appx!+2a12xlx2+ , .. +2Clp_"
2557,unknown,"Q(x)= Qllxi+ ... +appx!+2a12xlx2+ , .. +2Clp_l,,,.%p_IXp. Clearly. Q(O) = O. Definition (1) Q(x) is called a positive definite (p.d.) quadratic form if Q(x) > 0 for all "",., o. (2) Q(x) is called a positive semi-definite (p.s.d) quadratic form if Q(x);;;.O for all s >' o. (3) A symmerric marrix A is called p.d. (p.s.d) if Q(,,) is p.d. (p.s.d.) and we wrire A> 0 or A ;;;. 0 for A positive definire"
2558,unknown,"and we wrire A> 0 or A ;;;. 0 for A positive definire or positive semi -definite, respecrive/y. NegatIVe de(inire and negarive semi-definire quadraric fomls are similarly defined. For p=2. Q(x)=x~+xi is p.d. while Q(x)=(x,-x,.1' is p.s.d. Canonical foml Any quadratic form can be converted into a weighted sum of squares without cross-product terms with the help of the following theorem. Theorem A.7"
2559,unknown,"theorem. Theorem A.7.1 For allY symmerric marrix A . there exisrs all onhogonal rraftsformarion y=r'x (A.7.2) 475 APPENDlX A MATRIX ALGEBRA such rhar (A.7.3) Proof Consider rhe specrra/ decompositIOn gIVe"" in Theorem A.6.4: A=rAr' . (A.7.4) From (A.7.2). rAx= y'r'Ary = yT'rAr'ry = y'Ay. Hence (A.?3) follows. • It is imporlant to recall that r has as it. columns the eigenvectors of A and that AI •."
2560,unknown,"and that AI •...• A"" are the eigenvalues of A. Using this theorem. we can deduce the following results for a matrix A> O. Theorem A .7.2 If A>O rhen A,>O fnr i=l •...• p. If A;;;.O. then A,;;;'O. PlOQf If > O. we have. for all ,.,. O. O<x'A:I= AIY~+ .. +A.y;. From (A.7.2). ,.,.,0 implies y>'O . Choosing YI=l. y,= _ =y.=O. we deduce that A, >0. Similarly A, >0 for all i. If A;;;'O tbe above ine­ qu"
2561,unknown,"qualities are weak. • Corollary A.7.2.1 Tf A> O. then A is nOIl-singw/ar and \A\ > O. Proof Use the determinant of (A.7.4) with A, > O. • Corollary A.7.2.2 If A>O. rhl!11 A - '>O. Proof From (A.7.3). we have x'A-'x= Lv NA, . • (A.7.5) Corollary A.7.2.3 (Symmetric decomposition) Any Inarrix A;;;'O can be written as A=B 2 , (A.7.6) where B is a symmerric matrix. Proof Take B =rA'/2r ' in (A.7.4) . •"
2562,unknown,"Tbeorem A.7.3 If A;;;'O IS a (pxp) marrix. then for allY (pxn) marrix C, C'AC;;;'O. If A>O and C is nOli-singular (so P=II). tlten C'AC>O. Proof [f A""'O then for any n-vector x;<O, x'C'ACx =(Cx l'A(Cx);;;'O, SO C'AC;;;'O. MULTIVARIATE ANALYStS 476 If A> 0 and C is non-singular, tbe Cx '"" 0, so (Cx ), A (Cx) > 0, and hence C'AC >O . • CoroDary A.7.3.1 If A;;. 0 and B > 0 are (p x p) matrices, then "
2563,unknown,"non-zero eigenvalues of B - 'A are positive. Proof Since B>O, B -II' exists and, by Theorem A.6.2. B -JJ2AB -'I2, B -'A, and AB -1 have the same eigenvalues. By Theorem A .7.3, B -'12AB - I12;;.O, so all of the non-zero eigenValues are positive. • Remark., (1) There are other forms of interest: (a) Linear form .• 'x = a,x, + ... + a""x,. Generally called a linear com­ bination. (b) Bilinear form. x"
2564,unknown,"(2) We have noted in Corollary A.7.2.1 that IA I>O for A >0 . In fact, IA ""I>O for all partitions of A . The proof follows on considering x'Ax>(} for aLI x with ><., = . .. = x, = (). The converse is also true. (3) For ~ = (~ ~), the transformation (A.7.2) is given by (A.6.1S). YI = (XI + x2)1..fi, y,= (x,-x,)/..fi. Thus, from (A.7.3) and (A .7.S), x'Ix= X~+2PXIX2+ xi = (I + p)Yf+(l- p)y ~, 1 2 2 "
2565,unknown,"1 2 2 YT y~ ,,':1;-1,,= ( 2) (xl- 2PX, X~+ X2) =--+--' I- p l+p I-p A geometrical interpretation of these results will be found in Section A .lO.4. (4) Note that the centring matrix H;;.O because x'H,,=[(x,-x)';;.O. (5) For any matrix A , AA';;.O and A'A;;.O. Further, r(AA ')= r(A'A) = r(A ). * A.8 Generalized Inverse We now consider a metbod of defining an inverse [or any matrix. Definition For a"
2566,unknown,"477 A.PPENDIX A MATRIX ALGEBRA inverse) of A if (A .S.I) A generalized inverse always exists although in general it is nOt unique. • Method~ of construction (1) Using the singular value decomposition theorem . (The.orem A.6 .S) for A (n X p), write A = ULV' . Then it is easily checked that A - = VL -'U' (A.S.2) defines a g·inverse. (2) If rCA ) = r, re-arrange the rows and columns of A(n x p) and "
2567,unknown,"partition A so tbat A ll is an (rX r) non-singular matrix. Then it can be verified that A -= 11 (A -, 0) o 0 IA .8.3) is a g-inverse. The result follows on noting that there exist B and C such tbat A I2= A lIB , AZI = C A u and A 22 =C AIlB . (3) If A(p X p) is non-singular then A - = A - I is uniquely defined. (4) ff A(p x p) is symmetric of rank r, then, using Remark 4 after Theorem A.o.4. A can"
2568,unknown,"Theorem A.o.4. A can be written as A =f,A,n , where f , is a column orthonormal matrix of eigenvectors corresponding to the non-zero eigen­ values Aj = diag (A "" ... , A,) of A . Then it is easily checked tbat (A.R.4) is a g-inverse. Applications (I) Linear equations. A particular solution of the consistent equatIons Ax = b. (A .8.S) is (A .S.O) Proof From (A.8.l), AA -A1< = Ax ~ A (A -b)= b which"
2569,unknown,"AA -A1< = Ax ~ A (A -b)= b which when compared with (A.8.S) leads to (A.S.o). • It can be shown that a general solution of a consistent equation is X=AD +(J- G )Z, where z is arbitrary and G = A -A . For b = 0, a general solution is (1- G )z. MULllVARlAlC ANALYSlS 478 (2) Quadraric forms. Let A(p x p) be a symmetric matrix of rank r~p. Then there exists an orthogonal transformation such that for :"
2570,unknown,"to M(A) the subspace spanned by the columns of A , x'A -x can be written as (A,8.7) where A I"" .. ,A, are the non -zero eigenvalues of A , Proof First note that if x lies in M(A) we can write x=Ay for some y, so that x'A -x=y'AA -Ay = ,'Ay does not depend upon the particular g-inverse chosen. From the spectral decomposition of A we see that M(A) is spanned by the eigenvectors of A corresponding to"
2571,unknown,"A corresponding to non-zero eigenvalues, say by ('Yen,· .. , 'VI"") = r ,. Then if xE M(A), it can be written as :o:=r,u for some r-vector D, Defining A-by (A.SA) , we see that (A,S.7) follow, Remarks (1) For the equicorrelation matrix E , if 1 +(p-I)p =0, then (1-prll is a g-inverse of E. (2) Under the following conditions A - is defined uniquely: AA - and A -A symmetric, *(3) For A:;"" 0, A - IS n"
2572,unknown,"ition (see Remark 4, Section A,2 .4.). A.9 Matrix DiHel'entiatioD and Maximization ProbJems Let us define the derivative of !(X) with respect to X(n x p) as the matrix af(X) = (af(X»). ax aX"" We have the following results: 0..'"" (I) -=ll. ax ax'x ax'Ax ax'Ay (11) ~=2x, --a;-=(A + A')x, --a;-=A)'. (A,9.t) (A.9.2) 479 APPENDIX A MATRIX ALGEBRA (ill) a IXI = x,; if all elements of X ("" x ,,) are dist"
2573,unknown,"aX,; = {2-;' i=i} if x is symmetric, F~j, l:;t. J where x,/ is the (i, ilth cofactor of X . (IV) a t~:V = Y ' if all elements of X l"" x p) are distinct, = Y + Y' - Diag (Y) if X ("" x n ) is symme tric. aX -' (V) --= -X -'J""X -1 if all elements of X(n x ,,) are distinct axi, { - X -l J"" X -I, = - X- '(J,, ~J )X -I I, ,. . i = i,} 'fX ' . i"""" i I IS symmetnc. (A .9.3) (A .9A ) (A,9,5) where J,; deno"
2574,unknown,"(A .9A ) (A,9,5) where J,; denotes a matrix with a I in the (i. j)th place and zeros elsewhere. W e now consider some applications of these results to some stationary value problems. Theorem A.9.1 The vector"" which mInimizes « x) = (y - Ax )'(y-Ax) is given by A'Ax = A'y, (A.9.6) Proof Differentiate f(x) aDd set the derivative equal to 0 , ote Chat the second derivative matrix 2A 'A :;'0 so that t"
2575,unknown,"second derivative matrix 2A 'A :;'0 so that the solution to (A,9.6) will give a minimum, Also note that from (A.4 .3e), (A .9,6) is a consistent set of equations. • Theorem A.9.2 Let A and B be rwo symmetric matrices. Suppose that B >O. Then the maximum (minimum) of x'Ax given ""'B,, = I (A.9.7) is attained whetl "" is the eigenvector of B -1 A correspotlding to tire largest (sm allest) eigenvalue o"
2576,unknown,"(sm allest) eigenvalue of B -'A, Thus if AI and A. are the largest atld smallest eigenvalues of B -'A , theil, subject to tlte constraint (A.9.7). max x'Ax= A I, min x'Ax = Ar- (A.9.S) a a Proof Let B ""2 denote the symmetric square root of B. and let y = B 112,.. MULTIVARlA""IE ANALYSIS 480 Then the maximum of x'Ax subject to (A9.?) can be written as max y'B-'/2AB'II'y subject to y'y = 1. (A.9.9) •"
2577,unknown,"• Let B -'12 AB -,n = r Ar' be a spectral decomposition of the symmetric matrix B -1I2A B - '12. Let z=r'y. Then z'z=yTr'y=y'y so that (A9.9) can be written max z' Az = max L A,z ~ subject to z'z= I. . , (A.9.lD) If the eigenvalues are written in descending order then (A .9.l0) satisfies max LAi Z~~AJ max L Z~=A.I ' Further this bound is attained for z = (1,0, ... ,0)', that is for y = 'Y()). and "
2578,unknown,"for x =B "" r2'Y(1)' By Theorem A.6.2, B- ' A and B -'''AB''12 bave the same eigenvalues and _ = B '1I2y(\) is an eigenvector of B "" A correspond· ing to A ,. Thus the theorem is proved for maximization. The same technique can be applied to prove the minimization result. • CoroUary A.9.2.l If R(x)=_'Ax/x'Bx rhen, for ""pO, Ap "".R(x)""'A,. (A9 .1I) Proof Since R (x) is invariant under changes of scale"
2579,unknown,"the problem as maximizing (minimizing) x' Ax given (A.9.7). • CoroUary A.9.2.2 The maximum of a'x subjecr to (A.9.7) is (a'8-'8),n . (A.9.12) Funher max {(a'x)2/(x'Bx )} = .'B"" . (A .9.l3) . and rhe maximum is attained at x= B "" ./(8'B"" .)'/2. ¥roof Apply Theorem A.9.2 with x'Ax=(a'x),=x'(u')x . • Remarks (I) A direct method is sometimes instructive. Consider the problem of maximizing the squared "
2580,unknown,"x2 +y"" of a point (x, y) on the ellipse (A.9 .14) 481 APPENDIX A MATRIX ALGEBRA When y2 is eliminated. the problem reduces to finding the maximum of x'+b 2(x2/a2 -1), xE[-a.aj. Setting the derivative equal to 0 yields the stationary point x = 0 which, from (A.9.14). gives y= ±b. Also. at tbe endpoints of the interval (x =±a) , we get y = D. Hence max (x2+ y2) = max (a', b2). This solution is not a"
2581,unknown,"This solution is not as elegant as the proof of Theorem A .9.2, and does not generalize neatly to more complicated quadratic forms. (2) The results (A9.1)--{A.9.2) follow by direct substitution, e.g. d, a ( ) -ax=-a ,x,+ ... + a.x"" =a , dX, ax, proves (A.9.l). For (A .9.3) use (A2.3d ). A.lO Geometrical Ideas A.lO.l /I' dimensional geometry Let e, denote the vector in R "" with 1 in the ith place a"
2582,unknown,"Let e, denote the vector in R "" with 1 in the ith place and zeros elsewhere so that (e"" ... , e.) forms an orthonormal basis of Rn . In terms of tbis basis, vectors x can be represented as x = I xie"" and x, is called the ith coordinUle axis. A point 8 in R "" is represented in terms of these coordinates by x, = a .....• x.. = a.. The point 8 can also be interpreted as a directed line segment from 0"
2583,unknown,"a directed line segment from 0 to a. Some generalizations of various basic concepts of two- and three·dimensional analytic Euclidean geometry are summarized in Table AlD .l. A.tO.2 Orthogonal lransfonnations Let r be an orthogonal matrix. Then rei =Y (j). i = I, ... , n, also form an orthonormal basis and points x can be represented in terms of this new basis as x = L Xie, = L Yi'Y(i), whe .re y, "
2584,unknown,"whe .re y, = ""(i,x are new coordinates. If ,,(1) and :1m are two points with new coordinates y'"" and y(2) note that (yO) _ y(2»'(y( "" _ y(2) = (,,(1\ _ ,,""')'rr' (x'"" - x(2) = (xtl) - x(2)),(x'"" - x12,), MULTIVARIATE ANALYS IS 482 Table A.lO.l B asic concepts in n-dimensiona1 geometry Concept Point _ Distance between _ and b Une passing through •• b Line passing through 0, I Angle between lines [r"
2585,unknown,"Angle between lines [rom OtooandOtob Direction cosine vector of a line from 0 to • Plane P Plane through b , •... , b, Plane through O. b"" ... ,b,. Hypersphere with centre a and radius r Ellipsoid Description (11xI1 = (L xt)""') 110-bli ={ L (a, - b.y}'"" x = Ao + (1-A lb is the equation J:=Aa 6 where cos 6= a'b/UlallIIbla"""". 0,.;6,.;.,,­ (cos y"" ... ,cos y.). cos y, = a,~ioU; 'Yl = angle between li"
2586,unknown,"'Yl = angle between line and ith axis . ']( = c is general equation x=LA .b .. LA ,=I X=LA ,bi ('-Il'(s-I)=r' (x-o)'A-'(x-I)=c'. A > O so that orthogonal transformations preserve distances. An orthogonal transformation represents a rotation of the coordinate axes (plus a reflection if Irl = -1). When It = 2 and Irl = 1, r can be represented as ( COS e -sin e) sin e cos e and represents a rotation "
2587,unknown,"an angle e. A.10.3 Projections Consider a point 8 , in It dimensions (see Figure A.10 .l). Tts projection onto a plane P (or onto a line) through the origin is the point i at the foot of the perpendicular from a to P. The vector i is called the orthogonal projection of the vector a onto the plane. Let the plane P pass through points 0, bl> ... ,b. so that its equation from Table A.lO.l is x= LA ,b"
2588,unknown,"from Table A.lO.l is x= LA ,bi> 8=(b"" ... ,b.J. Suppose rank (B ) = k so that the plane is a k -dimensional subspace. The 483 APPENDIX A MATRIX ALGEBRA G Figure A .IO.I i is the. projection of a onto rhe plant P. point i. is defined by x= L A,b;, where A"" ... , A. minimize ~a- I,\, bl ~ since i. is the point on the plane closest to a. Using T heorem A.9.1, we deduce the following result. Theorem A"
2589,unknown,"Theorem A..I0.1 The point i is giuen by i =B(B 'B )-'B '.. • (A .l0.3a) Note that 8 (8 '8 )-'B ' is a symmetric idempotent matrix. 10 fact, any symmetric idempotent matrix can be used to represent a projection_ A.10.4 EUipsoids Let A be a p.d. matrix_ Then (x-(lrA -'(x-(l) =c 2 (A.l0.4a) represents an ellipsoid in n dimensions. We note that the centre or the ellipsoid is at ,,=a. On shifting the c"
2590,unknown,"ellipsoid is at ,,=a. On shifting the centre to x= 0, the equation becomes (A.IO.4h) Definition Let"" be a point on the ellipsoid defined by (A .IO.4a) and ret felt) = I _a1l2 denote the squared distance belllleen a Qltd x. A (ilte through Ot and l< for which x is a srarionary point of {(x) is called a principal axis of MULTIVARlATIl ANALYSIS 484 Ihe ellipsoid. The dis lance IIx -otll is called Ihe"
2591,unknown,"Ihe ellipsoid. The dis lance IIx -otll is called Ihe lenglh of Ihe principal semi-axis. Theorem A.10.2 LelA, •...• A,. be 11.e eigenvalues of A salisfying A,> A,> ... > A,.. Suppose Ihal 'Y0l.' ..• 'Y,., are Ihe corresponding eigenvec­ lars. For Ihe ellipsoids (A .10.4a) and (A.IOAb). we have (J) The direction cosine vector of Ihe ilh principal axis is 'Y,l)' (2) The lenglh of Ihe ilh principal se"
2592,unknown,"Proof It is sufficient to prove the result for (A.10.4b). The problem reduces to finding the stationary points of fix) = x'x subject to x lying on the ellipsoid x'A-'x=c'. The derivative of ""'A-',, is 2A -'"". Thus a point y represents a direction tangent to the ellipsoid at "" if 2y' A -'x = O. The derivative of fix) is 2 .. so the directional derivative of fix) in the direction y is 2y'x. Then x i"
2593,unknown,"direction y is 2y'x. Then x is a stationary point if and only if for all points y representing tangent directions to the ellipsoid at It. we have 2y'"" = 0; that is if y'A - 'x= O => y·x= O. This condition is satisfied if and only if A -'x is proportional to x; that is if and only if x is an eigenvector of A -'. -, ~~----------~--------~~ o Figure AJO .2 Ellipsoid ""'A-',,= 1. Un .. defined by)"" and"
2594,unknown,"and s.rond principal axes. liall = A!"". I = A;"". 485 APPENDIX A MATRIX ALGEBRA Setting x= f3'Y(1) in (A. IOAb) gives f3'/A, = c', so (3 = c,A""n. Thus . the theorem is proved. • If we rotate tbe toordinate axes with the transformation y = r ·x. we find that (A .lO.4b) reduces 10 L yfJA,=c'. Figure A .I0.2 gives a pictorial representation. With A=l . (A.I0.4b) reduces to a hyperspbere with A, = ... "
2595,unknown,"so that the .\S are not distinct and tbe above theorem fails; that is, the position of 'Y(,). i = 1 •...• n, through the sphere is not unique and any rotation will suffice; that is. aU the n components are isometric. In general. if A, = A, + .. the section of tbe ellipsoid is circular in the plane generated by 'Y'i). 'YU .l)' Although we can construct two perpendicu­ lar axes for the common root. "
2596,unknown,"lar axes for the common root. their position through the circle is not unique. If A equals the equicorrelation matrix. there are p -1 isotropic principal axes corresponding to the last p -I eigenvalues. Appendix B Univariate Statistics B.l Introduction In this appendix, we summarize tbe univariate distributions needed in the text, together with some of tbeir simple properties. Note that we do not "
2597,unknown,"distinguIsh notationally between a random variable x and its p.d.f. fix). B.2 Normal Distribution If a random variable x has p.d.1. fix} = (21T)-I12e-·>12, (8.2.1) then x is said to have a standardized normal disrribution, and we write x - N(O, I). Its cbaracteristic function (c.f.) is given by </>(t) = E(e""') = e-""12 _ (B.2.2) Using the formula for the rtb moment of x, where r is a positive integ"
2598,unknown,"11-2.= 1, 1-'-.=3. The p.d.f. of N(,..., u 2 ) is given by (21TU')-I12 exp (-!(x-I-'-)'Ju'), -00< x <"""", where u'>O, -00< I-'-<00. lis characteristic function is given by exp (il-'-I-~ t 2U'). (B .2.3) 487 APPENDIX B UNIVARIA""TE STATISTICS B.3 Cbi-squared Distribution Let x ..... , x"" be independent N (O. I) variables and set y=xi+""'+x;, (B.3.tl Then y is said to have a chi-squared disrriburion wi"
2599,unknown,"freedom and we write y -X;. The density of y is given by f(y )={2./2r(~p>r l .tH e-,/2, Here r(x) is the gamma function defined by . r(x) = J x""-'e-- du , o O<y<oo. (B.3.2) x>O. (B .3.3) which satisfies tbe relations r(x ""'I)=xr(x) for x >0 and nn + 1)=,Ii for integer n"""" O. Note that (B.3.2) is a density for any real p >0; p need not be an integer. Us ing (B.3.3), the moments of y are easily calc"
2600,unknown,"integer. Us ing (B.3.3), the moments of y are easily calculated as E(y') =2T(~p + r)!r(~ p). for any real number r for which r> -!p. Using r = 1.2. we get E(y)=p, V(y) = 2p. Also, if p >2, taking r=-l in (B .3.4), we get E(l/y)= 1/(p-2). B.4 F and Beta Variables Let u - X~ independently of v - X~. and set ulp x =-. vlq (B.3.4) (B .3.5) (8.3.6) (B.4.1) Then x is said to have an F disrributiolt with"
2601,unknown,"We write x - F •.•. Using the same notation. set y = ,,/(14 +v ). (B.4.2) Then y is said to bave a beta distriburiolt (sometimes called tbe bela type I dislriburioll) with parameters!p and iq. We write y - BGp. !q). Its density MULTIVAR IATE ANALYSIS is given by f(y)= [r(ip + ~q )I(r(~p ) r(~q))]}'.I2-I (1 - y)<U2- ' , 488 O<y < L (B.4.3) The beta type II disrribution is proportional to the F dist"
2602,unknown,"defined in the above notation by w = pxlq = ulv. The F and beta type I distributions are related by the one-to-one transformation y = pxl(q + px), (B.4.4) or, equivalently, x = qyl{p(1-y)}. (B.4.5) The following theorem shows an important relationship between the beta and chi-squared distributions. Theorem B.4.1 Let u - K~ independently of v - K;. Then 2 = u + v - K! •• independently of y = ul(u +"
2603,unknown,"Proof Transform the variables (u. v) to (z. y) where u = zy and v = z(l-y). The p.d.f. of (u. v) is known from (B.3.2), and simplifying yields the desired result. • B.S I Distribution Let x - N (O, 1) independently of u - K;. Th en r = X{(ulp)'{2 (B .S.1) is said to have a r dislribulion with p degrees of freedom. We write I- t"". Its p.d.!. is given by [rG p +!)I{(""."".)'''f(}p)}](l + t'/p)-<,>I){2"
2604,unknown,"From (B.3.1) and (B.4.1), DOle that if t - t"", then t'l_ F l.p. (B.S.2) The mean and variance of tbe I distribution can be calculated from (B.2.3) and (B.3.6) to be E(I) = 0, V (I) = p/(p - 2), (B.S.3) provided p> 1 and p> 2, respectively. Appendix C Tables MULTIVARLATE ANALYSIS 490 491 APPENDIX C TABLES Tabl. C.I Upper percentage points of the x;-discributiant Table C2 Upper percentage points of "
2605,unknown,"I-a I-a . v 0.90 0.95 0.975 0.99 0.995 0.999 "" 0.90 0.95 0.975 0.99 0.995 1 2.71 3.84 5.02 6.63 7.88 10.83 1 3.0777 6.3138 12.706 31.821 63.657 2 4.61 5 .99 7.38 9.21 10.60 13.81 2 1.8856 2.9200 4.3027 6.9646 9.9248 3 6.25 7.81 9.35 11.34 12.84 16.27 3 1.6377 2.3534 3.1824 4.5407 5.8409 4 7.78 9.49 11.14 13.28 14.86 18.47 4 1.5332 2.\318 2.7764 3.7469 4.6041 5 9.24 11.07 12.83 15.09 16.75 20.52 5 "
2606,unknown,5 9.24 11.07 12.83 15.09 16.75 20.52 5 1.4759 2.0150 2.5706 3.3649 4.0321 6 10.64 12.59 14.45 16.81 18.55 22.46 Ii 1.4398 1.9432 2.4469 3.1427 3.7074 7 12.02 14.07 16.01 18.48 20.28 24.32 7 1.4149 1.8946 2.3646 2.9980 3.4995 8 13.36 15.51 17.53 20.09 21.95 26.12 8 1.3968 1.8595 2.3060 2.8965 3.3554 9 14.68 16.92 19.02 21.67 23.59 27.88 9 1.3830 1.8331 2.2b22 2.8214 3.2498 10 15.99 18.31 20.48 23.2
2607,unknown,10 15.99 18.31 20.48 23.21 25.19 29.59 10 1.3722 1.8-125 2.2281 2.7638 3.1693 11 17.28 19.68 21.92 24.73 26.76 31.26 11 1.3634 1.7959 2.2010 2.7181 3.1058 12 18.55 21.03 23.34 26.22 28.30 32.91 12 1.3562 1.7823 2.1788 2.6810 3.0545 J3 19.81 '22.36 24.74 27.69 29.82 34.53 13 1.3502 1.7709 2.1604 2.6-03 3.0123 14 21.06 23.68 26.12 29.14 31.32 36.12 14 1.3450 1.7613 2.1448 2.6245 2.9768 15 22.31 25.0
2608,unknown,15 22.31 25.00 27.49 30.58 32.80 37.70 15 1.3406 1.7531 2.1314 2.6025 2.9467 16 23.54 26.30 28.83 32.00 34.27 39.25 16 l.3368 1.7459 2.1199 2.5 35 2.9208 17 24.77 27.59 30.19 33.41 35.72 40.79 17 1.3334 1.7396 2.1098 2.5669 2.8982 18 25.99 28.87 31.53 34.81 37.16 42.31 18 1.3304 1.7341 2.1009 2.5524 2.8784 19 27.20 30.14 32.85 36.19 38.58 43.82 19 1.3277 1.7291 2.0930 2.5395 2.8609 20 28.41 31.41 
2609,unknown,20 28.41 31.41 34.17 35.57 40.00 45.31 20 1.3253 1.7247 2.0860 2.5280 2.8453 21 29.62 32.67 35.48 38.93 41.40 46.80 21 1.3232 1.7207 2.0796 2.5176 2.8314 22 30.81 33.92 36.78 40.29 42.80 48.27 22 1.3212 1.7171 2.0739 2.5083 2.8188 23 32.01 35.17 38.08 41.64 44.18 49.73 23 1.3195 1.7139 2.0687 2.4999 2.8073 24 33.20 36.42 39.36 42.98 45.56 51.18 24 1.3178 J.7 t09 2.0639 2.4922 2.7969 25 34.38 37.65
2610,unknown,"25 34.38 37.65 40.65 44.31 46.93 52.62 25 1.3163 1.708 1 2.0595 2.4851 2.7874 26 35.56 38.89 41.92 45.64 48.29 54.05 26 1.3150 1.7056 2.0555 2.4786 2.7787 27 36.74 40.1 I 43.19 46.96 49.64 55.48 27 1.3137 1.7033 2.0518 2.4727 2.7707 28 37.92 41.34 44.46 48.28 50.99 56.89 28 1.3125 1.70 I I 2.0484 2,4671 2.7633 29 39.09 42.56 45.72 49.59 52.34 58.30 29 1.3114 1.6991 2.0452 2.4620 2.7564 30 40.26 43"
2611,unknown,"30 40.26 43.77 46.98 50.89 53.67 59.70 30 1.3104 1.6973 2.0423 2.4573 2.7500 40 51.81 55.76 59.34 63.69 66.77 73.40 31 1.3095 1.6955 2.0395 2.4528 2.7440 50 63.17 67.50 71.42 76.15 79.49 86.66 32 1.3086 1.6939 2.0369 2.4487 2.7385 60 74.40 79.08 83.30 88.38 91.95 99.61 33 1.3077 1.6924 2.0345 2.4448 2.7333 • 70 85.53 90.53 95.02 100,4 104.2 112.3 34 1.3070 1.6909 2.0322 2.4411 2.7284 80 96.58 101."
2612,unknown,80 96.58 101.9 106.6 112.3 116.3 124.8 35 1.3062 1.6896 2.0301 2.4377 2.7238 90 107.6 \13.1 118.1 124.1 128.3 \37.2 36 1.3055 1.6883 2.0281 2.4345 2.7195 100 118.5 124.3 129.6 135.8 140.2 149.4 37 1.3049 1.6871 2.0262 2.4314 2.7154 38 1.3042 1.6860 2.0244 2.4286 2.7116 39 1.3036 1.6849 2.0227 2.4258 2.7079 For v> 100. J'ix;-N(J2.-I.I) 40 1.3031 1.6839 2.0211 2.4233 2.7045 60 1.2958 1.6706 2.0003 2
2613,unknown,"60 1.2958 1.6706 2.0003 2.3901 2.6603 Abridged from Catherine M. Thompson : Tables of percentage points of the X.L distribution. 120 1.2886 1.6577 1.9799 2.3578 2.6174 Biometrika. vot. 32 (194 t), pp . 187-191, and published nere with the kind permission of the '"" 1.2816 1.6449 1.9600 2.3263 2.5758 au tho r and the editor of Bjom~' rika. Al I' _~. I. tends to N(O. 1). Further. t;""'"" F, .... tTaken"
2614,unknown,"tTaken [rom Table 2 of K . V Mardia and P. J. Zemroc.b (1978). Table C.3 , -CIr 10'.1 09<) 00' 091$ .o. 0.'"" 0.95 0.975 '2. 0."" 0."" 0,95 O.91!1 0.99 ..... 0.95 0.975 0.9'> 090 09' 0.975 0 .. 09(1 0.95 0.975 0."" 0.90 0.95 0,975 0."" 0."" 0.95 om! 0."" Mil 0.95 0.915 0."" 0.'IIl 09' 0,915 0 .. 0.9() 0,95 0.975 O.9\J 0.90 O.S 0.915 0."" 0·9() 0,9$ 0975 0 .. 0.90 0,95 n.9?S 0."" 0.90 0,95 0.915 .... 0."" 09'"
2615,unknown,"0 .. 0.90 0,95 n.9?S 0."" 0.90 0,95 0.915 .... 0."" 09' l 6 o '0 "" IS '"" ,,) .. 0.91' 1211 0 .. 0.'1() (),9' U.975 0"" Upper percentage points of the F .. , .. , distribut;ont ''I' '6' .. , .,050 • .s' ISJi "".s ..... 5,5. 10,1 11 .• ,.., 4.54 771 11.2 '"" .... 6.61 10.0 16.) 49.05 .00 .00 '.000 '.00 19.0 39,0 "".0 5.46 9,55 ,.0 30.8 4.32 6.94 10,6 18,0 l.78 5.19 .. , IJJ 3.78 3.""<J 5.99 5,\4 881 1.26 1"
2616,unknown,"18,0 l.78 5.19 .. , IJJ 3.78 3.""<J 5.99 5,\4 881 1.26 13.' 10.9 3~S9 5.59 8,01 III ,.. '"" 1.51 JU 3."" 5.11 '"" IO,ft ""'I .... 6.94 10.0 3.18 4.10$ 6.55 9.33 307 .... , .... .... 2.97 0' 5.87 8.10 '2. 88 4.17 5.57 'Ufl 17' '.00 ""'I 7."" 175 3.\12 S.lS 6.85 2.71 HI. ',02 6.63 3.26 •• 14 6.S"" !.I.55 311 •. 46 6.0' 8.6~ 3.01 "", 5.71 8.02 2.92 "".10 5.46 1.54 U, 3.89 5.ill 61)3 270 3.68 '.71 .,. 2.S9 '"" 4"
2617,unknown,"5.46 1.54 U, 3.89 5.ill 61)3 270 3.68 '.71 .,. 2.S9 '"" 4,46 5.8S 2.49 3"" '.11;1 539 2_39 '"" ),93 4.98 2.3S 3.07 3.80 4.19 2.30 3.00 3.69 ·461 SM "". ... 5,400 ... 19.2 39.2 "".l 5.39 9.28 15.4 29.5 4.19 6.59 9.98 16."" 3.62 $.41 7.76 12.1 3.>' 4"" .... \1.78 3.()7 '"" , ... ...... ,., '""' 5,42 , ... 2.81 HI6 S.OK fi.99 2.71 37' 4.11) ."" 2.61 , .... 4.4"" 's.9~ 1 ,; 3.29 '"" 5.41 2.36 3.10 )JUi 4.04 '"" 2."
2618,unknown,", .... 4.4"" 's.9~ 1 ,; 3.29 '"" 5.41 2.36 3.10 )JUi 4.04 '"" 2.92 J.59 4.51 2.18 2.7~ 334 4.11 2.1] ,~ .. 3.23 3.95 '.08 '.60 3.12 ).78 "". 11.< 000 5,620 9.24 19.2 39.2 99,' 5.34 9.12 15.1 28.7 '.\1 ."" 'I."" 16.0 ).5""2 S:19 7.39 11.4 31' 4.S3 .» '"" 1 .. 4.1l 5.52 '.8.5 2.1$1 3.84 5.05 7.01 1 .. 363 '"" '41 261 3.48 4.47 , ... 2.4K 3.26 "".12 '""~ 1.36 306 3.110 ... 215 U, 35' 4.43 2.14 2,69 3.25 , n, , "
2619,unknown,"'""~ 1.36 306 3.110 ... 215 U, 35' 4.43 2.14 2,69 3.25 , n, , .. '"" ).01 . ,~ .... 2.45 2.80 l.4tl 1,9>4 2.17 2. 711 3.32 $1.2 23. m 5."""""" 9.29 19.3 ""., "".3 '"" 9.01 14.9 ,. , • 0' .,. ,-', IB lA' , 0' 7"" lI.O 311 '''' , ... 8.7S 2.88 ).97 5.29 , ... 2.13 3.M 4,82 6.63 16' , .. 4 •• 8 '.116 2.$2 3.33 >4.2. , .. 2..19 '"" 3.89 S ... '17 , .. 1.58 "". 2.16 2.7\ "". 4.10 1.OS 1.S3 J OJ 3.70 , "" ,-', 2.79 "
2620,unknown,"1.58 "". 2.16 2.7\ "". 4.10 1.OS 1.S3 J OJ 3.70 , "" ,-', 2.79 3.3. , ... 2.29 '"" 3.17 I.tl5 1.11 1."" [0' • 7 ,., 23' m 5.860 OJ, ,,-' 39 J 99.3 "". 8.94 14.7 27.9 il,(11 6.16 010 !S.2 3.·0 4,9,5 6.911 10.7 1.05 4,18 5.82 .. , 1-8J '"" '"" '10 '2,61 "". 4.65 6.31 2.55 3,)7 4.n '""0 "". 1.22 • 0' '"" 2.33 '.on 3.73 4.8:2- 221 27, "", ."" 2 .. 2.60 3"" J,1j7 1.'18 2.42 2.87 3.47 1.[11 1.25 UJ ""2 '"" 2,lil 2.51 ,."
2621,unknown,"2.60 3"" J,1j7 1.'18 2.42 2.87 3.47 1.[11 1.25 UJ ""2 '"" 2,lil 2.51 ,.. '"" 2.1(1 2.41 2.1IU 58,9 137 ... 5,930 iJ)!i 19.4 39 •• 99.4 5.27 II.M 14.6 177 ).98 . .. • 0' ISO '"" '88 6.IIS '0, 3.01 '"" '.m 8.26 "". )70 ... 6,99 16' j,50 . "" 6,18 '"" '29 420 '61 '"" "". 1.9's s.~o 2.21$ 2.91 l.'d ... lIQ 21' '"" '"" , ... 'l.SI , 0' 371) 1.93 2.)3 2.75 """" I."" '17 251 , 'I' '"" 2.00 2.39 2.79 1.71 2 0' 2.29 2.M .."
2622,unknown,""""" I."" '17 251 , 'I' '"" 2.00 2.39 2.79 1.71 2 0' 2.29 2.M .. , 59 .• 139 '"" S.980 9)1 '9 .. 304 ""A n, .os .. , 27.S '"" '.(14 8.99 ''',8 3.34 ,., 61_ 1(lJ 2,98 '.15 , .. 8.11) 2.7S ).1l '.00 .... 2,S? , .. 4.0 6.03 '"" S.U 41. 5,41 '"" l.U7 lll!\ S ... "". 2.11' 35' .. 0 2.12 164 310 '.00 '00 2.45 l.PI l.!I6 !.tIl 211 H' )17 '"" 2.10 '"" 2.81 172 >0, 2.30 2.66 '61 ... 2.1\) 1St o , .. ,., .., 0.020 "". 1"
2623,unknown,"172 >0, 2.30 2.66 '61 ... 2.1\) 1St o , .. ,., .., 0.020 "". 19,4 ]9 .• ""A S.,. '81 '4.5 11.3 3.9' '.110 .... '''.7 3.31 4.77 fi.~"" 10.2 , .. •• 10 '"" , ... 2.72 l."" "".82 6,12 2.56 3.39 4.)6 $.~I , .. "". •. 03 SJ' ,"" 3 n, 3.78 , ,- 2.21 2.80 3.44 4.30 '00 1.59 ;\.11 3K9 196 2,)9 .o. ,., 1.115 2.'l1 2 "" 3.01 I,N , .. 2)) 2.12 1.6& , ... 2.21 2.5(i .. , \.lUI '"" 2,41 \0 601 '"" .. , 6060 9.39 19."" ' ."
2624,unknown,"2.5(i .. , \.lUI '"" 2,41 \0 601 '"" .. , 6060 9.39 19."" ' .. .... .s,n 8.79 14,4 272 3.92 , .. . ... 14 . .5 JJO "".74 6.62 to, "", .... 5.46 7.87 2.70 ).oj"" •• 76 6.62 '.S< '"" 430 '"" 2,42 3.14 "". $.26 2.32 ,., 3.72 4.SS 2.19 '"" '"" 4_10 206 '"" "")6 .0 1 0"", 'l.U '"" '"" I.Rl ~ 16 lSI ,.a '71 , ... 211 2: 63 ,'"" '9' 21. '"" 1.60 \,113 2.05 2.32 "" ... , 24' m 6.IIQ 9,.1 19.4 , .. .. .. 5.22 ."" , .. ] 21' l"
2625,unknown,"24' m 6.IIQ 9,.1 19.4 , .. .. .. 5.22 ."" , .. ] 21' lOU , .. .n ... 3.17 •• 6R 6.52 9.S\) 2.'"" '.00 5.37 1.11 1.61 .l..s7 .. , 6.47 BU 3.28 4.20 5.67 2.38 '.01 3.87 '"" "". 29' ).62 .,. lIS 2.69 3.211 ',16 , "". 2,48 , ... '"" 18' 2l' 2.bI! 3.23 1.71 '.09 2.4' 2.114 "" .. 191 117 "". '.60 1.83 2./15 2.34 I.!5S 1.75 19. VIJ "" '"" , .. ,a, 6.160 9 •• 2 19.· 39,4 ... 5,'20 8,711 14.3 26.0 3.87 $.86 .... '4·"
2626,unknown,"9 •• 2 19.· 39,4 ... 5,'20 8,711 14.3 26.0 3.87 $.86 .... '4·.2 ),2' 4.62 6.4' 971 '"" l."" 5.27 ,56 .. , ).5' 4.S7 6.31 2 .• 6 3.22 'In '"" 2.3. Hn ),77 .... 2.24 2.84 35> 4.56 2.10 2.62 "", 4.01 '91 ""0 2116 '"" , .. "". '"" 3.00 172 2,111 131 270 1.61) , ... 2 .. 2.3S 1.54 '15 1.\)4 1.10 "". .. , 183 """" .,' ."" lOS .. , 6.210 9."" 19,5 39 •• .. .. 5,18 .... 14.2 26.' "". , ... "". 140 '"" •. 56 6,33 .~, 2,84"
2627,unknown,".. .. 5,18 .... 14.2 26.' "". , ... "". 140 '"" •. 56 6,33 .~, 2,84 3.tl7 ,"" 7.'0 2,S9 3 .... 4.47 b.16 1.4Z j.U '111' 5.36 2.)0 '94 367 .01.81 2,20 '2.77 3.42 4.41 2."" 2 ~01 '.117 3.86 1.02 233 176 3.3i 179 2.1""2 2 ... 6 1,1.14 U17 1,93 22"" 2.5!! U4 '"" 1.94 2.20 '48 '.66 .82 20' 1.42 U7 1.71 I.MII ,. 62.') lS0 1.000 6,260 ,.46 19.5 39.5 "".S '"" 8.62 14' 26.S J~' '"" tl.""'6 1).' J.J7 4.50 6.23 1.I.3A 1"
2628,unknown,"39.5 "".S '"" 8.62 14' 26.S J~' '"" tl.""'6 1).' J.J7 4.50 6.23 1.I.3A 1.80 3JU '.07 72J 1."" ,-', ,,. , ... 23. , ... 3.89 )20 2.2 .• .... 3.56 US 2.15 27. 'JI '"" 1 0' 2 .• 7 2,96 3.70 ,,, 2.25 , ... 3.21 '"" 2."" 2.)5 DII 1.1'11 1.84 207 139 I.., '65 1.82 2.tJ3 '41 1.5,5 1.6!.! 1.86 '."" 1.46 U7 17n 60 ."" m 1,010 6,]10 9 •• 7 19.5 39.' "".' S.U $.51 \4.0 26.3 3.79 5.69 ,,. '31 ),1. 443 6.12 010 176 3.7' "
2629,unknown,"S.U $.51 \4.0 26.3 3.79 5.69 ,,. '31 ),1. 443 6.12 010 176 3.7' '."" , ... 2.SI '.30 4.2.5 ).82 ,,. 3 0' 3"" S.03 ""' 119 3 .• 5 .., ""' '0> ).'20 , O. I."" 2.38 2.85 :i S. ,., 21' 2..s2 1 ""' '0< , ., 2.21 1.61 "". 1.74 I,Q4 2.21 14(' 1.5) 1.67 1.8'"" 131 '"" '"" 1.66 '"" 132 1;\(,1 1.""7 '20 til. 1 '"" 1,010 6.3AO 9,48 19,5 19.5 ... S 5.U '"" '39 26.2 '"" , ... .lI 1).6 3.12 uo '.1)1 9,11 2.74 3.70 •. on 6.97 "
2630,unknown,"26.2 '"" , ... .lI 1).6 3.12 uo '.1)1 9,11 2.74 3.70 •. on 6.97 2,49 3.27 .. 21) ,,. 2)1 2.97 J.7l 495 2,\8 2.7S 3.39 4AO 108 "", "". '00 19' 13' "". '"" \,19 2.11 2.'fi '.96 , .... 1.90 .16 '2.52 1.50 1.611 .. ' 21' 1 3~ '47 1.,Ii: 173 1.26 I,JS 14) 1.53 1.17 1.22 1.17 1.32 63.3 "". 1,020 6.)7U 9.9 19 .. 19.s ... , 5,13 1j • .$3 1J,9 26.1 3.76 ).63 8.26 1).05 ),11 "".31 • O. '02 '"" ).67 . .., ... 2.47 )"
2631,unknown,"3.76 ).63 8.26 1).05 ),11 "".31 • O. '02 '"" ).67 . .., ... 2.47 ).21 ',1' 5.6~ 2.29 1:.93 3.67 ..... lI • 2.71 13) >4,J I '.0. 2, ~4 3.(l~ 3·t '00 2.30 'Z.71 3.36 "". '0' 2.""0 2IJ1 ,6' '84 .... H' t.-46 1.62 179 2.01 I.~ 139 ,~ 1.~ 1,J9 I""~ l.lI 1.3A '00 ,00 ,­ '00 t Abridged rrom Merrington and Thompson (t 943) amJ published here Wilh the kind permission of the author and Ihe editor or Biom~trika. "
2632,unknown,"~ ~ ~ ~ ~ '"" ... .., '"" > .. ~ "" (') ~ '"" ei Tabl. C.4 Upper percen,age points 8. of 8(p, v"" v,), 'he largest eigenvalue of IB -8(W + 8)1 ~ 0 for p -2.1 v, -hypo.hesls 3: c degrees of freedom; IIt:::O: error degrees or freedom ~ v, ~ , -, I-~ 2 3 5 7 9 II 13 IS 17 19, 21 0.90 0.7950 0.8463 0.8968 0.9221 0.9374 0.9476 0.9550 0.9605 0,9649 0.9683 0.9712 ~ ~ 5 0.95 0.8577 U.8943 0.9296 0.9471 0.9576 "
2633,unknown,"~ 5 0.95 0.8577 U.8943 0.9296 0.9471 0.9576 0.9645 0.9696 0.9733 0.9763 0.9787 0.9806 -< 0.99 0.9377 0.9542 0.9698 0.9774 0.9R19 0.9850 0.9872 0.9888 0.9900 0.9910 0.9918 Iii 0.90 0.6628 ().7307 0.8058 0,8474 0.8741 0.8928 0.9066 0.9'73 0.9257 0.9326 0.9383 7 0.95 11.7370 0.7919 0.8514 0,$839 0.9045 0.9189 0.9295 0.9376 0.9440 0.9493 0,9536 0.99 0.8498 0.8826 0.9173 0.9358 0.9475 0,9556 0.9615 0.9"
2634,unknown,"0.99 0.8498 0.8826 0.9173 0.9358 0.9475 0,9556 0.9615 0.966(1 0.9695 0.9724 0,9748 C).90 0.5632 0.6366 0.7244 0.7768 0.8120 0.8374 0.8567 0.8720 0.8842 0.8943 0.9027 9 0.95 0.6383 0.7017 0.7761 0.8197 0.8487 0.8696 U.R853 0.8976 0.9076 0.9157 0.9225 0.99 0.7635 0.8074 0.8575 0.8862 0.9051 0.9185 0.9286 0.9364 0.9427 0.9478 0.9521 0.90 0.4880 0.5617 0.6551 0.7138 0.7548 0.7854 0.8089 0.8278 0.8433 "
2635,unknown,"0.90 0.4880 0.5617 0.6551 0.7138 0.7548 0.7854 0.8089 0.8278 0.8433 0.8561 08670 II 0.95 0.5603 0.6267 0.7091 0.7600 0.7952 0.8212 0.8413 0.8573 0.8702 0.8810 0,8902 0.99 0.6878 0.7381 0.7989 0.8357 0.8607 0.8790 0.8929 0.9039 0.9128 0.9202 0.9265 0.90 0.4298 0.5016 0.5965 0.6587 0.7035 0.7375 0.7644 0.7862 0.8042 0.8194 0.8324 13 0.95 0.4981 0.5646 0.6507 0.7063 0.7459 0.7757 0.7992 0.8181 0.8337"
2636,unknown,13 0.95 0.4981 0.5646 0.6507 0.7063 0.7459 0.7757 0.7992 0.8181 0.8337 0.8468 0.8580 0.99 0.6233 0.67711 0.7446 0.7872 0.8171 0.8394 0.8568 0.8706 0.8821 0.8915 0.8997 0.90 0.3837 0.4527 0.5468 0.6106 0.6577 0.6942 0.7235 0.7475 0.7675 0.7847 0.7993 15 0.95 0.4478 0.5130 0.6003 0.6584 0.7011 0.7338 0.7598 0.7810 0.7989 0.8138 0.8268 0.99 0.5687 0.6237 0.6954 0.7422 0.775~ 0.R013 0.8216 0.8378 0.85
2637,unknown,"0.99 0.5687 0.6237 0.6954 0.7422 0.775~ 0.R013 0.8216 0.8378 0.8512 0.8629 0.8726 0.90 0.3463 0.4122 0.5043 0.5685 0.6169 0.6550 0.6860 0.7116 0.7334 0.7519 0.7681 17 0.95 0.4065 0.4697 0.5564 0.6160 0.6605 0.6951 0,7232 0.7464 0.7659 0.7825 0.7969 0.99 0.5222 0.5773 0.6512 0.7008 0.7373 0.7652 0.7875 0.8061 0.8212 0.8346 0.8458 0.90 0.3155 0.3782 0.4677 0.5315 0.5805 0.6196 0.6517 0.6786 0.7016 0"
2638,unknown,0.90 0.3155 0.3782 0.4677 0.5315 0.5805 0.6196 0.6517 0.6786 0.7016 0.7214 0.7387 19 0.95 0.3119 0.4327 0.5182 0.5782 0.6238 0.6599 0.6894 0.7139 0.7349 0.7528 0.7684 ~ 0.99 0.4823 0.5369 0.6116 0.6630 0.7014 0.7313 0.7555 0.7156 0.7926 0.8071 0.8198 '<> ~ Ci.9!J 0.2697 0.3493 0.4358 il.4988 0.5479 0.5875 0.6204 0.6482 0.6721 0.6929 0.7' 12 21 (1.95 0.3427 0.4012 0.4847 0.5445 0.5906 0.6277 0.6581
2639,unknown,"21 (1.95 0.3427 0.4012 0.4847 0.5445 0.5906 0.6277 0.6581 0.6R38 0.7058 0.7248 0.7415 ~ '"" n.99 0.4479 0.5014 0.5762 0.6285 0.6685 0.6997 0.7254 0.7469 0.7652 0.78\0 0.7946 OJ> 0.90 11.2677 0.3244 0.4080 0.4699 0.5185 0.5584 0.5918 0.6202 0.6448 0.6663 0.6853 23 (\.95 0.3177 0.3131 0.4551 0.5143 0.5606 0.5981 CI.6294 0.6558 0.6787 0.6986 0.7160 0.99 0.4179 0.4701 0.5443 0.5971 06380 0,6703 0.6970 "
2640,unknown,"0.99 0.4179 0.4701 0.5443 0.5971 06380 0,6703 0.6970 0.7197 0.7391 0.7558 0.7705 0.90 0.2488 0.3027 0.3834 0.4439 0.4921 0.5319 0.5655 0.5944 0.6194 0.6415 0.6610 25 0.95 0.2960 (1.3498 0.4287 1).4872 0.5333 0.5710 0.6027 0.6298 11.6533 0.6738 0.6920 0.99 0.3915 0.4424 0.5155 0.5685 0.6096 0.6429 0.6708 0.694, 0.7143 (1.7319 0.7474 11.90 0.2324 0.2839 0.3616 0.4206 0.4682 0.5077 (1.5413 0.57114 0."
2641,unknown,11.90 0.2324 0.2839 0.3616 0.4206 0.4682 0.5077 (1.5413 0.57114 0.595~ 0.6183 0.6383 27 0.95 0.2171 0.3286 0.4052 0.4626 11.5084 (1.5462 0.5781 0.6056 0.6296 0.6506 0.6693 0.99 0.3682 0.4176 0.4895 0.5422 0.5837 0.6175 {I.6458 0.6700 0.690'1 0.7092 (1.7254 0.90 0.2180 0.2672 0.3420 0.3996 0.446:1 0.4855 0.5191 (1.5482 0.5738 0.5966 0.6170 29 0.95 0.2604 0.3l199 0.3840 11.44(14 0.4855 05232 0.5553 
2642,unknown,"29 0.95 0.2604 0.3l199 0.3840 11.44(14 0.4855 05232 0.5553 0.5830 0.6074 0.6288 0.6480 0.99 (1.3475 0.3954 11.4659 11.5182 0.5597 0.5938 (J,622S 0.6472 0.6687 0.6876 (I. 7M3 090 (1.205:1 0.2523 0.3246 n.3S05 U.4264 1).41,51 11.4985 0.5277 11.5534 11.5763 05969 31 0.95 11.2457 0.2931 0.3650 0.4200 11.4641 11.5022 0.5342 11.5620 0.5866 0.6082 0.6278 n 99 0.3290 0.3754 0.4444 11.4%1 0.5374 11,5717 0."
2643,unknown,"n 99 0.3290 0.3754 0.4444 11.4%1 0.5374 11,5717 0.60118 11.6258 n.M77 0.6671 0.6843 0.9(1 0.194(1 (1.2389 0.3087 (1.3632 11.41181 11.4464 0.4794 0.5084 0.5342 0.5572 0.5781 33 0.95 0.2124 0.2781 0.3478 0.41115 0.4455 04825 0.5145 0.5424 0.567( 0.5890 0.6088 (1.99 0.3123 11.3573 0.4247 11.4751 11.5168 11.5511 0.5803 0.6057 0.6279 0.6476 0.6653 11.90 0.1838 0.2270 0.2943 0,3473 0.3913 0.4290 04617 0"
2644,unknown,"11.90 0.1838 0.2270 0.2943 0,3473 0.3913 0.4290 04617 0.4906 0.5163 U.s394 0.5603 35 0.95 0.2206 0.2645 03320 0.3845 0.4277 0.4644 0.4962 0.5240 0.5487 0.5708 0.5907 0.99 0.2972 0.3408 0.4066 0.4569 0.4974 11.5318 0.56\2 0.5867 0.6091 0.6292 0.6471 0.90 0.1747 0.2161 11.2~12 0.3328 0.3759 OAI29 0.4453 0.4739 0.4995 11.5226 0.5434 > ~ 37 0.95 (1.209~ n.2521 ~:.~~~~ 0.36K9 0.4113 (1.4475 0.4791 0.50"
2645,unknown,"~ 37 0.95 (1.209~ n.2521 ~:.~~~~ 0.36K9 0.4113 (1.4475 0.4791 0.5068 0.5315 0.5536 0.5737 ~ n.99 0.2834 0.3257 0.4394 11.4797 (1.5138 0.5430 0.5688 0.5915 O.~111 0.6299 (1.90 O.16M fI 2062 0.2691 U.3194 0.3616 0.3980 0.4299 0.4583 0.4837 11.5(1(,7 11.5277 x 39 O.9~ 0.2001 0.2408 0.3044 0.3544 11.3961 0.4319 11.4631 0.4906 0.5152 0.5375 11.5576 ,., ... f1.99 0.2709 0.3119 0.3747 0.4232 0.4631 11,49"
2646,unknown,"... f1.99 0.2709 0.3119 0.3747 0.4232 0.4631 11,4969 11 .. 12611 0.5517 0.5747 0.5950 0.6134 ;. '"" II 911 01589 11.1972 0.2582 0.3070 0.3483 0.384(\ 0.4155 0.443(, 0.4689 0.4918 0.5127 ~ 41 (195 n 1912 0.2306 0.2922 0.341 I (URI9 (lAI72 0.4480 0.4754 0.4999 0.5221 0.5423 0.99 11.2594 112993 0.3605 11.41179 0.4412 0.4812 n.~ 1110 0.5358 0.5585 11.5792 0.5977 MUL TIV ARIA TE ANALYSIS • o ~~~N~--'<tC"
2647,unknown,"~~~N~--'<tC()N~-ao~""<fVlN"""" ~~Oo-V'l-oO""<f-CIOCVl-N""<fo.o.~ NV':;-r-OV'l""<f>O_OMr-CIOO~lnr-_ ..,.""<tV':;M""<t""<fM~""<fMMMN""'MNNM cddcicicicicicociddciciocci _O:)M N\COCI -r-o -0000 -r""'Io, r-o...,. MMM NM'"" cioci cicici OV'lM C()Or.n ~--oV')OClO N""<ft-t-o.­ NNN ...---N cicio cicici .... ...,.c Vl""''''' 0\ \Q _ M I:-V'I ON..o -Cr-C NNN--N 6cicicicici «>'>O'<t N..:)oo \Or""Ir-r-Or­ (""-0 0> f'I M V'I r-"
2648,unknown,"NNN--N 6cicicicici «>'>O'<t N..:)oo \Or""Ir-r-Or­ (""-0 0> f'I M V'I r- - - N ooeodo N.,...r-""'...,'"" ~--=c --..... '<t""<fV':; ..o...,M 0""""1'0'> M>ON ON..o 000..,. ""1r-c 1""1,.,-0 NNN -NN --(""I --- cicici cicici cicici doo -0\00 ""<to .... r-.I-M M-N COlD - or, ...... '"" M - '"" '''''''. - -N ~~~ ~~~ ~~~ g~::g 0;;;;; NMM Nt""IM NNN NNN -N N cioci cicici cod cicici cicci cicici cicid - '"" N .-'"" '"" '"" .."
2649,unknown,"- '"" N .-'"" '"" '"" ..... NN M cicici N""<fo,O-oO ...,....., (""\I Moo­ -...,.0000100 NNM -NN dcici cicici r- 0-. ..,. \0 C 0'1 IoC ..,. - NP:""'\t-VI""'''<t -0--.0 r-O'-M VIr-..... ""<f""""'o, --N --N coo ocio coo f""-""<tM r-O""<f ~~~ ;~c; __ N - -('.I I""lf""-""<t InVloe -.cor. f""-In""'"" VI('.It-t-NN N""<tOC _M\O o.-""<f ci C ci ci ci ::i 0-- cicici 000 do: M '"" M ono ... OOOM 0-- cicici r-:;7If""'l -... '"" .-co-"
2650,unknown,"r-:;7If""'l -... '"" .-co-00- cicici -'<>"" 000'"" =~:!. coo ~~~ ~~~ ~~~ g~g: ~6!~ ~~& ~~g: ~~~ ccicicicicicicicicicicidcicidciciciciciccici """" References Aitchison. J., Habbema. J. D . F., and Kay , J. W . (1977). A critical comparison of two methods of discrimination. J. Ro y. Srar;""r. Soc. C. 26, 15-25. Anderson , E. (J 960). A semigraphical method for the analysis of complex problems. Technomerric"
2651,unknown,"problems. Technomerrics. 2, 387-391. Anderson , T. W. (1958). All [norod""Clion ro M""frivariate Srarisricaf Analys;"". Wiley, ew York . Anderson. T . W . (1963). Asymptotic theory for principal component analysis, Ann . Math . Star;"" ... 34, 122-148. Anderson. T . W. and Bahadur. R . R . (1962). Classification into two multivariate normal distributions w ith different covariance matrices. Ann_ Math."
2652,unknown,"normal distributions w ith different covariance matrices. Ann_ Math. Statist, 33, 42(}-431. Anderson. T. W .. Das Gupta , S. P. and Styan, G. P. H , (1972). A Bibliography of Muirivariaoe AnalysiS. Otiver and Boyd, Edinburgh, Andrews, D. F . (1972). Plots of high dimensional data. Biomelrics, 28, 12~-136. Arrow, K. 1 .. Chenery. H. B .. Minha., B . S., and Solow , R. M . (1961). Capital­ labor sub"
2653,unknown,"labor substitution and economic efficiency. Rev. £Can. Stansr., 43, 225-250. Balakrishnan, V . and Sanghvi, L. D. (1968). Distance between populations on the basis of attribute data. Biometrics, 24, 859-865. Ball, G . H . and Hall. D . J. (1970). Some implications of inter-active graphic compu ter systems for data analysis and statistics. Technomerrics. 12, 17-31. Barnard, G . A. (1976). Discussio"
2654,unknown,"Barnard, G . A. (1976). Discussion to ""The ordering of multivariate data"" by Barnett, V . D .. J. Roy. Slansr. Soc. A. 139, 318-355. Barnett. V . D . (1976). The ordering of multivariate data. J. Roy . Star;""r. Soc. A. 139,318-355. Barnett, V . D. and Lewis. T . (1963). A study of the relation between GCE and degree results. 1. R oy. SrariS!. Soc. A. 126, 187-216 . Bar~ett, M . S. (1939). The stan"
2655,unknown,"Bar~ett, M . S. (1939). The standard errors of discriminant function coefficients. J. Roy. Srarisr, Soc., Suppl. 6, 169-173 . Bartlett, M. S. (1947). M ultivariate analysis, J. Roy. SruriS!. Soc. B , 9, 176-197 . Bartlett. M . S. (1951). The effect of standardization on a X ' approximation in factor analysis (with an appendix by Lederman. W .). Bionoerrika, 38, 337-344 , Bartlett, M . S. (1954). A"
2656,unknown,"Bartlett, M . S. (1954). A note on multiplying factors fo; various chi·squared approximations. J. Roy . Starisr. Soc. B. 16, 296-298 . ~ULllVARlATE ANALYSIS 498 Bartlett. M . S . (! 965). Multivariate stalistics. In nleoretical a""d Mathematical Biology (Waterman. T . H. and Morowitz . H. 1.. eds). Blaisdell. New York . Beale, E. M. L. (1970). Selecting an optimum subset. In llileger and Nonlillear"
2657,unknown,"Programming (Abadie . 1.. ed.). North-Holland. Amsterdam. Beale. E . M . L., Kendall. M . G ., and Mann . D . W . (1967). The discarding of variables in multivariate analysis. Biomerrika. 54, 357-366. Bennett. B. M . (1951). Note on a solution of the generalised Behrens-Fisher problem. Anll. lnst. Statist. Math . 2, 87-90. B eran, R. J. (1968). Testing for uniformity on OJ compact homogeneous spac"
2658,unknown,"Appl. Ptob .• 5, 177-195. . . .' Beree. R. and Wilbaux. R . (1935). R echerche statlstlque des relations eXlsta~t entre Ie rendment des plantes de grande cultures et les facteurs meteorologl­ ques en Belgique. B«II. llist. Awon. Stll. R eel •. Gembloux. 4. 32-81 . Bhattacbaryya. A. (1946). On a meaSUIe of divergence between twO m ultinomial populations. Sallkl.ya. 7, 401-406. . . Bingham , C . (19"
2659,unknown,"Ph.D . thesis. Yale University. Bingham . C. (1972). An asymptotic expansion for the distributions of tbe eigen­ values of a 3 by 3 Wi shart matrix. A,In. Math. Statist., 43, 1498-1506 . Bingbam . C. (974). Ao aotipodally symmetric distribution On the sphere. All"". Statist.. 2, 1201-1225 . Bingham . M . S. and Mardi., K. V . (1975). Maximum likelihood characterization of the von Mises distribution"
2660,unknown,"of the von Mises distribution. 10 A Modem Course On Statistical Distriburiotls in Scientific Work (PatU, G. P .• Kotz. S .. and Ord. J. K .. eds). D . Reidel. Boston. Vol. 3. pp. 387-398 . Bock. R. D. (1975). Muttitlariat€. Statistical Methods in Behar..1;our;al Research. McGraw-Hill. ew York . Box , G . E . P. (1949). A gene<al distribution theory for a class of likelihood criteria. Biomerrika.. "
2661,unknown,"criteria. Biomerrika.. 36,317-346. Box, G. E. P . and Tiao, G. C. (1973). Bayesian Inference in Statistifal Researclt. Addison Wesley. R eading. Mass . Cavalli-Sforza. L L. and Edwards. A. W . F. (1967). Phylogenetic analysis: models and estimation procedures. Evolution. 21, 550-570 . Cattell. R. B . (1966). The scree test for the number of factors. Multivariate Behav . R es .• 1, 245-276 . Cberno"
2662,unknown,"Cbernoff, H . (l973). Using faces 10 represent points in k-dimensional space graphically. I. Amer . Statist. Assoc., 68, 361-368. Coc hran, W . G. (1934). The di~tTihution of 'lu;:tn[aric fnems in a DO£D1al system, with applications to the analysis of variance. PrO<.:. Camb. Phil. Soc., 30, 178-191. Cochran . W . G . (1962). On the performance of tbe linear discriminant function. Bull. Inst. Inter"
2663,unknown,"Bull. Inst. Intern. Soarist .. 39. 435-447 . Cormack. R . M . (1971). A review of classification. J. Roy . Statist. Soc. A . 134, 321-367 . Cornish. E . A , (t 954). The muhivariate t-distribution associated wi1h a set of normal sample deviates. Austral. f. Phys .. 1, 531-542 . Cox, D. R . (1972). The analysis of multivariate binary duta. Appl. Srarisr .. 21, 113-120 . Cox. D . R . and Bramhvood. "
2664,unknown,"113-120 . Cox. D . R . and Bramhvood. L (1959). On a discriminatory problem connected with the works of Plato. 1. Roy . Sta.ist. 50<'. B . 21, 195-200. 499 Cox, D. R. and Hinkley. D . V . (1974). l1teoretical Sratistics. Chapman and Hall, London. C raig. A . T . (1943). A note on the independence of certain quadratic forms. Ann. Math . Statist., 14, 195-197 . Dagnelie. P. (1975). Analyse Slatisliq"
2665,unknown,"Dagnelie. P. (1975). Analyse Slatislique ii Plusieurs Variables, Vander. Brussels. Day. . E. (1969). Estimating the components of a mixture of normal distribu­ tions. Biometrika. 56, 301-312 . Deemer. W . Land Olkin. I. (1951). The Iacobians of certain matrix transforma­ tions useful in muJtjvariate analysis. Biometrika. 38, 345-367 . Delany. M . J. and H ealy, M . J. R . (1966). Variation in whit"
2666,unknown,"the British Isles. Proc. Roy . Soc. B , 164, 63-74. Dempster , A . P. (1969). El~menrs of Conrinuous Multivariate Analysis. Addison­ Wesley , Reading. Mass . Dempster, A . P. (1971). An overview o[ multivariate data analysis. j, Multivariate Anal., 1, 3L6-346 . Dempster. A . P. (1972). Covar iance selection. Biometrics, 28, 157-175 . Dbrymes . P. J. (1970). Economerrics: Statistical Foundar'ions a"
2667,unknown,"Harper and Row . London. Dickey. J. M. (1967). Matrix variate generalisations of the Jaultivariate I distribu­ tion and the inverted multivariate 1 distribution. Ami . Math. Stalis •. , 38, 511-5JS. Downs . T . D . (1972). Orientation statistics. Biometrika. 59, 665-675. Dunnett, C. W . and Sobel. M. (1954 ). A bivariate generalization of Student's t distribution with tables for certain cases. Bio"
2668,unknown,"distribution with tables for certain cases. Biometrika. 41, 153-169 . Ealon. M . L. (19721. Mulrivariate Statistical Analysis. Institute of Mathemati cs and S tatistics, University of Copenhagen. Edwards , A . W . F. and Cavalli-Sforza, L L (1965). A method of cluster analysis. Biometric •. 21. 362-375 . Enis. P. (1973). On the relation E( Y) = E(E(X I Y). Biometrika. 60, 432-433 . Everitt. B . (1"
2669,unknown,"Everitt. B . (1974). Clt .. t.r Analysi.s. Heineman Educational, London . Fisher. L and van Ness. J. W . (1971). Admissible clustering procedures. Biom et­ rika. 58, 91-104. rlSher. L. and van ess, 1. W. (1973). Admissible di<criminant analysis. I. Amer. Statist. Assoc .. 68, 603-607 . Fisher. R . A. (1936). The use of multiple measurements in taxonomk problems. Ann . Euge"" .. 7, 179-188 . Fisher."
2670,unknown,"Fisher. R. A. (1947). The analysis of covariance method for Ihe relation between a pan and a whole . Biometrics. 3, 65-68. Fisher. R. A.. (1953). Dispersion on a sphere. Proc. Roy . Soc. A , 217, 295-305 . Fisher, R. A . (1970). Statistical Merhods for Research Workers. 14th ed. Oliver and Boyd . Edinburgh. Foster. F. G. and Ree s. D. H. (1957). Upper percentage points of the generali7.<:u beta di"
2671,unknown,"beta distribution I. Biometrika. 44. 237-247 . Frechel. M . (195l). Sur les tableaux de correlation dont les mages sont donnees . Annates de I'Universitf de Lyon. Section A . Series 3,14, 53-77. Frets, G . P. (1921). Heredity of head form in man . Genetica, 3, J93-3B4 . Friedman. H . P. and Rubin. J. (1967). On some invariate criteria for gTouping data. J. Am.,. Statist. Assoc .. 62, 115Y-117~. Ga"
2672,unknown,"Gabriel, K . R . ([970). On the relation between union intersection and likelihood ratio tests. In Essays in Probabiliry and Statistics, Universily of orth Carolina. Rayleigh. MULTIVARIATE ANALYS1S 500 Gen tleman. W. M. (L965). Robust estimation of multivariate location by minimiz­ ing pili power derivatives. Ph.D . thesis. Princeton University. Giri, N. (1968). On t"",,1S of lbe equality of two co"
2673,unknown,"Statisr., 39, 275-277 . Ginltick. M. A. (1939). On the sampling theory of 1'oots of determinantal equations. Ann. Math. Statist., 10, 203-224 . Girshick. M . A . and Haavelmo , T . (1947). Statistical analysis of the demand for food: examples of simultaneous estimation of structural equations. Ec.onomt l­ rica, 15, 79-110. Glass, D . V . (ed.) (1954). Social Mobility in Britain. Routledge and Kega"
2674,unknown,"Glass, D . V . (ed.) (1954). Social Mobility in Britain. Routledge and Kegan Paul, London . Gnanadesikan , R . (1977). Methods for Statistical Data Analysis of Mul!ivariatt Obseroarions. Wiley, New York . Gnanadesikan , M . and Gupta, S. S. (1970). A selection procedure for multivariate normal distributions in ten:ns of t.he generalised variances. T ecknomelrics., 12, 103-117. Gnanadesikan . R. an"
2675,unknown,"103-117. Gnanadesikan . R. and Kettenring, J. R . (1972). Robust estimates. residuals and outlier detection with multiresponse data. Biomtrrics,28, 8 1-124 . Goodman, L. A. (1972). Some multiplicative models for tbe analysis of cros., classified data. Proc. 6th Berk. Symp . Math . Statist. Prob .. 1, 649-696 . Goodman , N. R. (1963). Statistical analysis based on a certain multivariate complex Gau"
2676,unknown,"complex Gausian distribution (an introduction). Ann. Malh . Statist.. 34, 152- 177. Gower, J. C. (1966). Some distanoe properties of latent root and vector method s in multivariate analysis. Biometrika. 53, 315-328 . Gower. J. C . (1968). Adding a point to vector diagrams in multivariate anal ysi.s ~ Biometrika. 55, 582-585 . Gower , J. C. (197Ia). A general coefficient of similarity and some of i"
2677,unknown,"properties, Biometrics. 27, 857-874 . Gower . J. C. (197Ib). Statistical methods of comparing different llIultivariate analyses of the same data. In _Mathematics ill tlte Archaeolog ical ami Hislorical Sciences. (Hodson. F. R .. Kendall. D . G .. and Tautu. P .. eds). Edinburgh University Press, Edinburgh, pp. 138-149 . Gower, J. C. and Ross. G . J. S. (1969). Minimum spanning trees and single lin"
2678,unknown,"linkage cluster analysis. Appl. Stalist., 18, 54-64. .. . . Graybill, F. A . (L969). lntroducuoll to Matrices wull AppllCQllOl1S In Stallstrcs. Wadsworth , Belmont, CA. Green. B . F . (1952). The orthogonal approximation of an oblique structure in factor analySiS. Psychometrika, 17, 429-440. Guttman , L. (1968). A general non-metric technique [or finding the smallest co-ordinate space for a config"
2679,unknown,"co-ordinate space for a configuration of points. Psychomelrika, 33, 469-506. Hartigan, J. A , (1973). Minimum mutation fits to a given tree. Biometrics, 29, 53-65. Hartigan, J. A . (1975). Clustering Algorithms. Wiley. ew York . Hartigan, J. A.. (1978). Asymptotic distributions for clustering criteria. Ann. Statist., 6, 117-131. Hill. M . O. (1974). Correspondence analysis: a neglected mu ltivaria"
2680,unknown,"Appl. Statist .. 23, 340-354 . Hill, R . C "" Formby , T. B ., and Johnson, S. R. (1977). Component selection norms for principal components regression. Comm . Sfat.-Thtor. Me"" •. A . 6~ 309-334 . 501 REFERENCES Hoerl, A . E . and Kennard , R . W . (1970). Ridge regression. Technometrics,12, 55-67 and 69-82. Holgate. P. (1964). Estimation for the bivariate Poisson distribution. Biometrika, 51, 152-"
2681,unknown,"51, 152-177. Hooper , J. W . (1959). Simultaneous equations and canonical correlation theory. Econometrica, 27, 245-256 . Hopkins, J. W . (1966). Some considerations in multivariate allometry. Biometrics, 22, 747-760 . Horst. P. (1965). Faewr Analysis of Data Matrices. Holt, Rinebart, and Winston, New York. Hotelling, H . (1931). The generalization of Student's ratio. Alln. Math. Statist., 2, 360-"
2682,unknown,"Hotelling, H . (1931). The generalization of Student's ratio. Alln. Math. Statist., 2, 360-378 . H otelling, H. (1933). Analysis of a complex of statistical variables into principal components. J. £du c. Psych., 24, 417-441. 498-520 . Hotelling, H . (1936). Relations between two sets of variables. Biomelrika, l8, 321 -377 . Huber, J. P. (1972). Robust statistics: a review. Ann . Malh. Statist., 43"
2683,unknown,"Huber, J. P. (1972). Robust statistics: a review. Ann . Malh. Statist., 43, 1041- 1067. James. A . T. (1964). Distributions of matrix variates and latent roots derived from normal samples. Ann. Mati •. Statist.. 35, 475-501. Jame s, G . S. (1954). Tests of linear hypotheses in univariate and muJtivariate analysis when tbe ratios of the population variances are unknown. Biometrika, 41, 19-43. Jardi"
2684,unknown,"41, 19-43. Jardine, . (1970). Algorithms, method s and models in the simplification of complex data. Camp . J .. 13, 116-117 . Jardine. N . and Sibson, R. (1971). Malhemalical Taxonomy . Wiley, New York. Jeffers, J. N . R. (1967). Two case studies in the application of principal compo­ nent analysis. Appl. Slatisl., 16, 225-236 . Jeffreys, H. (1961). Thear)' of Probability. Clarendon Press, Oxford"
2685,unknown,"Johnson. N . Land Kotz, S. (1972). Distributio .. s ill Statistics: Colltinuous Mul ­ tivariare Distributions. Wiley. New York . Jolicoeur, P. and Mosimann, J. ([960). Size and shape variation in the painted turtle: a principal component analysis. Growlh , 24, 339-354 . Jolliffe, I. T . (1972). Discarding variables in principal oomponen t analysis r: artificial data. Appl. Stalist., 21, 160-173. J"
2686,unknown,"Jolliffe. I. T . (1973). Discarding variables in principal component analysis lJ: real data. Appl. Slalisl., 22,21-31. Joreskog, K. G . (1967). Some contributions to maximum likelihood factor analysis. Psychometrika, 32. 443-482 . Joreskog. K . G. (1970). A general method for analysis of covariance structures. Biometrika, 57. 239-251. Joreskog, K. G . and Sorbom, D . (1977). Statistical models ana"
2687,unknown,"analysis of longtitudinal data. Tn Latent Variables in Socio-economic Models (Aigner, D . J. and Goldberger, A. S., eds). North-Holland. Amsterdam . Kagan , A. M .. Linnik, Y . V., and Ra o, C. R . (1973). Characterizatioll Problems irl Mathematical Statistics. Wiley. New York . Kaiser, H . F. (1958). The varimax crilerion for analytic rotation in (actor analysis. Psychometrik.a, 23. 187-200 . Kel"
2688,unknown,"Kelker. D . (1972). Distribution theory of spherical distributions and a location­ scale parame ter generalization. Sankhya A , 32,419-430. Kendall, D . G . (1971). Seriation from abuodance matrices. In Malh,matics in Ihe MULTIVARIATE ANALystS 502 Archa,ological and Hisrorical Sciences (Hodson. F. R.. Kendall. D . G .• and Tautu. p .. eds.). Edinburgh University Press. Edinburgh. pp. 215-251. Kend"
2689,unknown,"Kendall. M . G. (1975). Multivariare Analysis. Grillin. London . Kendall. M. G . and Stuart. A . (1958). 'rhe Advan ced Th eor)' of Statistics. Vol. I. Griffin. London . Kendall. M . G . and Stuart. A . (1967). TI .. Advanced Th ,ory of Stolislles. V ol. 2. Grillin. London . Kendall. M. G. and Stuart. A . (1973). The Advanced Theory of SlaliStics. Vol. 3. Griffin, London . Kent, J. T . (1978). Lim"
2690,unknown,"Kent, J. T . (1978). Limiting behaviour of the von Mises-Fisher distribution. Marh. Proc. Comb . Phil. Sac .. 84. 531-536 . Khatri. C. G . (1965). Oassical statistical analysis based on a certain multivariate complex Gau..'iosian distribution. Ami . Mafh . Stati.st. 36, 98-114 . Khatri. C. G . and Mardia. K . V. (1977). The von Mises-Fisher matrix distribution in orientalion statistics. J. Roy . S"
2691,unknown,"in orientalion statistics. J. Roy . Stalisl. Sac. B. 39, 95-106 . Khatri, C . G . and Pillai. K . C. s. (1965). Some re.ults o n the non-central multivariate beta distribution and moments of tTaces of two mattices. Ann . Marh . Slolisl.. 36, 1511-1520 . Kiefer. J. and Schwartz. R . (1965). Admissible B ayes character of T '. R ' and other fully invarjant tests for classicaJ multivariate norm al pr"
2692,unknown,"Matla. Starist.. 36, 747-770. Kmenta. J. (1971). Elements of Econometrics. Collier-Macmillan. London . Korin. B . P. (1968). On the distribution of a statistic used for testing a covariance matrix. Biometrika, S5, 17l-178. Krishnamoorthy. A. S. (1951). Multivariate binomial and Poisson di'tribulions. Sankhyil. 11, 11 7-124. Kruskal. J. B . (1964). No n-metric multidimensio nal scaling. P.,,}'cllom"
2693,unknown,"1-27. 115- 129. Kshin;agar. A . M . (1960). Some extensions of multivariate I-distribution and the multivariate generalization of rhe distribution o( the regression coefficient. Proc. Call1b. Phil. Sac .. 57, 80-86. Kshin;agar, A. M . (196 1). Th e non-central mullivariate beta distribution. Anll. Mallo. Stali."" ., 32, 104-111. Kshin;agar. A . M . (1972). Multivariate Altalysis. Maroell Dekker . e"
2694,unknown,"Lancaster. H . O . (1969 ). The Chi-S4l1ared Distriblltiolt. Wiley, New York . Lance. G . N . and William s. W . J. (1967). A general theory of classificatory sorting strategies: I hierarchial systems. Camp . J .. 9. 373--380. Lawley , D . N . (1959). T est of significance in cano nical analysis. Biome.trika. 46, 59-66. Lawley. D . N .. and Max-weU . A. E . (1971). F actor Analy is as a Slatistica"
2695,unknown,"Merhod , 2nd ed. Butterworths. Londo n. Lewis. T. (1970). 'The Statistician as a Member of Society'. Inaugural leeture. University of Hull. Levy , P. (1937). TIleorie de I'Addition des Variables Atealnires. Gauthier-Villars, Paris. Lingoes. J. C. (971 ). Some boundary conditions for a monolone analysis of symmetric matrices. Psychometrika. 36, 195-203. Lingoes, J. C. and Ro skam , E . E. (1973). A"
2696,unknown,"Lingoes, J. C. and Ro skam , E . E. (1973). A mathematical and empiric,,1 analysis of two multidimensional scaling algorithms. PsycilOmetrika. 38. Monograph Suppl. o. 19. 503 REFERENCES Lubischew. A. A. (1962). On the: use of discriminant functions in taxonomy . Biometrics, 18, 455-477. Lyttleton. R. A . (1953). TIle Comets and tlteir Orig;"". Cambridge University Press, Cambridge . Lyttleton. R. A"
2697,unknown,"Lyttleton. R. A. (1961). On the stalistical loss of long period comets from the solar system 1. Prac. 4th Berkeley Symp . Marh . Srolist. Prob .• 3, 229-244. McRae. D. 1. (1971). MICKA . a FORTRA IV iterative K-means cluster analysis program. Behavioural Sci. 16. 423-424 . Mahalanobis. P. C. (1957 ). The foundations or stalistics. (llikhya. A . 18, UB - 194. Mardia . K. v. (1962). Multivariare Par"
2698,unknown,"Mardia . K. v. (1962). Multivariare Pareto distTibutions. Arln. Matk Sratisl.. 33, 1008-1015. Mardia . K . V. (l964a). Some results on the order statistics of the multivariate normal and Pareto type 1 populations. Aun . Mafh . Stalin .. 35, 1815-1818 . Mardia. K. V . (l964b). Exact distributions of extremes. ranges and midranges in samples from any mu1tivariate population. 1. Indian Statist Assoc "
2699,unknown,"samples from any mu1tivariate population. 1. Indian Statist Assoc .. 2~ 126--130. Mardia. K . V . (1967a). Correlation of the ranges of correlated samples . B iomet­ rika. 54, 529-539 . Mardia , K . V . (l967b). A.. non-parametric test for the bivariate two -sample location problem . J. Roy. Sratist. Sac. B. 29, 320-342 . Mardia , K. V . (1967c). Some contributions to contingency-type bivariatedis"
2700,unknown,"tions. Biometrika, 54, 235-249 . Mardi •. K . V . (J 968 ). Small sample pow er of a no n-parametric test (or Ihe bivariate two-samp le location problem . J. Roy . Statist. Soc. B . 30. 83-92. Mardia. K. V . (1969). On the null distribution of a non-parametric test for the two-sample problem. J. Roy . Sratist. Sac. B . 31, 9 -102. Mardia. K . V . (1970a ). Measures of multivariate skewnes .. 1i: a"
2701,unknown,"applications Bjomerrika, 57. 519-530 . Mardia, K . V . (1970b). Families of Bivariate Disrribwion •. Griffin. London. (No . 27 of Griffin's Statistical Monograph s and Courses.) Mardia . K . V. (197Uc). A translation family ur bivariate distributions an d Frechet bound s. Sanklryii. A. 32. 119-122. Mardia. K. V . (1971). The effect of non-normality on some multivariate tests and robustness to non-"
2702,unknown,"robustness to non-normality in the linear model. Biometrika. 58. 105-121. Mardia . K . V . (1972a ). Statistics of Directional D ata. Academic Press. L ondon . Mardia , K . V (191 2b ). A mulltsamp le uniform sco ~ e:!l test on a clTcie and It parametric competitor. 1. Roy . Slatisi. Soc .. B, 34. lO2-113. Maroia. K . V . (1974 ). Applicatiolls of some measures of multivariate skewnes~ and kurtosi"
2703,unknown,"and kurtosis in testing normality and robustness studies. Sankhyd B . 36, 115-128 . Mardia. K . V . 0975a). Assessment of multinormality and the robustness of Ho telling's T ' test. J. Roy . Statist. Soc. C, 24, 163-171. Mardia, K . V . (1975b) . Distribution theory for the von Mises-Fisher distribution and its applications. In A Modem Course on Statistical Distributions in Scumfijic Work (patil. "
2704,unknown,"Work (patil. G. p .. Kotz, S .• and Ord. J. K .. eds). D. Reidel. Boston. pp. 113-130 . MarcHa, K . V . (1975c). Characterizatio[J of directional distributions. In A Modem Course on Statisrical Dislributions itt Scienlific Work (Patil. G . P .. KOlZ . S .. and Ord , J. K .• eds). D. R eidel. Boston. pp. 364-385. MULTJVARIATE A.JrlfALYS1S 504 Mardia , K. V. (l975d). Statistics of directional data ("
2705,unknown,"SllItist. Soc. B, 37, 349-393. Mardi., K . V . (J977) M'Ihal • .nobis distance """"d angles, In ""Multivariate Analysis-IV"". Krishnaiah. P. R . (ed.). orth Holland, Amsterdam. Mardia , K. V . (1978). Some properties of classical multidimensional scaling. Comm. Statist.-Theor. Meth A, 7. 1233-1241. Mardia. K . V. and Spurr. B . D . (1977). On some tests for the bivariate two­ sample location problem. "
2706,unknown,"sample location problem. J. Amer . Statist. Assoc .. 72, 994-995 . Mardia. K. V. and Thompson, J, W . (I 972). Unified treatment of moment fonnulae. Sakhyu A. 34, 121-132. Mardia. K. V , and Zemroch. P. J. (1918). Tables of the F- and Related Distributions with Algorithms. Academic Press, London. Marriott. F. H . (1952). Tests of significance in canonical analysis. Biometrika. 39, 58-M. Marriott, "
2707,unknown,"Biometrics. 27, 501-514. Marshall. A . W . and Ollun. I. (1967). A multivariate exponential distribution. J, Amer. Stanst. Assoc., 62, 30-44. Massy , W , F. (1965), Principal component analysis in exploratory data research, J. Amer . Statist. Assoc .. 60, 234-256 . Maxwell. A. E. (1961). Recent trends in factor analysis. J. Ro y. Starist, Soc. A. 124,49-59. Merrington, M . and Thompson , C. M. (19"
2708,unknown,"124,49-59. Merrington, M . and Thompson , C. M. (1943). Tables of percentage points of the inverted beta (F) distribution. Biometrika, 33, 14-88. Mitra. S. K. (1969). Some characteristic and non-characteristic properties of the Wishart distribution. Sallkhyu A , 31, 19-22. Morgenstern, D. (1956). Einfache Beispiele Zweidirnesionaler Verteilungen. Mill. Marh , Statist., 8, 234-235 , Morrison. D . F"
2709,unknown,"Morrison. D . F, (1976), Mulrivariate Staristical Methods. 2nd ed. McGraw -Hili. New York. Mosimarm , J. E. (1970). Size allometry: size and shape variables with characteri· zations of the Jognormal and generalized gamma distributions. J. Amer . Starlst. Ass"""" .. 65, 930-945 , Narain. R. D . (1950). On the completely unbiased character of tests of indepen· dence in multivariate normal system. An n"
2710,unknown,"dence in multivariate normal system. An n. Marh . StatiSl .. 21, 293-298 . Ogawa . j, (1949). On the independence of linear and quadratic forms of a random sample from a normal population, Ann. lnsl. Marh . StariSl .. I, S3-108 . aikin, 1. and Tomsky, T . L. (1975). A new class of multivariate tests based on the union-intersection principle. Bull. Insl. Statist Insr .• 46. 202-204 . Pearce, S. C ."
2711,unknown,"Pearce, S. C . (1965). The measurement of a living organism. Biometrie· praximetrie, 6, 143-J52. Pearce. S. C. and Holland. D. A. (1960). Some applications of multivariate methods in botany. Appl. Srarisl., 9, 1-7. Pearson, E. S. (1956). Some aspects of the geometry of statistics. 1. R oy. Srarisl. Soc. A, 119, 125-146 . Pearson. E . S. and Hartley, H. O. (1972). Bimnerrika Tables for SrariSlician"
2712,unknown,"2. Cambridge University Press. Cambridge. Pearson, K. (1901). On lines and planes of closest fit to systems of points in space. Phil. Mag . (6). 2, 559-512 . Piliai. K . C. S. (1955). Some new test criteria in multivariate analysis. Ann . Mar""­ Sratisl., 26, 117-121. 505 REFERENCE..<; PilIai, K. C. S, and Jay.chandran, K. (1967), Power comparison of tests of two multivariate. hypotheses bao;ed on "
2713,unknown,"multivariate. hypotheses bao;ed on four criteria. Biometrika, 54, 195-210. Press. S, J, (1972). Applied Multivariare Allalysis, Holt. Rinehart, and Winston. New York. Puri. M . L. and Sen. P. K . (1971). NOli-parametric Merhods in M""lrivariate Analysis. Wiley. New York . Ramabhandran , V. R . (1951). A multivariate gamma-type distribution. Sankhyii, 11,45-46. Rao~ C. R . (J 948). Tests of signific"
2714,unknown,"Rao~ C. R . (J 948). Tests of significance in multivariate analysis. Biometrika. 35, 5R-79 . Rao. C. R . (1951), An asympto tic expanSIon of the distribution of Wilks' criterion. Bull. Itisl. btremal. StcHisr., 33, 177- 180. Rao. C. R. (1964). The use and mterpretation of principal component analysis in applied research. Sankltyu A. 26, 329-358 . Rao . C. R (1966 ). Covariance adjustment and reiat"
2715,unknown,"Rao . C. R (1966 ). Covariance adjustment and reiated problems in multiv3[iate analysis. In Multivariate Analysis (Krishnaiah. P. R., ed,). Academic Press, ew York . Rao . C . R . (J971), Taxonomy in anthropology. tn Mathematics in the Ar­ chaeological and Hisroncal Sciellces (Hodson . F. R .. KendaH, D, G .• and Tautu. P ., eds). Edinburgh University Press. Edinburgh . Rao , C . R . (1973). Linea"
2716,unknown,"Rao , C . R . (1973). Linear Srarisllcal b tferellt< alld its Appiicario,as. Wiley, New York. Ra o, C. R. and Mitra, S. K . (197 1), Generalised Inverse of Marrices and irs Applicar;o""s. Wiley. New York . Rayleigh, Lord (19 I 9). On the problems of random vibrations and of random flights in one , two or three dimensions. Phil. Mag , (6). 37, 321-347. Reeve . E . C. R . (1941 ). A statistical analy"
2717,unknown,"Reeve . E . C. R . (1941 ). A statistical analysis of taxonomic differences within the genus T amandu Gray (X enorthra). Pro<:. Zool. Soc. Lond. A . 111, 279-302. Rosenblatt, M . (1952). Rem arks o n a multivariate translorrnation. Ann. Marh . Star"",t .. 23, 470-472 . Ross. G , I. S. (1969). Single linkage cluster analysis (Algorithms AS 1.3-15). Appl. Starisl., 18, I 03-1I O. Rothkopf , E. Z. (19"
2718,unknown,"Rothkopf , E. Z. (1957). A measure of stimulus similarity and errors in some paired-associate learning tasks. J. Exp . Psychol .. 53, 94-101. Roy. 5, N . (1957). Some Aspects of Mulrivariate Allalysis. Wiley, ew York. Roy . S. . and Bose. R , C. (J953), Simultaneous confidence interval estimation . Anti, Mat h. Statist., 24, 513-536 . Sanghvi. L. D . (1953). Comparison of genetical and morphologic"
2719,unknown,"study of biological ditrcr~""c~. , Anter. J. Phys. Ant/trap., 11, 385-404. Sanghvi. L. D. and Balakrishnan. V . (1972). Comparison of different measures of genetic distance between human populations. In The Assessmellr of Population Affinities ill Man (Weiner, J. A. and Huizinga, H ., eds). Clarendon Press. Oxford . Schatrot!. M, (1966). Exact distributions of Wilks's likelihood ratio criterion. Bi"
2720,unknown,"Biometrika. 53, 347-358 . Schmidt-Koenig, K. (1965), Current problems in bird orientation, In Advan ces ill the Study of Behaviour (Lehrman , D. 5 .• et 01.. eds), Academic Press, New York . Schoenberg . 1, J. (1935). Remarks to Maurice Fredlet's article ""Sur la definition axiomatique d'une classe d'espace dislancies vectorieltement applicable sur I'espace de Hilbert"". Anti. Marh ., 36, 724-732. M"
2721,unknown,"I'espace de Hilbert"". Anti. Marh ., 36, 724-732. MULTIVARIATE ANALYSlS 506 Scott. A. J. and Symonds . M. J. (1971). Clustering methods based on likelihood ratio criteria. Biometrics. 27. 387-397 . SeaL H. L. (1964). Mulriuariare Sraristical Analysis for Biologists. Methuen. London. Shaver. R. H. (196()). The Pennsylvanian ostracode Bairdia oklahontaensis in Indiana. J. Paleonu>logy. 34. 656-{i70. "
2722,unknown,"Shepard. R . N . (1962a). The analysis of proximilies: multidimensional scaling with an unknown distance function r. Psychometrika. 27, 125-139. Shepard. R . . (l962h). The anal~i!;. of pro)(imities: multidimensional scaling with an unknown distance function 11. Psychomefrika. 27, 219-246. Shepard . R . N . (1963). An analysis of proximities as a technique lor the study of information processing i"
2723,unknown,"information processing in man Human Farrors. S. 19-34 . Sibson. R. (1978). Studies in the. robustness or multidimensional scaling: Proc­ rustes statistics. I. R . Statist. Soc .• B, 40, 234-238 . Silvey. S. D. (1970). SIal/SIICaIInferellce. Penguin. Baltimore. Siskind. V . (1972). Second moments of inverse Wishart-matri elements. Biomet­ rika. 59.691 -<'>92. Smith. C . A . B . (1947) Some example<"
2724,unknown,"Smith. C . A . B . (1947) Some example< 01 discrimination. A ..... Eugell .. 13, 272-282. Sneath. P . H . A . and Sobl. R. R . (1973). N"""",,,ical Taxonomy . W . H . Freeman & Co .. San Francisco. Sokal. R . R. and Michener. C. D . (1958). A statistical method for evaluating systemalic relationships. Uruu. Kallsas Sci. Bull .• 38, L409-1438 . Spearman . C . (904). The proof and measurement uf assoc"
2725,unknown,"things. Am . J. Psycho/.. 15. 72 and 202 . Sprent. P. (1969). Model s ill R egressio"" a/l(l Related Topics. Methuen. London. Sprent. P. (1972). The mathematics 01 size and shape. Biomerrics. 28, 2}-37. Stein. C. M . (1956). lnadmissibility of the usual estimator lor the mean o[ a multivariate nonnal distribution. Proc. 3rd Berk. Symp . Math. Stat Prob .. 1, 197-20<'>. Stephen., M . A . (1962). Exa"
2726,unknown,"Stephen., M . A . (1962). Exact and appropriate tests lor directions. 1. Biomerrika, 49, 463-477 Subrahmaniam. K. aDd Subrahmaniam. K. (1973). Multivariare Analysis. A Selecred a .. d Ab .• rracred Bibliograph)' 1957-1972. Marcel Dekker. ew York. Sugiyama. T . and Tong. H . (1976). On a statistic uselul in dimensionality reduction in multivariate linear stochastic system. Conumm. Slalisl.-Theor. M"
2727,unknown,"reduction in multivariate linear stochastic system. Conumm. Slalisl.-Theor. Merll. A. 5. 711-721. Thompson . C . M . (1941). Tables of percentage points 01 the x' distribution. Brometrika. 32. IlS7-191. Thompson . M . L. (1978). Selection of variables in multiple regression 1. n. It1t. Sratist. Rev .• 46, 1-20. 129-146 , Torgerson. W . S. (1958). Theory WId Methods of Scaling. Wiley. New York. Tyr"
2728,unknown,"Tyror. J. G . (19-7). The distribution of the directions 01 perihelion of long-period comets. MOil. Not. Ro),. Astro ... Soc.. 117. 369-379. Vinod . H . D. (1976). Canonical ridge and econometrics of joint production. J. Ecot1omtrrics.4, 147-166. Von Mises. R . (1918). Uber die ""Ganzahligkeit"" der Atomgewicht UDd verwanle Fragen. Physikal. Z .. 19, 491}-500. Watson. G. N. (1944). A Trealise on Ihe"
2729,unknown,"Cambridge University Press. Cambridge. 507 REFERENCES Watson. G . S. (1956). Analysis 01 dispersion on a sphere. Mon . ot. Roy . AStron. Soc. Geophys. Suppl .• 7. IS}-159. Watson . G . S. and Williams. E. J. (1956). On the construction 01 significance tests on the circle and the sphere. Biomerrika. 43, 344-352. Weinman. D. G. (1966). A multivariate e tension of the exponential distribution. Ph. D "
2730,unknown,"Ph. D . thesis, Arizona Slate University. Welch. B . l. (1939). Note on discriminant functions. Biom<lrika. 31. 218-220. Welch. B . L. (19~7). TIle generalization 01 Student's problem when several populations are involved. Biometrika. 34, 28-35. Wilk. M . B .• Gnanadesikan. R .. and Huyelt. M. 1. (1962). Probability plots for the gamma distribution. TedUiomerrics. 4, 1-20. Wilkinson. 1. H. (1965)."
2731,unknown,"Wilkinson. 1. H. (1965). nle Algebraic EigenvaI"". Problem. Clarendon Press. Oxford . Wilks, S. S. (1962). Mathemalical Stalisrics. Wiley. New York. Wolle. J. H . (1971). A Monte Carlo study 01 the sampling distribution of the likelihood ratio for mixtures of multinormal distribution~. Naval Personne.l a""d Trai""ing Res. Lab. Tech. B«II .• STB 72-2. Wooaing , R . A. (1956). The multivariate dislribu"
2732,unknown,"ables. Biollltrrika. 43. 329-350 \Vright. S. (1954). The inte.rpretation of multivariate systems. In SrarisliCl and Mathematics in BIOlogy (Kemplhorne. 0 .. el al .• eels), Jowa State University Press. Yao . Y . (1965). An approximate degrees of freedom solution to the multivariate Behrens-Fisher problem . Biomerrika. 52. 139-147 Young. G. and H o useholder. A. S. (1938). Discussion o( a set of po"
2733,unknown,"of their mutual distances. Ps),chomelrika. 3, 19-22 List of Main Notations and Abbreviations B B (' .-) BLUE C(x, y) c.£. o D 2 d.£. E F (·) fO FIML GLS H l i.i.d. ILS TV L I LIML LRT between groups SSP matrix beta variable best linear unbiased estimate covariance between x and y characteristic fun ction distance matm Mabalanobis distance distribution function expectation distribution function pro"
2734,unknown,expectation distribution function probability function full information maximum likelihood generalized least squares centring matrix identity matrix independent and identically distributed indirect least squares instrumental variables likelihood log likelihood limited information maximum likelihood likelihood ratio test MANOV A multivariate analysis of variance MDS multidimensional scaling ML maxi
2735,unknown,"ML maximum likelihood m.l.e. N.(.,., I) OLS PCA p.d.f. S SLC SSP maximum likelihood estimate multinormal distribution ordinary least squares principal component analysis probability density function sample covariance matrix standardized linear combination sums of squares and products 509 s. T 2 UIT V(x) W W.(l:, m) X 11 (J UST OF MAIN NOTATIONS M't£> ABBREVlATIONS unbiased sam ple covariance matri"
2736,unknown,"unbiased sam ple covariance matrix Hotelling T 2 statistic union intersection test variance-covariance matrix of x within groups SSP matrix Wishart distribution data matrix Wilks' A statistic greatest root statistic Subject Index ahundancc matrix, 393 adm issibility. 105,306-8 agreeme nt m easures. 408 alienation coefficient. vector. 171 allometry, 239-43 analysis of covariance structures. 276 ant"
2737,unknown,"ant-eater da .. , 335-{,. 35<>-7 union intersection test. 349 apple tree data, 241-2 applicant data. 271-3 archaeology data, 238-9 correspondence analysis. 238-9 seriation, 238-9, 409-11 similarity coefficients. 383 asymptotic distributions Bartlett's approximation, 84 canonical correlation. 298 centraJ limit Iheorem. S 1-2 likelihood ratio statistic. 124 B Bartlett's approximation. 84 Banletfs fa"
2738,unknown,"Banletfs factor score, 274-5 Bayesian inferem.""e. 109-11 duster analysis, 366 discriminant analysis. 304-8.316-7, 329 regression. 180 Behrens-Fisher problem. 142-4 beta distribution (types I and 10, 487-8 matrix (types 1 and 0), 86 Bhattacharyya distance, 378-81 . 393 Bingham distribution. 433-5 entropy 447 estimation 444. 45. blood group data, 3 O. <121 C canonical coordinates 341 , 343 canonical"
2739,unknown,"canonical correlation analysis. 5.281- 99 contingency table, 290-3. 2Y9 discriminant analysis. 320. 330 prediction, 289-90 ridge tecbnique. 298 score. 289-90 canonical correlation coefficient, 283 canonical correlation variable, 283 canon ical correlation veClor, 283 canonical mean. 343. 386 , 419-20 canOnical variate, cluster analysis, 385-7 discriminant analysis, 320, 330 multidimensional scalin"
2740,unknown,"multidimensional scaling, 419-20 test of dimenslonality, 338-50 cat dala. 22-3, 107-8 Cauchy distribution. muHi .. 'ariale, 43 density, 57 discrimination. 326-7 stable 49 central limit theorem, 51-2 centring malrix H , 10. 461-2 characteristic function. 33-4 directional data. 435-6 multlnormal distribution, 39-41, 6 1- 2 chi-squared distribution, 487, 490 cluster analysis, 5, 360-93 distances and "
2741,unknown,"distances and similarities, 375-84 graphical mel hods, 385-7 hierarchical methods, 369-75 511 probabilistic methods, 361-9 Cochran's theorem. 68-9, 90. 93 one -way classification, 139 cofactor, 456-7 comet data, 445 COmmOn factor. 256 commun ality. 257 complele linka&\, (furtbest neighbour) cluster analysis, 374-5 com plex multinormal distribution. 85 conditional dislribution, 27 multi normal dist"
2742,unknown,"multi normal distribution, 63-4, 66. 87 consistency 102 instrumental variables, J87 regression. 174-5, 184, 204 simultaneous equation systems. 208 two-stage least squares, 191 contingency table, 290-3. 299 contrast 167. 337-8 convergence in probability. 52 cork deposit data, 11-12. 15-16 graphical representation. 17-20 hypothesis lesting. 133. 152-3 simultaneous confidence intervals, 145-6 lest fo"
2743,unknown,"145-6 lest for normality, 149, 156 correlation see also canonical correla- tion analysis between veClors 170-171. 183 discriminant analysis 324-5 multiple, J 67-9 discarding of variables, 175 population, 182 sample. 167-9 parllal, 169-70. 183 populalion, 30 l'IampJe, Ll correspondence analysis. 237-9. 252 covariance matrix.. population. 29-30 multinormal. 37. 61-2 samp le, 10,66,69,90,91 distribut"
2744,unknown,"samp le, 10,66,69,90,91 distribution under multinormality, 66 unbiased, 11. 140 Craig's theorem, 69-70. 90. 93, 139 one -way classification, 139 Cramer-Rao lower bound, 99-100. 115 SUBJECT INDE.X Cramer-Wold theorem. 34. 60 cumulative distribution function. 26 o data matrix, 8 multi normal distribution. 64-5 dendrogram, 372-5, 390 design matrix, 157 degenerate rank, 164-7 determinan t, 456-8 deten"
2745,unknown,"determinan t, 456-8 detenninant correlation. 171 , 183 diagonal ma trix. 454-5 directional data, 5, 424-51 basic dislributions. 428-35 descriptive measures , 426-8 distribulion theory, 435-7 estimation, 437-9 hypothesis testing, 439-45 principal component analysis. 443--5 Dirichlet distribution. 44 di~carding of componen ts, 223-5 discarding o( variables, dependence analysis, 175-8. 246 discrimina"
2746,unknown,"discriminant analysis. 322-4, 33 J interdependence analysis, 179-80. 242-3 discrete random vector. 26-7 discriminant analysis, 5, 300-32 Bayesian. 304-8. 316-7. 329 discarding of variables. 322-4 estimated parameters, 309-17 Fisher's rule. 318-20 known parameters. 301-8 misclassification probabilities, 320-2 discriminant (unction, 304 discriminant rule. 300 Bayesian, 304-5, 3 16-7, 329 fisher'S, 3"
2747,unknown,"fisher'S, 318-20 likelihood ratio. 316 maximum likelihood. 30 l, 309-15. 320 distance cluster analysis. 369-75. 389-QO multidimensional scaling, 395 types, 375-81 E econometrics, 185-212 simu ltaneous equation systems , 191- 208 SUBJECT INDEX single~equation esrjmation. 199- 203 system estimation. 203-R techniques. 186--191 efficient score-see score eigenvalue. 214,466--74 eigenvector. 466-74 elli"
2748,unknown,"eigenvector. 466-74 ellipsoid. 38-9. 482-3 endogenous variable. 185-6. 191.210 entropy. 46. 447 equicorrelalion matrix. 461-2 conditional distribution, 64 discriminant analysis. 325-6 estimation. 118. 230 Mahalanobis distance, 54 principal components, 2 J 6 principal factor analysis, 278 estimable linear combinations. 165 estimation. 96--119 Bayesian. 109-11 maximum likelihood. 102-8 robust. 111-3"
2749,unknown,"robust. 111-3 ""euo r in equation"", l89 ""errors in variables"", 189. 221 Euclidean distance-see distance exact identity, 192 exactly identified, 195-7,210 examination data of Hull University students, 293-5 exogenous variable. 185-6. 191. 210 expectation. 28 exponential distribution. multivariate Marshall and Olkin. 45. 56 Weinman , 45, 102-3. 155 exponential family. 45-6, 113-4 extended 47 simple 4"
2750,unknown,"extended 47 simple 46 F F distribution, 487-S relationsbip to Hotelling T"", 74-5 relationship to W ilks' A, S3-4 lables, 492-3 factor analysis, 5, 255-S0 goodness of fit, 267-S maximum likelihood. 263-7 model. 256--61 principal,261-3 relationship wilh principal compo­ nent analysis 275-6 rotalion, 257-S. 26S-73 seores, 273-5 512 Fisher information matri=<, 98. L t4 Cramer-Rao lower bound, 99-100 t"
2751,unknown,"transformation, 116 vague prior. 109-10 Fisher's discriminant rule. 318-20 Hea-beelle data. 32S-9 fowl bone measuremenl data. 252-3 Frechet inequalities, 54 Fret's head dala-see head length and breadlh data Crog dala, 153 full information maximum likelihood (FTML), 207-8 comparison with other estimatOrs. 20S, 212 functional relationship, 222 G gamma distribution, multivariate, 45 Gauss-Markov theo"
2752,unknown,"Gauss-Markov theorem, 172. 184. 204 general linear hypothesis, 161-4 general linear model , 151 generalized (g-) inverse, 459, 476--8 generalized least squares (GlS) consistency. 174-5. 184 regression, 172-3 seemingly unrelated regressions. 204,211-2 generalized variance. 13 geological dala, 150-1 geometrical ideas. 481-5 graphical techniques 17-20 bivariate scatters, 19-20 canonical variates. 385"
2753,unknown,"canonical variates. 385-7 discriminant analysis, 310-311, 312 factoT loadings, 271 harmonic curves. 20. 385-7 principal componenls, 227-8 univariate scatters. 17-9 greatest root statistic, 84--5 tables. 494-6 union intersection test, 139-40, 163- 4.348-50 H harmonic curves, 20, 385-7 head length and breadth data. 120-3 513 canonical correlations. 287-8 independence of brothers. 136--7 sphericity. "
2754,unknown,"sphericity. 134-5 tests on covariances. 127 tests on means. 124-5, 126 Heimerl transformation. 461 ""Heywood case"". 261, 265. 277 hierarchical methods. 369-75, 384-5 homoscedastic. 32 horseshoe effect, 412-3 Hotelling T"" distribulion, 73-5 matrix T,86 non-central. 93 one·sample Slatistic. 125. 130. 144-5 relationship to F distribution. 74-5 simultaneous confidence intervals. 144-6 two-sample statis"
2755,unknown,"144-6 two-sample statistics. 76--7. 139-40. 146 hypothesis of linear constraints, 132 hypothesis testing, 120-56-5.. also likelihood ratio test, union in· tersection test and non­ paramelric test idempotent. 461 identification problem. 164-5 econometrics. 194--5, 210 regression, 164--5 identity matrix. 454 incidence matrix, 383 inconsistency of ordinary lea')t squares, 186--7, 208-9 independence. "
2756,unknown,"independence. 28 multinonnal distribution. 62-3 hypothesis testing, 135 linear forms. 65 of sample mean and cO\13riance matrix, 66. 69. 90, 91 quadratic form . 69-70, 90-1 indirect least squares (ILS), 196.211 informauve prior, 110 inner product, 463 instrumental variables. 186-96 invariant prior-see Jeffreys' invariant prior inverse. 458-9 inverted Wishart distribution. 85 Baye ian inference. 110"
2757,unknown,"Baye ian inference. 110. I 19 iris data, 5-7 SUBIEcr INDEX cluster analYSIS, 360, 363-4 discrimjnant analysis. 309-12, 327 discarding of variables. 323-4 misclassification probabilities, 321-2 multivariate analysis of variance. 344--5, 350 robust estimation, 1l2-3 isOtTOPY lesl. 235. 251 jack.knifing, 322 Jacobian. 35 J Jeffers' data-see pine pitprop data Jeflreys' invariant prior, 109-10. IIS-9 d"
2758,unknown,"discriminant analysis. 317 K Karl Pearson distance, 3771 38l Kronecker product, 8S, 459-60 kurtosis-, multivariate-see skewness and kurtosis, multivariate L least squares estimation-see also under specific headings generalized 172-3 indirect 196 ordinary 171-2 three·stage 205-7 Iwo-stage 190-1. 199-200 likelihood, 96--108 likelihood ratio discriminant rule. 316 likelihood ratio le't, 120-4 asympto"
2759,unknown,"asympto tic distribution. 124 comparison with union inter eetlOn test. 147-S directional data, 439-45 mult'inorrnal data, Behrens-Fisher problem. 142-3 canonical correlation analy~i~. 2S8-9 cluster analysis, 366 factor analysis, 267-8 isotropy, 235. 251 multi-sample tests, 138-42, .134-8 one-sample tests on co\ariances, 126--7, 133-8 one-sample tests on means. 124-6. 131-3 regression, 16l-3, 181 S"
2760,unknown,"SUBJECT INDEX test of dimensionality. 338-48 limited information maximum likeli. hood (LIML ). 200-3 comparison with other estimators. 208.211.212 linear equation, 465 , 47 7 li.near transformation. 2-5, 465-6 Jacobian. 35 multinormal distribution. 62--6 sample. 13-4 logit m odel, 46 log-normal. m ultivariate, 43 M magnet ism data from Icelandic lava flows, 427-8 MahaJ anobis distance, 17. 3 J clu"
2761,unknown,"cluster analysis. 368. 377, 381 multinomial data. 378-9, 393 decompo sition. 78-9 depend ence on dimension. 154-5 discriminant analysis. 303. 323 relation to HotcH ing two·sample r- tatistk. 76-7 Mahalanobis transformation, 14-5. 23 cluster analysis, 365 marginal distribution, 27 matrix. 452 matrix algebra. 452-85 matrix differentiation, 478-9 matrix maximization. 479-8 t matrix T distribution. 86"
2762,unknown,"ma ximum likelihood estimation, 102-8 directional data, 43 7-9. 449, 451 exponential. 102-3 exponential family, 114 multinormal, 103-8 cluster analysis. 36l-9. 386-9 factor analysis. 263-8. 280 full information. 207-8 limited information, 200-3 principal components. 229-30 regression, 158-60 m ean direction, 426 . 441 -3 mean vector. popu lation, 29 multinormal 37, 6J-2 sample. 9-10 distribution u"
2763,unknown,"distribution under multinormality, 66, 69. 90, 91 meteorological data, 247-8 metric, 376 metric methods. 394-404 514 mixture of normals. 55, 388-9 Morrison rar data-see cat data Morse code data. 394-5. 403-4 multidimensional scaling, 5, 394-423 classical solution, 397-404 mulLi-sample problem . 419-20 non-metric meth ods, 396. 413-15 optimality properties, 406-9 principal coordinate analysis. 404-"
2764,unknown,"principal coordinate analysis. 404-6 seriation,409-13 . multinomial distribution. 44. 57 cluster analysis. 377-81. 392 discriminant analysis, 313, 327-8 estimation. 100, 1l4, 115 limit theorem. 52-3 mu ltinormal distribution on se:e also m axim um likelihood estim ation, likelihood ratio test and union intersection test basic properties. 36-41, 59-60 Bayesian inference. 109-1 J central limit theor"
2765,unknown,"characterization, 60--2 conditional. 63-4, 66 , 87 data matrix, ·n. 64 expone ntial family. 46 likelihood, 96-7 ma ximum likelihood estimation. 103-9 mixture. 55. 388-9 singular. 41-3. 63 di criminant analysis, 304 principal compo nent analysis. 220 stable, 49 transforma tions 62-6 multiple correlation-set: correlation mul tiple regression. 158 multiplicity, 467 multi-sample hypothesis. 138-42 clu"
2766,unknown,"multi-sample hypothesis. 138-42 cluster analysis, 361, 367-9 multidimen sional ~caling. 4 I 9-20 multivariate analysis of variance. 333-59 multivariate analysis o f variance (MAN OVA ). 276. 333-59 one-way cJas ification, t 3R-9 . 333- 50 515 discriminant analySiS, 318 profile analysis, 348 regression, 165-7, 1 2 test of dimensionality, 33S--48 two-way classification 350-6 multivariate normal-see "
2767,unknown,"multivariate normal-see multinormal distribution multivariate regression model, 157, 204 non-cen tral Hotelling T o:! distribution, 93 non-central Wishart distribution. 85 non·informative prior-see Jeffreys' in· va.riant prior non-metric meth ods. 396, 413-15 non-normality, 148-9 non -parame tric test. 149-51 non-singular matrix, 457 . .+58 norm, 463 normal distribution. 486-see also muhino rmal d"
2768,unknown,"muhino rmal distribution null space. 464 o open /closed book data. 2-4 canonical correlation analysis. 281-2 prediction , 290 scores, 298 factor analysis goodness 01 fit, 268 maxim um likelihood, 266-7 principal, 262-3 rotation, 270-1 principal compon ents 213 confidence intervals, 234 correlation m alrix, 2J9-20 covariance matrix. 218 graphical representations. 227, 228 ignoring components. 223-4"
2769,unknown,"ignoring components. 223-4- scores. 249 order in probability. (0.0 and 0.('». 52 ordinary least squares. (OLS ) Gauss·Markov theorem , t 72 inconsistency, 186-7, 20S--9 regression, 171-2, 173-4 seemingly unrelated regressions. 204. 211 SUBJECf Il'iUE.X orthogonal matrix, 460-1 orthogonal transformations, 48 1-2 over-identified, 195-7.210 P Pareto diStribution, multivariate, 44. 53. 56-7 estimation"
2770,unknown,"estimation. 118 partial correlation-see: correlation partitioned maLrix, 454 , 457. 459 Pearson distance-see Karl Pearson distance peeling. 111-2 Perron-Frobenius theorem , 252 pine pitprop daLa. 176-8 principal component analysis discarding of components, 225 -7 discarding of variables, 242-3 regression. 245-6 regression analysis. discarding of var· iables, 176-80 Possion distribution. multivaria"
2771,unknown,"56 polar coordinates. 35-6, 424-6 spherical symmetry , 48 positive definite (p.d.), 46 1. 474-6 posirjve semi-definite (p.s.d.), ~61, 474-{) principal axis, 483-4 principal component analysis. 5. 213- 54 allometry 239-42. 252-3 corresponden ce analysis, 237-9, 252 directional data. 44 3-5 discarding 01 components . 223-7 discarding of variables, 242-3 ""errors in variables"", 221-2 graphical represe"
2772,unknown,"graphical representations, 227. 228 hypothesis tests, 233-7 non-scale-invariance, 219-20 , 249 population. 214-6 principal coordinates, 404-6, 420-1 properties, 214-22 regression. 244-6 relationship to factor analysis. 275-6 sample , 217-8 sampling properties, 229--33 principal comp onent transformation population. 38, 214, 275 samp le, 15.217 , SUBJEcr Ir-.'OEX principal coordinate analysis, 404-"
2773,unknown,"principal coordinate analysis, 404-6. 42(}-1 principal (actor analysis. 26l-3, 278 probability density (unctIon (p.d.!.), 26 Procrustes rotation, 416-9 profile analysis, 348 projection, 482 Q Q-techniques, 16 quadratic form l 474-6, 478 qualitative data. c,anonical correlation analysis. 290-5 cluster analysis. 382-4 discriminant analysis. 313 quantitative data cluster analysis, 383-4 R R .techniqo"
2774,unknown,"R R .techniqoes. l6 random sample, 49. 64-5 random walk, 436. 448 rank. 463-5, 47 I rat data. 353-6 Rayleigh test, 439-41 reduced correlatjon rnalrix. 26l-3 reduced (orm. 193-4 Reev e ant·eater data-se£ ani-eater data regression, 2, 157-184 multiple. 158 canonical correlation, 290. 296-7 discarding of variables, 175-80 (east squares estimation. 17l-5 principal components, 244-6 ridge, 253-4 multiv"
2775,unknown,"ridge, 253-4 multivariate design matrix 157, 164-7 estimation, 158--61 hypothesis tests. 161-4 measures of correlation. 167-71 simultaneous confidence intervals. 164 seemingly unrelated, 203-5, 211 regression-like models, 185-6. 191-4 errors in variables. 189, 221-2 resubstitution method , 321-2 resuhant length, 427-8 resullant vector. 427 516 ridge techniques, canonical correlation. 29 regression"
2776,unknown,"regression. 253-4 road distance data, 394-5 classical solution, 409-10 figure, 410 non -metric solution, 413. 418-9 robustness, 111-3, 20S rotation o( (actors. 268-73 s scale invariance, canonical correlation analysis. 286 (actor analysis, 257 lack of in principal component analysis. 219. 249 scaling transformation. 14 scedastic curve, 32 score canonical correlation analysis. 289- 90,298 efficient"
2777,unknown,"90,298 efficient, 97-8. I 14 (actor analysis, 273-5 principaJ component analysis. 217, 249 seemi ngly unrelated regressions. 203- 5, 2 I I seriation. 238-9. 396-7. 409-13 shape, discrimmant analYSIS. 326 principal component analysis, 216. 24(}-1 Shepard-Kruskal solution. 413-5 sbrew data cluster analysis complete linkage, 375 graphical, 385-7 probabilistic. 368-9 single linkage. 372-3 test of dime"
2778,unknown,"single linkage. 372-3 test of dimensionality, 345-8. 358-9 similarity, 381-4. 392-3 multidimensional scaling, 394, 402-4 ..imultaneous confidence interval, 144-5 multivariate analysis of variance. 349-50,355 one-sample, 144-6 regressiol), 164 two·sample, 146-7 simu ltaneous equation system, 191-4 517 single-equation estImation. 199-203 singular matrix. 457 singular multinormal- see multinormal dis"
2779,unknown,"distribution singular value decomposition theorem, 473-~ size discriminant analysis. 326 principal component analysis, 216, 240-1 kewness and kurtosis, multivariate populalion. 31 mu1tinormal. 41 sarople, 2(}-2, 57-8 distribution under muhinormality , 148-9, J56 social mobility data. 290-3 specific (actor. 256 specific variance, 257 spectral decomposition theorem. 469- 70 spherical >-ymmelry. ~7-9"
2780,unknown,"70 spherical >-ymmelry. ~7-9, 57, 134-5 sphericity test, 134-5 stable distribution. ~9 standard lrans(ormatiM, 402 standardized linear (.'Ombination (SLC) 213 slress-. 414 structural (orm. 191-3 st!uctural relationship. 222. 277 Student I distribution univariate. 488 lables, 491 multivariate. 43. 57. llO sufficiency. 10(}-1 sum o( squares and products (SSP) matrix one-way classification, 138-9, 33"
2781,unknown,"disl.Timinanl analysis 318-9 regression 167. J82 regression analysis, 159 two·way classification, 350-3 symmetric matrix. 454 system estimation. 203-8 T t distTibution-see Student , distribu· lion r' distribution-see Hotelling T' dis­ tribution SUBJECT INDEX testable hypothesis, 165, 182 three-stage least squares (3SLS), 205-7 comparison with other estimators. 208.212 threshold cluster analysis. 3"
2782,unknown,"cluster analysis. 370 principal component analysis. 243 Thompson's factor score. 274-5 total variation, 13 principal component analysis, 218-9, 224. 250 hypothesis test 233-4 Irace, 456 trace correlation. 171. 183 transpose. 453. 455-6 triangular matrix. 454 turtle data, 141,236 Iwo-stage leasl squares (2SLS ), I 9(}-1, 209 single equation estimation. 199-200 compar ison with' other estjmaton. 20~"
2783,unknown,"compar ison with' other estjmaton. 20~, 212 sYS tem estimation 206 u U method-see jack-knifing ultrametric distance. 391 ultrametric inequality, 374 under-identified, 195-7. 2l0 uniform distribution, 429. 435-7 hypothesis test, 439-41 uniform score. 150,446 union intersect jon test, 120.127-31 comparison with likelihood ratio test 147-8 muhinormal data Behrens-Fisher problem, 143-4 mUlti-sample te"
2784,unknown,"mUlti-sample tests, 139-40 multivariate analysis of vanancc. 348-50 one-sample tests on covariance-s, 130-1, 136-7 one-sample tests on means . 127- 30. 132-3 regression. 163-4 simultaneous confidence intervals. 144-7 unique factor. 256 unique variance. 257 unit matrix. 454 SUBJECT INDEX v vague prior-see Jeffreys! invariant prior varimax Totation, 269 vector a1ienation ooefficient. 171 von ~ses-Fi"
2785,unknown,"von ~ses-Fisher distribution, 429-33 """",ai, 434 bimodal, 434 entTOpy, 447 estimation, 437-9, 448-9 hypothesis tests, 441-3, 449-50 relationship to multinormal distribu- tion, 448 W W~ks' J\ distribution, 81-5 WlSban distribution. 43, 66-73 inverted, 85 moments. 66. 92 non-central, 85 partitioned. 70-3 518 relation to chi-squared distribution~ 67-8,89 statistics based on , 80--5 Z Zellner's two-sta"
2786,unknown,"67-8,89 statistics based on , 80--5 Z Zellner's two-stage estimator, 204--51 212 Open Access © The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original"
2787,unknown,"author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate- rial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by sta"
2788,unknown,"exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to t"
2789,unknown,"RESEARCH Alakus et al. BMC Bioinformatics (2023) 24:258 https://doi.org/10.1186/s12859-023-05377-y BMC Bioinformatics Covariance regression with random forests Cansu Alakus1*, Denis Larocque1 and Aurélie Labbe1 Abstract Capturing the conditional covariances or correlations among the elements of a multivariate response vector based on covariates is important to various fields includ- ing neuroscien"
2790,unknown,"ing neuroscience, epidemiology and biomedicine. We propose a new method called Covariance Regression with Random Forests (CovRegRF) to estimate the covariance matrix of a multivariate response given a set of covariates, using a random forest framework. Random forest trees are built with a splitting rule specially designed to maximize the difference between the sample covariance matrix estimates of"
2791,unknown,maximize the difference between the sample covariance matrix estimates of the child nodes. We also propose a significance test for the partial effect of a subset of covariates. We evaluate the performance of the proposed method and significance test through a simulation study which shows that the proposed method provides accurate covariance matrix estimates and that the Type-1 error is well contro
2792,unknown,"matrix estimates and that the Type-1 error is well controlled. An application of the pro- posed method to thyroid disease data is also presented. CovRegRF is implemented in a freely available R package on CRAN. Keywords: Covariance regression, Multivariate response, Random forests, Variable importance Introduction Most existing multivariate regression analyses focus on estimating the conditional m"
2793,unknown,"mean of the response variable given its covariates. For example, in traditional regres - sion analysis, the expectation of the response variables is related to linear combina - tions of covariates. While estimating the conditional covariances or correlations among multiple responses based on covariates is also important, it is a less studied problem. For example, functional brain connectivity focu"
2794,unknown,"problem. For example, functional brain connectivity focuses on the exploration of the co-occurrence of brain activity in different brain regions, and this co-variability can be explained as a function of covariates [1 ]. As another example, human biomarkers such as glucose, cholesterol, iron, albumin, and so on, are important for biomedical research and the covariance of these biomarkers is influe"
2795,unknown,"research and the covariance of these biomarkers is influenced by age [2 ]. In micro - biome studies, the changes in the co-occurrence patterns among taxa with respect to the covariates have been studied [3 , 4]. In tasks of cognitive and physical perfor - mance, a research question is whether the correlation between speed and accuracy is influenced by other covariates, such as sustained attention "
2796,unknown,"influenced by other covariates, such as sustained attention or age [5 ]. In neuroscience, the associations of functional criticality with intelligence can be affected by age [6 ]. In all these examples, the main goal could be just to estimate the conditional covariance *Correspondence: cansu.alakus@hec.ca 1 Department of Decision Sciences, HEC Montréal, Montréal, Canada Page 2 of 19Alakus et al. B"
2797,unknown,"Montréal, Canada Page 2 of 19Alakus et al. BMC Bioinformatics (2023) 24:258 between multiple responses, e.g. for microbiome data, the goal is to estimate net - work changes with respect to a set of covariates. Another interesting application of estimating covariance matrices based on covariates is to verify the homoscedasticity assumption in classical multivariate regression. In cases where testin"
2798,unknown,"assumption in classical multivariate regression. In cases where testing the effect of covariates on the covariability of the response variables leads to the rejection of the null hypothesis, conditional estimates of the covariance matrix can be used to have valid inference, for example to build multivariate confidence or prediction regions. In general terms, let Yn×q be a matrix of q response vari"
2799,unknown,"vations, where y⊤ i represents the ith row of Y . Similarly, let Xn×p be a matrix of p covariates available for all n observations, where x⊤ i represents the ith row of X . For an observation with covariates xi and responses yi , the goal is to estimate the con - ditional covariance of the response variables Cov [yi|xi] ≜ /Sigma1xi , which is a measur - able matrix function of covariates xi , and "
2800,unknown,"able matrix function of covariates xi , and to analyze how this conditional covariance matrix varies with respect to the covariates. For this problem, [ 7] use a kernel esti - mator to estimate the conditional covariance matrix for a single continuous covari - ate. However, it is not clear how to extend this approach to situations with multiple covariates. [ 8] propose a linear covariance regressi"
2801,unknown,"where the mean and covariance of the multivariate response is parameterized as func - tions of covariates. This model can also be interpreted as a special random-effects model where Aq×(p+1) and Bq×(p+1) characterize the fixed and random parts of the model, respectively. The scalar γi can be interpreted as an individual-level variability in addi - tion to the random error ǫi . The rows of B indica"
2802,unknown,"tion to the random error ǫi . The rows of B indicate how much this additional variability affects yi . The vector ǫi is of dimension q × 1 and is assumed to be normally distrib - uted. In this framework, they assume that E [γi]= 0 , E [ǫi]= 0 , E [γiǫi]= 0 , Var [γi]= 1 , Var [ǫi]= � , leading to the following covariance matrix [9] illustrate an application of this model with a four-dimensional he"
2803,unknown,[9] illustrate an application of this model with a four-dimensional health outcome. [10] propose a Bayesian nonparametric model for covariance regression within a high- dimensional response context. Their approach relates the high-dimensional multivariate response set to a lower-dimensional subspace through covariate-dependent factor load - ings obtained with a latent factor model. The conditional
2804,unknown,"ings obtained with a latent factor model. The conditional covariance matrix is a quadratic function of these factor loadings. The method is limited to data sets with smaller sample sizes. [11] proposes a parametric Bayesian model for high-dimensional responses. In this model, the conditional covariance matrices vary with continuous covariates. [ 12] pro- pose another covariance regression model wh"
2805,unknown,"pose another covariance regression model where the covariance matrix is linked to the linear combination of similarity matrices of covariates. [13] propose a covariance regres- sion method called Covariate Assisted Principal Regression (CAPR). Unlike the other covariance regression methods described in this section, the CAPR aims to find a linear projection of the multivariate response data such t"
2806,unknown,"yi = (A + γi B) 1 xi + ǫi , /Sigma1xi = /Psi1+ B ( 1 xi )( 1 xi )⊤ B⊤. Page 3 of 19 Alakus et al. BMC Bioinformatics (2023) 24:258 the data variation in the projected space. The model assumes that in the eigendecompo - sition of covariance matrices, all covariance matrices in the sample are diagonalized by the same orthogonal matrix which results in a restrictive covariance matrix form. In this st"
2807,unknown,"In this study, we propose a nonparametric covariance regression method for estimating the covariance matrix of a multivariate response given a set of covariates, using a random forest framework. The above-mentioned methods are very useful in modeling covariance matrix but compared to them the proposed method offers higher flexibility in estimating the covariance matrix given the set of covariates."
2808,unknown,"the covariance matrix given the set of covariates. For example, with the proposed method, we can estimate the conditional covariance matrix for a set of covariates including multi- ple continuous and categorical variables, and the proposed method can be used to capture complex interaction patterns with the set of covariates. Moreover, the proposed method is nonparametric and needs less computation"
2809,unknown,"nonparametric and needs less computational time compared to the parametric models, and can be applied to data sets with larger sample sizes. Random forest [14] is an ensemble tree-based algorithm involving many decision trees, and can also be seen as an adaptive nearest neighbour predictor [15–21]. In the proposed random forest framework, we grow each tree with a splitting rule specially designed "
2810,unknown,"maximize the difference in the sample covariance of Y between child nodes. For a new observation y∗ with covariates x∗ , the proposed random forest finds the set of nearest neighbour observations among the out-of-bag (OOB) observations that are not used in the tree growing process. This set of nearest neighbour observations is then used to estimate the conditional covariance matrix of y∗ given x∗ "
2811,unknown,"the conditional covariance matrix of y∗ given x∗ . In each tree built in the proposed random forest framework, the set of covariates is used to find subgroups of observations with simi- lar conditional covariance matrices, assuming that they are related to conditional covari- ance matrices. We propose a hypothesis test to evaluate the effect of a subset of covariates on the estimated covariance ma"
2812,unknown,"on the estimated covariance matrices while controlling for the others. We investigate two particular cases, the global effect of the covariates and the partial effect of a single covariate. This paper is organized as follows. In Section Method, we give the details of the proposed method, significance test and variable importance measure. The simulation study results for accuracy evaluation, global"
2813,unknown,"for accuracy evaluation, global and partial effects of covariates, and variable importance are presented in Section Simulations. We provide a real data example in Section Real data example, and conclude with some remarks in Section Concluding remarks. Method Let /Sigma1xi be the true conditional covariance matrix of yi based on covariates xi , and /Sigma1X be the collection of all conditional cova"
2814,unknown,"/Sigma1X be the collection of all conditional covariance matrices for n observations, /Sigma1X ={ /Sigma1xi : i = 1, ... ,n} . Similarly, let ˆ/Sigma1xi be the estimated conditional covariance matrix of yi based on covariates xi , and ˆ/Sigma1X be the collection of all estimated conditional covariance matrices for n observations, ˆ/Sigma1X ={ ˆ/Sigma1xi : i = 1, ... ,n } . In this section, we desc"
2815,unknown,"posed method in detail. Tree growing process and estimation of covariance matrices for new observations with random forests We aim to train a random forest with the set of covariates X to find subgroups of obser - vations with similar covariance matrices of Y , based on many unsupervised decision trees built with a specialized splitting criterion. The tree growing process follows the Page 4 of 19A"
2816,unknown,"Page 4 of 19Alakus et al. BMC Bioinformatics (2023) 24:258 CART approach [22]. The basic idea of the CART algorithm is to select the best split at each parent node among all possible splits, all evaluated with a selected splitting cri - terion, to obtain the purest child nodes. The algorithm evaluates all possible splits to determine the split variable and split point. Instead of considering all p"
2817,unknown,"determine the split variable and split point. Instead of considering all possible splits at each parent node, the best split search in random forests is confined to a randomly cho - sen subset of covariates that varies from node to node. The splitting process continues until all nodes are terminal. Our goal is to obtain subgroups of observations with distinct covariance matrices. Hence, we propose"
2818,unknown,"Hence, we propose a customized splitting rule that will seek to increase the difference in covariance matrices between two child nodes in the tree [17, 20, 21, 23]. We define /Sigma1L as the sample covariance matrix estimate of the left node as follows: where tL is the set of indices of the observations in the left node, nL is the left node size and ¯YL = 1 n L ∑ i∈tL yi . The sample covariance ma"
2819,unknown,"and ¯YL = 1 n L ∑ i∈tL yi . The sample covariance matrix estimate of the right node, /Sigma1R , is computed in the same way, where nR is the right node size. The proposed splitting crite - rion is where d(/Sigma1L, /Sigma1R) is the Euclidean distance between the upper triangular part of the two matrices and computed as follows: where Dq×q and Eq×q are symmetric matrices. The best split among all p"
2820,unknown,"the one that maximizes (1). The final covariance matrices are estimated based on the random forest. For a new observation, we use the nearest neighbour observations to estimate the final covariance matrix. The idea of finding the nearest neighbour observations, a concept very similar to the ‘nearest neighbour forest weights’ [15, 16], was introduced in [17] and later used in [18–21]. [19] called t"
2821,unknown,"in [18–21]. [19] called this set of observations the Bag of Observations for Prediction (BOP). For a new observation x∗ , we form the set of nearest neighbour observations with the out-of-bag (OOB) observations [24, 25]. We can define the BOP oob for a new observation as where B is the number of trees and Ob(x∗) is the set of OOB observations in the same terminal node as x∗ in the bth tree. Each t"
2822,unknown,"terminal node as x∗ in the bth tree. Each tree is built with a selected random sub-sam - ple instead of a bootstrap sample, i.e. in-bag observations ( Ib ), which has 63.2 percent distinct observations from the original sample. The remaining training observations, /Sigma1L = 1 n L − 1 ∑ i∈tL ( yi − ¯YL )( yi − ¯YL )⊤, (1)√nLnR ∗ d(/Sigma1L, /Sigma1R), (2)d (D,E) = √ q∑ i=1 q∑ j=i ( Dij − Eij )2"
2823,unknown,"(2)d (D,E) = √ q∑ i=1 q∑ j=i ( Dij − Eij )2 , BOP oob(x∗) = B⋃ b=1 O b(x∗), Page 5 of 19 Alakus et al. BMC Bioinformatics (2023) 24:258 namely Ob , are OOB observations for that tree and are not used to build the bth tree. BOP oob is slightly different than the nearest neighbour sets in the previous papers who use in-bag observations to form BOP . Since the OOB observations are not used in the"
2824,unknown,"use in-bag observations to form BOP . Since the OOB observations are not used in the tree building process, for the trees where they are OOB, they act as new observations. Therefore, OOB observations represent a new observation better than in-bag observa - tions. Using OOB observations for neighbourhood construction is similar to the idea of honesty in the context of forests. An honest double-samp"
2825,unknown,of honesty in the context of forests. An honest double-sample tree splits the training subsample into two parts: one part for tree growing and another part for estimating the desired response [26]. We use the nearest neighbour construction idea to estimate the covariance matrices for the new observations. Algorithm 1 describes how to estimate the covariance matrix with OOB observations for a new o
2826,unknown,"the covariance matrix with OOB observations for a new or training observation. After training the random forest with the specialized splitting criterion, for a new observa - tion x∗ , we form BOP oob(x∗) and then we estimate the covariance matrix by computing the sample covariance matrix of the observations in BOP oob(x∗) . See the Supplementary figures 1 and 2 in the Additional file 1 for the res"
2827,unknown,figures 1 and 2 in the Additional file 1 for the results of the simulation study comparing different ways of estimating the final covariance matrix. nodesize tuning The number of observations in the nodes decreases as we progress down the tree dur - ing the tree-building process. The nodesize parameter is the target average size for the terminal nodes. Lowering this parameter results in deeper tre
2828,unknown,"the terminal nodes. Lowering this parameter results in deeper trees, which means more splits until the terminal nodes. Tuning the nodesize parameter can potentially improve the prediction performance [16]. In typical supervised problems where the target is the observed true response, ran - dom forests search for the optimal level of the nodesize parameter by using out-of- bag (OOB) prediction erro"
2829,unknown,"bag (OOB) prediction errors computed using the true responses and OOB predictions. The nodesize value with the smallest OOB error is chosen. However, in our problem, the target is the conditional covariance matrix which is unknown. Therefore, we propose a heuristic method for tuning the nodesize parameter. For nodesize tuning, we use the OOB covariance matrix estimates, as described in Algorithm 1"
2830,unknown,"The general idea of the nodesize tuning method is to find the nodesize level where the average difference between OOB covariance matrix predictions at two consecutive nodesize levels is the smallest among the set of nodesize values. We first train sepa- rate random forests for a set of nodesize values (see the Parameter settings section in simulation study). Then, we compute the OOB covariance mat"
2831,unknown,"Page 6 of 19Alakus et al. BMC Bioinformatics (2023) 24:258 in Algorithm 1 for each random forest. Define MAD (D,E) = 2 q(q+1) ∑q i=1 ∑q j=i|Dij− Eij| . Let ˆ/Sigma1 s xi be the estimated covariance matrix for observation i when nodesize= s . Let s(1) < ... < s(M ) be a set of increasing node sizes. For j={ 1, ... M − 1} , let Then we select s(j) that corresponds to the value j for which MAD j is t"
2832,unknown,"Then we select s(j) that corresponds to the value j for which MAD j is the minimum among {MAD1 ,... ,MADM } . See the Additional file 2 for the results of a nodesize tuning experiment and the illustration of the process with an example. When a node sample size nd is smaller than the number of responses q, the sample covariance matrix becomes highly variable. In fact, if nd − 1 < q , the estimate i"
2833,unknown,"covariance matrix becomes highly variable. In fact, if nd − 1 < q , the estimate is singular and hence non-invertible. Therefore, the tuning set of nodesize levels should be larger than q. In fact, we need more than q distinct values, so we use sub-sampling instead of bootstrap resampling for each tree building step of the proposed method to guarantee distinctness, assuming the observations in the"
2834,unknown,"distinctness, assuming the observations in the original sample are distinct. Significance test The proposed method uses covariates to find groups of observations with similar covari - ance matrices with the assumption that the set of covariates is important to distinguish between these covariance matrices. However, some (or all) covariates might not be rel - evant. In this paper, we propose a hypo"
2835,unknown,"evant. In this paper, we propose a hypothesis test to evaluate the effect of a subset of covariates on the covariance matrix estimates, while controlling for the other covariates. If a subset of covariates has an effect on the covariance matrix estimates obtained with the proposed method, then the conditional covariance matrix estimates given all covari - ates should be significantly different fro"
2836,unknown,"ates should be significantly different from the conditional covariance matrix estimates given the controlling set of covariates. We propose a hypothesis test to evaluate the effect of a subset of covariates on the covariance matrix estimates for the null hypothesis where /Sigma1X is the conditional covariance matrix of Y given all X variables, and /Sigma1Xc is the conditional covariance matrix of "
2837,unknown,"conditional covariance matrix of Y given only the set of controlling X variables. The proposed significance test is described in Algorithm 2. After computing the covariance matrix estimates for all covariates and control variables only, we compute the test statis - tic with where d(., .) is computed as (2). The test statistic specifies how much the covariance matrix estimates given all covariates "
2838,unknown,"matrix estimates given all covariates differ from the estimates given only the controlling set of covariates. As T becomes larger, we have more evidence against H0. We conduct a permutation test under the null hypothesis (3) by randomly permuting rows of X . Let R be the total number of permutations and T r be the global test statistic (4) computed for the rth permuted X . We estimate the test p-v"
2839,unknown,"MAD j = 1 n n∑ i=1 MAD ( ˆ/Sigma1 s(j) xi , ˆ/Sigma1 s(j+1) xi ) . (3)H 0 : /Sigma1X = /Sigma1Xc, (4)T = 1 n n∑ i=1 d (ˆ/Sigma1xi, ˆ/Sigma1xc i ) , Page 7 of 19 Alakus et al. BMC Bioinformatics (2023) 24:258 and we reject the null hypothesis (3) at a pre-specified level α if the p-value is less than α. In the significance test described above, we need to apply the proposed method many times: for t"
2840,unknown,"many times: for the original data with (i) all covariates and (ii) the set of control covariates, and at each permutation for the permuted data with (iii) all covariates and (iv) the set of control covariates. The proposed method applies a nodesize tuning as described in the previous section. Since tuning the nodesize parameter can be computationally demanding, we tune the nodesize for the origina"
2841,unknown,"computationally demanding, we tune the nodesize for the original data with all covariates and with the set of control covariates only and use those tuned values for their corresponding permutation steps. The proposed significance test has two particular cases of interest. The first is to evaluate the global effect of the covariates on the conditional covariance estimates. If X has a global effect "
2842,unknown,"X has a global effect on the covariance matrix estimates obtained with the proposed method, then the conditional estimates /Sigma1X should be significantly different from the unconditional covariance matrix estimate /Sigma1root which is computed as the sample covariance matrix of Y . The null hypothesis (3 ) becomes See the Supplementary Algorithm 1, Additional file 3 for the details of the global"
2843,unknown,"See the Supplementary Algorithm 1, Additional file 3 for the details of the global signifi - cance test. The second case is to evaluate the effect of a single covariate when the other covariates are in the model. In that particular case, the null hypothesis (3) remains. The only difference between the global and partial significance tests is the number of forests we need to train. In the partial s"
2844,unknown,"we need to train. In the partial significance test, we need to train two random forests per sample, one for all covariates and one for the controlling variables, which makes a total 2R + 2 random forests. However, when we test for the global effect, we need to train only one random forest per sample (in total R + 1 random forests) since we do not need to build a random forest for the root node. (5"
2845,unknown,"(5)p = 1 R R∑ r=1 I(T r > T ), (6)H 0 : /Sigma1X = /Sigma1root. Page 8 of 19Alakus et al. BMC Bioinformatics (2023) 24:258 Variable importance For traditional regression tree problems, we can get the variable importance (VIMP) measures by computing the average change in prediction accuracy using the OOB samples. However, the covariance regression problem does not have an observed tar - get. We can"
2846,unknown,"get. We can compute the VIMP measures by using the fit-the-fit approach which has been applied to enhance interpretability of the covariates on the response [21, 27–31]. In the univariate response case, we get the importance measures by fitting a regres - sion forest to re-predict the predicted values. However, in covariance regression, we have a predicted covariance matrix for each observation an"
2847,unknown,"we have a predicted covariance matrix for each observation and not a single value. Therefore, we use a multivariate splitting rule based on the Mahalanobis distance [32] to re-predict the predicted covariance matrices. We begin by applying the proposed method using the original covariates and responses and estimate the covariance matrices as described in Algorithm 1. Next, we train a random forest"
2848,unknown,"matrices as described in Algorithm 1. Next, we train a random forest with the original covariates and the vector of upper-triangular estimated covariance matrix elements as a multivariate response. VIMP measures are obtained from this random forest. Covariates with higher VIMP measures indicate higher importance for the estimation of covariance matrices. The proposed VIMP computation is described "
2849,unknown,"tary Algorithm 2 in the Additional file 4 . Software We have developed an R package called CovRegRF. We used the custom splitting feature of the randomForestSRC package [33] to implement our specially designed splitting criterion in the tree building process. The package is available on CRAN, https:// CRAN.R- proje ct. org/ packa ge= CovRe gRF. Simulations In this section, we perform a simulation "
2850,unknown,"Simulations In this section, we perform a simulation study to demonstrate the performance of the proposed method, validate the proposed significance test with two particular cases- global and partial significance tests-and evaluate the variable importance estimations of the covariates. Data generating process We carry out a simulation study using four Data Generating Processes (DGPs). The details "
2851,unknown,"details of the DGPs are given in the Additional file 5. The first two DGPs are variations of the first simulated data set used in [8 ]. Both DGPs include one covariate and two response variables. The covariate x is generated uniformly on [−1, 1] . In DGP1, the covariance matrix for the observation x i is /Sigma1xi = /Psi1+ Bxi x⊤ i B⊤ where x⊤ i = (1, xi)⊤ . DGP2 is similar to DGP1, except that we"
2852,unknown,"i = (1, xi)⊤ . DGP2 is similar to DGP1, except that we add a quadratic term to the covariance matrix equation such as /Sigma1xi = /Psi1+ B ˙xi ˙x⊤ i B ⊤ where ˙x⊤ i = (1,(xi + x 2 i ))⊤. In DGP3, the vector of covariates includes seven independent variables generated from the standard normal distribution. For the covariance structure, we use an AR(1) structure with heterogeneous variances. The cor"
2853,unknown,"structure with heterogeneous variances. The correlations are generated with all seven covariates according to a tree model with a depth of three and eight terminal nodes. The variances are functions of the generated correlations. In DGP4, the covariance Page 9 of 19 Alakus et al. BMC Bioinformatics (2023) 24:258 matrix has a compound symmetry structure with heterogeneous variances. Both variances "
2854,unknown,"variances and correlations are functions of covariates. The covariates are generated from the standard normal distribution. The correlations are generated with a logit model and the variances are functions of these generated correlations. The number of covariates and response variables varies depending on the simulation settings. For all DGPs, after generating /Sigma1xi , yi is generated from a mu"
2855,unknown,"N (0, /Sigma1xi). Simulation design Accuracy evaluation We perform a simulation study based on the four DGPs described above to evaluate the accuracy of the proposed method for estimating the covariance matrices. For DGP3 and DGP4, we consider five response variables. For each DGP , we use several values of the training sample size n train ={ 50, 100, 200, 500, 1000 } , which generates a total of "
2856,unknown,"settings (4 DGPs × 5 training sample sizes). We repeat each setting 100 times. In each run of the simulations, we generate an independent test set of new observations with n test = 1000. We evaluate the performance of the covariance matrix estimates using the mean abso- lute errors (MAE) computed for both the estimated correlations and standard deviations separately. For the estimated correlations"
2857,unknown,"separately. For the estimated correlations, we compute the MAE between the upper tri - angular (off-diagonal) matrices of the true and estimated correlations over all observa - tions as follows: where CX and ˆCX are the collection of all correlation matrices corresponding to /Sigma1X and ˆ/Sigma1X , respectively. The values ρijk and ˆρijk represent the correlations in row j and column k of Cxi and"
2858,unknown,"of Cxi and ˆCxi , respectively. For the estimated standard deviations, we compute the normalized MAE between the true and estimated standard deviations over all observations as follows: The values σ2 ij and ˆσ2 ij represent the jth diagonal element of /Sigma1xi and ˆ/Sigma1xi , respectively. Smaller values of MAE cor and MAE sd indicate better performance. We compare our proposed method with the o"
2859,unknown,"proposed method with the original Gaussian-based covariance regression model cov - reg developed in [8] which was presented in the Introduction. This method is currently available in the covreg R package [34]. Moreover, as a simple benchmark method, we compute the sample covariance matrix without covariates, which is then used as the covariance matrix estimate for all new observations from the tes"
2860,unknown,"covariance matrix estimate for all new observations from the test set. Variable importance For the variable importance evaluation simulations, we use DGP3 and DGP4 in which we add five noise variables X to the covariates set. As above, we consider several MAE cor( ˆCX,CX) = 2 q(q − 1)n test ntest∑ i=1 q∑ j=1 q∑ k=j+1 |ˆρijk− ρijk|, MAE sd( ˆ/Sigma1X,/Sigma1X) = 1 qntest ntest∑ i=1 q∑ j=1 ⏐⏐⏐⏐ ˆσij"
2861,unknown,"j=1 ⏐⏐⏐⏐ ˆσij− σij σij ⏐⏐⏐⏐. Page 10 of 19Alakus et al. BMC Bioinformatics (2023) 24:258 values for the training sample sizes n train ={ 50, 100, 200, 500, 1000 } , for a total of 10 scenarios studied. We examine whether the estimated VIMP measures tend to rank the important variables first. The variable with the highest VIMP measure has a rank of 1. For each scenario, we compute the average rank "
2862,unknown,"group and for the noise variables group. Evaluating the power of the global significance test We studied four scenarios to evaluate the global effect of the covariates, two of which are under the null hypothesis (6 ) and the other two under the alternative hypothesis. We generate the data sets for these scenarios as follows: 1 H0 (case 1): we generate 5 Y with a constant population covariance matr"
2863,unknown,"X variables which are all independent following a standard normal distribution. In this case, the covariance of Y is independent of X and we are therefore under the null hypothesis. 2 H0 (case 2): we first generate 7 X and 5 Y under DGP3. Then, we replace the X matrix with 10 independent X variables generated from a standard normal distribution. In this case, the covariance of Y varies with some o"
2864,unknown,"this case, the covariance of Y varies with some of the X variables but those X vari - ables are not available in the training set. Therefore, we are again under the null hypothesis. 3 H1 (without noise): we generate 7 X and 5 Y under DGP3, and the covariates are available in the training set. In this case, the covariance of Y varies with all X vari - ables. 4 H1 (with noise): we generate 7 X and 5"
2865,unknown,"ables. 4 H1 (with noise): we generate 7 X and 5 Y under DGP3 and we add 3 independent X variables to the covariates’ training set. In this case, the covariance of Y varies with some of the X variables but not all. Evaluating the power of the partial significance test We can consider three scenarios to evaluate the effect of a single covariate, where one is under the null hypothesis (3 ) and the ot"
2866,unknown,"is under the null hypothesis (3 ) and the other two under the alternative hypothesis. We generate the data sets for these scenarios as follows: 1 H0 : We first generate 2 X and 5 Y with DGP4 and we add 1 independent X variable to the covariates’ training set. In this case, the covariance of Y varies only with the first two X variables. The control set of variables is {X1 ,X2 } and we evaluate the "
2867,unknown,"the X3 variable. Therefore, we are under the null hypothesis. 2 H 1(weakest) : We generate 3 X and 5 Y with DGP4. In this case, the covariance of Y varies with all X variables. The control set of variables is {X1 ,X2 } and we evaluate the effect of X3 , which has the weakest effect on the covariance matrix. 3 H 1(strongest) : We generate 3 X and 5 Y with DGP4. In this case, the covariance of Y aga"
2868,unknown,"again varies with all X variables. But now the control set of variables is {X2 ,X3 } and we evaluate the effect of X1 , which has the strongest effect on the covariance matrix. Page 11 of 19 Alakus et al. BMC Bioinformatics (2023) 24:258 For both the global and partial significance test simulations, we use training sample sizes of n train ={ 50, 100, 200, 300, 500} . The number of permutations and"
2869,unknown,"of n train ={ 50, 100, 200, 300, 500} . The number of permutations and the number of rep - lications for each scenario are set to 500. We estimate the type-1 error as the propor - tion of rejection in the scenarios simulated under H0 and the power as the proportion of rejection in the scenarios simulated under H1 . We estimate a p-value for each repli - cation and we reject the null hypothesis if "
2870,unknown,"cation and we reject the null hypothesis if the p-value is less than the significance level α = 0.05 . Finally, we compute the proportion of rejection over 500 replications. Parameter settings For the simulations, we use the following parameters for the proposed method. We set the number of trees to 1000. Letting p be the number of covariates, then the number of covariates to randomly split at eac"
2871,unknown,"number of covariates to randomly split at each node, mtry, is set to ⌈p/3⌉ . The number of random splits for splitting a covariate at each node, nsplit, is set to max {n train/50, 10} . We tune the nodesize parameter with the set of node - size= {[sampsize × (2−1 ,2−2 ,2−3 ,... ) ] > q} where q is the number of responses and sampsize= 0.632n train . In each replication, covreg is run in four indep"
2872,unknown,"for 8000 iterations, with the first half taken as burn-in. Results Accuracy evaluation Figures 1 and 2 present the accuracy results for 100 repetitions. For each method, we can see the change in MAE cor and MAE sd computed for 100 repetitions with an increasing training sample size. As demonstrated in Fig. 1, for DGP1 and DGP2 when n train = 50 , the proposed method and covreg both have a similar "
2873,unknown,"the proposed method and covreg both have a similar performance with respect to the correlation estimation, with a slight advantage for covreg. For DGP1, covreg per - forms better for both the correlation and standard deviation compared to the proposed method as the sample size increases. This is expected since DGP1 is generated exactly under the covreg model. However, the proposed method still rem"
2874,unknown,"under the covreg model. However, the proposed method still remains competitive. For DGP2, in which a quadratic term is added, the proposed method performs better for the correlation than covreg with increasing sample size. covreg shows better stand - ard deviation estimation performance for smaller sample sizes, but after n train = 500 the proposed method performs slightly better. As demonstrated "
2875,unknown,"proposed method performs slightly better. As demonstrated in Fig. 2, for DGP3, the pro- posed method shows a significantly smaller MAE cor and MAE sd than covreg for all sample sizes. Moreover, for the smaller sample sizes, the proposed method has consider- ably lower variance in MAE. For DGP4, both methods improve with increasing sample size, but the proposed method shows smaller or equal MAEs fo"
2876,unknown,"size, but the proposed method shows smaller or equal MAEs for both correlation and standard deviation estimations. For DGP3 and DGP4, these results are expected, since the proposed method can capture a nonlinear effect. Supplementary figures 5 and 6 in the Additional file 6 present the difference in MAE between the proposed method and covreg. Moreover, we evaluate the accuracy with Stein’s loss wh"
2877,unknown,"covreg. Moreover, we evaluate the accuracy with Stein’s loss which is the Kullback– Leibler divergence between the estimated and true covariance matrices. The conclusions remain the same. See Supplementary Figure 7, Additional file 6. For the nodesize tuning, we compare the accuracy results for different levels of nodesize along with the proposed tuning method. Supplementary figures 3 and 4 in Pag"
2878,unknown,"Page 12 of 19Alakus et al. BMC Bioinformatics (2023) 24:258 the Additional file 2 present the MAE results for all DGPs which show that the tuning method works well. Variable importance Supplementary Figure 8 in the Additional file 4 presents the average ranks of the VIMP measures for both the important and noise sets of variables for DGP3 and DGP4. In all scenarios, the important variables have sm"
2879,unknown,"scenarios, the important variables have smaller average ranks than noise variables. As the sample size increases, the difference between the average ranks of important and noise variables increases, as expected. Global significance test The left plot in Fig. 3 presents the estimated type-1 error and power for different train - ing sample sizes for the two H0 scenarios and two H1 scenarios, respect"
2880,unknown,"ing sample sizes for the two H0 scenarios and two H1 scenarios, respectively. We expect the type-1 error to be close to the significance level ( α = 0.05 ) and we can see that it is well controlled in both cases studied. In both H1 scenarios, the power increases with the sample size. When the sample size is small, adding noise covariates slightly decreases the power, but this effect disappears as "
2881,unknown,"power, but this effect disappears as the sample size increases. Fig. 1 Accuracy evaluation results for DGP1 and DGP2. Smaller values of MAE cor and MAE sd are better Page 13 of 19 Alakus et al. BMC Bioinformatics (2023) 24:258 Partial significance test The right plot in Fig. 3 presents the estimated type-1 error and power for different training sample sizes for the H0 scenario and two H1 scenarios"
2882,unknown,"training sample sizes for the H0 scenario and two H1 scenarios, respectively. As can be seen from the H0 line, the type-1 error is close to the significance level ( α = 0.05 ). In both H1 scenarios, the power increases with the sample size as expected. How - ever, the power is much smaller when one tests the weakest covariate compared to the strongest covariate. Real data example Thyroid hormone, "
2883,unknown,"Thyroid hormone, the collective name for two hormones, is widely known for regulating several body processes, including growth and metabolism [35, 36]. The main hormones produced by the thyroid gland are triiodothyronine (T3) and thyroxine (T4). The syn - thesis and secretion of these hormones are primarily regulated by thyroid stimulating hormone (TSH), which is produced by the pituitary gland. P"
2884,unknown,"hormone (TSH), which is produced by the pituitary gland. Primary hypothyroidism is a condition that occurs when the thyroid gland is underactive and the thyroid hormone produced is insufficient to meet the body’s requirements, which leads to an increase of TSH. Contrarily, when the thyroid gland produces levels of thyroid hormones that are too high, leading to decreased levels of TSH, the resultin"
2885,unknown,"too high, leading to decreased levels of TSH, the resulting condition is hyperthyroidism. Fig. 2 Accuracy evaluation results for DGP3 and DGP4. Smaller values of MAE cor and MAE sd are better Page 14 of 19Alakus et al. BMC Bioinformatics (2023) 24:258 Serum levels of the thyroid hormones and TSH are used to evaluate subjects’ thy - roid function status and to identify subjects with a thyroid dysfu"
2886,unknown,"roid function status and to identify subjects with a thyroid dysfunction. Therefore, establishing reference intervals for these hormones is critical in the diagnosis of thy - roid dysfunction. However, reference ranges are affected by age and sex [37– 41]. Fur- thermore, there is a relationship between TSH and thyroid hormone, and the effects of age and sex on this relationship have not been well "
2887,unknown,"of age and sex on this relationship have not been well described [27, 42]. Serum lev - els of these hormones are also affected by the subject’s diagnosis, i.e. hormone levels would be within the reference ranges for normal subjects and out of range for sub - jects with thyroid dysfunction. The conditional mean of these hormones based on the covariates is studied in the literature, but to our knowl"
2888,unknown,"covariates is studied in the literature, but to our knowledge, no study has yet explic - itly investigated the effect of covariates on the conditional covariance matrix of these hormones. Hence, our contribution is to study the effect of age, sex and diagnosis on the covariance matrix of the thyroid hormones and TSH. In this study, we investigate the thyroid disease data set from the UCI machine l"
2889,unknown,"learning repository [43]. This data set originally included 9172 subjects and 30 vari - ables including age, sex, hormone levels and diagnosis. Following the exclusion crite - ria applied in [42] and [40], we exclude pregnant women, subjects who have euthyroid sick syndrome (ESS), goitre, hypopituitarism or tumour, subjects who use antithy - roid medication, thyroxine or lithium, who receive I131 "
2890,unknown,"roid medication, thyroxine or lithium, who receive I131 treatment, or who have had thyroid surgery. The subjects have different diagnoses including hypothyroidism and hyperthyroidism, as well as normal subjects. Since the sample size of hyperthyroid - ism subjects is small, we exclude them from the analysis. We also exclude the very young and very old subjects, since there are only a few subjects "
2891,unknown,"young and very old subjects, since there are only a few subjects on the extremes. The remaining data set consists of 324 hypothyroidism and 2951 normal subjects ( n = 3275 ) between 20 and 80 years of age (2021 females/1254 males). We want to estimate the covariance matrix of four thyroid-related hormones-TSH, T3, TT4 (total T4) and FTI (free thyroxine index/free T4)-based on covariates and invest"
2892,unknown,"T4) and FTI (free thyroxine index/free T4)-based on covariates and investigate how Fig. 3 Significance test results. The left and right plots present the results for global and partial significance tests, respectively. The proportion of rejection corresponds to the type-1 error for H0 scenarios, and power for H1 scenarios. The dotted line represents the significance level of α = 0.05 Page 15 of 19"
2893,unknown,"Page 15 of 19 Alakus et al. BMC Bioinformatics (2023) 24:258 the relationship between these hormones varies with the covariates. We apply the proposed method with the covariates age, sex and diagnosis to estimate the covari - ance matrix of the four hormones. We first perform the significance test with 500 per - mutations to evaluate the global effect of the three covariates. The estimated p -valu"
2894,unknown,"mutations to evaluate the global effect of the three covariates. The estimated p -value with (5) is 0 and we reject the null hypothesis (6 ), which indicates that the conditional covariance matrices vary significantly with the set of covariates. Next, we apply the proposed method and obtain the covariance matrix estimates. We analyze the cor - relations between hormones as a function of covariates"
2895,unknown,"relations between hormones as a function of covariates, and as shown in Fig. 4, age seems not to have much effect on the estimated correlations. We also compute the variable importance measures, and age (0.001) is found to be the least important vari - able where diagnosis (1.000) is the most important variable, followed by sex (0.011). Therefore, we apply the significance test to evaluate the eff"
2896,unknown,"Therefore, we apply the significance test to evaluate the effect of age on covariance matrices while controlling for sex and diagnosis. Using 500 permutations, the esti - mated p-value with (5 ) is 0.42 and we fail to reject the null hypothesis (3 ), indicating that we have insufficient evidence to prove that age has an effect on the estimated covariance matrices while sex and diagnosis are in the"
2897,unknown,"covariance matrices while sex and diagnosis are in the model. Although the mean lev - els of TSH and thyroid hormones differ with age [37– 39, 41], the correlation between these hormones may not be affected by aging. Similarly, we apply the significance test for diagnosis and sex while controlling for the remaining two covariates, and the estimated p-values for both tests are 0, which indicates th"
2898,unknown,"estimated p-values for both tests are 0, which indicates that both diagnosis and sex, taken individually, have an effect on the covariance matrix of the four hormones. We compare the estimated correlations using the proposed method to the sample correla - tions computed using the whole sample, which are represented with the black dashed lines in Fig. 4. For example, the sample correlation between "
2899,unknown,"lines in Fig. 4. For example, the sample correlation between TSH and T3 over all sam - ples is −0.28 which is not close to the estimated correlation of either hypothyroidism or normal subjects. Furthermore, the estimated variances of the four hormones as a function of age, sex and diagnosis are presented in Supplementary Figure 9 of the Additional file 7. We can see that the variances also differ "
2900,unknown,"Additional file 7. We can see that the variances also differ with covariates. For a mean regression analysis for any of these hormones, assuming a constant variance could yield misleading results. The findings of this analysis suggest that there may be sex and diagnosis specific differ- ences in the regulation of thyroid function, which could have important implications for the diagnosis and treat"
2901,unknown,"the diagnosis and treatment of thyroid disorders in men and women. Clinicians can use this information to better understand the relationship between TSH and thyroid hormones in their patients, and to tailor their diagnostic and treatment approaches accordingly. It is known that the mean levels of TSH and thyroid hormones are different for hypothyroid- ism subjects compared to normal subjects. Howe"
2902,unknown,"ism subjects compared to normal subjects. However, in Fig. 4, we also observe that there is a difference in correlation between hypothyroidism and normal subject classes. Moreover, we see that there is a difference between genders for hypothyroidism subjects for TSH and thyroid hormone correlations, Cor(TSH, T3), Cor(TSH, TT4), Cor(TSH, FTI). Concluding remarks In this study, we propose a nonparam"
2903,unknown,"In this study, we propose a nonparametric covariance regression method, using a ran - dom forest framework, for estimating the covariance matrix of a multivariate response given a set of covariates. Random forest trees are built with a new splitting rule designed Page 16 of 19Alakus et al. BMC Bioinformatics (2023) 24:258 to maximize the distance between the sample covariance matrix estimates of t"
2904,unknown,"to maximize the distance between the sample covariance matrix estimates of the child nodes. For a new observation, the random forest provides the set of nearest neighbour out-of-bag (OOB) observations which is used to estimate the conditional covariance matrix for that observation. We perform a simulation study to test the performance of the proposed method and compare it to the original Gaussian-"
2905,unknown,"the proposed method and compare it to the original Gaussian-based covariance regres - sion model covreg. The average computational times of both methods for the simula - tions are presented in Supplementary Table 1 of the Additional file 8. We can see from the table that the proposed method is significantly faster than covreg. For the real data analysis, the computational time was 200.14 s. It sho"
2906,unknown,"data analysis, the computational time was 200.14 s. It should also be noted that covreg accounts for the uncertainty quantification in estimation of parameters which inevita - bly results in higher computational times compared to non-Bayesian methods. Further - more, we propose a significance test to evaluate the effect of a subset of covariates while the other covariates are in the model. We inve"
2907,unknown,"the other covariates are in the model. We investigate two particular cases: the global effect of covariates and the effect of a single covariate. We also propose a way to com - pute variable importance measures. In this paper, we use the Euclidean distance between the upper triangular part of the two covariance matrices as splitting criterion. This is to avoid double counting the off-diagonal elem"
2908,unknown,"off-diagonal elements since covariance matrices are symmetric. However, several alternative splitting criteria are possible using other measures for computing distance between covariance matrices. We can use alternative distance metrics such as Frobe - nius norm, log-Euclidean, Kullback-Leibler divergence, Fisher Information metric, Bhattacharyya distance [44– 46]. Another possibility is to use te"
2909,unknown,"Bhattacharyya distance [44– 46]. Another possibility is to use test statistics as splitting criteria. There is a large literature on testing the equality of covariance matrices. Here Fig. 4 Estimated correlations between the four hormones as a function of age, sex and diagnosis. Dashed lines represent the sample correlations computed using the whole sample Page 17 of 19 Alakus et al. BMC Bioinform"
2910,unknown,"Page 17 of 19 Alakus et al. BMC Bioinformatics (2023) 24:258 are a few examples [47– 51] and an R package [52] that implements them. Finally, we could use a weighted Euclidean distance between covariance matrices as d (D,E) = √∑q i=1 ∑q j=i w ij(Dij− Eij)2 . This allows to finely control the weight we wish to give to each element of the matrix. This way, the splitting criterion could be based only"
2911,unknown,"based only on the the variance terms or on the covariance terms, for example. Another possibility is that for the final covariance matrix estimation for a new obser - vation, we can use sparse or robust covariance matrix estimations [53, 54] using the nearest neighbour observations. Similarly, it is theoretically possible to use the sparse or robust covariance matrix estimations instead of the sam"
2912,unknown,"or robust covariance matrix estimations instead of the sample covariance matrix for the tree building process. However, the computational time could be a limiting factor. The proposed method can be applied to larger X dimensions. The computational time increases linearly with mtry which is the number of covariates to randomly split at each node. It can also be adapted to larger Y dimensions, but t"
2913,unknown,"each node. It can also be adapted to larger Y dimensions, but the computational time could be a limitation for very large Y dimensions. Computing the sample covariance matrix has a time complexity O(nq2) for q response variables and we compute covari - ance matrix for each node split in each tree of the forest which necessitates many covariance matrix computations. In [21], we proposed a method, R"
2914,unknown,"In [21], we proposed a method, Random Forest with Canonical Correlation Analysis (RFCCA), which estimates the conditional canonical correlation between two multivari - ate data sets given the subject-related covariates. This method conditionally estimates a single parameter, the canonical correlation, that summarizes the strength of the depend- ency between two sets of variables. In this paper, we"
2915,unknown,"ency between two sets of variables. In this paper, we conditionally estimate the whole covariance matrix for one set of variables. Both methods use a splitting criterion that aims at maximizing the heterogeneity of the target parameter to build a forest of trees to obtain a set of local observations that is used to compute the final estimate. Hence, the general methodology in both papers is simila"
2916,unknown,Supplementary information The online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859- 023- 05377-y. Additional file 1. Results of a simulation study comparing different ways of estimating the final covariance matrix Additional file 2. Results of a nodesize tuning experiment Additional file 3. Details of the global significance test Additional file 4. VIMP c
2917,unknown,"Additional file 5. Details of the DGPs Additional file 6. Figures presenting the difference in MAE and accuracy evaluation with Stein’s loss Additional file 7. Figure presenting the estimated variances of four thyroid-related hormones Additional file 8. Table presenting the computational times Acknowledgements Not applicable. Author contributions CA, DL and AL developed the method, designed the si"
2918,unknown,"simulations, prepared figures, drafted manuscript and implemented the R package. DL and AL revised the manuscript. All authors read and approved the final manuscript. Funding This research was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) and by Fondation HEC Montréal. Page 18 of 19Alakus et al. BMC Bioinformatics (2023) 24:258 Availability of data and materi"
2919,unknown,"Availability of data and materials The thyroid disease data set is available at https:// archi ve. ics. uci. edu/ ml/ datas ets/ thyro id+ disea se. CovRegRF is imple- mented in a freely available R package on CRAN, available at https:// cran.r- proje ct. org/ packa ge= CovRe gRF. Declarations Ethics approval and consent to participate Not applicable. Consent for publication Not applicable. Compet"
2920,unknown,"Not applicable. Competing interest The authors declare that they have no competing interests. Received: 8 December 2022 Accepted: 2 June 2023 References 1. Seiler C, Holmes S. Multivariate heteroscedasticity models for functional brain connectivity. Front Neurosci. 2017;11. 2. Le Goallec A, Patel CJ. Age-dependent co-dependency structure of biomarkers in the general population of the United States"
2921,unknown,"United States. Aging. 2019;11(5):1404–26. 3. Levy R, Borenstein E. Metabolic modeling of species interaction in the human microbiome elucidates community- level assembly rules. Proc Natl Acad Sci. 2013;110(31):12804–9. 4. McGregor K, Labbe A, Greenwood CMT. MDiNE: a model to estimate differential co-occurrence networks in microbi- ome studies. Bioinformatics. 2020;36(6):1840–7. 5. Tu D, Mahony B, "
2922,unknown,"with association size. Biostatistics. 2022. 6. Jiang L, Qiao K, Li C. Distance-based functional criticality in the human brain: Intelligence and emotional intel- ligence. BMC Bioinformatics. 2021;22(1):1–17. 7. Yin J, Geng Z, Li R, Wang H. Nonparametric covariance model. Stat Sin. 2010;20:469. 8. Hoff PD, Niu X. A covariance regression model. Stat Sin. 2012;22(2):729–53. 9. Niu X, Hoff PD. Joint m"
2923,unknown,"2019;13(1):321–39. 10. Fox EB, Dunson DB. Bayesian nonparametric covariance regression. J Mach Learn Res. 2015;16(1):2501–42. 11. Franks AM. Reducing subspace models for large-scale covariance regression. Biometrics. 2021. 12. Zou T, Lan W, Wang H, Tsai CL. Covariance regression analysis. J Am Stat Assoc. 2017;112(517):266–81. 13. Zhao Y, Wang B, Mostofsky SH, Caffo BS, Luo X. Covariate Assisted P"
2924,unknown,"comes. Biostatistics. 2021;22(3):629–45. 14. Breiman L. Random forests. Mach Learn. 2001;45(1):5–32. 15. Hothorn T, Lausen B, Benner A, Radespiel-Tröger M. Bagging survival trees. Stat Med. 2004;23(1):77–91. 16. Lin Y, Jeon Y. Random forests and adaptive nearest neighbors. J Am Stat Assoc. 2006;101(474):578–90. 17. Moradian H, Larocque D, Bellavance F. L1 splitting rules in survival forests. Lifet"
2925,unknown,"18. Moradian H, Larocque D, Bellavance F. Survival forests for data with dependent censoring. Stat Methods Med Res. 2019;28(2):445–61. 19. Roy MH, Larocque D. Prediction intervals with random forests. Stat Methods Med Res. 2020;29(1):205–29. 20. Tabib S, Larocque D. Non-parametric individual treatment effect estimation for survival data with random forests. Bioinformatics. 2020;36(2):629–36."
2926,unknown,"Bioinformatics. 2020;36(2):629–36. 21. Alakuş C, Larocque D, Jacquemont S, Barlaam F, Martin CO, Agbogba K, et al. Conditional canonical correlation estimation based on covariates with random forests. Bioinformatics. 2021;37(17):2714–21. 22. Breiman L, Friedman J, Stone CJ, Olshen RA. Classification and regression trees. Boca Raton: CRC Press; 1984. 23. Athey S, Tibshirani J, Wager S. Generalized "
2927,unknown,"23. Athey S, Tibshirani J, Wager S. Generalized random forests. Ann Stat. 2019;47(2):1148–78. 24. Lu B, Hardin J. A unified framework for random forest prediction error estimation. J Mach Learn Res. 2021;22(8):1–41. 25. Alakuş C, Larocque D, Labbe A. The R Journal: RFpredInterval: an R package for prediction intervals with random forests and boosted forests. R J. 2022;14(1):300–20."
2928,unknown,"forests and boosted forests. R J. 2022;14(1):300–20. 26. Wager S, Athey S. Estimation and inference of heterogeneous treatment effects using random forests. J Am Stat Assoc. 2018;113(523):1228–42. 27. Lee K, Bargagli-Stoffi FJ, Dominici F. Causal rule ensemble: Interpretable inference of heterogeneous treatment effects. arXiv preprint arXiv: 2009. 09036. 2020. 28. Spanbauer C, Sparapani R. Nonpara"
2929,unknown,"and Bayesian additive regression trees with mixed models. Stat Med. 2021;40(11):2665–91. 29. Bargagli-Stoffi FJ, De Beckker K, Maldonado JE, De Witte K. Assessing sensitivity of machine learning predictions. A novel toolbox with an application to financial literacy. arXiv preprint arXiv: 2102. 04382. 2021. 30. Bargagli-Stoffi FJ, Witte KD, Gnecco G. Heterogeneous causal effects with imperfect comp"
2930,unknown,"machine learning approach. Ann Appl Stat. 2022;16(3):1986–2009. 31. Meid AD, Gerharz A, Groll A. Machine learning for tumor growth inhibition: Interpretable predictive models for transparency and reproducibility. CPT Pharmacometrics Syst Pharmacol. 2022;11(3):257. 32. Ishwaran H, Tang F, Lu M, Kogalur UB. randomForestSRC: Multivariate splitting rule vignette; 2021. Page 19 of 19 Alakus et al. BMC "
2931,unknown,"• fast, convenient online submission • thorough peer review by experienced researchers in your ﬁeld • rapid publication on acceptance • support for research data, including large and complex data types • gold Open Access which fosters wider collaboration and increased citations maximum visibility for your research: over 100M website views per year • At BMC, research is always in progress. Learn mo"
2932,unknown,"At BMC, research is always in progress. Learn more biomedcentral.com/submissions Ready to submit y our researc hReady to submit y our researc h ? Choose BMC and benefit fr om: ? Choose BMC and benefit fr om: 33. Ishwaran H, Kogalur UB. Fast unified random forests for survival, regression, and classification (RF-SRC); 2022. R pack- age version 3.1.0. 34. Niu X, Hoff P . covreg: A simultaneous regre"
2933,unknown,"35. Yen PM. Physiological and molecular basis of thyroid hormone action. Physiol Rev. 2001;81(3):1097–142. 36. Shahid MA, Ashraf MA, Sharma S. Physiology, thyroid hormone. Treasure Island, FL: StatPearls Publishing;2022. 37. Kapelari K, Kirchlechner C, Högler W, Schweitzer K, Virgolini I, Moncayo R. Pediatric reference intervals for thyroid hormone levels from birth to adulthood: a retrospective s"
2934,unknown,"hormone levels from birth to adulthood: a retrospective study. BMC Endocr Disord. 2008;8(1):15. 38. Aggarwal N, Razvi S. Thyroid and aging or the aging thyroid? An evidence-based analysis of the literature. J Thyroid Res. 2013;2013. 39. Biondi B. The normal TSH reference range: what has changed in the last decade? J Clin Endocrinol Metab. 2013;98(9):3584–7. 40. Strich D, Karavani G, Edri S, Chay C"
2935,unknown,"Endocr Pract. 2017;23(7):803–7. 41. Park SY, Kim HI, Oh HK, Kim TH, Jang HW, Chung JH, et al. Age-and gender-specific reference intervals of TSH and free T4 in an iodine-replete area: data from Korean National Health and Nutrition Examination Survey IV (2013– 2015). PLoS ONE. 2018;13(2): e0190738. 42. Hadlow NC, Rothacker KM, Wardrop R, Brown SJ, Lim EM, Walsh JP . The relationship between TSH and"
2936,unknown,"large population is complex and nonlinear and differs by age and sex. J Clin Endocrinol Metab. 2013;98(7):2936–43. 43. Dua D, Graff C. UCI machine learning repository; 2017. 44. Dryden IL, Koloydenko A, Zhou D. Non-Euclidean statistics for covariance matrices, with applications to diffusion tensor imaging. Ann Appl Stat. 2009;3(3):1102–23. 45. Costa SIR, Santos SA, Strapasson JE. Fisher informatio"
2937,unknown,"2015;197:59–69 46. Bhattacharyya A. On a measure of divergence between two multinomial populations. Sankhyā Indian J Stat (1933- 1960). 1946;7(4):401–406. 47. Nagao H. On some test criteria for covariance matrix. Ann Stat. 1973;1(4):700–9. 48. R Schott J. Some tests for the equality of covariance matrices. J Stat Plann Inference. 2001;94(1):25–36. 49. Ledoit O, Wolf M. Some hypothesis tests for th"
2938,unknown,"sample size. Ann Stat. 2002;30(4):1081–102. 50. Schott JR. A test for the equality of covariance matrices when the dimension is large relative to the sample sizes. Comput Stat Data Anal. 2007;51(12):6535–42. 51. Srivastava MS, Yanagihara H, Kubokawa T. Tests for covariance matrices in high dimension with less sample size. J Multivar Anal. 2014;130:289–309. 52. Barnard B, Young D. Covariance matrix"
2939,unknown,"52. Barnard B, Young D. Covariance matrix Tests; 2018. R package version 0.1.4. 53. Rousseeuw PJ, Driessen KV. A fast algorithm for the minimum covariance determinant estimator. Technometrics. 1999;41(3):212–23. 54. Bien J, Tibshirani RJ. Sparse estimation of a covariance matrix. Biometrika. 2011;98(4):807–20. Feature Article: Introduction to Gaussian Processes An Intuitive Tutorial to Gaussian Pr"
2940,unknown,"Regression Jie Wang, University of Waterloo, Waterloo, ON, N2L 3G1, Canada Abstract—This tutorial aims to provide an intuitive introduction to Gaussian process regression (GPR). GPR models have been widely used in machine learning applications due to their representation flexibility and inherent capability to quantify uncertainty over predictions. The tutorial starts with explaining the basic conc"
2941,unknown,"basic concepts that a Gaussian process is built on, including multivariate normal distribution, kernels, non-parametric models, and joint and conditional probability. It then provides a concise description of GPR and an implementation of a standard GPR algorithm. In addition, the tutorial reviews packages for implementing state-of-the-art Gaussian process algorithms. This tutorial is accessible to"
2942,unknown,"accessible to a broad audience, including those new to machine learning, ensuring a clear understanding of GPR fundamentals. G aussian Process is a key model in proba- bilistic supervised machine learning, widely applied in regression and classification tasks. It makes predictions incorporating prior knowledge (kernels) and provides uncertainty measures over its predictions [1]. Despite its broad "
2943,unknown,"standing GPR can be challenging, especially for pro- fessionals outside computer science, due to its reliance on complex concepts like multivariate normal distribu- tion, kernels, and non-parametric models. This tutorial aims to explain GPR in a clear, ac- cessible way, starting from fundamental mathematical concepts including multivariate normal distribution, ker- nels, non-parametric models, and"
2944,unknown,"probability. To facilitate an intuitive understanding, the tutorial extensively utilizes plots and provides prac- tical code examples, available at https://github.com/ jwangjie/Gaussian-Process-Regression-Tutorial. This tutorial is designed to make GPR accessible to a diverse audience, ensuring that even those new to the field can grasp its core principles. MATHEMATICAL BASICS This section explain"
2945,unknown,"sential for understanding Gaussian process regression (GPR). We start with the Gaussian (normal) distribu- XXXX-XXX © 2023 IEEE Digital Object Identifier 10.1109/XXX.0000.0000000 tion, followed by an explanation of multivariate normal distribution (MVN) theories, kernels, non-parametric models, and the principles of joint and conditional probability. The objective of regression is to formulate a f"
2946,unknown,"a function that accurately represents observed data points and then utilize this function for predicting new data points. Considering a set of observed data points depicted in Fig. 1(a), an infinite array of potential functions can be fitted to these data points. Fig. 1(b) il- lustrates five such sample functions. In GPR, Gaussian processes perform regression by defining a distribution over this i"
2947,unknown,"over this infinite number of functions [2]. Gaussian Distribution A random variable X is Gaussian or normally dis- tributed with mean µ and variance σ2 if its probability density function (PDF) is [3]: PX (x) = 1√ 2πσ exp −(x − µ)2 2σ2 ! . Here, X represents random variables and x is the real argument. This normal distribution of X is usually represented by PX (x) ∼ N(µ, σ2). The PDF of a uni- var"
2948,unknown,"variate normal (or Gaussian) distribution was plotted in Fig. 2, where 1000 points from a uni-variate normal distribution were randomly generated and plotted along the X axis. These randomly generated data points can be ex- pressed as a vector x1 = [ x1 1 , x2 1 , ... ,xn 1 ]. By plotting the vector x1 on a new Y axis at Y = 0, we projected December Published by the IEEE Computer Society Computing"
2949,unknown,"arXiv:2009.10862v5 [stat.ML] 28 Jan 2024 THEME/FEATURE/DEPARTMENT 0.0 0.2 0.4 0.6 0.8 1.05.0 2.5 0.0 2.5 5.0 7.5 10.0 (a) Data point observations 0.0 0.2 0.4 0.6 0.8 1.0 2.5 0.0 2.5 5.0 7.5 10.0 (b) Five possible functions by GPR FIGURE 1: A regression example: (a) Observed data points, (b) Five sample functions fitting the observed data points. 4 2 0 2 4 x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 "
2950,unknown,"0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 PX(x) FIGURE 2: Visualization of 1000 normally distributed data points as red vertical bars on the X-axis, along- side their PDF plotted as a two-dimensional bell curve. the points [ x1 1 , x2 1 , ... ,xn 1 ] into a different space shown in Fig. 3. We did nothing but vertically plot points of the vector x1 in a new Y, x coordinates space. Similarly, another "
2951,unknown,"[x1 2 , x2 2 , ... ,xn 2 ] can be plotted at Y = 1 within the same coordinate framework, as demonstrated in Fig. 3. It’s crucial to remember that both x1 and x2 are a uni- variate normal distribution depicted in Fig. 2. Next, we selected 10 points randomly in vector x1 and x2 respectively and connected these points in order with lines as shown in Fig. 4(a). These connected lines look like linear f"
2952,unknown,"[0, 1] domain. We can use these functions to make predictions for regression tasks if the new data points are on (or proximate to) these linear lines. However, the assumption that new data points will consistently 0.0 0.2 0.4 0.6 0.8 1.0 Y 3 2 1 0 1 2 3 x FIGURE 3: Two independent uni-variate Gaussian vec- tor points plotted vertically within the Y , x coordinates space. lie on these linear functi"
2953,unknown,"we plot more random generated uni-variate Gaussian vectors, say 20 vectors like x1, x2, ... ,x20 within [0, 1] interval, and connecting 10 randomly selected sample points of each vector as lines, we get 10 lines that look more like functions within [0, 1] shown in Fig. 4(b). Y et, we still cannot use these lines to make predictions for regression tasks because they are too noisy. These functions m"
2954,unknown,"are close to each other should have similar output values. These “functions” generated by connecting points from independent Gaussian vectors lack the re- quired smoothness for regression tasks. Therefore, it is necessary to correlate these independent Gaussians, forming a joint Gaussian distribution, as described by the theory of multivariate normal distribution. Multivariate Normal Distribution "
2955,unknown,"Multivariate Normal Distribution It is quite usual and often necessary for a system to be described by more than one feature variable (x1, x2, ... ,xD) that are correlated to each other. If we would like to model these variables all together as one Gaussian model, we need to use a multivariate Gaussian/normal (MVN) distribution model [3]. The PDF of a D-dimensional MVN is defined as [3]: N(x|µ, Σ)"
2956,unknown,"N(x|µ, Σ) = 1 (2π)D/2|Σ|1/2 exp  −1 2(x − µ)TΣ−1(x − µ)  , Here, D represents the number of the dimensionality, x denotes the variable, µ = E[x] ∈ RD is the mean vector, and Σ = cov[x] is the D × D covariance matrix. The Σ is a symmetric matrix that stores the pairwise covariance of all jointly modeled random variables, with Σij = cov(yi , yj ) as its ( i, j) element. A bi-variate normal (BVN) d"
2957,unknown,pler example to understand the MVN concept. A BVN distribution can be visualized as a three-dimensional 2 An Intuitive Tutorial to Gaussian Process Regression December 2023 THEME/FEATURE/DEPARTMENT 0.0 0.2 0.4 0.6 0.8 1.0 Y 3 2 1 0 1 2 x (a) Two Gaussian vectors 0.0 0.2 0.4 0.6 0.8 1.0 Y 2 1 0 1 2 3 x (b) Twenty Gaussian vectors FIGURE 4: Connecting points of independent Gaus- sian vectors by line
2958,unknown,"in vectors x1 and x2, (b) Ten randomly selected points in twenty vectors x1, x2, ... ,x20 . (3-D) bell curve, where the vertical axis (height) rep- resents the probability density, as shown in Fig. 5(a). The ellipse contours on the x1, x2 plane, illustrated in Fig. 5(a) and 5(b), are the projections of this 3- D curve. The shape of ellipses shows the correlation degree between x1 and x2 points, i."
2959,unknown,"of x1 relates to another variable of x2. The function P(x1, x2) denotes the joint probability density of x1 and x2. For a BVN, the mean vector µ is a two- dimensional vector "" µ1 µ2 # , where µ1 and µ2 represent the independent means of x1 and x2, respectively. The covariance matrix is "" σ11 σ12 σ21 σ22 # , with the diagonal terms σ11 and σ22 being the independent variance of x1 and x2, respective"
2960,unknown,"σ21 represent correlations between x1 and x2. The BVN is expressed as: "" x1 x2 # ∼ N "" µ1 µ2 # , "" σ11 σ12 σ21 σ22 #! = N(µ, Σ) . It is intuitively understandable that we need condi- tional probability rather than joint probability for regres- sion tasks. If slicing the 3-D bell curve of a BVN at a certain constant point, as shown in Fig. 5(a), we can obtain the conditional probability distributio"
2961,unknown,"x13 2 1 0 1 2 3x2 3 2 1 01234 P(x1,x2) 0.00 0.05 0.10 0.15 0.20 (a) 3-D bell curve 4 2 0 2 x1 2 1 0 1 2 3 4 x2 (b) 2-D ellipse contours FIGURE 5: BVN PDF visualization: (a) a 3-D bell curve with the height representing the probability density, (b) 2-D ellipse contour projections showing the correlation between x1 and x2 points. with x = x2 = constant, shown in Fig. 6. This conditional distribution"
2962,unknown,"distribution is also Gaussian [1]. Kernels Having introduced the MVN distribution, we want to smooth the functions in Fig. 4(b) for regression tasks. The kernel, or covariance function, plays a pivotal role in this smoothing process, encapsulating our prior knowledge about the functions we aim to model. In regression, we desire the predictions to be smooth and logical: similar inputs should yield "
2963,unknown,"For example, consider two houses, A and B, with comparable size, location, and features; we expect their market prices to be similar. A natural measure of ‘similarity’ between two inputs is the dot product A ·B = ∥A∥∥B∥cosθ, where θ is the angle between two input vectors. Smaller angles, indicating high similarity, correspond to larger dot products, and vice versa. Imagine a scenario in which we c"
2964,unknown,"into a ‘magical’ space, where doing this dot product becomes more powerful and tells us even more about December 2023 An Intuitive Tutorial to Gaussian Process Regression 3 THEME/FEATURE/DEPARTMENT x1 x 2 P (x 1 , x 2 ) P (x1 |x = x2 ) = P (x1 |x2 ) FIGURE 6: The conditional probability distribution P(x1|x2) obtained by cutting a slice on the PDF 3-D bell curve of a BVN. how similar our houses are"
2965,unknown,"called “feature space”. The function that helps us do this lift and enhanced compression in the feature space is named as “kernel function”, denoted as k(x, x′). We do not actually move our data into this new high- dimensional “feature space” (that could be computa- tionally expensive); instead, the kernel function facili- tates the comparison of data though providing us the same dot product resul"
2966,unknown,"is known as the famous “kernel trick”. Formally, the kernel function k(x, x′) computes the similarity be- tween data points in a high-dimensional feature space without explicitly transforming the inputs [1]. Instead of directly computing the dot product of transformed inputs, ⟨ϕ(x), ϕ(x′)⟩, with ϕ being the feature mapping function, the kernel function accomplishes the same result in a computation"
2967,unknown,"result in a computationally efficient manner. The squared exponential (SE) kernel, also known as the Gaussian or Radial Basis Function (RBF) ker- nel, is widely used in Gaussian processes due to its exceptional properties [4]. It is recognized for its adaptability across various functions. Additionally, ev- ery function in its prior is smooth and infinitely differ- entiable, leading to naturally s"
2968,unknown,"model predictions. The SE kernel function is defined as 1: cov(xi , xj ) = exp − (xi − xj )2 2 ! . 1This is a simplified SE kernel without hyperparameters for simplicity. The general SE kernel will be further explained in Sec. Hyperparameters Optimization. 0.0 0.2 0.4 0.6 0.8 1.0 2 1 0 1 2 (a) Ten samples of the 20-VN prior with an identity kernel 0.0 0.2 0.4 0.6 0.8 1.0 1.5 1.0 0.5 0.0 0.5 1.0 1."
2969,unknown,"1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 (b) Ten samples of the 20-VN prior with a RBF kernel FIGURE 7: Samples of twenty-variate normal (20- VN) distribution kernelized prior functions: (a) Ten 20- VN with identity covariance, (b) Ten 20-VN with RBF covariance. In Fig. 4(b), we plotted 20 independent Gaussian vectors by connecting 10 randomly selected sample points from each vector in order by lines. Inst"
2970,unknown,"plotting 20 independent Gaussian, we can generate 10 twenty-variate normal (20-VN) distributions with an identity covariance function as shown in 7(a). It is the same as Fig. 4(b) due to the absence of correlations among points by using identity as its kernel function. Employing an RBF kernel as the covariance function, on the other hand, we got smooth lines observed in 7(b). By integrating covari"
2971,unknown,"smoother lines, and they start to look like functions. It is natural to consider continuing to increase the di- mension of MVN. Here, dimension refers to the number of variables in the MVN. When the dimension of MVN becomes larger, the region of interest will be filled up with more points. When the dimension reaches infinity, there will be a point to represent every possible input point. Utilizing"
2972,unknown,"to fit functions with infinite parameters for regression tasks, thus enabling predictions throughout the region of interest. In Fig. 8, We illustrate 200 samples from a two hundred-variate normal (200-VN) distribution 4 An Intuitive Tutorial to Gaussian Process Regression December 2023 THEME/FEATURE/DEPARTMENT FIGURE 8: Two hundred kernelized prior functions from a two hundred-variate normal distr"
2973,unknown,"to conceptualize functions with infinite parameters. We call these functions “kernelized prior functions”, because there are no observed data points yet. All functions are randomly generated by the MVN model with kernel functions as prior knowledge before having any observed data points. Non-parametric Model This section explains the distinction between paramet- ric and non-parametric models [3]. "
2974,unknown,"assume that the data distribution can be modeled in terms of a set of finite numbers of parameters. In regression, given some data points, we would like to predict the function value y = f(x) for a new specific x. If we assume a linear regression model, y = θ1 + θ2x, we need to identify the parameters θ1 and θ2 to define the function. often, a linear model is insufficient, and a polynomial model w"
2975,unknown,"θ1 +θ2x +θ3x2 is needed. We use the training dataset D comprising n observed points, D = [(xi , yi ) |i = 1, ... ,n] to train the model, i.e. establish a mapping x to y through basis functions f(x). After the training process, all information in the dataset is assumed to be encap- sulated by the feature parameters θ, thus predictions are independent of the training dataset D. This can be expressed"
2976,unknown,"are predictions made at unobserved data points X∗. Thus, when conducting regressions using parametric models, the complexity or flexibility of models is inher- ently limited by the number of parameters. Conversely, if the parameter number of a model grows with the size of the observed dataset, it’s a non-parametric model. Non-parametric models do not imply that there are no parameters; but rather "
2977,unknown,"parameters; but rather they entail an infinite number of parameters. GAUSSIAN PROCESSES Before delving into the Gaussian processes, we first do a quick review of the foundational concepts we have covered. In regression, our objective is to model a function f based on observed data points D (the training dataset) from the unknown function f. Tradi- tional nonlinear regression methods often give a s"
2978,unknown,"function that is considered to best fit the dataset. However, there could be more than one function that fits the observed data points equally well. We ob- served that when the dimension of MVN was infinite, we could make predictions at any point using these infinite numbers of functions. These functions are MVN because it is our (prior) assumption. More formally, the prior distribution of these i"
2979,unknown,"representing the expected outputs of f over inputs x before observing any data. When we start to have observations, instead of infinite numbers of functions, we only keep functions that fit the observed data points, forming the posterior distribution. This posterior is the prior updated with observed data. When we have new observations, we use the current posterior as prior, and new observed data "
2980,unknown,"posterior. Definition of Gaussian processes : A Gaussian process model describes a probability distribution over possible functions that fit a set of points. Because we have the probability distribution over all possible functions, we can compute the means to represent the maximum likelihood estimate of the function, and the variances as an indicator of prediction confidence. Key points include: i"
2981,unknown,"new observations; ii) a Gaussian process model is a probability distribution over possible functions, with any finite samples of functions being jointly Gaussian distributed; iii) the mean function derived from the pos- terior distribution of possible functions is the function used for regression predictions. Now, it is time to explore the standard Gaussian process model. All the parameter definit"
2982,unknown,"classic textbook by Rasmussen (2006) [1]. Besides the covered basic concepts, Appendix A.1 and A.2 of [1] are also recommended reading. The regression function modeled by a multivariate Gaussian is given by: P(f |X) = N(f |µ, K) , where X = [ x1, ... ,xn] represents the observed data points, f = [f(x1), ... ,f(xn)] the function values, µ = [m(x1), ... ,m(xn)] the mean function, and Kij = k(xi , xj"
2983,unknown,"the kernel function, which is a positive definite. With no observation, we default the mean function to m(X) = 0, assuming the data is normalized to zero mean. The Gaussian process model is thus a distribution over functions whose shapes (smoothness) are defined by K. If points xi and xj are considered similar by the December 2023 An Intuitive Tutorial to Gaussian Process Regression 5 THEME/FEATUR"
2984,unknown,"THEME/FEATURE/DEPARTMENT FIGURE 9: A illustrative process of conducting re- gressions by Gaussian processes. The red points are observed data, the blue line represents the mean function estimated by the observed data points, and predictions will be made at new blue points. kernel, their respective function outputs, f(xi ) and f(xj ), are expected to be similar too. The regression process using Gau"
2985,unknown,"using Gaussian processes is illustrated in Fig. 9: given observed data (red points) and a mean function f (blue line) estimated from these observed data points, we predict at new points X∗ as f(X∗). The joint distribution of f and f∗ is expressed as: "" f f∗ # ∼ N "" m(X) m(X∗) # , "" K K ∗ KT ∗ K∗∗ #! , where K = K (X, X), K∗ = K (X, X∗) and K∗∗ = K (X∗, X∗). The mean is assumed to be  m(X), m(X∗) "
2986,unknown,"The mean is assumed to be  m(X), m(X∗)  = 0. While this equation describes the joint probability distribution P(f, f∗ |X, X∗) over f and f∗, in regressions, we need the conditional distribution P(f∗ |f, X, X∗) over f∗ only. The derivation of the conditional distribution P(f∗ |f, X, X∗) from the joint distribution P(f, f∗ |X, X∗) is achieved by using the Marginal and conditional distributions of "
2987,unknown,"is: f∗ |f, X, X∗ ∼ N  KT ∗ K−1 f, K∗∗ − KT ∗ K−1 K∗  . In realistic scenarios, we typically have access only to noisy versions of true function values, y = f(x) + ϵ, where ϵ represents additive independent and identi- cally distributed (i.i.d.) Gaussian noise with variance σ2 n. The prior on these noisy observations then be- comes cov( y) = K + σ2 nI. The joint distribution of the observed value"
2988,unknown,"points is: y f∗ ! ∼ N 0, "" K + σ2 nI K ∗ KT ∗ K∗∗ #! . By deriving the conditional distribution, we get the predictive equations for Gaussian process regression: ¯f∗ |X, y, X∗ ∼ N  ¯f∗, cov(f∗)  , where ¯f∗ ∆ = E[¯f∗ |X, y, X∗] = KT ∗[K + σ2 nI]−1y , cov(f∗) = K∗∗ − KT ∗[K + σ2 nI]−1K∗ . In this expression, the variance function cov(f∗) reveals that the uncertainty in predictions depends solely o"
2989,unknown,"the input values X and X∗, not on the observed out- puts y. This characteristic is a distinctive property of Gaussian distributions [1]. ILLUSTRATIVE EXAMPLE This section demonstrates an implementation of the standard GPR, adhering to the algorithm outlined in Rasmussen (2006) [1, Algorithm 2.1]. L = cholesky(K + σ2 nI) α = L⊤ \ (L \ y) ¯f∗ = K⊤ ∗ α v = L \ K∗ V[¯f∗] = K (X∗, X∗) − v⊤v. log p(y | "
2990,unknown,"log p(y | X) = −1 2y⊤(K + σ2 nI)−1y − 1 2 log det(K + σ2 nI) − n 2 log 2π The inputs of this algorithm are X (inputs), y(targets), K (covariance function), σ2 n(noise level), and X∗(test input). The outputs include ¯f∗ (mean), V[¯f∗] (variance), and log p(y | X) (log marginal likelihood). An example result is illustrated in Fig. 10. We con- ducted regression within the [-5, 5] interval. Observed d"
2991,unknown,"data points (training dataset) were generated from a uniform distribution between -5 and 5. The functions were evaluated at evenly spaced points between -5 and 5. The regression function is composed of mean values estimated by a GPR model. Twenty samples of posterior mean functions, along with 3 times variances, were also plotted. Hyperparameters Optimization We have covered the basics of GPR and "
2992,unknown,"We have covered the basics of GPR and provided a straightforward example. However, practical GPR models often present more complexity. The selection of the kernel function is critical, as it greatly affects the model’s ability to generalize [6]. Kernel functions range from well-established options like the RBF to custom designs tailored to specific needs based on model requirements such as smoothn"
2993,unknown,"changes, and differentiability [4]. Selecting an appropri- ate kernel function for a specific GPR task is detailed 6 An Intuitive Tutorial to Gaussian Process Regression December 2023 THEME/FEATURE/DEPARTMENT 4 2 0 2 4 3 2 1 0 1 2 3 4 True function Observed data points Estimated mean function FIGURE 10: An illustrative example of standard GPR. Black crosses represent observed data points gen- erat"
2994,unknown,"erated by the blue dotted line (true function). Given these data points, infinite possible posterior functions were obtained, with 20 samples plotted in different colors. The mean function, derived from the probability distribution of these functions, is plotted as a red solid line. The blue shaded area around the mean function indicates 3 times prediction variances. in Duvenaud (2014) [4]. Additi"
2995,unknown,"optimization plays an essential role in kernel-based methods. For example, consider the widely used RBF kernel: k(xi , xj ) = σ2 f exp  − 1 2l (xi − xj )T(xi − xj )  , In this kernel, σf (vertical scale) and l (horizontal scale) are hyperparameters. The parameter σf determines the vertical span of the function, while l indicates the rate at which the correlation between two points de- creases wi"
2996,unknown,"hyperparameter l on the smoothness of the function is demonstrated in Fig. 11. Increasing the value of l results in a smoother function, while a smaller l value leads to a function with more fluctuations or ‘wiggles’. The optimal hyperparameters Θ∗ are determined by maximizing the log marginal likelihood [1]: Θ∗ = arg max Θ log p(y |X, Θ) . Thus, considering hyperparameters, a more general- ized p"
2997,unknown,"ized prediction equation at new testing points is [7]: ¯f∗ |X, y, X∗, Θ ∼ N  ¯f∗, cov(f∗)  . Note that after learning/optimizing the hyperparame- ters, the predictive variance cov( f∗) depends on not only the inputs X and X∗ but also the outputs y [8]. With the optimized hyperparameters, σf = 0.0067 and l = 0.0967, the regression result of the observed data points shown in Fig. 11 is depicted in "
2998,unknown,"the hyperparameters optimization was conducted by the GPy package, which will be introduced in the next section. 0.0 0.2 0.4 0.6 0.8 1.04 2 0 2 4 6 (a) l = small 0.0 0.2 0.4 0.6 0.8 1.0 2.5 0.0 2.5 5.0 7.5 10.0 12.5 (b) l = medium 0.0 0.2 0.4 0.6 0.8 1.0 2 0 2 4 6 (c) l = large FIGURE 11: Effect of the hyperparameter l on function smoothness: A larger l yields a smoother function, while a smaller "
2999,unknown,"0.0 0.2 0.4 0.6 0.8 1.0 10 5 0 5 10 15 Observed data points Estimated mean function FIGURE 12: Regression result with the optimized hy- perparameters σf and l. Gaussian Processes Packages This section reviews three Python packages for im- plementing Gaussian processes. GPy is a mature and well-documented package in development since 2012 [9]. It utilizes NumPy for computations, offering sufficient"
3000,unknown,"sufficient stability for tasks that are not computationally intensive. However, GPR is computationally expensive in high dimensional spaces (beyond a few dozen). For complex and computationally intense tasks, packages incorporating advanced algorithms and GPU acceler- ation are especially preferable. GPflow [9] originates December 2023 An Intuitive Tutorial to Gaussian Process Regression 7 THEME/F"
3001,unknown,"THEME/FEATURE/DEPARTMENT from GPy with a similar interface. It leverages Tensor- Flow as its computational backend. GPyTorch [10] is a more recent package that provides GPU accelera- tion through PyTorch. Like GPflow, GPyTorch supports automatic gradients, which simplifies the development of complex models, such as those embedding deep neural networks within GP frameworks. CONCLUSION A Gaussian pr"
3002,unknown,CONCLUSION A Gaussian process is a probability distribution over possible functions that fit a set of points [1]. A Gaussian process regression model provides prediction values together with uncertainty estimates. The model in- corporates prior knowledge about the nature of the functions through the use of kernel functions. The GPR model discussed in this tutorial is the standard or “vanilla” appr
3003,unknown,"[11]. There are two primary limitations with it: 1) The computational complexity is O(N3), where N repre- sents the dimension of the covariance matrixK . 2) The memory consumption increases quadratically with data size. Due to these constraints, standard GPR models become impractical for large datasets. In such cases, sparse Gaussian Processes are employed to alleviate computational complexity [12"
3004,unknown,computational complexity [12]. ACKNOWLEDGMENTS The author would like to express sincere gratitude to Prof. Krzysztof Czarnecki from the University of Waterloo. His insightful and constructive feedback have significantly contributed to the progression and en- hancement of the quality of this tutorial. The author is deeply thankful for his invaluable guidance and support. REFERENCES 1. C. E. Rasmuss
3005,unknown,"Processes for Machine Learning . The MIT Press, 2006. 2. Z. Ghahramani, “A Tutorial on Gaussian Processes (or why I don’t use SVMs),” in Machine Learning Summer School (MLSS) , 2011. 3. K. P . Murphy, Machine Learning: A Probabilistic Per- spective. The MIT Press, 2012. 4. D. Duvenaud, “Automatic model construction with Gaussian processes,” Ph.D. dissertation, University of Cambridge, 2014. 5. C. "
3006,unknown,"5. C. M. Bishop and N. M. Nasrabadi, Pattern recogni- tion and machine learning . Springer, 2006. 6. D. Duvenaud, “The Kernel Cookbook,” Available at https://www.cs.toronto.edu/~duvenaud/cookbook, 2016. 7. Z. Dai, “Computationally efficient GPs,” Available at https://www.youtube.com/watch?v=7mCfkIuNHYw, 2019. 8. Z. Chen and B. Wang, “How priors of initial hy- perparameters affect Gaussian process "
3007,unknown,"models,” Neurocomputing, vol. 275, pp. 1702–1710, 2018. 9. A. G. De G. Matthews, M. Van Der Wilk, T. Nickson, K. Fujii, A. Boukouvalas, P . León-Villagrá, Z. Ghahra- mani, and J. Hensman, “GPflow: A Gaussian process library using TensorFlow,” The Journal of Machine Learning Research , vol. 18, no. 1, pp. 1299–1304, 2017. 10. J. R. Gardner, G. Pleiss, D. Bindel, K. Q. Weinberger, and A. G. Wilson, "
3008,unknown,"Gaussian Process Inference with GPU Acceleration,” in Advances in Neural Information Processing Sys- tems, 2018. 11. R. Frigola, F . Lindsten, T. B. Schön, and C. E. Rasmussen, “Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC,” in Advances in Neural Information Process- ing Systems, 2013, pp. 3156–3164. 12. H. Liu, Y .-S. Ong, X. Shen, and J. Cai, “When Ga"
3009,unknown,"sian process meets big data: A review of scalable GPs,” IEEE Transactions on Neural Networks and Learning Systems, 2020. Jie Wang is a postdoctoral research associate at the University of Waterloo. He earned his Ph.D. degree in Mechanical Engineering from the University of Calgary. His research bridges machine learning and traditional robotics, primarily focusing on enhancing the perfor- mance of "
3010,unknown,"ensuring safety and efficiency. For further informa- tion or collaboration inquiries, he can be reached at jwangjie@outlook.com. 8 An Intuitive Tutorial to Gaussian Process Regression December 2023 Gaussian Process Regression Networks Andrew Gordon Wilson∗ David A. Knowles† Zoubin Ghahramani‡ University of Cambridge University of Cambridge University of Cambridge agw38@cam.ac.uk dak33@cam.ac.uk zo"
3011,unknown,"agw38@cam.ac.uk dak33@cam.ac.uk zoubin@eng.cam.ac.uk Abstract We introduce a new regression framework, Gaussian process regression networks (GPRN), which combines the structural properties of Bayesian neural networks with the nonparametric ﬂexibility of Gaussian processes. This model accommodates input dependent signal and noise correlations between multiple response variables, input dependent len"
3012,unknown,"correlations between multiple response variables, input dependent length-scales and amplitudes, and heavy-tailed predictive distributions. We derive both eﬃcient Markov chain Monte Carlo and variational Bayes inference procedures for this model. We apply GPRN as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multi"
3013,unknown,"over eight popular multiple output (multi-task) Gaussian process models and three multivariate volatility models on benchmark datasets, including a 1000 dimensional gene expression dataset. 1 Introduction Gaussian process models have become exceptionally popular for solving non-linear regression and classi- ﬁcation problems. They are expressive, interpretable, avoid over-ﬁtting, and have impressiv"
3014,unknown,"performance in many thorough empirical comparisons (Rasmussen, 1996; Kuss and Rasmussen, 2005; Ras- mussen and Williams, 2006). In machine learning, Gaussian process regression developed out of neural networks research. Neal (1996) showed that Bayesian neural networks became Gaussian processes as the number of hidden units approached inﬁnity, and conjectured that “there may be simpler ways to do i"
3015,unknown,"simple inference techniques became the cornerstone of subsequent Gaussian process models. However, neural networks had been motivated in part by their ability to capture correlations between multiple outputs (responses), by using adaptive hidden units that were shared between the outputs. In the inﬁnite limit, this ability was lost. Recently there has been an explosion of interest in extending the"
3016,unknown,"to account for ﬁxed correlations between output variables (Alvarez and Lawrence, 2011; Yu et al., 2009; Alvarez and Lawrence, 2008; Bonilla et al., 2008; Osborne et al., 2008; Teh et al., 2005; Boyle and Frean, 2004). These are often called ‘multi-task’ learning or ‘multiple output’ regression models. Capturing correlations between outputs (response variables) can be used to make better prediction"
3017,unknown,"wish to predict cadmium concentrations in a region of the Swiss Jura, where geologists are interested in heavy metal concentrations. A standard Gaussian process regression model would only be able to use cadmium training measurements. With a multi-task method, we can also make use of correlated heavy metal measurements to enhance cadmium predictions (Goovaerts, 1997). We could further enhance pred"
3018,unknown,"predictions if we make use of how these (signal) correlations change with geographical location. ∗http://mlg.eng.cam.ac.uk/andrew †http://mlg.eng.cam.ac.uk/dave ‡http://mlg.eng.cam.ac.uk/zoubin 1 arXiv:1110.4411v1 [stat.ML] 19 Oct 2011 There has similarly been great interest in extending Gaussian process (GP) regression to account for input dependent noise variances (Goldberg et al., 1998; Kerstin"
3019,unknown,"2008; Turner and Sahani, 2008; Turner, 2010; Wilson and Ghahramani, 2010a,b; L´ azaro-Gredilla and Titsias, 2011). Wilson and Ghahramani (2010b, 2011) and Fox and Dunson (2011) further extended the GP framework to accommodate input dependent noise correlations between multiple output (response) variables. Other extensions include Gaussian process regression with non-stationary covariance function "
3020,unknown,"Other extensions include Gaussian process regression with non-stationary covariance function ampli- tudes (Turner and Sahani, 2008; Adams and Stegle, 2008) and length-scales (Gibbs, 1997; Schmidt and O’Hagan, 2003), and with heavy tailed predictive distributions (Neal, 1997; Vanhatalo et al., 2009) for outlier rejection (De Finetti, 1956; Dawid, 1973; O’Hagan, 1979). In this paper, we introduce a "
3021,unknown,"which combines the structural properties of Bayesian neural networks (Neal, 1996) with the nonparamet- ric ﬂexibility of Gaussian processes. This network is an adaptive mixture of Gaussian processes, which naturally accommodates input dependent signal and noise correlations between multiple output variables, input dependent length-scales and amplitudes, and heavy tailed predictive distributions, w"
3022,unknown,"or numerically unstable computations. We start by introducing the GPRN framework, and show how to perform eﬃcient inference using both Markov chain Monte Carlo (MCMC) and variational Bayes (VB). Carefully following Alvarez and Lawrence (2011), we compare to eight multiple output GP models on gene expression and geostatistics datasets. We then compare to multivariate volatility models on several be"
3023,unknown,"and Ghahramani (2010b). In the Appendix, we review Gaussian process regression and the notation of Rasmussen and Williams (2006). 2 Gaussian Process Regression Networks Given vector valued data pointsD= {y(xi) : i= 1,...,N }, where x∈X is an arbitrary input variable, we aim to predict E[y(x∗)|x∗,D] and cov[y(x∗)|x∗,D] at a test input x∗. We assume that the noise and signal correlations between the"
3024,unknown,"particular component of the p-dimensional vector y(t) could be the expression level of a particular gene at time t, and Σ(t) would then represent the variances and correlations for these genes at time t. Instead of assuming only time dependence, we could have a more general input (predictor) variable x∈X. Then we can imagine y(x) representing diﬀerent heavy metal concentrations at a geographical l"
3025,unknown,"These are two examples from the experiments in Section 4. We model y(x) as y(x) = W(x)[f(x) + σfϵ] + σyz (1) where ϵand z are i.i.d. N(0,I) white noise 1, W(x) is a p×q matrix of independent Gaussian processes such that W(x)ij ∼GP(0,kw), and f(x) = (f1(x),...,f q(x))⊤is a q×1 vector of independent GPs with fi(x) ∼GP(0,kfi ). Each of the latent Gaussian processes in f(x) have additive Gaussian nois"
3026,unknown,"variables to include the noise σfϵwe let ˆfi(x) ∼GP(0,k ˆfi ), where kˆfi (x,x′) = kfi (x,x′) + σ2 fδxx′ , (2) and δxx′ is the Kronecker delta. We represent this Gaussian process regression network (GPRN) in Figure 1, labelling the length-scale hyperparameters for the kernels kw and kf as θw and θf respectively. We see the latent node functions ˆf(x) are connected together to form the outputs y(x)"
3027,unknown,"ˆf(x) are connected together to form the outputs y(x). The strengths of the connections change as a function of x; the weights themselves – the entries of W(x) – are functions. Old connections can break and 1The distribution of z could be Student-t, Laplace, or something diﬀerent. Using diagonal noise would also be a straight- forward extension. 2 ˆfi(x) ˆf1(x) ˆfq(x) y1(x) yj(x) W11(x) Wpq(x) W 1"
3028,unknown,"y1(x) yj(x) W11(x) Wpq(x) W 1q(x) W p1(x) yp(x) σf W(x) ˆf(x) y(x) σy a) b) θfθw... ... ... ... Figure 1: The Gaussian process regression network. Latent random variables and observables are respectively labelled with circles and squares, except for the weight functions in a). Hyperparameters are labelled with dots. a) This neural network style diagram shows theq components of the vectorˆf (GPs wi"
3029,unknown,"components of the vectory. The links in the graph, four of which are labelled, are latent random weightfunctions. Every quantity in this graph depends on the inputx. This graph emphasises the adaptive nature of this network: links can change strength or even disappear asx changes. b) A directed graphical model showing the generative procedure with relevant variables. new connections can form. This"
3030,unknown,"the components of y(x) vary with x. 2 To explicitly separate the dynamic signal and noise correlations, we re-write (1) as y(x) = W(x)f(x)   signal + σfW(x)ϵ+ σyz   noise . (3) Conditioning on W(x) in (3), we can better understand the signal correlations. In this case, each of the outputs yi(x), i= 1,...,p , is a Gaussian process with kernel kyi (x,x′) = q∑ j=1 Wij(x)[kfj (x,x′) + σ2 f]Wij"
3031,unknown,"kyi (x,x′) = q∑ j=1 Wij(x)[kfj (x,x′) + σ2 f]Wij(x′) + σ2 y. (4) Even if σ2 f and σ2 y are zero, so that this is a noise free regression, there are still signal correlations; the components of y are coupled through the matrix W(x). Once the network has been trained, W(x) is conditioned on the data D, and so the predictive covariances of y(x∗)|Dare now inﬂuenced by the values of the observations th"
3032,unknown,"x1,...,x N as is the case for independent GPs; we can view (4) as an adaptive kernel learned from the data. There are three other interesting features in equation (4): 1) the amplitude of the covariance function∑q j=1 Wij(x)Wij(x′) is non-stationary (input dependent); 2) even if each of the kernels kfj has diﬀerent stationary length-scales, the mixture of the kernels kfj is input dependent and so "
3033,unknown,"length-scale is non-stationary; 3) the kernels kfj may be entirely diﬀerent: some may be periodic, others squared exponential, others Brownian motion, etc. . So the overall covariance function (kernel) may be continuously switching between regions of entirely diﬀerent covariance structures. 2Coincidentally, there is an unrelated paper called “Gaussian process networks” (Friedman and Nachman, 2000)"
3034,unknown,"is about learning the structure of Bayesian networks – e.g. the direction of dependence between random variables. 3 In addition to modelling signal correlations, we can see from equation (3) that the GPRN is simulta- neously a multivariate volatility model. The noise covariance is σ2 fW(x)W(x)⊤+ σ2 yI. Since the entries of W(x) are GPs, this noise model is an example of a generalised Wishart proce"
3035,unknown,"2010b, 2011). The number of nodesqinﬂuences how the model accounts for signal and noise correlations. Ifqis smaller than p, the dimension of y(x), the model performs dimensionality reduction and matrix factorization as part of the regression on y(x) and cov[y(x)]. However, we may want q >p, for instance if the output space were one dimensional ( p = 1). In this case we would need q >1 to realise f"
3036,unknown,"For a given dataset, we can vary q and select the value which gives the highest marginal likelihood on training data. We can also use ‘automatic relevance determination’ (MacKay and Neal, 1994) as a proxy for model selection for q for a given dataset. This is achieved by introducing {aj}, signal variances for each node function j, so that kˆfj →ajkˆfj , and comparing magnitudes of the trained aj. "
3037,unknown,"When q= p= 1, the GPRN essentially becomes the nonstationary GP regression model of Adams and Stegle (2008) and Turner and Sahani (2008). Likewise, when the weight functions are constants the GPRN becomes the Semiparametric Latent Factor Model (SLFM) of Teh et al. (2005), except that the resulting GP regression network is less prone to over-ﬁtting through its use of full Bayesian inference. 3 Inde"
3038,unknown,"an implementation of GPRN, one can switch features on or oﬀ; to switch oﬀ changing correlations and multivariate volatility, set σ2 f = 0 and the length-scales for weight function kernels ( kw) to large ﬁxed values. 3 Inference Now that we have speciﬁed a priorp(y(x)) at all pointsxin our domainX, we wish to predictE[y(x∗)|x∗,D] and cov[y(x∗)|x∗,D] at a test input x∗, given vector valued data D= {"
3039,unknown,"and cov[y(x∗)|x∗,D] at a test input x∗, given vector valued data D= {y(xi) : i = 1 ,...,N }. We do this using two diﬀerent approaches – variational Bayes and Markov chain Monte Carlo (MCMC) – and we compare between these approaches. We also use variational Bayes to estimate all hyperparameters γ = {θf,θw,σf,σy}, where, as before, θf and θw are the length-scales of the node and weight function kern"
3040,unknown,"kernels. As a ﬁrst step, we re-write the prior in terms of u= (ˆf,W), a vector composed of all the node and weight Gaussian process functions, evaluated at the training points {x1,...,x N}. There will be q node functions and p×q weight functions. Therefore p(u|σf,θf,θw) = N(0,CB) , (5) where CB is an Nq(p+ 1) ×Nq(p+ 1) block diagonal matrix, since the weight and node functions are independent in t"
3041,unknown,"independent in the prior. The way we have ordered u, the ﬁrst q blocks are N ×N covariance matrices Kˆf from the node kernel kˆf, and the last blocks are N×N covariance matrices Kw from the weight kernel kw. Next we specify our likelihood function, so we can use Bayes’ theorem to ﬁnd the posterior p(u|D,γ). From (1), our likelihood is p(D|u,σy) = N∏ i=1 N(y(xi); W(xi) ˆf(xi),σ2 yI) . (6) By incorp"
3042,unknown,"need for costly or numerically unstable matrix inversions. For other multivariate volatility models, like multivariate GARCH (Bollerslev et al., 1988), or multivariate stochastic volatility (Harvey et al., 1994), the likelihood takes the form p(D|β) = ∏N i=1 N(µi,Σi), and requires inversions of p×p covariance matrices. There are three other notable advantages to the inference with GPRN: 1) it is e"
3043,unknown,"estimate µi and Σi. Usually in the multivariate volatility setting, µi is assumed to be a constant; 2) we 3In Teh et al. (2005), the weight constants are a large matrix of hyperparameters, determined through maximising a marginal likelihood. 4 can use a Student- t observation model instead of a Gaussian observation model, by letting z in (1) be t distributed, with minimal changes to the inference "
3044,unknown,"product W(xi) ˆf(xi) so that the priors on the components of y(xi) become copula processes (Wilson and Ghahramani, 2010a) and have whatever marginals we desire. We can also do this without signiﬁcantly changing inference procedures. Now that we have speciﬁed our prior and likelihood, we can apply Bayes’ theorem: p(u|D,γ) ∝p(D|u,σy)p(u|θf,θw,σf) . (7) In the next sections, we discuss how to either "
3045,unknown,"posterior in (7), so that we can estimate p(y(x∗)|D). We also use variational Bayes to learn the hyperpa- rameters γ. 3.1 Markov chain Monte Carlo To sample from (7), we could use a Gibbs sampling scheme which would have conjugate posterior updates, alternately conditioning on weight and node functions. However, this Gibbs cycle would mix poorly because of the tight correlations between the weight"
3046,unknown,"of the tight correlations between the weights and the nodes. In general, MCMC samples from (7) mix poorly because of the strong correlations in the prior imposed by CB. The sampling process is also often slowed by costly matrix inversions in the likelihood. We use Elliptical Slice Sampling (Murray et al., 2010), a recent MCMC technique speciﬁcally designed to sample from posteriors with tightly co"
3047,unknown,"parameters. We ﬁnd that it mixes well. And since there are no costly or numerically unstable matrix inversions in the likelihood of (6) we also ﬁnd sampling to be highly eﬃcient. With a sample from (7), we can sample from the predictive p(W(x∗),f(x∗)|u,σf,D). Let Wi ∗,fi ∗ be the ith such joint sample. Using (3) we can then construct samples of p(y(x∗)|Wi ∗,fi ∗,σf,σy), from which we can construct"
3048,unknown,"p(y(x∗)|D) = lim J→∞ 1 J J∑ i=1 p(y(x∗)|Wi ∗,fi ∗,σf,σy) . (8) We see that even with a Gaussian observation model, the predictive distribution in (8) is an inﬁnite mixture of Gaussians, and will generally be heavy tailed and therefore robust to outliers. Mixing was assessed by looking at trace plots of samples, and the likelihoods of these samples. Speciﬁc information about how long it takes to sa"
3049,unknown,"3.2 Variational Bayes We perform variational EM (Jordan et al., 1999) to ﬁt an approximate posterior q to the true posterior p, by minimising the Kullback-Leibler divergence KL(q||p) = −H[q(v)]− ∫ q(v) logp(v)dv,where H[q(v)] = − ∫ q(v) logq(v)dv is the entropy and v = {f,W,σ2 f,σ2 y,aj}. E-step. We use Variational Message Passing (Winn and Bishop, 2006) under the Infer.NET frame- work (Minka et a"
3050,unknown,"f,σ2 y,aj}. We specify inverse Gamma priors on {σ2 f,σ2 y,aj}: σ2 fj ∼IG(ασ2 f ,βσ2 f ), σ2 y ∼IG(ασ2y ,βσ2y ), aj ∼IG(αa,βa). For mathematical and computational convenience we introduce the following variables which are deter- ministic functions of the existing variables in the model: wnij := Wij(xn), f ′ nj := fj(xn) (9) tnij := wnij ˆfnj, s in := ∑ j tnij (10) 5 Note that the observations yi(xn"
3051,unknown,"y) and that ˆfnj ∼N(f′ nj,σ2 fj ). Variational message passing uses these deterministic factors and the associated “pseudo-marginals” as conduits to pass appropriate moments, resulting in the same updates as standard VB (Winn and Bishop, 2006). The full model can now be written as p(v) ∝IG(σ2 y; ασ2y ,βσ2y ) Q∏ j=1 ( N(fj; 0,ajKfj ) IG(σ2 fj; ασ2 f ,βσ2 f )IG(aj; αa,βa) P∏ i=1 [ N(Wij; 0,Kw) N∏ n="
3052,unknown,"N∏ n=1 δ(wnij −Wij(xn))δ(f′ nj −ˆfj(xn))N( ˆfnj; f′ nj,σ2 fj ) δ(tnij −wnij ˆfnj)δ(sin − ∑ j tnij)N(yi(xn); sin,σ2 y) ]) We use a variational posterior of the following form: q(v) = qσ2y (σ2 y) Q∏ j=1 qfj (fj)qσ2 fj (σ2 fj)qaj (aj) P∏ i=1 qWij (Wij) N∏ n=1 qwnij (wnij)qf′ nj (f′ nj)qˆfnj ( ˆfnj)qtnij (tnij)qsin (sin) where qσ2y ,qσ2 fj and qaj are inverse Gamma distributions; qwnij ,qf′ nj ,q ˆfnj"
3053,unknown,"fj and qaj are inverse Gamma distributions; qwnij ,qf′ nj ,q ˆfnj ,qtnij and qsin are univariate normal distributions; and qfj and qWij (Wij) are multivariate normal distributions. The updates for f,W,σ2 f,σ2 y are standard VB updates and are available in Infer.NET. The update for the ARD parameters aj however required speciﬁc implementation. The factor itself is log N(fj;0,ajKf) c = −1 2 log |ajK"
3054,unknown,"2 log |ajKj|− 1 2fT j (ajKf)−1fj = −N 2 log aj −1 2 log |Kj|− 1 2a−1 j fT j K−1 f fj (11) where c = denotes equality up to an additive constant. Taking expectations with respect to f under q we obtain the VMP message to aj as being IG ( aj; N 2 −1,1 2 ⟨fT j K−1 f fj⟩ ) . Since the variational posterior on f is multivariate normal the expectation ⟨fT j K−1 f fj⟩is straightforward to calculate. M-st"
3055,unknown,"M-step. In the M-step we optimise the variational lower bound with respect to the log length scale parameters {θf,θw}, using gradient descent with line search. When optimising θf we only need to consider the contribution to the lower bound of the factor N(fj; 0,ajKfj ) (see (11)), which is straightforward to evaluate and diﬀerentiate (see Appendix). For θw we consider the contribution of N(Wpq; 0,"
3056,unknown,"3.3 Computational Considerations GPRN is mainly limited by taking the Cholesky decomposition of the block diagonal CB, an Nq(p+ 1)× Nq(p+ 1) matrix. But pq of these blocks are the same N ×N covariance matrix Kw for the weight functions, and q of these blocks are the covariance matrices Kˆfi associated with the node functions, and chol(blkdiag(A,B,... )) = blkdiag(chol( A),chol(B),... ). Therefore "
3057,unknown,"the same covariance function (which they do in our experiments), the complexity of this operation is only O(N3), the same as for regular Gaussian process regression. At worst it is O(qN3), assuming diﬀerent covariance functions for each node. 6 Sampling also requires likelihood evaluations. Since there are input dependent noise correlations be- tween the elements of the pdimensional observations y"
3058,unknown,"require inverting a p×pcovariance matrix N times, like MGARCH (Bollerslev et al., 1988) or multivariate stochastic volatility models (Harvey et al., 1994). This would lead to a total complexity of O(Nqp+ Np3). However, by working directly with the noisy ˆfinstead of the noise free f, evaluating the likelihood requires no costly or numerically unstable inversions, and thus has a complexity of onlyO"
3059,unknown,"to scale to high dimensions; indeed we have a 1000 dimensional gene expression experiment in Section 4. The computational complexity of VB is dominated by the O(N3) inversions required to calculate the covariance of the node and weight functions in the E-step. Naivelyqand qpsuch inversions are required per iteration for the node and weight functions respectively, giving a total complexity of O(qpN"
3060,unknown,"under VB the covariances of the weight functions for the same pare all equal, reducing the complexity to O(qN3). If p is large the O(pqN2) cost of calculating the weight function means may become signiﬁcant. Although the per iteration cost of VB is actually higher than for MCMC far fewer iterations are typically required to reach convergence. Overall, even though the GPRN accounts for input depend"
3061,unknown,"Overall, even though the GPRN accounts for input dependent signal correlations (rather than ﬁxing the correlations like other multi-task methods), the computational demands of GPRN compare favourably to most multi-task GP models, which commonly have a complexity of O(p3N3) (Alvarez and Lawrence, 2011). 4 Experiments We compare the GPRN to multi-task learning and multivariate volatility models, and"
3062,unknown,"We compare the GPRN to multi-task learning and multivariate volatility models, and we also use the GPRN to gain new scientiﬁc insights into the data we model. Furthermore, we compare between variational Bayes (VB) and Markov chain Monte Carlo (MCMC) inference within the GPRN framework. To keep our comparisons up to date, we exactly reproduce many of the experiments in recent papers by Alvarez and "
3063,unknown,"Lawrence (2011) and Wilson and Ghahramani (2010b) on benchmark datasets. In the multi-task setting, there are p dimensional observations y(x), and the goal is to use the correlations between the elements of y(x) to make better predictions of y(x∗), for a test input x∗, than if we were to treat the dimensions independently. A major diﬀerence between GPRN and alternative multi-task models is that th"
3064,unknown,"independently. A major diﬀerence between GPRN and alternative multi-task models is that the GPRN accounts for signal correlations that change with x, rather than ﬁxed correlations. It also accounts for changing noise correlations (multivariate volatility). We compare to the following multi-task GP methods: 1) the linear model of coregionalisation (LMC) (Journel and Huijbregts, 1978; Goovaerts, 199"
3065,unknown,"1997), 3) ordinary co-kriging (Cressie, 1993; Goovaerts, 1997; Wackernagel, 2003), 4) the semiparametric latent factor model (SLFM) (Teh et al., 2005), 5) convolved multiple output Gaussian processes (CMOGP) (Barry and Jay, 1996; Ver Hoef and Barry, 1998; Boyle and Frean, 2004), 6) standard independent Gaussian processes (GP), 7) and the DTC (Csat´ o and Opper, 2001; Seeger et al., 2003; Qui˜ none"
3066,unknown,"Rasmussen, 2005; Rasmussen and Williams, 2006) , 8) FITC (Snelson and Ghahramani, 2006), and 9) PITC (Qui˜ nonero-Candela and Rasmussen, 2005) sparse approximations for CMOGP (Alvarez and Lawrence, 2011), which we respectively label as MDTC, MFITC and MPITC. Detail about each of these methods is in Alvarez and Lawrence (2011). We compare on a 3 dimensional geostatistics heavy metal dataset from"
3067,unknown,"the Swiss Jura, where 28% of the observations for one of the outputs (response variables) is missing, and on gene expression datasets with 50 and 1000 dimensional time dependent outputs y(t). In the multi-task experiments, the GPRN accounts for input dependent noise covariance matrices cov[y(x)] = Σ(x). To speciﬁcally test GPRN’s ability to model input dependent noise covariances (mul- tivariate v"
3068,unknown,"models – full BEKK MGARCH (Engle and Kroner, 1995), generalised Wishart processes (Wilson and Ghahramani, 2010b), the original Wishart process (Bru, 1991; Gouri´ eroux et al., 2009), and empirical esti- mates – on benchmark return series datasets which are especially suited to MGARCH (Poon and Granger, 2005; Hansen and Lunde, 2005; Brownlees et al., 2009; McCullough and Renfro, 1998; Brooks et al."
3069,unknown,"In all experiments, GPRN uses a squared exponential covariance function for its node functions, and another squared exponential covariance function for its weight functions. 7 4.1 Gene Expression Tomancak et al. (2002) measured gene expression levels every hour for 12 hours during Drosophila embryo- genesis; they then repeated this experiment for an independent replica (a second independent time s"
3070,unknown,"Gene expression is activated and deactivated by transcription factor proteins. We focus on genes which are thought to at least be regulated by the transcription factor twi, which inﬂuences mesoderm and muscle development in Drosophila (Zinzen et al., 2009). The assumption is that these gene expression levels are all correlated. We would like to use how these correlations change over time to make b"
3071,unknown,"of time varying gene expression in the presence of transcription factors. In total there are 1621 genes (outputs) at N = 12 time points (inputs), on two independent replicas. For training, p= 50 random genes were selected from the ﬁrst replica, and the corresponding 50 genes in the second replica were used for testing. We then repeated this experiment 10 times with a diﬀerent set of genes each tim"
3072,unknown,"the results. We then repeated the whole experiment, but with p= 1000 genes. We used exactly the same training and testing sets as Alvarez and Lawrence (2011). We use a relatively small p = 50 dataset so that we are able to compare with popular alternative multi-task methods (LMC, CMOGP, SLFM) which have a complexity of O(N3p3) and would not scale to p= 1000 (Alvarez and Lawrence, 2011). For p= 100"
3073,unknown,"GP methods (MFITC, MDTC, and MPITC) of Alvarez and Lawrence (2011). In both of these regressions, the GPRN is accounting for multivariate volatility; this is the ﬁrst time a multivariate stochastic volatility model has been estimated for p> 50 (Chib et al., 2006). We assess performance using standardised mean square error (SMSE) and mean standardized log loss (MSLL), as deﬁned in Rasmussen and Wil"
3074,unknown,"square error (SMSE) and mean standardized log loss (MSLL), as deﬁned in Rasmussen and Williams (2006) on page 23, and discussed in Alvarez and Lawrence (2011) on page 1469. Using the empirical mean and variance to ﬁt the data would give an SMSE and MSLL of 1 and 0 respectively. The smaller the SMSE and more negative the MSLL the better. The results are in Table 1, under the headings GENE (50D) and"
3075,unknown,"The results are in Table 1, under the headings GENE (50D) and GENE (1000D). For SET 1 of the 50D dataset, we used Replica 1 and Replica 2 in Alvarez and Lawrence (2011) respectively as training and testing replicas. We follow Alvarez and Lawrence (2011) and reverse training and testing replicas to create SET 2. The results for LMC, CMOGP, MFITC, MPITC, and MDTC are reproduced from Alvarez and Lawr"
3076,unknown,"Lawrence (2011). GPRN signiﬁcantly outperforms all of the other models, with between 46% and 68% of the SMSE, and similarly strong results on the MSLL error metric. On the 50D dataset the MCMC and VB results are comparable. However, on the 1000D dataset GPRN with VB noticeably outperforms GPRN with MCMC, likely because MCMC is not mixing as well in high dimensions. Indeed VB may have an advantage "
3077,unknown,"an advantage over MCMC in high dimensions. On the other hand, GPRN with MCMC is still robust, outperforming all the other methods on the 1000 dimensional dataset. On both the 50 and 1000 dimensional datasets, the marginal likelihood for the network structure is sharply peaked at q = 1. This is evidence for the hypothesis that there is only the one transcription factor twi controlling the expressio"
3078,unknown,"Lawrence’s GPSIM toolbox: http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/gpsim/ Typical GPRN (VB) runtimes for the 50D and 1000D datasets were respectively 12 seconds and 330 seconds. 4.2 Jura Geostatistics Here we are interested in predicting concentrations of cadmium at 100 locations within a 14.5 km 2 region of the Swiss Jura. For training, we have access to measurements of cadmium at 259 nei"
3079,unknown,"We also have access to nickel and zinc concentrations at these 259 locations, as well as at the 100 locations we wish to predict cadmium. While a standard Gaussian process regression model would only be able to make use of the cadmium training measurements, a multi-task method can use the correlated nickel and zinc measurements to enhance predictions 4. With GPRN we can also make use of how the co"
3080,unknown,"between nickel, zinc, and cadmium change with location to further enhance predictions. Here the network structure with by far the highest marginal likelihood has q= 2 latent node functions. The node and weight functions learnt using VB for this setting are shown in Figure 2. Since there are 4This can be seen as a multivariate missing data problem, with p= 3 outputs. 8 f2(x) f1(x) W11(x) W12(x) W21"
3081,unknown,"8 f2(x) f1(x) W11(x) W12(x) W21(x) W22(x) W31(x) W32(x) y1(x) y2(x) y3(x) Figure 2: Network structure for the Jura dataset learnt by GPRN. The layout here is as in Figure 1, with the spatially varying node and weight functions shown, along with the predictive means for the observations. The three output dimensions are cadmium, nickel and zinc concentrations respectively. p= 3 output dimensions, th"
3082,unknown,"correlated. Indeed, using our model we can observe the spatially varying correlations between heavy metal concentrations, as shown for cadmium and zinc in Figure 3. Although the correlation between cadmium and zinc is generally positive (with values around 0.6), there is a region where the correlations drop of noticeably, perhaps corresponding to a geological structure. The quantitative results in"
3083,unknown,"that the ability of GPRN to learn these spatially varying correlations is beneﬁcial in terms of being able to predict cadmium concentrations. We assess performance quantitatively using mean absolute error (MAE) between the predicted and true cadmium concentrations. We restart the experiment 10 times with diﬀerent initialisations of the parameters, and average the MAE. The results are marked by JUR"
3084,unknown,"parameters, and average the MAE. The results are marked by JURA in Table 1. This experiment follows Goovaerts (1997) and Alvarez and Lawrence (2011). The results for SLFM, ICM and CMOGP are from Alvarez and Lawrence (2011), and the results for co-kriging are from Goovaerts (1997). It is unclear what preprocessing was performed for these methods, but we found log transforming and normalising each d"
3085,unknown,"dimension to have zero mean and unit variance to be beneﬁcial due to the skewed distribution of the y-values (but we also include results on untransformed data, marked with *). All of the multiple output methods give lower MAE than using an independent GP, and GPRN outperforms SLFM and the other multiple output methods. For the JURA dataset, the improved performance of GPRN is at the cost of a sli"
3086,unknown,"However, GPRN is accounting for input dependent signal and noise correlations, unlike the other methods. Moreover, the complexity of GPRN scales with p as O(Nqp), unlike the other methods which scale as O(N3p3) (Alvarez and Lawrence, 2011). This is why GPRN runs relatively quickly on the 1000 dimensional gene expression dataset, for which the other methods are intractable. This data is available f"
3087,unknown,"//www.ai-geostats.org/. 4.3 Multivariate Volatility In the previous experiments the GPRN implicitly accounted for multivariate volatility (input dependent noise covariance) in making predictions of y(x∗). The GPRN incorporates a generalised Wishart process (Wilson and Ghahramani, 2010b, 2011) noise model into a more general model which can also account for signal correlations and other nonstationa"
3088,unknown,"input dependent correlations in a multiple output regression setting, we here test the GPRN explicitly as 9 1 2 3 4 5 longitude 1 2 3 4 5 6latitude 0.15 0.00 0.15 0.30 0.45 0.60 0.75 0.90 Figure 3: Spatially dependent correlation between cadmium and zinc learnt by the GPRN. Markers show the locations where measurements were made. a model of multivariate volatility, and assess predictions of Σ( t) "
3089,unknown,"time dependent. We make historical predictions at observed time points, and one day ahead forecasts. Historical predictions can be used, for example, to understand a past ﬁnancial crisis. We follow Wilson and Ghahramani (2010b) exactly, and predict Σ( t) for returns on three currency exchanges ( EXCHANGE) and ﬁve equity indices (EQUITY) processed exactly as in Wilson and Ghahramani (2010b). These "
3090,unknown,"are especially suited to MGARCH, the most popular multivariate volatility model, and have become a benchmark for assessing GARCH models (Poon and Granger, 2005; Hansen and Lunde, 2005; Brownlees et al., 2009; McCullough and Renfro, 1998; Brooks et al., 2001). We make 200 historical predictions of Σ(x) and 200 one step ahead forecasts. The forecasts are assessed using the log likelihood of the new "
3091,unknown,"observations under the predicted covariance, denoted LForecast. We compare to full BEKK MGARCH (Engle and Kroner, 1995), the generalised Wishart process (Wilson and Ghahramani, 2010b), the original Wishart process (Bru, 1991; Gouri´ eroux et al., 2009), and using the empirical covariance of the training set. We see in Table 1 that GPRN (VB) is competitive with MGARCH, even though these datasets ar"
3092,unknown,"set. We see in Table 1 that GPRN (VB) is competitive with MGARCH, even though these datasets are particularly suited to MGARCH. The Historical MSE for EXCHANGE is between the learnt covariance Σ(x) and y(x)y(x)⊤, so the high MSE values for GPRN on EXCHANGE are essentially training error, and are less meaningful than the encouraging step ahead forecast likelihoods. The historical predictions are mo"
3093,unknown,"relevant in EQUITY, where we can compare to the true covariances. See Wilson and Ghahramani (2010b) for details. GPRN and GWP are both highly ﬂexible but fully Bayesian models for multivariate volatility, so it understandable that their performance is comparable. While GPRN (MCMC) sometimes outperforms MGARCH, the GWP, and the WP, it is often outperformed by GPRN (VB) on the multivariate volatilit"
3094,unknown,perhaps suggesting convergence problems. These data were obtained using Bloomberg ( http://www. bloomberg.com/). 10 Table 1: Comparative performance on all datasets. GENE (50D) Average SMSE Average MSLL SET 1: GPRN (VB) 0 .3356 ±0.0294 −0.5945 ± 0.0536 GPRN (MCMC) 0.3236 ± 0.0311 −0.5523 ±0.0478 LMC 0 .6909 ±0.0294 −0.2687 ±0.0594 CMOGP 0 .4859 ±0.0387 −0.3617 ±0.0511 SLFM 0 .6435 ±0.0657 −0.2376 
3095,unknown,SLFM 0 .6435 ±0.0657 −0.2376 ±0.0456 SET 2: GPRN (VB) 0 .3403 ±0.0339 −0.6142 ± 0.0557 GPRN (MCMC) 0.3266 ± 0.0321 −0.5683 ±0.0542 LMC 0 .6194 ±0.0447 −0.2360 ±0.0696 CMOGP 0 .4615 ±0.0626 −0.3811 ±0.0748 SLFM 0 .6264 ±0.0610 −0.2528 ±0.0453 GENE (1000D) Average SMSE Average MSLL GPRN (VB) 0.3473 ± 0.0062 −0.6209 ± 0.0085 GPRN (MCMC) 0 .4520 ±0.0079 −0.4712 ±0.0327 MFITC 0 .5469 ±0.0125 −0.3124 ±0
3096,unknown,MPITC 0 .5537 ±0.0136 −0.3162 ±0.0206 MDTC 0 .5421 ±0.0085 −0.2493 ±0.0183 JURA Average MAE Training Time (secs) GPRN (VB) 0.4040 ± 0.0006 3781 GPRN* (VB) 0 .4525 ±0.0036 4560 SLFM (VB) 0 .4247 ±0.0004 1643 SLFM* (VB) 0 .4679 ±0.0030 1850 SLFM 0 .4578 ±0.0025 792 Co-kriging 0.51 ICM 0 .4608 ±0.0025 507 CMOGP 0 .4552 ±0.0013 784 GP 0 .5739 ±0.0003 74 EXCHANGE Historical MSE LForecast GPRN (VB) 3 .8
3097,unknown,GPRN (VB) 3 .83 ×10−8 2073 GPRN (MCMC) 6 .120 ×10−9 2012 GWP 3.88 × 10−9 2020 WP 3.88 × 10−9 1950 MGARCH 3 .96 ×10−9 2050 Empirical 4 .14 ×10−9 2006 EQUITY Historical MSE LForecast GPRN (VB) 0 .978 ×10−9 2740 GPRN (MCMC) 0.827 × 10−9 2630 GWP 2 .80 ×10−9 2930 WP 3 .96 ×10−9 1710 MGARCH 6 .69 ×10−9 2760 Empirical 7 .57 ×10−9 2370 11 5 Discussion A Gaussian process regression network (GPRN) has a si
3098,unknown,"many of the recent extensions to the Gaussian process regression framework. The model naturally accom- modates input dependent signal and noise correlations between multiple output variables, heavy tailed predictive distributions, input dependent length-scales and amplitudes, and adaptive covariance functions. Furthermore, GPRN has scalable inference procedures, and strong empirical performance on"
3099,unknown,"mark datasets. In the future, it would be enlightening to use GPRN with diﬀerent types of adaptive covariance struc- tures, particularly in the case where p= 1 and q >1; in one dimensional output space it would be easy, for instance, to visualise a process gradually switching between brownian motion, periodic, and smooth covariance functions. It would also be interesting to apply this adaptive net"
3100,unknown,"the GPRN will inspire further research into adaptive networks, and further connections between diﬀerent areas of machine learning and statistics. 6 Acknowledgements Thanks to Mauricio ´Alvarez and Neil Lawrence for their valuable feedback about the gene expression datasets. 7 Appendix 7.1 Gaussian processes Since we extend the Gaussian process framework, we brieﬂy review Gaussian process regressio"
3101,unknown,"Since we extend the Gaussian process framework, we brieﬂy review Gaussian process regression, some notation, and expand on some of the points in the introduction. For more detail see Rasmussen and Williams (2006). A Gaussian process is a collection of random variables, any ﬁnite number of which have a joint Gaussian distribution. Using a Gaussian process, we can deﬁne a distribution over functions"
3102,unknown,"w(x) ∼GP(m(x),k(x,x′)), (12) where w is the output variable, x is an arbitrary (potentially vector valued) input variable, and the mean m(x) and covariance function (or kernel) k(x,x′) are respectively deﬁned as m(x) = E[w(x)] , (13) k(x,x′) = cov[w(x),w(x′)] . (14) This means that any collection of function values has a joint Gaussian distribution: (w(x1),w(x2),...,w (xN))⊤∼N(µ,K) , (15) where th"
3103,unknown,"where the N×N covariance matrix K has entries Kij = k(xi,xj), and the mean µhas entries µi = m(xi). The properties of these functions (smoothness, periodicity, etc.) are determined by the kernel function. The squared exponential kernel is popular: kSE(x,x′) = Aexp(−0.5||x−x′||2/l2) . (16) Functions drawn from a Gaussian process with this kernel function are smooth, and can display long range trend"
3104,unknown,"values w(x) and w(x+ a) depend on one another, for some constant a ∈X . When the length-scale is learned from data, it is useful for determining how far into the past one should look in order to make good forecasts. A∈R is the amplitude coeﬃcient, which determines the marginal variance of w(x) in the prior, Var[w(x)] = A, and the magnitude of covariances between w(x) at diﬀerent inputs x. 12 The O"
3105,unknown,"12 The Ornstein-Uhlenbeck kernel is also widely applied: kOU(x,x′) = exp(−||x−x′||/l) . (17) In one dimension it is the covariance function of an Ornstein-Uhlenbeck process (Uhlenback and Ornstein, 1930), which was introduced to model the velocity of a particle undergoing Brownian motion. With this kernel, the corresponding GP is a continuous time AR(1) process with Markovian dynamics: w(x+ a) is "
3106,unknown,"independent of w(x−a) given w(x) for any constant a. Indeed the OU kernel belongs to a more general class of Mat´ ern kernels, kMat´ ern(x,x′) = 21−α Γ(α) ( √ 2α||x−x′|| l )αKα( √ 2α||x−x′|| l ) , (18) where Kα is a modiﬁed Bessel function (Abramowitz and Stegun, 1964). In one dimension the correspond- ing GP is a continuous time AR( p) process, where p = α+ 1/2.5 The OU kernel is recovered by set"
3107,unknown,"α= 1/2. There are many other useful kernels, like the periodic kernel (with a period that can be learned from data), or the Gibbs kernel (Gibbs, 1997) which allows for input dependent length-scales. Kernels can be combined together, e.g. k = a1k1 + a2k2 + a3k3, and the relative importance of each kernel can be determined from data (e.g. from estimating a1,a2,a3). Rasmussen and Williams (2006) and "
3108,unknown,"have a discussion about how to create and combine kernels. Suppose we are doing a regression using points {y(x1),...,y (xN)}from a noisy function y= w(x) +ϵ, where ϵ is additive i.i.d Gaussian noise, such that ϵ ∼N (0,σ2 n). Letting y = (y(x1),...,y (xN))⊤, and w = (w(x1),...,w (xN)⊤, we have p(y|w) = N(w,σ2 nI) and p(w) = N(µ,K) as above. For notational simplicity, we assume µ= 0. For a test poin"
3109,unknown,"[w(x∗) w ] ∼N(0, [ k(x∗,x∗) k⊤ ∗ k∗ K+ σ2 nI ] ) , (19) where K is deﬁned as above, and ( k∗)i = k(x∗,xi) with i= 1,...,N . We can therefore condition on yto ﬁnd p(w(x∗)|y) = N(µ∗,v∗) where µ∗= k⊤ ∗(K+ σ2 nI)−1y, (20) v∗= k(x∗,x∗) −k⊤ ∗(K+ σ2 nI)−1k∗. (21) We can ﬁnd this more laboriously by noting that p(w|y) and p(w(x∗)|w) are Gaussian and integrating, since p(w(x∗)|y) = ∫ p(w(x∗)|w)p(w|y)dw. We"
3110,unknown,"We see that (21) doesn’t depend on the data y, just on how far away the test point x∗ is from the training inputs {x1,...,x N}. In regards to the introduction, we also see that for this standard Gaussian process regression, the observation model p(y|w) is Gaussian, the predictive distribution in (20) and (21) is Gaussian, the marginals in the prior (from marginalising equation (15)) are Gaussian, "
3111,unknown,"covariance functions given, the amplitude and length-scale are constant. A brief discussion of multiple outputs, noise models with dependencies, and non-Gaussian observation models can be found in sections 9.1, 9.2 and 9.3 on pages 190-191 of Rasmussen and Williams (2006), available free online at the book website www.gaussianprocess.org/gpml. An example of an input dependent length-scale is in se"
3112,unknown,"4.2 on page 43. 7.2 Constraining W It is possible to reduce the number of modes in the posterior by somehow constraining the weights W to be positive. For MCMC it is straightforward to do this by exponentiating the weights, as in Adams 5Discrete time autoregressive processes such as w(t+ 1) = w(t) +ϵ(t), where ϵ(t) ∼N(0,1), are widely used in time series modelling and are a particularly simple spe"
3113,unknown,"modelling and are a particularly simple special case of Gaussian processes. 13 and Stegle (2008) and Adams et al. (2010). For VB it is more straightforward to explicitly constrain the weights to be positive using a truncated Gaussian representation. We found that these extensions did not signiﬁcantly improve empirical performance, although exponentiating the weights sometimes improved numerical st"
3114,unknown,"exponentiating the weights will have been more valuable because they use Expectation Propagation which is known to perform badly in the presence of multimodality. MCMC and VB approaches are more robust to this problem. 7.3 VB M-step From (11) we have: ⟨log N(fj; 0,ajKfj )⟩q c = −N 2 log aj −1 2 log |Kfj |− 1 2⟨a−1 j ⟩⟨fT j K−1 fj fj⟩ We will need the gradient with respect to θf: ∂⟨log N(fj; 0,ajKf"
3115,unknown,"∂θf = −1 2tr ( K−1 fj ∂Kfj ∂θf ) −1 2⟨a−1 j ⟩⟨fT j K−1 fj ∂Kfj ∂θf K−1 fj fj⟩ The expectations here are straightforward to compute analytically. 7.4 VB predictive distributions The predictive distribution is calculated as p(y∗(x)|D) = ∫ p(y∗(x)|W(x),f(x))p(W(x),f(x)|D)dWdf (22) VB ﬁts the approximation p(W(x),f(x)|D) = q(W)q(f), so the approximate predictive is p(y∗(x)|D) = ∫ p(y∗(x)|W(x),f(x))q(W"
3116,unknown,We can calculate the mean and covariance of this distribution analytically: ¯y∗(x)i = ∑ k E(Wik)E[fk] (24) cov(y∗(x))ij = ∑ k [E(Wik)E(Wjk)var(fk) + δij(var(Wik)E(f2 k))] + δijσ2 y (25) It is also of interest to calculate the noise covariance. Recall our model can be written as y(x) = W(x)f(x)   signal + σfW(x)ϵ+ σyz   noise (26) Let n= σfW(x)ϵ+ σyzbe the noise. The covariance of n is then
3117,unknown,"cov(n)ij = ∑ k [E[σ2 fk ]E(Wik)E(Wjk) + δijvar(Wjk)] + δijσ2 y (27) 14 References Abramowitz, M. and Stegun, I. (1964). Handbook of mathematical functions with formulas, graphs, and mathematical tables. Dover publications. Adams, R. and Stegle, O. (2008). Gaussian process product models for nonparametric nonstationarity. In Proceedings of the 25th international conference on Machine learning . ACM"
3118,unknown,"Proceedings of the 25th international conference on Machine learning . ACM. Adams, R. P., Dahl, G. E., and Murray, I. (2010). Incorporating side information into probabilistic matrix factorization using Gaussian processes. In Gr¨ unwald, P. and Spirtes, P., editors,Proceedings of the 26th Conference on Uncertainty in Artiﬁcial Intelligence , pages 1–9. Alvarez, M. and Lawrence, N. (2008). Sparse c"
3119,unknown,"In NIPS. Alvarez, M. and Lawrence, N. (2011). Computationally eﬃcient convolved multiple output gaussian pro- cesses. Journal of Machine Learning Research , 12:1425–1466. Barry, R. and Jay, M. (1996). Blackbox kriging: spatial prediction without specifying variogram models. Journal of Agricultural, Biological, and Environmental Statistics , pages 297–322. Bishop, C. M. (2006). Pattern Recognition "
3120,unknown,"Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer. Bollerslev, T., Engle, R. F., and Wooldridge, J. M. (1988). A capital asset pricing model with time-varying covariances. The Journal of Political Economy , 96(1):116–131. Bonilla, E., Chai, K., and Williams, C. (2008). Multi-task Gaussian process prediction. In NIPS. Boyle, P. and Frean, M. (2004). Dependent Gaussian process"
3121,unknown,"Boyle, P. and Frean, M. (2004). Dependent Gaussian processes. In NIPS. Brooks, C., Burke, S., and Persand, G. (2001). Benchmarks and the accuracy of GARCH model estimation. International Journal of Forecasting, 17:45–56. Brownlees, C. T., Engle, R. F., and Kelly, B. T. (2009). A practical guide to volatility forecasting through calm and storm. Available at SSRN: http://ssrn.com/abstract=1502915. B"
3122,unknown,"Bru, M. (1991). Wishart processes. Journal of Theoretical Probability, 4(4):725–751. Chib, S., Nardari, F., and Shephard, N. (2006). Analysis of high dimensional multivariate stochastic volatility models. Journal of Econometrics , 134(2):341–371. Cressie, N. (1993). Statistics for spatial data (wiley series in probability and statistics). Csat´ o, L. and Opper, M. (2001). Sparse representation for"
3123,unknown,"information processing systems 13: proceedings of the 2000 conference , volume 13, page 444. The MIT Press. Dawid, A. (1973). Posterior expectations for large observations. Biometrika, 60(3):664. De Finetti, B. (1956). The Bayesian approach to the rejection of outliers. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability: held at the Statistical Laboratory, U"
3124,unknown,"versity of California, June 30-July 30, l960 , page 199. University of California Press. Engle, R. and Kroner, K. (1995). Multivariate simultaneous generalized ARCH. Econometric theory, 11(01):122–150. Fox, E. and Dunson, D. (2011). Bayesian nonparametric covariance regression. Arxiv preprint arXiv:1101.2017. 15 Friedman, N. and Nachman, I. (2000). Gaussian process networks. In Proc. Sixteenth Con"
3125,unknown,"Uncertainty in Artiﬁcial Intelligence (UAI00) , pages 211–219. Gibbs, M. (1997). Bayesian Gaussian Process for Regression and Classiﬁcation . PhD thesis, Dept. of Physics, University of Cambridge. Goldberg, P. W., Williams, C. K., and Bishop, C. M. (1998). Regression with input-dependent noise: A Gaussian process treatment. In NIPS. Goovaerts, P. (1997). Geostatistics for natural resources evaluat"
3126,unknown,"Gouri´ eroux, C., Jasiak, J., and Sufana, R. (2009). The Wishart autoregressive process of multivariate stochastic volatility. Journal of Econometrics , 150(2):167–181. Hansen, P. R. and Lunde, A. (2005). A forecast comparison of volatility models: Does anything beat a GARCH(1,1). Journal of Applied Econometrics , 20(7):873–889. Harvey, A., Ruiz, E., and Shephard, N. (1994). Multivariate stochasti"
3127,unknown,"Economic Studies, 61(2):247–264. Jordan, M., Ghahramani, Z., Jaakkola, T., and Saul, L. (1999). An introduction to variational methods for graphical models. Machine learning, 37(2):183–233. Journel, A. and Huijbregts, C. (1978). Mining geostatistics. Academic Press (London and New York). Kersting, K., Plagemann, C., Pfaﬀ, P., and Burgard, W. (2007). Most likely heteroscedastic gaussian process reg"
3128,unknown,"400. ACM. Kuss, M. and Rasmussen, C. (2005). Assessing approximate inference for binary Gaussian process classi- ﬁcation. The Journal of Machine Learning Research , 6:1679–1704. L´ azaro-Gredilla, M. and Titsias, M. (2011). Variational heteroscedastic gaussian process regression. MacKay, D. and Neal, R. (1994). Automatic relevance determination for neural networks.Technical report. McCullough, B. "
3129,unknown,"McCullough, B. and Renfro, C. (1998). Benchmarks and software standards: A case study of GARCH procedures. Journal of Economic and Social Measurement , 25:59–71. Minka, T. P., Winn, J. M., Guiver, J. P., and Knowles, D. A. (2010). Infer.NET 2.4. Microsoft Research Cambridge. http://research.microsoft.com/infernet. Murray, I., Adams, R. P., and MacKay, D. J. (2010). Elliptical Slice Sampling. JMLR:"
3130,unknown,"Neal, R. (1996). Bayesian learning for neural networks . Springer Verlag. Neal, R. (1997). Monte Carlo implementation of Gaussian process models for Bayesian regression and classiﬁcation. Arxiv preprint physics/9701026. O’Hagan, A. (1979). On outlier rejection phenomena in Bayes inference. Journal of the Royal Statistical Society. Series B (Methodological), pages 358–367. Osborne, M., Roberts, S.,"
3131,unknown,"tion processing of sensor network data using computationally eﬃcient multi-output gaussian processes. In Proceedings of the 7th international conference on Information processing in sensor networks , pages 109–120. IEEE Computer Society. Poon, S.-H. and Granger, C. W. (2005). Practical issues in forecasting volatility. Financial Analysts Journal, 61(1):45–56. 16 Qui˜ nonero-Candela, J. and Rasmuss"
3132,unknown,"regression. The Journal of Machine Learning Research , 6:1939–1959. Rasmussen, C. E. (1996). Evaluation of Gaussian Processes and Other Methods for Non-linear Regression. PhD thesis, Dept. of Computer Science, University of Toronto. Rasmussen, C. E. and Williams, C. K. (2006). Gaussian processes for Machine Learning. The MIT Press. Schmidt, A. and O’Hagan, A. (2003). Bayesian inference for non-sta"
3133,unknown,"via spatial deformations. Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 65(3):743–758. Seeger, M., Williams, C., and Lawrence, N. (2003). Fast forward selection to speed up sparse gaussian process regression. In Workshop on AI and Statistics , volume 9, page 2003. Snelson, E. and Ghahramani, Z. (2006). Sparse gaussian processes using pseudo-inputs. Advances in neur"
3134,unknown,"information processing systems, 18:1257. Teh, Y., Seeger, M., and Jordan, M. (2005). Semiparametric latent factor models. In Workshop on Artiﬁcial Intelligence and Statistics , volume 10. Tomancak, P., Beaton, A., Weiszmann, R., Kwan, E., Shu, S., Lewis, S., Richards, S., Ashburner, M., Hartenstein, V., Celniker, S., et al. (2002). Systematic determination of patterns of gene expression during dro"
3135,unknown,"during drosophila embryogenesis. Genome Biol, 3(12):0081–0088. Turner, R. and Sahani, M. (2008). Modeling natural sounds with modulation cascade processes. Advances in neural information processing systems , 21. Turner, R. E. (2010). Statistical Models for Natural Sounds . PhD thesis, University College London. Uhlenback, G. and Ornstein, L. (1930). On the theory of brownian motion. Phys. Rev., 36"
3136,unknown,"Uhlenback, G. and Ornstein, L. (1930). On the theory of brownian motion. Phys. Rev., 36:823–841. Vanhatalo, J., Jylanki, P., and Vehtari, A. (2009). Gaussian process regression with Student-t likelihood. In NIPS. Ver Hoef, J. and Barry, R. (1998). Constructing and ﬁtting models for cokriging and multivariable spatial prediction. Journal of Statistical Planning and Inference , 69(2):275–294. Wacker"
3137,unknown,"Wilson, A. G. and Ghahramani, Z. (2010a). Copula processes. In NIPS. Wilson, A. G. and Ghahramani, Z. (2010b). Generalised Wishart Processes. Arxiv preprint arXiv:1101.0240. Wilson, A. G. and Ghahramani, Z. (2011). Generalised Wishart Processes. In Uncertainty in Artiﬁcial Intelligence. AUAI Press. Winn, J. and Bishop, C. M. (2006). Variational message passing. Journal of Machine Learning Research"
3138,unknown,"6(1):661. Yu, B., Cunningham, J., Santhanam, G., Ryu, S., Shenoy, K., and Sahani, M. (2009). Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity. Journal of neuro- physiology, 102(1):614. Zinzen, R., Girardot, C., Gagneur, J., Braun, M., and Furlong, E. (2009). Combinatorial binding predicts spatio-temporal cis-regulatory activity. Nature, 462(7"
3139,unknown,"17 Analytic natural gradient updates for Cholesky factor in Gaussian variational approximation Linda S. L. Tan Department of Statistics and Data Science, National University of Singapore, 117546 Sin- gapore. E-mail: statsll@nus.edu.sg Summary. Natural gradients can improve convergence in stochastic variational inference significantly but inverting the Fisher information matrix is daunting in high "
3140,unknown,"significantly but inverting the Fisher information matrix is daunting in high dimensions. Moreover, in Gaussian variational approximation, natural gradient updates of the precision matrix do not ensure positive definiteness. To tackle this issue, we derive analytic natural gradient updates of the Cholesky factor of the covariance or precision matrix, and con- sider sparsity constraints representin"
3141,unknown,"sider sparsity constraints representing different posterior correlation structures. Stochas- tic normalized natural gradient ascent with momentum is proposed for implementation in generalized linear mixed models and deep neural networks. Keywords: Gaussian variational approximation, Natural gradients, Cholesky factor, Positive definite, Sparse precision matrix, Normalized stochastic gradient desce"
3142,unknown,"Positive definite, Sparse precision matrix, Normalized stochastic gradient descent 1. Introduction Variational inference is fast and provides an attractive alternative to Markov chain Monte Carlo (MCMC) methods for approximating intractable posterior distributions in Bayesian analysis. Stochastic gradient methods (Robbins and Monro, 1951) further enabled variational inference for high-dimensional "
3143,unknown,"enabled variational inference for high-dimensional models and large data sets (Sato, 2001; Hoffman et al., 2013; Salimans and Knowles, 2013). Euclidean gradients are often used in optimizing the variational objective function, called evidence lower bound. However, the steepest ascent direction in the parameter space, where distance between densities is measured using Kullback-Leibler (KL) divergen"
3144,unknown,"measured using Kullback-Leibler (KL) divergence, is actually given by the natural gra- dient (Amari, 1998), which is obtained by premultiplying the Euclidean gradient with the inverse Fisher information. Stochastic optimization based on natural gradients can be more robust, with the ability to escape plateaus, yielding faster convergence (Rattray et al., 1998). Martens (2020) showed that natural g"
3145,unknown,"et al., 1998). Martens (2020) showed that natural gradient descent can be viewed as a second order optimization method, with the Fisher information in place of the Hessian. Various alternatives to natural gradient descent similarly account for geometry of the evidence lower bound. The KL proximal-point algorithm (Khan et al., 2015) uses KL divergence as a proximal term, and is equivalent to natura"
3146,unknown,"KL divergence as a proximal term, and is equivalent to natural gradient descent for conditionally-conjugate exponential-family models. Martens and Grosse (2015) used a Kronecker-factored approximation to the Fisher information matrix in natural gradient descent for neural networks, while Zhang et al. (2018) approximated the negative log- likelihood Hessian in the noisy natural gradient with the Fi"
3147,unknown,"arXiv:2109.00375v9 [stat.CO] 19 May 2024 2 Linda Tan point estimation. The variational predictive natural gradient (Tang and Ranganath, 2019) improves on the natural gradient by accounting for curvature of the expected log- likelihood in the evidence lower bound. Kim et al. (2023) provide convergence guarantees for proximal stochastic gradient descent in black-box variational inference. Computing "
3148,unknown,"Computing natural gradients is complicated, but natural gradient updates for con- jugate exponential family models can be simpler than Euclidean ones (Hoffman et al., 2013). For variational densities in the minimal exponential family (Wainwright and Jordan, 2008), natural gradient of the evidence lower bound with respect to the natural parameter is just the Euclidean gradient with respect to the e"
3149,unknown,"parameter is just the Euclidean gradient with respect to the expected sufficient statistics (Khan and Lin, 2017). In Gaussian variational approximation (Opper and Archambeau, 2009), stochastic natural gradient updates of the mean and precision, which depend re- spectively on the first and second derivatives of the log posterior density (Khan et al., 2018) can be derived using the theorems of Bonne"
3150,unknown,"precision matrix’s update does not ensure positive definiteness. To handle the positive definite constraint, Khan and Lin (2017) use back-tracking line search. Khan et al. (2018) use the generalized Gauss-Newton approximation for the log likelihood Hessian in variational online Gauss-Newton (VOGN), which is later applied to deep networks by Osawa et al. (2019). Ong et al. (2018) compute natural gr"
3151,unknown,"for Cholesky factor of the precision matrix by solving a linear system numerically. For one-one transformations of the natural parameter, the inverse Fisher information can be computed as a Jacobian-vector product via automatic differentiation (Salimbeni et al., 2018). Using a factor structure for the covariance, Tran et al. (2020) compute natural gradients via a conjugate gradient linear solver b"
3152,unknown,"gradients via a conjugate gradient linear solver based on a block diagonal approximation of the Fisher information. Lin et al. (2020) use Riemannian gradient descent with a retraction map to obtain a precision matrix update that has an additional term to ensure positive definiteness. Tran et al. (2021) derive an update of the covariance matrix based on an approximation of the natural gradient, and"
3153,unknown,"on an approximation of the natural gradient, and a popular retraction for the manifold of symmetric positive definite matrices. Lin et al. (2021) use local parameter coordinates to derive natural gradient updates for low-rank, square root and sparse matrices. We consider Cholesky decomposition of the covariance or precision matrix, and derive the inverse Fisher information in closed form to obtain"
3154,unknown,"the inverse Fisher information in closed form to obtain analytic natural gradient updates. Our derivation is based on properties of elimination and commutation matrices for han- dling lower triangular matrices (Magnus and Neudecker, 1980) and is independent of Lin et al. (2021)’s local parameter coordinates approach. Extending Lin et al. (2019b), we obtain unbiased natural gradient estimates with "
3155,unknown,"obtain unbiased natural gradient estimates with respect to the Cholesky factor in terms of the first or second order derivative of the log posterior via Stein’s Lemma (Stein, 1981). First order gradient estimates are more efficient computationally and storage wise, but second order estimates have lower variance close to the mode and are espe- cially useful for high-dimensional dense covariance mat"
3156,unknown,"cially useful for high-dimensional dense covariance matrices. Compared with Euclidean updates of the mean and Cholesky factor (Titsias and L´ azaro-Gredilla, 2014), natural gradient updates require more computation, but can improve convergence significantly. Updating the Cholesky factor instead of the precision matrix also has advantages in terms of storage, computation (finding the determinant an"
3157,unknown,"terms of storage, computation (finding the determinant and inverse) and simulations. Gaussian variational approximation has been applied in many contexts, such as likelihood-free inference (Ong et al., 2018), deep Bayesian neural networks (Khan et al., Natural gradient updates in Gaussian variational approximation 3 2018), exponential random graph models (Tan and Friel, 2020) and factor copula mod"
3158,unknown,"els (Nguyen et al., 2020). A Gaussian variational approximation can be specified for variables which have undergone independent parametric transformations, resulting in a Gaussian copula variational approximation (Han et al., 2016; Smith et al., 2020). To ac- commodate constrained, skewed or heavy-tailed variables, Lin et al. (2019a) developed natural gradient variational inference for mixture of "
3159,unknown,"natural gradient variational inference for mixture of exponential family distributions. The multivariate Gaussian can also be used as a prior for parameters in mean-field variational approximation for other types of latent variables, to expand the variational model hierarchically and induce dependencies among latent variables (Ranganath et al., 2016). Our natural gradient updates are also useful i"
3160,unknown,"2016). Our natural gradient updates are also useful in these contexts. For high-dimensional models, we derive efficient natural gradient updates under spar- sity constraints, where (i) the covariance matrix is a block diagonal representing the variational Bayes assumption (Attias, 1999), and (ii) the precision matrix has a sparse structure mirroring posterior conditional independence in a hierarch"
3161,unknown,"structure mirroring posterior conditional independence in a hierarchical model where local variables are independent given global ones (Tan and Nott, 2018). Sparsity con- straints can be easily imposed on Euclidean gradients by setting relevant entries to zero, but the same does not apply to natural gradients due to premultiplication by the inverse Fisher information. In comparison, the automatic "
3162,unknown,"Fisher information. In comparison, the automatic differentiation variational inference algorithm in Stan (Kucukelbir et al., 2017) only allows Gaussian variational approxima- tions with a diagonal or full covariance matrix, and uses Euclidean gradients to update the Cholesky factor in stochastic gradient ascent. Finally, we demonstrate that the learning rate, Adam (Kingma and Ba, 2015), which can "
3163,unknown,"can be interpreted as a sign-based approach with per dimension variance adaptation (Balles and Hennig, 2018), is incompatible with natural gradients as it neglects their scale information. We propose stochastic normalized natural gradient ascent with momentum (Snngm) as an alternative, which is shown to converge if the objective function is L- Lipschitz smooth with bounded gradients. Our approach "
3164,unknown,"Lipschitz smooth with bounded gradients. Our approach differs from Cutkosky and Mehta (2020) in normalization of the natural gradient instead of momentum of the Euclidean gradients. Stochastic normalized gradient descent is suited to non-convex optimization as it can overcome plateaus/cliffs in objective functions (Hazan et al., 2015). Section 2 introduces variational inference, where intractable "
3165,unknown,"2015). Section 2 introduces variational inference, where intractable lower bounds are op- timized using stochastic gradient ascent, and Section 3 motivates the use of natural gradients. Section 4 derives the natural gradient update of the natural parameter in Gaussian variational approximation, and compares it with Euclidean gradients and other parametrizations. In Section 5, we present natural gr"
3166,unknown,"parametrizations. In Section 5, we present natural gradient updates of the mean and Cholesky factor of the covariance/precision matrix, and unbiased estimates based on the first or second derivative of the log posterior for intractable lower bounds. Incompatibil- ity of Adam with natural gradients is demonstrated in Section 6 and Snngm is proposed for implementation. Section 7 presents natural gra"
3167,unknown,for implementation. Section 7 presents natural gradient updates for high-dimensional models where sparsity constraints are imposed. Performance of natural gradient up- dates is investigated using generalized linear mixed models (GLMMs) and deep neural networks in Section 8. We conclude with a discussion of future work in Section 9. 4 Linda Tan 2. Stochastic gradient variational inference Let p(y|θ
3168,unknown,"Let p(y|θ) denote the likelihood of variables θ ∈ Rd given observed data y, and p(θ) be a prior density for θ. In variational inference, an intractable posterior distribution, p(θ|y) = p(y|θ)p(θ)/p(y), is approximated by a more tractable density qλ(θ) with pa- rameter λ, that minimizes the KL divergence between qλ(θ) and p(θ|y). As log p(y) = Z qλ(θ) log qλ(θ) p(θ|y)dθ | {z } KL divergence + Z qλ("
3169,unknown,"p(θ|y)dθ | {z } KL divergence + Z qλ(θ) log p(y, θ) qλ(θ) dθ | {z } Evidence lower bound, L(λ) , minimizing the KL divergence is equivalent to maximizing the lower bound on the log marginal likelihood, log p(y), with respect to λ. Let L(λ) = E q[h(θ)] denote the lower bound, where h(θ) = log p(y, θ) − log qλ(θ). When L is intractable, stochastic gradient ascent can be used for optimization. Starti"
3170,unknown,"λ ← λ + ρt b∇λL, is performed at iteration t, where b∇λL is an unbiased estimate of the Euclidean gradient ∇λL. Applying chain rule, ∇λL = R {∇λqλ(θ)}h(θ)dθ, since Eq[∇λ log qλ(θ)] = 0. Under regularity conditions, the algorithm will converge to a local maximum ofL if the stepsize ρt satisfies P∞ t=1 ρt = ∞ and P∞ t=1 ρ2 t < ∞ (Spall, 2003). 3. Natural gradient Search for the optimal λ is performe"
3171,unknown,"Search for the optimal λ is performed in the parameter space of qλ(θ), which has its own curvature, and the Euclidean metric may not be appropriate for measuring the distance between densities indexed by differentλs. For instance, N(0.1, 1000) and N(0.2, 1000) are similar, while N(0.1, 0.1) and N(0.2, 0.1) are vastly different, but both pairs have the same Euclidean distance. Amari (2016) defines "
3172,unknown,"have the same Euclidean distance. Amari (2016) defines the distance between λ and λ + dλ as 2KL(qλ∥qλ+dλ) for small dλ. Using a second order Taylor expansion, this is approximately equal to 2Eq h log qλ(θ) − {log qλ(θ) + dλ⊤∇λ log qλ(θ) + 1 2dλ⊤∇2 λ log qλ(θ)dλ} i = dλ⊤Fλdλ, where Fλ = −Eq[∇2 λ log qλ(θ)] is the Fisher information of qλ(θ). Thus, the distance between λ and λ + dλ is not dλ⊤dλ as i"
3173,unknown,"between λ and λ + dλ is not dλ⊤dλ as in a Euclidean space, but dλ⊤Fλdλ. The set of all distributions qλ(θ) is a manifold and the KL divergence provides the manifold with a Riemannian structure, with Fisher norm ∥dλ∥Fλ = p dλ⊤Fλdλ if Fλ is positive definite. Steepest ascent direction of L at λ is defined as the vector a that maximizes L(λ + a), where ∥a∥Fλ is equal to a small constant ϵ > 0 (Amari,"
3174,unknown,"multipliers, L = L(λ + a) − α(∥a∥2 Fλ − ϵ2) ≈ L(λ) + a⊤∇λL −α(a⊤Fλa − ϵ2). Setting ∇aL ≈ ∇λL− 2αFλa to zero yields a = ϵ(e∇λL)/∥e∇λL∥Fλ. Thus, steepest ascent direction in the parameter space is given by the natural gradient, e∇λL = F−1 λ ∇λL. Natural gradient updates in Gaussian variational approximation 5 Table 1. Natural gradient updates. ∇µL and ∇ΣL are evaluated at (µ(t), Σ(t)). κ ξ λ (natura"
3175,unknown,"Σ(t+1) = Σ(t) + 2ρtΣ(t)∇ΣLΣ(t) Σ−1(t+1) = Σ−1(t) − 2ρt∇ΣL Σ−1(t+1) = Σ−1(t) − 2ρt∇ΣL µ(t+1) = µ(t) + ρtΣ(t)∇µL µ(t+1) = µ(t) + ρtΣ(t)∇µL µ(t+1) = µ(t) + ρtΣ(t+1)∇µL Replacing the unbiased Euclidean gradient estimate with that of the natural gradient yields the stochastic natural gradient update, λ ← λ + ρt F−1 λ b∇λL. Another motivation for using the natural gradient is that, provided qλ(θ) is a g"
3176,unknown,"approximation to p(θ|y), then close to the mode, ∇2 λL = Z ∇2 λqλ(θ) {log p(y, θ) − log qλ(θ)}dθ − Fλ ≈ −Fλ, as the first term is approximately zero. Thus the natural gradient update resembles Newton-Raphson, a second-order optimization method, where λ ← λ −(∇2 λL)−1∇λL. If ξ ≡ ξ(λ) is a smooth invertible reparametrization of qλ(θ) and J = ∇ξλ, then e∇ξL = F−1 ξ ∇ξL = (JFλJ⊤)−1J∇λL = (∇λξ)⊤ e∇λL. "
3177,unknown,"e∇ξL = F−1 ξ ∇ξL = (JFλJ⊤)−1J∇λL = (∇λξ)⊤ e∇λL. (1) 4. Gaussian variational approximation A popular option for qλ(θ) is the multivariate Gaussian, N( µ, Σ), which is a member of the exponential family. It can be written as qλ(θ) = exp n s(θ)⊤λ − A(λ) o , s (θ) =  θ vech(θθ⊤)  , λ =  Σ−1µ −1 2D⊤vec(Σ−1)  , (2) where s(θ) is the sufficient statistic, λ is the natural parameter and A(λ) = 1 2µ⊤Σ−"
3178,unknown,"2µ⊤Σ−1µ + 1 2 log |Σ| + d 2 log(2π) is the log-partition function. For any square matrix X, vec( X) is the vector obtained by stacking columns of X from left to right and vech( X) is obtained from vec(X) by omitting supradiagonal elements. If X is symmetric, D is the duplication matrix such that Dvech(X) = vec(X). For (2), m = E[s(θ)] = ∇λA(λ) and Var[s(θ)] = ∇2 λA(λ) = ∇λm = Fλ. Applying chain ru"
3179,unknown,"(Khan and Lin, 2017). Thus, the natural gradient, e∇λL = ∇mL = ∇µL −2(∇ΣL)µ D⊤vec(∇ΣL)  , where vec( ∇ΣL) = ∇vec(Σ)L, can be obtained without finding F−1 λ explicitly. The derivation is in the supplement S1 and the natural gradient update for λ is in Table 1. Consider other parametrizations, κ = (µ⊤, vech(Σ)⊤)⊤ and ξ = (µ⊤, vech(Σ−1)⊤)⊤, which are one-one transformations of λ. The natural gradie"
3180,unknown,"which are one-one transformations of λ. The natural gradients e∇κL and e∇ξL are derived using (1) in the supplement S1, and corresponding updates are in Table 1. The update for ξ is almost identical to λ, except that the update of µ for λ relies on the updated Σ, unlike that for ξ. The Fisher information of κ and ξ are block diagonal matrices, which imply that κ and ξ are orthogonal parametrizatio"
3181,unknown,"imply that κ and ξ are orthogonal parametrizations. However, it is only through the non-orthogonal parametrization λ, that we discover that the updated Σ can be used to improve the update of µ, due to the correlation between Σ −1µ and Σ−1. 6 Linda Tan Table 2. Loglinear model: number of iterations and smallest ρt used. Starting point Euclidean ( κ) Natural ( κ) Natural ( ξ) Natural ( λ) (0, 0.1) 1"
3182,unknown,"(0, 0.1) 141 (1e-05) 15 (0.01) 11 (0.01) 6 (1) (0.5, 0.02) 107 (1e-06) 12 (0.1) 8 (0.1) 5 (1) (2, 0.01) 115 (1e-06) 9 (0.01) 8 (0.1) 5 (1) 4.1. An illustration using Poisson loglinear model To gain insights on how natural gradient updates in Table 1 compare with the Euclidean gradient update, µ(t+1) = µ(t) + ρt∇µL, Σ(t+1) = Σ(t) + ρt∇ΣL, consider the loglinear model. Let yi ∼ Poisson(δi) and log δ"
3183,unknown,"consider the loglinear model. Let yi ∼ Poisson(δi) and log δi = x⊤ i θ for i = 1, . . . , n, where xi and θ denote covariates and regression coefficients respectively. Consider a prior, θ ∼ N(0, σ2 0I) where σ2 0 = 100, and a Gaussian approximation N( µ, Σ) of the true posterior of θ. The lower bound L is tractable and hence its curvature can be studied easily. Expressions of L, ∇µL and ∇ΣL are gi"
3184,unknown,"easily. Expressions of L, ∇µL and ∇ΣL are given in the supplement S2. To visualize the gradient vector field, we consider intercept-only models and write Σ as σ2. Variational parameters ( µ, σ2) are estimated using gradient ascent and the largest possible stepsize ρt ∈ {1, 0.1, 0.01, ...} is used in each iteration, provided that the update of σ2 is positive and L is increasing. We use as observati"
3185,unknown,"the update of σ2 is positive and L is increasing. We use as observations the number of satellites of 173 female horseshoe crabs (Table 3.2, Agresti, 2018). Figure 1 shows the gradient vector field and gradient ascent trajectories from 3 starting points marked by squares. L is maximized at (µ, σ2) = (1.07, 0.002), which is marked by a circle. Number of iterations to converge and smallest ρt used ar"
3186,unknown,0.0 0.5 1.0 1.5 2.0 0.0 0.1 0.2 0.3 0.4 Euclidean gradient (κ) µ σ2 0.0 0.5 1.0 1.5 2.0 0.00 0.02 0.04 0.06 0.08 0.10 Natural gradient (κ) µ σ2 0.0 0.5 1.0 1.5 2.0 0 100 200 300 400 500 Natural gradient (ξ) µ 1 σ2 0 500 1000 1500 0 200 400 600 800 1200 Natural gradient (λ) µ σ2 1 σ2 Fig. 1.Gradient vector field and trajectories for gradient ascent from 3 starting points. The first two plots show a
3187,unknown,"The first two plots show a sharp contrast between the Euclidean and natural gradient vector fields for κ = (µ, σ2), especially when σ2 is close to zero. While natural gradients are collectively directed at the mode, Euclidean gradients have strong vertical compo- nents, causing longer zigzag trajectories. Number of iterations required for Euclidean gradients is an order of magnitude larger than na"
3188,unknown,"gradients is an order of magnitude larger than natural gradients, and a much smaller stepsize has to be used at some point to avoid a negative σ2 update or L decreasing. Natural gradient ascent for λ is most efficient, likely because natural gradient updates of stepsize 1 correspond to fixed point iteration for the natural parameter (Tan and Nott, 2013). In the one-dimensional case, we can reparam"
3189,unknown,"Natural gradient updates in Gaussian variational approximation 7 instance, and update w without using smaller stepsize to ensure positiveness, but such techniques are hard to apply in higher dimensions. We chose d = 1 for visualization but issues of ensuring positive definiteness of Σ occur similarly in higher dimensions. 5. Natural gradient updates for mean and Cholesky factor In many application"
3190,unknown,"In many applications, the mean and Cholesky factor of the covariance/precision matrix in Gaussian variational approximation are updated using stochastic gradient ascent, which avoids positive definite constraints and allows flexibility in choice of stepsize, reduction in computation/storage costs and ease in simulations. However, existing analytic updates for Cholesky factors are based on Euclidea"
3191,unknown,"for Cholesky factors are based on Euclidean gradients (Titsias and L´ azaro-Gredilla, 2014; Tan and Nott, 2018). We derive the natural gradient counterparts by considering λ = (µ⊤, vech(C)⊤)⊤ where Σ = CC⊤, or λ = (µ⊤, vech(T)⊤)⊤ where Σ−1 = T T⊤, such that C and T are lower triangular matrices. In these cases, λ is not the natural parameter and we need to find the inverse Fisher information expli"
3192,unknown,"and we need to find the inverse Fisher information explicitly to get the natural gradient. The Fisher information for both cases turn out to be block diagonal matrices of the same form. Hence the inverse can be found using a common result in (ii) of Lemma 1, while (iii) is useful in simplifying F−1 λ ∇λL. The Fisher information and natural gradient for these two parametrizations are presented in T"
3193,unknown,"Let X ∈ Rd×d. In Lemma 1, K is the commutation matrix such that Kvec(X) = vec(X⊤), and L is the elimination matrix such that Lvec(X) = vech(X). If X is lower triangular, then L⊤vech(X) = vec( X). Magnus and Neudecker (1980) highlight that one should differentiate with respect to vech( X) instead of vec(X) if X is symmetric or lower triangular. The transformation matrices, K, L and N = (K + Id2 )/2"
3194,unknown,"in handling the redundancy of supradiagonal elements. Let ¯X be the lower triangular matrix derived from X by replacing supradiagonal elements by zero, while ¯¯X is obtained from ¯X by halving the diagonal. Lemma 1. Let C be any d × d lower triangular matrix and I(C) = L{(C−1 ⊗ C−⊤)K + (Id ⊗ C−⊤C−1)}L⊤. (i) I(C) = 2L(Id ⊗ C−⊤)N(Id ⊗ C−1)L⊤. (ii) I(C)−1 = 1 2L(Id ⊗ C)L⊤(LNL⊤)−1L(Id ⊗ C⊤)L⊤. (iii) I"
3195,unknown,"(iii) I(C)−1vech(G) = vech(C ¯¯H) for any d × d matrix G, where H = C⊤ ¯G. Theorem 1. (i) If λ = (µ⊤, vech(C)⊤)⊤, let H = C⊤ ¯G and ∇vech(C)L = vech(G). The natural gradient update at iteration t is µ(t+1) = µ(t) + ρtΣ(t)∇µL(t), C(t+1) = C(t) + ρtC(t) ¯¯H(t), since Fλ =  Σ−1 0 0 I(C)  and e∇λL =  Σ∇µL vech(C ¯¯H)  . (ii) If λ = ( µ⊤, vech(T)⊤)⊤, let H = T⊤ ¯G and ∇vech(T)L = vech(G). The natur"
3196,unknown,"gradient update at iteration t is µ(t+1) = µ(t) + ρtΣ(t)∇µL(t), T(t+1) = T(t) + ρtT(t) ¯¯H(t), since Fλ =  Σ−1 0 0 I(T)  and e∇λL =  Σ∇µL vech(T ¯¯H)  . 8 Linda Tan From Theorem 1, if C is a diagonal matrix, then G is also diagonal and H = CG while ¯¯H = 1 2CG. Thus the update for C simplifies to C ← C + ρt 2 C2G. Inspired by the superior performance of the natural parameter in Section 4.1, we"
3197,unknown,"the superior performance of the natural parameter in Section 4.1, we consider a one-one transformation of λ = (µ⊤, vech(T)⊤)⊤ in Corollary 1, which reveals that the updated T can be used to update µ. Unfortunately, it is not possible to obtain similar updates for C. Proofs of Lemma 1, Theorem 1 and Corollary 1 are given in the supplement S3. Corollary 1.Let ξ = ((T⊤µ)⊤, vech(T)⊤)⊤, H = T⊤ ¯G and ∇"
3198,unknown,"The natural gradient update of ξ at iteration t is T(t+1) = T(t) + ρtT(t) ¯¯H(t), µ(t+1) = µ(t) + ρtT(t+1)−⊤ T(t)−1 ∇µL(t). To investigate the differences between updates of ( µ, T) in Theorem 1 and Corollary 1, consider the loglinear model in Section 4.1 again, this time fitting Model 1: Sa ∼ Width, and Model 2: Sa ∼ Color + Width. The largest stepsize ρt ∈ {1, 0.1, 0.01, ...} is used provided L "
3199,unknown,"is used provided L is increasing. Figure 2 shows that updates in Corollary 1 converge faster and are more resilient to larger stepsizes. 2 4 6 8 10 12 14 0.01 0.05 0.20 1.00 Model 1 Iteration (t) Stepsize (ρt) Thm 1 Cor 1 2 4 6 8 10 12 14 0.01 0.05 0.20 1.00 Model 2 Iteration (t) Stepsize (ρt) Thm 1 Cor 1 Fig. 2.Stepsize ρt used at each iteration in natural gradient update of (µ, T). 5.1. Stochast"
3200,unknown,"5.1. Stochastic natural gradient updates If ∇λL is not analytic, stochastic natural gradient ascent can be performed using an unbiased estimate. The Fisher information in the natural gradient e∇λL = F−1 λ ∇λL can be computed exactly given λ, and stochasticity arises only from substitution of ∇λL with b∇λL. Hence, an unbiased estimate of the natural gradient is obtained if Eq(b∇λL) = ∇λL. For updat"
3201,unknown,"Eq(b∇λL) = ∇λL. For updates of ( µ, Σ), Khan et al. (2018) obtained unbiased estimates of ∇µL and ∇ΣL using the Theorems of Bonnet (1964) and Price (1958), which are stated below. The proofs can be found in Lin et al. (2019b). ACL is an abbreviation for absolute continuity on almost every straight line (Leoni, 2017). The second equality in Bonnet’s Theorem is also known as Stein’s Lemma (Stein, 19"
3202,unknown,"Theorem is also known as Stein’s Lemma (Stein, 1981). As L = E q[h(θ)], if θ is a sample generated from qλ(θ) at iteration t, then the stochastic natural gradient update of natural parameter λ is Σ(t+1)−1 = Σ(t)−1 − ρt∇2 θh(θ), µ (t+1) = µ(t) + ρtΣ(t+1)∇θh(θ). Natural gradient updates in Gaussian variational approximation 9 The above result follows from Table 1 by replacing ∇µL and ∇ΣL by their un"
3203,unknown,"estimates, ∇θh(θ) and 1 2∇2 θh(θ), from Bonnet’s and Price’s Theorems. Bonnet’s Theorem (Stein’s Lemma).If θ ∼ N(µ, Σ) and h : R d → R is locally ACL and continuous, then ∇µEq[h(θ)] = Eq  Σ−1(θ − µ)h(θ)  = Eq[∇θh(θ)]. Price’s Theorem. If θ ∼ N(µ, Σ), h : R d → R is continuously differentiable, ∇θh(θ) is locally ACL and Eq[h(θ)] is well-defined, then ∇ΣEq[h(θ)] = 1 2Eq h Σ−1(θ − µ)∇θh(θ)⊤ i = 1 2"
3204,unknown,"∇ΣEq[h(θ)] = 1 2Eq h Σ−1(θ − µ)∇θh(θ)⊤ i = 1 2Eq  ∇2 θh(θ)  . For Cholesky factors, Price’s Theorem cannot be applied directly but there are sev- eral alternatives. The score function method uses ∇λqλ(θ) = qλ(θ)∇λ log qλ(θ) to write ∇λL = E q[∇λ log qλ(θ)h(θ)], but tends to have high variance leading to slow conver- gence, and variance reduction techniques are required (Paisley et al., 2012; Ran"
3205,unknown,"et al., 2014; Ruiz et al., 2016). The reparametrization trick (Kingma and Welling, 2014) introduces a differentiable transformation θ = Tλ(z) so that the density ϕ(z) of z is independent of λ. Making a variable substitution and applying chain rule, ∇λL = Eϕ[∇λθ ∇θh(θ)]. Reparametrization trick’s gradients typically have lower vari- ance than the score function method (Xu et al., 2019), but yields "
3206,unknown,"ance than the score function method (Xu et al., 2019), but yields unbiased estimates of ∇vech(C)L and ∇vech(T)L in terms of the first derivative of h(θ). We propose alternative unbiased estimates in terms of the second derivative of h(θ) in Theorem 2. Our results extend Bonnet’s and Price’s Theorems to gradients with respect to the Cholesky factor of the covariance/precision matrix. Lemma 2 is ins"
3207,unknown,"all proofs are given in the supplement S3. Lemma 2. If θ ∼ N(µ, Σ) and h : R d → R is locally ACL and continuous, then Eq  {Σ−1(θ − µ)(θ − µ)⊤ − Id}h(θ)  = Eq  ∇θh(θ)(θ − µ)⊤ . Theorem 2. Suppose h : R d → R is continuously differentiable, and h and ∇θh(θ) are locally ACL. Let θ ∼ N(µ, Σ), Σ = CC⊤ and Σ−1 = T T⊤ where C and T are lower triangular matrices, z = C−1(θ − µ) and v = T−1∇θh(θ). ∇ve"
3208,unknown,"∇vech(C)L = Eqvech(G1) = Eqvech(F1), where G1 = ∇θh(θ)z⊤, F1 = ∇2 θh(θ)C. ∇vech(T)L = Eqvech(G2) = Eqvech(F2), where G2 = −(θ − µ)v⊤, F2 = −Σ∇2 θh(θ)T−⊤. Theorem 2 is obtained by first finding ∇vech(C)L and ∇vech(T)L using the score func- tion method, which yields unbiased estimates in terms of h(θ). Applying Bonnet’s Theorem (Stein’s Lemma), we get estimates in terms of ∇θh(θ), which are identica"
3209,unknown,"those obtained from the reparametrization trick. Finally, estimates in terms of ∇2 θh(θ) are obtained using Price’s Theorem. The reparametrization trick is thus connected to the score function method via Stein’s Lemma. Since Price’s Theorem can be derived from Bonnet’s Theorem by applying Stein’s Lemma, we are applying Stein’s Lemma repeatedly to obtain unbiased estimates in terms of even higher d"
3210,unknown,"repeatedly to obtain unbiased estimates in terms of even higher derivatives of h(θ). Second order estimates are expensive computationally, but can be advantageous if ∇2 θh(θ) is not too complex as they are more stable close to the optimum where L will be 10 Linda Tan Table 3.Stochastic variational algorithms for updating (µ, C). Algorithm 1E (Euclidean gradient) Algorithm 1N (Natural gradient) Ini"
3211,unknown,"Initialize µ and C. For t = 1, 2, . . ., 1. Generate z ∼ N(0, Id) and compute θ = Cz + µ. 2. Find ¯G where G = ∇θh(θ)z⊤ or G = ∇2 θh(θ)C. 3. Update µ ← µ+ρt∇θh(θ). 4. Update C ← C + ρt ¯G. 3. Update µ ← µ+ρtCC⊤∇θh(θ). 4. Find ¯¯H where H = C⊤ ¯G. 5. Update C ← C + ρtC ¯¯H. Table 4. Stochastic variational algorithms for updating (µ, T). Algorithm 2E (Euclidean gradient) Algorithm 2N (Natural gradie"
3212,unknown,"Initialize µ and T. For t = 1, 2, . . ., 1. Generate z ∼ N(0, Id) and compute θ = T−⊤z + µ. 2. Find ¯G where G = −T−⊤zv⊤, v = T−1∇θh(θ) or G = −T−⊤T−1∇2 θh(θ)T−⊤. 3. Update µ ← µ + ρt∇θh(θ). 4. Update T ← T + ρt ¯G. 3. Find ¯¯H where H = T⊤ ¯G. 4. Update T ← T + ρtT ¯¯H. 5. Update µ ← µ + ρtT−⊤v. approximately quadratic. Suppose ℓ(θ) = log p(y, θ) is well approximated by a second order Taylor expa"
3213,unknown,"h(θ) ≈ ℓ(ˆθ) + 1 2(θ − ˆθ)⊤∇2 θℓ(ˆθ)(θ − ˆθ) + d 2 log(2π) + 1 2 log |Σ| + 1 2(θ − µ)Σ−1(θ − µ), ∇θh(θ) ≈ ∇2 θℓ(ˆθ)(θ − ˆθ) + Σ−1(θ − µ), ∇2 θh(θ) ≈ ∇2 θℓ(ˆθ) + Σ−1. While F1 and F2 are independent ofθ and hence have almost zero variation,G1 and G2 are subjected to stochasticity due to simulation of θ from qλ(θ). Later experiments indicate that this is important for inferring high-dimensional dens"
3214,unknown,"Gaussian variational approximation. Instead of computing ∇2 θh(θ), we can also reduce cost by using approximations. Khan et al. (2015) designed updates involving ∇2 θh(θ) for GLMs that scale linearly with the number of observations if d is large, while Khan et al. (2018) use the generalized Gauss-Newton (Graves, 2011) or gradient magnitude (Bottou et al., 2018) approximation for ∇2 θh(θ). Besides "
3215,unknown,"et al., 2018) approximation for ∇2 θh(θ). Besides second order gradient estimates, control variates can also be used for variance reduction (Ranganath et al., 2014). Stochastic variational algorithms obtained using Theorem 2 are outlined in Tables 3 and 4, and we have applied Corollary 1 in the update of µ in Algorithm 2N. Algorithms based on Euclidean and natural gradients are placed side-by-side"
3216,unknown,"based on Euclidean and natural gradients are placed side-by-side for ease of comparison. Those based on natural gradients have an additional step for computing ¯¯H, and the updates involve some form of scaling, which can help to improve convergence. 6. Choice of stepsize in stochastic natural gradient ascent For high-dimensional models, it is often important to use an adaptive stepsize ρt that is "
3217,unknown,"is robust to noisy gradients. Some popular approaches include Adagrad (Duchi et al., Natural gradient updates in Gaussian variational approximation 11 Table 5. Adam, Snngm and Nagm (scalar functions are performed elementwise on vectors). Adam (Kingma and Ba, 2015) Snngm Nagm Initialize m0 = 0, v0 = 0 and λ(1). α = 0.001, β1 = 0.9, β2 = 0.999, ϵ = 10−8. For t = 1, 2, . . ., 1. Compute gradient gt. "
3218,unknown,"2. mt = β1mt−1 + (1 − β1)gt. 3. vt = β2vt−1 + (1 − β2)g2 t . 4. bmt = mt/(1 − βt 1) bvt = vt/(1 − βt 2). 5. λ(t+1) = λ(t) + α bmt/(√bvt + ϵ). Initialize m0 = 0 and λ(1). α = α0 √ℓλ where ℓλ is length of λ, β = 0.9. For t = 1, 2, . . ., 1. Compute natural gradient estimate egt. 2. mt = βmt−1 + (1− β) egt ∥egt∥. 3. bmt = mt/(1 − βt). 4. λ(t+1) = λ(t) + α bmt. Initialize m0 = 0 and λ(1). β = 0.9, T ="
3219,unknown,"For t = 1, 2, . . ., 1. Compute Euclidean gradient estimate ˆgt. 2. ˆgt ← min  1, T ∥ˆgt∥  ˆgt 3. mt = βmt−1 +(1 −β)ˆgt. 4. λ(t+1) = λ(t) +αF−1 t mt. 2011), Adadelta (Zeiler, 2012) and Adam (Kingma and Ba, 2015), which compute ele- mentwise adaptive learning rates using past gradients. Adam (Table 5) introduces mo- mentum by computing the exponential moving average of the gradient (mt) and squar"
3220,unknown,"gradient (vt), and corrects for the bias due to initializing mt and vt at 0 using bmt and bvt. The effective step is α bmt/(√bvt + ϵ), where ϵ is a small constant added to avoid division by zero. Despite its wide applicability, we observe that use of natural gradients with Adam fails to yield significant improvement in convergence compared to Euclidean gradients. Indeed, Adam can be interpreted as"
3221,unknown,"gradients. Indeed, Adam can be interpreted as a sign-based variance adapted approach (Balles and Hennig, 2018), since (ignoring ϵ) the update step can be expressed as αsign( bmt)p bvt/ bm2 t ≈ α sign( bmt)p E(g2 t )/E(gt)2 = α sign( bmt)p 1 + Var(gt)/E(gt)2 . The update direction is dominated by the sign of bmt, while the per dimension magnitude is bounded by α, and reduced when there is high unce"
3222,unknown,"is bounded by α, and reduced when there is high uncertainty (measured by the relative variance). If we replace the Euclidean gradient estimate bgt by natural gradient estimate egt, Adam will update by focusing on the sign information inegt while the scaling is largely neglected. Loss of scale information is compounded by per dimension variance adaption. We explore two alternatives in Table 5 that "
3223,unknown,"We explore two alternatives in Table 5 that retain scale information: a basic approach Nagm that estimates the natural gradient based on the exponential moving average (mo- mentum) of the Euclidean gradient, and stochastic normalized natural gradient ascent with momentum (Snngm). For Nagm, Ft = Fλ(λ(t)) and gradient clipping is used to avoid exploding gradients. In practice, Nagm is applied to µ a"
3224,unknown,"avoid exploding gradients. In practice, Nagm is applied to µ and C separately as a smaller learning rate must be used for C to avoid divergence. We use an α that is 10 times and 100 times smaller than that for µ for diagonal and dense covariance matrices respectively. 6.1. Snngm Excluding momentum by setting β = 0, the update step in Snngm is αegt/∥egt∥, where ∥·∥ denotes the Euclidean norm. The n"
3225,unknown,"denotes the Euclidean norm. The norm of this step is fixed atα, and the effective stepsize is ρt = α/∥egt∥, which is common for all parameters to preserve the scaling information. 12 Linda Tan In the initial optimization stage when λ is far from the mode, ρt will be small as the gradient tends to be large. This is important for stability especially if the initialization is far from the mode. As λ "
3226,unknown,"is far from the mode. As λ approaches the optimum, ρt increases as the gradient tends to zero. Normalized natural gradient ascent can thus avoid slow convergence close to the mode and evade saddle points (Hazan et al., 2015). As the true natural gradient is unknown, we inject momentum using the exponential moving average for robustness against noisy gradients. As ∥egt∥ tends to increase with the l"
3227,unknown,"against noisy gradients. As ∥egt∥ tends to increase with the length of λ, scaling up α proportionally can prevent the stepsize from becoming too small in high-dimensions. To illustrate the difference between Adam and Snngm, consider the intercept-only loglinear model in Section 4.1 again. Figure 3 shows that natural gradients did not yield any improvements in Adam. Instead, more iterations were re"
3228,unknown,"any improvements in Adam. Instead, more iterations were required, and the run starting from (2, 0.01) was terminated due to a negative σ2 update. The second plot also shows that Adam does not follow the flow of natural gradients closely unlike Snngm in the third plot. This is likely caused by the loss of scale information in Adam. On the other hand, the number of iterations was reduced by about th"
3229,unknown,0.0 0.5 1.0 1.5 2.0 0.00 0.02 0.04 0.06 0.08 0.10 2197 1261 2591 Euclidean (Adam) µ σ2 0.0 0.5 1.0 1.5 2.0 0.00 0.02 0.04 0.06 0.08 0.10 5948 1940 14 Natural (Adam) µ σ2 0.0 0.5 1.0 1.5 2.0 0.00 0.02 0.04 0.06 0.08 0.10 767 431 658 Natural (Snngm) µ σ2 Fig. 3. Gradient vector field and trajectories of Adam and Snngm ( α = 0 .001 √ 2) from three starting points. Legend shows the total number of ite
3230,unknown,"We analyze the convergence of Snngm under assumptions (A1)–(A4), of which (A1)– (A3) are similar to that made by D´ efossez et al. (2020) in proving convergence of Adam. (A1) L(λ) ≤ L∗ ∀ λ ∈ Rd. (A2) ∥b∇λL(λ)∥ ≤R ∀ λ ∈ Rd. (A3) L(λ) is L-Lipschitz smooth: ∃ a constant L >0 such that ∥∇λL(λ′) −∇λL(λ)∥ ≤ L∥λ′ − λ∥ ∀λ, λ′ ∈ Rd. (A4) 0 < R1 ≤ ev(Fλ) ≤ R2 ∀λ ∈ Rd, where ev(Fλ) denotes the eigenvalues o"
3231,unknown,"Following D´ efossez et al. (2020), letτ be a random index such that P(τ = j) ∝ 1−βN−j+1 for j ∈ {1, . . . , N}. The proportionality constant of the distribution of τ is, C = NX j=1 (1 − βN−j+1) = N − β(1 − βN ) 1 − β ≥ N − β 1 − β = eN. For the distribution of τ, almost all values of j are sampled uniformly, except the last few which are sampled less often. Figure 4 shows the value of 1 −βN−j+1 f"
3232,unknown,"and β = 0.9. All values are greater than 0.99 except for the last 43 of them. Natural gradient updates in Gaussian variational approximation 13 0 2000 4000 6000 8000 10000 0.2 0.4 0.6 0.8 1.0 β= 0.9 j 1 − β(N −j+1) Fig. 4.Value of 1 − βN−j+1 for j = 1, . . . , Nwhere N = 10000. Theorem 3 provides bounds for the expected squared norm of the gradient at iteration τ in Snngm. The proof is given in th"
3233,unknown,"then eN ≈ N. Setting α = 1/ √ N then yields an O(1/ √ N) convergence rate. Theorem 3. In Snngm, under assumptions (A1)–(A4) and for any N > β/(1 − β), E∥gτ ∥2 ≤ RR2 R1 L∗ − L(λ(1)) eNα + NLα eN  β 1 − β + 1 2  . In later experiments, we compare Snngm and Nagm with Adam based on Euclidean or natural gradients (NAdam), variational online Gauss-Newton (VOGN, Khan et al., 2018) and ProxGenAdam (PG"
3234,unknown,"2018) and ProxGenAdam (PGA, Kim et al., 2023). VOGN uses the natural gradient update for the natural parameter of the Gaussian density. It ensures positive-definiteness of the precision matrix by using the generalized Gauss-Newton approximation for the Hessian of the log-likelihood, and considers bias-corrected momentum for µ to derive Adam-like updates. PGA uses Euclidean gradients, and combines "
3235,unknown,"Adam-like updates. PGA uses Euclidean gradients, and combines a variant of proximal stochastic gradient descent (Domke, 2020) called ProxGen (Yun et al., 2021) with Adam, where updates for diagonal elements of C are modified to keep them away from zero. We derive similar modified updates of T for PGA in the supplement S5, where VOGN and PGA are also described in more detail. Hyperparameters for Ad"
3236,unknown,"fixed at default values in Table 5. For Snngm, Nagm and VOGN, α requires tuning while other hyperparameters are generally set at default values (refer to code for values used). 7. Imposing sparsity For high-dimensional models, it is useful to impose sparsity on the Cholesky factor of the covariance or precision matrix to increase efficiency. For Algorithms 1E and 2E, updates of sparse Cholesky fac"
3237,unknown,"updates of sparse Cholesky factors can be obtained by extracting entries in the Euclidean gradients that correspond to nonzero entries in the Cholesky factor, but the same may not apply to natural gradients due to premultiplication by the Fisher information. Suppose λ = (λ⊤ 1 , λ⊤ 2 )⊤ and the Fisher information, Euclidean gradient and natural gradient are Fλ =  F11 F12 F21 F22  , g =  g1 g2  "
3238,unknown,"g2  , eg = F−1 λ g =  eg1 eg2  , 14 Linda Tan respectively. By block matrix inversion, eg1 = F−1 11 g1 − F−1 11 F12eg2. If we fix λ2 = 0 (λ2 is no longer an unknown parameter), then the natural gradient for updating λ1 is F−1 11 g1, which is equal to eg1 + F−1 11 F12eg2, but not eg1, unless F is a block diagonal matrix. In this section, we derive efficient natural gradient updates of the Choles"
3239,unknown,"in two cases, (i) the covariance matrix has a block diagonal structure corresponding to the variational Bayes assumption, and (ii) the precision matrix reflects the posterior conditional independence structure in a hierarchical model where local variables are independent conditional on the global variables. 7.1. Block diagonal covariance Let qλ(θ) = QK i=1 qλi(θi) for some partitioning θ = (θ⊤ 1 ,"
3240,unknown,"1 , . . . , θ⊤ K)⊤, where θi ∼ N(µi, Σi). Then Σ = blockdiag(Σ 1, . . . ,ΣK) and µ = (µ⊤ 1 , . . . , µ⊤ K)⊤. Let CiC⊤ i be the Cholesky decomposition of Σ i, where Ci is a lower triangular matrix for i = 1, . . . , K, and C = blockdiag(C1, . . . , CK). For the parametrization λ = (µ⊤, vech(C1)⊤, . . . ,vech(CK)⊤)⊤, the Fisher information, Fλ = blockdiag(Σ−1, I(C1), . . . ,I(CK)), where I(·) is def"
3241,unknown,"Lemma 1. Let ∇vech(Ci)L = vech(Gi) and Hi = C⊤ i ¯Gi for i = 1, . . . , K. Then it follows from Lemma 1 that the natural gradient, e∇λL = F−1 λ ∇λL =   Σ 0 . . . 0 0 I(C1)−1 . . . 0 ... ... ... ... 0 0 . . .I(CK)−1     ∇µL vech( ¯G1) ... vech( ¯GK)   =   Σ∇µL vech(C1 ¯¯H1) ... vech(CK ¯¯HK)  . This expression reveals the sparse structures of matrices that underlie com"
3242,unknown," . This expression reveals the sparse structures of matrices that underlie computation of the natural gradient. Let G = blockdiag(G1, . . . , GK) and H = blockdiag(H1, . . . , HK). Then H = C⊤ ¯G and C ¯¯H = blockdiag( C1 ¯¯H1, . . . , CK ¯¯HK). Thus C, ¯G, ¯¯H and C ¯¯H have the same sparse block lower triangular structure (see Figure 5), which is useful in improving storage and computationa"
3243,unknown,"improving storage and computational efficiency. ҧ𝐺𝐶𝑇 𝐻 Fig. 5.Shaded regions represent nonzero entries in C⊤, ¯G and H = C⊤ ¯G (K = 4). If the Euclidean gradient ∇λL is intractable, then unbiased estimates of ∇vech(Ci)L can be obtained using Theorem 2. As C is a block diagonal matrix, we only extract entries in G1 and F1 that correspond to C1, . . . , CK on the block diagonal. For i = 1, . . . , K"
3244,unknown,"1, . . . , K, ∇vech(Ci)L = Eqvech(∇θih(θ)z⊤ i ) = Eqvech(∇2 θih(θ)Ci), where zi = C−1 i (θi − µi). The resulting stochastic variational algorithm 1S is outlined in Table 6. Natural gradient updates in Gaussian variational approximation 15 Table 6. Stochastic natural gradient algorithms incorporating sparsity. Algorithm 1S (Update µ and C) Algorithm 2S (Update µ and T) Initialize µ and C = blockdia"
3245,unknown,"For t = 1, 2, . . ., 1. Generate z = (z1, . . . , zK)⊤ ∼ N(0, Id) and compute θ = Cz + µ. 2. Find ¯G = blockdiag( ¯G1, . . . ,¯GK) where Gi = ∇θih(θ)z⊤ i or ∇2 θih(θ)Ci. 3. Compute ¯¯H where H = C⊤ ¯G. 4. Update µ ← µ + ρtCC⊤∇θh(θ). 5. Update C ← C + ρtC ¯¯H. Initialize µ and T in (4). For t = 1, 2, . . ., 1. Generate z ∼ N(0, Id) and compute θ = T−⊤z + µ. 2. Find ¯G, which is given by −uv⊤ or −T−"
3246,unknown,"−T−⊤ d T−1∇2 θh(θ)T−⊤, where elements corresponding to zeros in T are set to 0. 3. Compute ¯¯H where H = TT d ¯G. 4. Update T ← T + ρtT ¯¯H. 5. Update µ ← µ + ρtT−⊤v. 7.2. Sparse precision matrix Consider a hierarchical model where the local variables specific to individual observa- tions, θ1, . . . , θn, are independent of each other given the global variables shared across all observations, θg. "
3247,unknown,"p(y, θ) = p(θg) nY i=1 p(yi|θi, θg)p(θi|θg), (3) where y = ( y1, . . . , yn)⊤, θ = ( θ⊤ 1 , . . . , θ⊤ n , θ⊤ g )⊤ and p(θg) is a prior density for the global variables. To reflect the fact θ1, . . . , θn are conditionally independent given θg a posteriori, let the Cholesky factor of Σ −1 be of the form T =   T1 . . . 0 0 ... ... ... ... 0 . . . Tn 0 Tg1 . . . Tgn Tg  , (4) where T1, . ."
3248,unknown,"1 , . . . , µ⊤ n , µ⊤ g )⊤ be the corresponding partitioning. Consider λ = (µ⊤, vech(T1)⊤, vec(Tg1)⊤, . . . ,vech(Tn)⊤, vec(Tgn)⊤, vech(Tg)⊤)⊤. Then the Fisher information is a block diagonal matrix which can be inverted analyti- cally. The natural gradient update for T in (4) is presented in Theorem 4 and the proof, which relies on Lemma 1, is in the supplement S6. Note that ¯G, ¯¯H and T ¯¯H hav"
3249,unknown,"same sparse structure as T, which allows efficient storage. Theorem 4. For i = 1 , . . . , n, let ∇vech(Ti)L = vech(Ai), ∇vec(Tgi)L = vec(Ggi) and Gi = Ai + T−⊤ i T⊤ gi Ggi. In addition, let ∇vech(Tg)L = vech(Gg) and Td = 16 Linda Tan Table 7. Loglinear model: lower bound at termination and runtime in brackets. Diagonal Covariance Full Covariance d = 50 d = 100 d = 50 d = 100 Adam -109491.4 (4.8) "
3250,unknown,d = 50 d = 100 d = 50 d = 100 Adam -109491.4 (4.8) -119170.4 (7.1) -109458.2 (7.5) -119073.7 (12.7) PGA -109198.6 (4.7) -115879.8 (7.1) -109162.2 (7.5) -115764.6 (12.5) NAdam -109568.6 (4.8) -125559.3 (7.2) -121200.5 (8.5) -377479.9 (13.9) VOGN -109114.5 (4.7) -115687.3 (7.2) -109037.6 (7.5) -115432.0 (18.4) Snngm -109063.8 (4.8) -115687.3 (7.1) -109030.5 (8.3) -115429.3 (13.5) Nagm -109118.4 (4.8
3251,unknown,"Nagm -109118.4 (4.8) -115658.2 (7.2) -109037.7 (8.5) -115431.7 (13.6) blockdiag(T1, . . . , Tn, Tg). The natural gradient update for T in (4) at iteration t is T(t+1) = T(t) + ρtT(t) ¯¯H, where H = T⊤ d ¯G and G =   G1 . . . 0 0 ... ... ... ... 0 . . . Gn 0 Gg1 . . . Ggn Gg  . 7.3. Stochastic natural gradient for sparse precision matrix If ∇λL is not tractable, an unbiased estimate of ¯G"
3252,unknown,"If ∇λL is not tractable, an unbiased estimate of ¯G can be obtained from Theorem 2 by extracting entries in G2 and F2 that correspond to nonzero entries in T. As the θis are conditionally independent a posteriori, ∇2 θi,θj h(θ) = 0 if i ̸= j and ∇2 θh(θ) is also sparse. Let u = (u⊤ 1 , . . . , u⊤ n , u⊤ g )⊤ = T−⊤ d T⊤(θ − µ) and v = (v⊤ 1 , . . . , v⊤ n , v⊤ g )⊤ = T−1∇θh(θ). In the supplement S6"
3253,unknown,"In the supplement S6, we show that an unbiased estimate of ¯G is given by −uv⊤ or −T−⊤ d T−1∇2 θh(θ)T−⊤, where elements that correspond to zeros in T are set to zero. The overall procedure is outlined in Algorithm 2S (Table 6). Compared with 2N, the computation of ¯G and ¯¯H differ in sparsity and usage of Td instead of T in some places. 8. Applications We apply proposed methods to GLMs, GLMMs and"
3254,unknown,"8. Applications We apply proposed methods to GLMs, GLMMs and deep neural networks. Variational approximation accuracy of different algorithms are compared using the lower bound L, and a higher value indicates an approximation closer to the true posterior. At termina- tion, L or its estimate is computed as the mean of h(θ) over simulations of θ from qλ(θ). We initialize µ = 0 in all cases except fo"
3255,unknown,"We initialize µ = 0 in all cases except for the weights in deep neural networks where Xavier initialization was applied, and C = Id/√n where n is the number of observations. The prior θ ∼ N(0, σ2 0Id) where σ0 = 10 is used in all models except deep net GLMs, which use adaptive shrinkage priors for regularization. Our code is written in Julia (Bezanson et al., 2017) and is available as supplementar"
3256,unknown,"(Bezanson et al., 2017) and is available as supplementary material. All experiments are run on an Intel Core i9-9900K CPU @ 3.60GHz and runtimes are in seconds. 8.1. Simulations from loglinear model First, consider the loglinear model (containing an intercept) in Section 4.1 with tractable lower bounds (see supplement S2), to study the impact of natural gradients apart from Natural gradient update"
3257,unknown,0 20 40 60 80 −180000 −140000 d=50 (diag) epoch lower bound Adam PGA NAdam VOGN Snngm Nagm 0 20 40 60 80 −450000 −300000 −150000 d=100 (diag) epoch lower bound 0 20 40 60 80 −180000 −140000 d=50 (full) time lower bound 0 20 40 60 80 −450000 −300000 −150000 d=100 (full) time lower bound Fig. 6. Loglinear model: average lower bound at each epoch. the use of higher derivatives for variance control. W
3258,unknown,"the use of higher derivatives for variance control. We generaten = 105 observations from the model with the dimension of θ, d ∈ {50, 100}, by simulating each covariate from U(0, 1) and letting {θi} be evenly distributed in [−1, 1]. For VOGN, ∇ΣEq[log p(yi|θ)] is used in place of the generalized Gauss-Newton approximation as it is positive definite. The data is processed in minibatches of 5000. Tab"
3259,unknown,"Table 7 shows that the (top 3) highest lower bounds are consistently achieved by VOGN, Snngm and Nagm, with PGA closely behind. For the loglinear model with tractable gradients, there is almost no difference in runtime across different algorithms when the covariance matrix is diagonal. For the full covariance matrix, natural gradient based approaches are more time-consuming and this becomes more a"
3260,unknown,"dimension increases. VOGN takes more time for d = 100 likely due to finding the Cholesky factor of the full precision matrix. Figure 6 shows that Nagm, VOGN and Snngm are among the fastest to converge followed by PGA. Adam and NAdam are much slower and NAdam is even stuck at poor local modes for the full covariance matrix case, highlighting the poor compatibility of natural gradients with Adam. 8."
3261,unknown,"case, highlighting the poor compatibility of natural gradients with Adam. 8.2. Logistic regression Next, consider logistic regression where the lower bound is intractable but the Hessian ∇2 θh(θ) can be computed analytically. The expression of h(θ) and its derivatives are given in the supplement S7. Given {(xi, yi)|i = 1, . . . , n} where xi ∈ Rd and yi ∈ {0, 1}, it is assumed that yi ∼ Bernoulli("
3262,unknown,"it is assumed that yi ∼ Bernoulli(pi), where logit(pi) = x⊤ i θ and xi contains an intercept. We simulate n = 10 5 observations from the model with d = 100, by letting {θi} be evenly distributed in [ −1, 1], and generating the covariates from a standard normal. In addition, we consider two real datasets from the UCI Machine Learning Repository. The German credit data classifies n = 1000 people des"
3263,unknown,"The German credit data classifies n = 1000 people described by 20 attributes (age, housing, etc) as good or bad credit risks, while the QSAR oral toxicity dataset contains 1024 binary attributes (molecular fingerprints) for classifying n = 8992 chemicals into toxic/not toxic. Continuous attributes are rescaled to have mean 0 and standard devi- ation 1, while categorical attributes are coded using "
3264,unknown,"ation 1, while categorical attributes are coded using dummy variables. After including an intercept and preprocessing, the dimension of θ for the German and toxicity data are 49 and 1025 respectively. The German data is processed in full batch, while minibatch sizes of 562 and 5000 are used respectively for the toxicity and simulated data. In Table 8, Adam1 and Adam2 denote algorithms that use Ada"
3265,unknown,"In Table 8, Adam1 and Adam2 denote algorithms that use Adam and updateC based 18 Linda Tan Table 8. Logistic regression: lower bound estimate at termination and runtime in (). Simulation German Toxicity Diagonal Full Diagonal Full Diagonal Full Adam1 -22936 (3.7) -71165 (4.2) -683.7 (0.3) -677.2 (0.3) -4538 (30.1) -15150 (416.0) PGA1 -21969 (3.6) -22444(4.2) -655.3 (0.2) -649.5 (0.3) -4524 (29.7) "
3266,unknown,PGA1 -21969 (3.6) -22444(4.2) -655.3 (0.2) -649.5 (0.3) -4524 (29.7) -32245 (392.5) NAdam1 -23841 (3.6) -150095 (5.2) -658.3 (0.2) -650.4 (0.4) -4516 (29.6) -219930 (572.3) Snngm1 -21951 (3.7) -30260 (5.2) -641.7 (0.2) -631.1(0.4) -4274(29.5) -6846(450.7) Nagm1 -21901(3.6) -22703 (5.2) -640.4(0.2) -633.0 (0.4) -4321 (29.5) -7044 (470.8) Adam2 -22765 (5.6) -22769 (8.6) -680.8 (0.3) -666.8 (0.6) -45
3267,unknown,Adam2 -22765 (5.6) -22769 (8.6) -680.8 (0.3) -666.8 (0.6) -4531 (50.0) -5469 (682.2) PGA2 -21958 (5.5) -21959 (8.6) -655.6 (0.3) -639.2 (0.6) -4518 (49.7) -3985 (650.2) NAdam2 -22096 (5.6) -67414 (9.8) -662.3 (0.3) -652.2 (0.7) -4512 (50.2) - Snngm2 -21903 (5.6) -21898(9.8) -641.3 (0.3) -625.6(0.7) -4226(50.0) -3518(703.6) Nagm2 -21899(5.5) -21946 (9.8) -640.2 (0.3) -626.0 (0.7) -4302 (49.9) -3893
3268,unknown,"VOGN -21949 (5.6) -21932 (13.0) -640.0(0.4) -626.7 (1.3) -4361 (49.5) -3609 (899.5) on the first and second derivative of h(θ) respectively (likewise for other methods). The highest lower bound in each case is achieved by Snngm, Nagm, VOGN or PGA. Use of second order derivatives generally led to a higher lower bound, albeit with an increase in runtime due to more intensive computations. For the si"
3269,unknown,"in runtime due to more intensive computations. For the simulations ( d = 100), lower bounds across various algorithms are poor when ∇h(θ) is used for inferring full covari- ances, but there are clear improvements when ∇2 θh(θ) is employed (except for NAdam2). This pattern did not appear for the German data ( d = 49) but reappeared for the tox- icity data ( d = 1025). Hence, first order derivatives"
3270,unknown,"icity data ( d = 1025). Hence, first order derivatives seem to be sufficient for diagonal covariances even in high dimensions, and are thus useful in reducing computation time. However, they may be too noisy and provide inadequate information for inferring dense covariance matrices in high dimensions, while second derivatives are more stable and useful. Figure 7 shows that Snngm is often among the"
3271,unknown,"useful. Figure 7 shows that Snngm is often among the fastest method to converge except when first derivatives are used to infer full covariances. On the other hand, Adam and NAdam tend to be slower to converge and may be stuck at poor local modes or diverge. Nagm’s behavior is often similar to VOGN, although VOGN tends to converge faster. Although PGA relies on Euclidean gradients, it performs ver"
3272,unknown,"and improves upon Adam significantly. 8.3. Convolutional neural networks In this section, we train a convolutional neural network (CNN), LeNet-5, on two datasets for computer vision, MNIST and CIFAR-10. The sigmoid activation functions and average pooling layers in LeNet-5 are replaced by ReLUs and max-pooling layers. We use the training sets of MNIST and CIFAR-10 from the MLDatasets Julia package"
3273,unknown,"MNIST contains 60,000 28 × 28 pixel gray-scale images of handwritten digits (0–9), while CIFAR-10 contains 50,000 32 × 32 pixel color images in 10 classes. The total number of parameters is d = 61, 706 for MNIST and d = 83, 126 for CIFAR-10. Due to the high dimensions, only Gaussian variational approximations with diagonal covariances are considered. The CNNs are trained using the Julia Flux libra"
3274,unknown,"Natural gradient updates in Gaussian variational approximation 19 0 20 40 60 80 −60000 −30000 Simulation (1st, Diag) epoch lower bound 0 20 40 60 80 −60000 −30000 Simulation (2nd, Diag) epoch lower bound 0 20 40 60 80 −60000 −30000 Simulation (1st, Full) epoch lower bound 0 20 40 60 80 −60000 −30000 Simulation (2nd, Full) epoch lower bound 0 200 600 1000 −900 −750 −600 German (1st, Diag) epoch low"
3275,unknown,"epoch lower bound 0 200 600 1000 −900 −750 −600 German (2nd, Diag) epoch lower bound 0 200 600 1000 −900 −750 −600 German (1st, Full) epoch lower bound 0 200 600 1000 −900 −750 −600 German (2nd, Full) epoch lower bound 0 200 600 1000 −10000 −6000 Toxicity (1st, Diag) epoch lower bound 0 200 600 1000 −10000 −6000 Toxicity (2nd, Diag) epoch lower bound 0 200 600 1000 −10000 −6000 Toxicity (1st, Full"
3276,unknown,"epoch lower bound 0 200 600 1000 −10000 −6000 Toxicity (2nd, Full) epoch lower bound Adam PGA NAdam VOGN Snngm Nagm Fig. 7.Logistic regression: average lower bound at each epoch. Table 9. CNNs: lower bound estimate at termination and runtime in (). Adam PGA NAdam Snngm Nagm VOGN MNIST -50535 (603) -82010 (630) -50786 (626) -42936(636) -49113 (621) -49247 (590) CIFAR10 -140971 (822) -151733 (835) -"
3277,unknown,"20 Linda Tan 0 100 200 300 400 500 600 −6e+05 −4e+05 −2e+05 MNIST time (s) lower bound 0 200 400 600 800 −1e+06 −6e+05 −2e+05 CIFAR10 time (s) lower bound Adam PGA NAdam Nagm Snngm VOGN Fig. 8.CNNs: average lower bound at each epoch against time. gradients are computed using automatic differentiation through Zygote. Both datasets are loaded in minibatches of size 128 for Adam, PGA, NAdam, Snngm an"
3278,unknown,"are loaded in minibatches of size 128 for Adam, PGA, NAdam, Snngm and Nagm, which are trained for 300 epochs. As VOGN requires individual gradients to compute the gener- alized Gauss-Newton approximation instead of only their sum, the computation is more intensive. As an online approach, VOGN is also more efficient with smaller batchsize when automatic differentiation is used to compute gradients."
3279,unknown,"when automatic differentiation is used to compute gradients. After experimenting with sizes 8, 16 and 32, VOGN is run with minibatches of size 8 (7 epochs) for MNIST, and size 16 (13 epochs) for CIFAR-10. At these settings, VOGN is more efficient and its runtime is also close to the other methods. From Table 9, Snngm, Nagm and VOGN achieved higher lower bounds than Adam, PGA and NAdam for both dat"
3280,unknown,"PGA and NAdam for both datasets under comparable runtimes. In this context, natural gradients seem to be very helpful in achieving better variational approximations to the posterior. Figure 8 also reveals that the average lower bounds per epoch for Adam, PGA, NAdam undergo a lot of fluctuations and are quite unstable. A larger batchsize or small stepsize may help to rectify this issue. Snngm conve"
3281,unknown,"or small stepsize may help to rectify this issue. Snngm converges faster than VOGN and Nagm initially as the normalized gradients admit larger stepsizes, but these may also result in more fluctuations closer to convergence. 8.4. Deep net generalized linear models Consider deep net GLMs (deepGLMs, Tran et al., 2021), where each response yi follows a density in the exponential family for i = 1 , . ."
3282,unknown,"a link function g(·) to a linear predictor x⊤ i θ as in a GLM, the covariates xi ∈ Rp are transformed via a deep feed forward neural network withL layers, and g(E(yi)) is a linear transformation of units in theLth (output) layer. An ReLU activation function is applied to each layer after the affine transformation except the Lth layer. For the deepGLM, θ consists of all weights and biases in the ne"
3283,unknown,"consists of all weights and biases in the neural network. To perform variable selection, a normal-Gamma mixture shrinkage prior is placed on the weights wxj ∈ R m connecting input xj to the m hidden units in the first layer: wxj |τj ∼ N(0, τjIm), τ j|γj ∼ Gamma(m+1 2 , γ2 j /2), j = 1, . . . , p. Natural gradient updates in Gaussian variational approximation 21 Table 10. DeepGLMs: lower bound esti"
3284,unknown,"Adam PGA NAdam Snngm Nagm VOGN Binary -4545 (107) -3393 (110) -4887 (108) -4474 (109) -2784(109) -4836 (116) Real -191259 (302) -181452(300) -224623 (301) -188845 (302) -193727 (304) -215894 (328) Census -9806 (53) -10185 (53) -9849 (53) -10217 (53) -9766(53) -10040 (58) Protein -135074 (73) -134869 (73) -137283 (73) -136333 (73) -134483(72) -136811 (74) A shrinkage prior N(0 , Idw /γw) is assigne"
3285,unknown,"A shrinkage prior N(0 , Idw /γw) is assigned to the remaining weights w of dimension dw to prevent overfitting, while a flat prior is used for the biases. The set of all variables is Θ = ( θ, v, τ), where v denotes any dispersion parameters in the exponential family, τ = ( τ1, . . . , τp)⊤, and γ = ( γ1, . . . , γp, γw)⊤. A mean-field variational approximation q(Θ) = qλ(θ)q(v) Qp j=1 q(τj) is cons"
3286,unknown,"q(Θ) = qλ(θ)q(v) Qp j=1 q(τj) is considered, where qλ(θ) is assumed to be N( µ, Σ) with Σ being a diagonal matrix. The optimal densities of q(v) and q(τj), and their parameter updates, are derived in the supplement S8. Updates of the shrinkage parameters γ, that maximize the evidence lower bound, are also derived. Some data are simulated using the models in Section 6.1.2 and 6.1.3 of Tran et al. ("
3287,unknown,"(2020). Let n = 105 and p = 100. In the first simulation, the binary response is generated as y = 1 {5 − 2(x1 + 2x2)2 + 4x3x4 + 3x5 ≥ 0}, where xj ∼ U(−1, 1) for j = 1, . . . , p. In the second simulation, a real-valued response is generated as y = 5 + 10x1 + 10 x2 2 + 1 + 5x3x4 + 2x4 + 5x2 4 + 5x5 + 2x6 + 10 x2 7 + 1 + 5x8x9 + 5x2 9 + 5x10 + ϵ, where ϵ ∼ N(0, 1) and x = (x1, . . . , xp)⊤ is gener"
3288,unknown,"where ϵ ∼ N(0, 1) and x = (x1, . . . , xp)⊤ is generated from a normal density with mean 0 and covariance matrix [0 .5|i−j|]i,j. We further consider two real datasets from the UCI machine learning repository. The first is a real-valued response dataset of the physicochemical properties of protein tertiary structure, with n = 45, 730 and p = 9. The second is the census income dataset that predicts "
3289,unknown,"The second is the census income dataset that predicts whether income exceed $50,000 per year (binary response) based on census data, with n = 30 , 148 and p = 38 after preprocessing. In each case, a 3-layer neural network (2 hidden layers each with 20 units and an output layer with a single unit) is used to transform the inputs. DeepGLMs are also trained using the Flux package. For Adam, PGA, NAda"
3290,unknown,"Snngm and Nagm, minibatches of 5000, 4573 and 1508 were used for the simulated, protein and census data respectively. For the binary response datasets, 1000 epochs was sufficient while 2000 epochs was used for the real-valued datasets. As explained in Section 8.3, VOGN is more efficient with smaller batchsize. After some tuning, we used batchsize of 50 for the simulated and census data, and 85 for"
3291,unknown,"batchsize of 50 for the simulated and census data, and 85 for the protein data. The number of epochs for VOGN was adjusted so that runtimes are comparable to other algorithms. Table 10 shows that the highest lower bounds were achieved by PGA and Nagm. Figure 9 reveals that the convergence rates of different algorithms vary across datasets. PGA converges rapidly in each case, and Nagm is often slow"
3292,unknown,"PGA converges rapidly in each case, and Nagm is often slower to converge initially but able to achieve a high lower bound eventually. VOGN, which requires the computation of individual gradients, is more time-consuming than other algorithms that rely only on first order derivatives. However, the second order derivative information incorporated by VOGN offers greater stability, and is very useful w"
3293,unknown,22 Linda Tan 0 20 60 100 −60000 −20000 0 Simulation (binary) time (s) lower bound 0 50 150 250 −6e+05 −4e+05 −2e+05 Simulation (real) time (s) lower bound 0 10 30 50 −25000 −15000 Census time (s) lower bound 0 20 40 60 −155000 −145000 −135000 Protein time (s) lower bound Adam PGA NAdam VOGN Snngm Nagm 0 20 40 60 80 100 0 2000 4000 Simulation (binary) j γj 0 20 40 60 80 100 0 400 800 1200 Simulatio
3294,unknown,Simulation (real) j γj 0 10 20 30 0 200 400 600 800 Census j γj 2 4 6 8 5 10 15 20 Protein j γj Fig. 9. DeepGLMs: average lower bound at each epoch against time (first row) and values of shrinkage parameters γj for input variable xj. very small minibatches or in an online fashion. The second row of Figure 9 shows the values of shrinkage parameters{γj}. The binary and real-valued simulated data onl
3295,unknown,"and real-valued simulated data only depend on the first five and ten inputs respectively, and this information has been captured by {γj}, which are small for the relevant inputs and large for the irrelevant inputs (thus shrinking them to zero). Different algorithms react slightly differently to {γj} although some common trends persist. 8.5. Generalized linear mixed models Let yi = (yi1, . . . , yi"
3296,unknown,"Let yi = (yi1, . . . , yini)⊤ denote the ith observation for i = 1, . . . , n. In GLMMs, each yij follows a distribution in the exponential family and g(E(yij)) = ηij for some link function g(·), where ηij = X⊤ ij β + Z⊤ ij θi is the linear predictor. Here Xij and Zij are covariates of length p and r respectively, β is the fixed effects and θi ∼ N(0, B−1) is the random effects. To transform all va"
3297,unknown,"random effects. To transform all variables onto R, consider the Cholesky decomposition B = W W⊤ where W is lower triangular with positive diagonal entries, and define W∗ such that W∗ ii = log(Wii) and W∗ ij = Wij if i ̸= j. Then the joint distribution of the GLMM is of the form in (3), where θg = [β⊤, ω⊤]⊤ and ω = vech(W∗). We assume the prior, θg ∼ N(0, σ2 0Ip), where σ0 = 10. We consider two var"
3298,unknown,"We consider two variational approximations. The first is GVA (Tan and Nott, 2018), where posterior conditional independence structure is captured via a sparse precision matrix, with Cholesky factor T of the form in (4). Thus GVA can be implemented using Algorithm 2S. The second is reparametrized variational Bayes (RVB, Tan, 2021), where posterior dependence between local and global variables is fi"
3299,unknown,"posterior dependence between local and global variables is first minimized by applying an invertible affine transformation on the local variables. Among the two transformations discussed in Tan (2021), RVB1 is more suited to Poisson and binomial models while Natural gradient updates in Gaussian variational approximation 23 Table 11. GLMMs: lower bound estimate at termination and runtime in (). Ada"
3300,unknown,Adam PGA NAdam Snngm VOGN Epilepsy GVA 3134.3 (9.7) 3133.2 (9.7) 3135.5 (16.6) 3138.7 (16.1) 3137.0 (32.7) RVB1 3139.6 (2.3) diverge 3139.6 (2.4) 3139.6 (2.4) 3139.0 (8.1) RVB2 3139.7 (5.8) diverge 3139.7 (5.9) 3139.7 (5.9) 3139.2 (11.1) Toenail GVA -645.2 (52.2) -644.8 (48.9) -644.8 (73.5) -644.8 (68.9) -647.3 (60.4) RVB1 -645.5 (7.1) -836.0 (7.0) -645.4 (7.5) -645.3 (7.5) -647.2 (12.4) RVB2 -644
3301,unknown,RVB2 -644.3 (24.6) -9822.5 (23.5) -644.3 (25.2) -644.3 (25.1) -646.4 (27.2) Hers GVA -5771.3 (100.7) -5044.6 (109.8) -5771.3 (169.7) -5042.7 (163.3) -5615.1 (121.7) RVB1 -5040.5 (22.0) -6959.1 (22.1) -5040.6 (23.6) -5040.7 (23.1) -5042.9 (44.0) RVB2 -5040.1 (78.4) -34313.1 (84.4) -5040.2 (79.8) -5040.4 (79.6) -5068.1 (93.1) Diabetes GVA -22835 (2872) -22691 (2834) -22835 (5461) -22712 (5325) -2270
3302,unknown,"RVB1 -29204 (415) -42008 (414) -39959 (442) -22599 (435) -22603 (5715) RVB2 -25431 (1048) diverge diverge -22599 (1062) -22603 (5034) RVB2 works better for Bernoulli models. Let ˜θ = ( ˜θ⊤ 1 , . . . ,˜θ⊤ n , θ⊤ g )⊤, where ˜θ1, . . . ,˜θn are the transformed local variables. Variational Bayes is then applied by assuming q(˜θ) = q(θg) Qn i=1 q(˜θi), and that all densities are Gaussian. Thus q(˜θ) ="
3303,unknown,"q(˜θ) = q(θg) Qn i=1 q(˜θi), and that all densities are Gaussian. Thus q(˜θ) = N(µ, Σ) where Σ is a block diagonal matrix with n + 1 blocks. If CC⊤ is a Cholesky decomposition of Σ, then RVB1 and RVB2 can be implemented using Algorithm 1S. In RVB, the local variables are transformed to be approximately Gaussian with mean 0 and variance 1. Hence, diagonal elements corresponding to local variables i"
3304,unknown,"Hence, diagonal elements corresponding to local variables in C are set at 1. We consider four benchmark datasets. The first is the Epilepsy data (Thall and Vail, 1990), where n = 59 epileptics are randomly assigned a new drug Progabide or a placebo, and yij is the number of seizures of patient i in the two weeks before clinic visit j for j = 1, . . . ,4. We fit the Poisson random slope model, log "
3305,unknown,"log µij = β1 + β2Basei + β3Trti + β4Basei × Trti + β5Agei + β6Visitij + bi1 + bi2Visitij, where the covariates for patient i are Basei (log(number of baseline seizures/4)), Trti (1 for drug and 0 for placebo), Age i (log(age of patient at baseline) centered at zero) and Visitij (coded as −0.3, −0.1, 0.1, 0.3 for j = 1, . . . ,4). The second is the Toenail data (De Backer et al., 1998), where the b"
3306,unknown,"The second is the Toenail data (De Backer et al., 1998), where the binary response yij of patient i at the jth visit is 1 if degree of separation of nail plate from nail bed is moderate or severe and 0 if none or mild. Consider the random intercept model, logit(pij) = β1 + β2Trti + β3tij + β4Trti × tij + θi, i = 1, . . . ,294, 1 ≤ j ≤ 7, where for the ith patient, Trt i = 1 if 250mg of terbinafine"
3307,unknown,"where for the ith patient, Trt i = 1 if 250mg of terbinafine is taken each day and 0 if 200mg of itraconazole is taken, and tij is the time in months when the patient is evaluated at the jth visit. The third dataset comes from the Heart and Estrogen/Progestin Study (HERS, Hul- ley et al., 1998). We examine 2031 women whose data for all covariates are available. The binary response yij of patient i"
3308,unknown,"The binary response yij of patient i at the jth visit indicates whether the systolic blood pressure is above 140. Consider the random intercept model, logit(pij) = β1 + β2agei + β3BMIij + β4HTNij + β5visitij + θi, 0 ≤ j ≤ 5, 24 Linda Tan where for patient i, agei is the age at baseline, BMI ij is the body mass index at the jth visit, HTNij indicates whether high blood pressure medication is taken "
3309,unknown,"visit, HTNij indicates whether high blood pressure medication is taken at the jth visit and visitij is coded as −1, −0.6, −0.2, 0.2, 0.6, 1 for j = 0, 1, . . . ,5 respectively. We normalize BMI and age to have mean 0 and standard deviation 1. Finally, we consider the diabetes dataset from the UCI machine learning repository, which contains 101766 hospital records of patients diagnosed with diabete"
3310,unknown,"hospitals from 1999–2008. We preprocess the data following Strack et al. (2014), and study n = 16, 341 patients each with at least 2 hospital records. The binary response yij of patient i at the jth record indicates if the patient is readmitted in less than 30 days. We fit a random intercept model that includes as main term the admission source, and the interactions between discharge disposition a"
3311,unknown,"the interactions between discharge disposition and time in hospital, discharge disposition and race, time in hospital and diagnosis, and HbA1c result and diagnosis. For each model and approach (GVA, RVB1 and RVB2), we implemented the algo- rithms, Adam, PGA, NAdam, Snngm and VOGN for the same number of iterations, and an estimate of the lower bound was computed by averaging over every 1000 itera- "
3312,unknown,"tions. Figure 10 plots the average lower bounds against time, while Table 11 shows the lower bound attained and runtime at termination. These results are based on first order gradient estimates as ∇2 θh(θ) is highly complex for GVA and RVB and it is unlikely that second order gradient estimates will be more efficient than first order ones. For GVA, natural gradient based algorithms (NAdam, Snngm a"
3313,unknown,time-consuming than Euclidean gradient based algorithms (Adam and PGA) for the 0 5 10 20 30 2200 2600 3000 Epilepsy (GVA) time (s) lower bound Adam PGA NAdam Snngm VOGN 0 20 40 60 −1600 −1200 −800 Toenail (GVA) time (s) lower bound 0 50 100 150 −12000 −8000 Hers (GVA) time (s) lower bound 0 2000 4000 −80000 −40000 Diabetes (GVA) time (s) lower bound 0 2 4 6 8 3000 3060 3120 Epilepsy (RVB1) time (s
3314,unknown,time (s) lower bound 0 2 4 6 8 10 −1600 −1000 Toenail (RVB1) time (s) lower bound 10 20 30 40 −11000 −8000 −5000 Hers (RVB1) time (s) lower bound 50 200 1000 5000 −80000 −40000 Diabetes (RVB1) time (s) lower bound 2 4 6 8 10 3020 3080 3140 Epilepsy (RVB2) time (s) lower bound 5 10 15 20 25 −1400 −1000 Toenail (RVB2) time (s) lower bound 20 40 60 80 −11000 −8000 −5000 Hers (RVB2) time (s) lower bou
3315,unknown,"Hers (RVB2) time (s) lower bound 100 500 2000 −90000 −50000 Diabetes (RVB2) time (s) lower bound Fig. 10.GLMMs: Average lower bound attained against time. Natural gradient updates in Gaussian variational approximation 25 same number of iterations, as more matrix inversions are required to compute natural gradients. However, there are still advantages to using natural gradients especially for small"
3316,unknown,"small to moderate datasets, where the cost of matrix inversions is cheaper. From Figure 10, Snngm converges much faster than Adam for the Epilepsy, Toenail and Hers datasets and achieves a higher lower bound. For Toenail and Hers, Adam and NAdam were stuck at a poor local mode much longer than PGA, VOGN and Snngm. PGA converges rapidly for GVA across all datasets, although it does not perform well"
3317,unknown,"after the learning rate is reduced. For RVB1 and RVB2, Snngm performs very well both in terms of convergence rate and posterior approximation accuracy, yielding the highest (or second highest) lower bound for each dataset. RVB considers the Cholesky factor of the covariance matrix instead of the precision matrix, whose natural gradients are more efficient to compute as they do not require any matr"
3318,unknown,"do not require any matrix inversions. On the other hand, VOGN still requires matrix inversions as it updates the precision matrix, and is more computationally intensive. 9. Conclusion Gaussian variational approximation is widely used and natural gradients provide a means of improving the convergence in stochastic gradient ascent, which is particularly impor- tant when suboptimal local modes are pr"
3319,unknown,"tant when suboptimal local modes are present. However, the natural gradient update of the precision matrix does not ensure positive definiteness and may require expensive matrix inversion or Cholesky decomposition. To tackle this issue, we consider Cholesky decomposition of the covariance or precision matrix. We show that the inverse Fisher in- formation can be found analytically and present natur"
3320,unknown,"formation can be found analytically and present natural gradient updates of the Cholesky factors in closed form. We also derive unbiased gradient estimates in terms of the first or second derivative of the log posterior when the gradient of the lower bound is not available analytically. First order gradient estimates are more efficient in most cases, but second order gradient estimates are more st"
3321,unknown,"second order gradient estimates are more stable and can lead to more accurate variational approximations when high-dimensional dense covariance matrices are considered. For high-dimensional models, we impose sparsity constraints on the covariance or precision matrix to incorporate assumptions in variational Bayes or conditional indepen- dence structure in the posterior, and show that efficient nat"
3322,unknown,"dence structure in the posterior, and show that efficient natural gradient updates can also be derived in these cases. Finally, we observe that Adam does not always perform well with natural gradients and we propose a basic approach that implements natural gradient with momentum using a constant stepsize (Nagm), and stochastic normalized natural gradient ascent with momentum (Snngm) as alternative"
3323,unknown,"Snngm is proven for L-Lipschitz smooth functions with bounded gradients. We compare these alternatives with some popular approaches in machine learning such as VOGN and ProxGenAdam for a range of statistical and deep learning models. While the performance of Nagm and Snngm have some variation across models and datasets, they are often able to converge faster or achieve a higher lower bound than Ad"
3324,unknown,"Adam, and are competitive with VOGN and PGA. However, the application of Nagm, Snngm and VOGN requires some tuning of the learning rate unlike Adam and PGA, whose default settings work quite well generally. Hence, it would be useful to develop an adaptive global learning rate for these methods in future work. 26 Linda Tan References Agresti, A. (2018). An introduction to categorical data analysis "
3325,unknown,"Sons, Inc. Amari, S. (1998). Natural gradient works efficiently in learning. Neural Computation 10 , 251–276. Amari, S. (2016). Information Geometry and Its Applications . Springer. Attias, H. (1999). Inferring parameters and structure of latent variable models by variational Bayes. In K. Laskey and H. Prade (Eds.), Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence, San "
3326,unknown,"Intelligence, San Francisco, CA, pp. 21–30. Morgan Kaufmann. Balles, L. and P. Hennig (2018). Dissecting adam: The sign, magnitude and variance of stochastic gradients. In J. Dy and A. Krause (Eds.),Proceedings of the 35th International Conference on Machine Learning, Volume 80, pp. 404–413. PMLR. Bezanson, J., A. Edelman, S. Karpinski, and V. B. Shah (2017). Julia: A fresh approach to numerical c"
3327,unknown,"computing. SIAM Review 59 , 65–98. Bonnet, G. (1964). Transformations des signaux al´ eatoires a travers les syst` emes non lin´ eaires sans m´ emoire.Annales des T´ el´ ecommunications 19, 203–220. Bottou, L., F. E. Curtis, and J. Nocedal (2018). Optimization methods for large-scale machine learning. SIAM Review 60 , 223–311. Cutkosky, A. and H. Mehta (2020). Momentum improves normalized SGD. In "
3328,unknown,"Cutkosky, A. and H. Mehta (2020). Momentum improves normalized SGD. In H. D. III and A. Singh (Eds.), Proceedings of the 37th International Conference on Machine Learning, Volume 119, pp. 2260– 2268. PMLR. De Backer, M., C. De Vroey, E. Lesaffre, I. Scheys, and P. D. Keyser (1998). Twelve weeks of continuous oral therapy for toenail onychomycosis caused by dermatophytes: A double-blind comparative"
3329,unknown,"of terbinafine 250 mg/day versus itraconazole 200 mg/day. Journal of the American Academy of Dermatology 38, 57–63. D´ efossez, A., L. Bottou, F. Bach, and N. Usunier (2020). A simple convergence proof of Adam and Adagrad. arXiv:2003.02395. Domke, J. (2020). Provable smoothness guarantees for black-box variational inference. In H. D. III and A. Singh (Eds.), Proceedings of the 37th International C"
3330,unknown,"A. Singh (Eds.), Proceedings of the 37th International Conference on Machine Learning , Volume 119 of Proceedings of Machine Learning Research, pp. 2587–2596. PMLR. Duchi, J., E. Hazan, and Y. Singer (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research 12 , 2121–2159. Graves, A. (2011). Practical variational inference for neural"
3331,unknown,"processing systems 24, 2348––2356. Han, S., X. Liao, D. Dunson, and L. Carin (2016). Variational Gaussian copula inference. In A. Gretton and C. C. Robert (Eds.), Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, Volume 51, pp. 829–838. PMLR. Hazan, E., K. Levy, and S. Shalev-Shwartz (2015). Beyond convexity: Stochastic quasi-convex optimiza- tion. In C. C"
3332,unknown,"tion. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (Eds.), Proceedings of the 29th Annual Conference on Neural Information Processing Systems, Volume 1, pp. 1594–1602. Curran Associates, Inc. Hoffman, M. D., D. M. Blei, C. Wang, and J. Paisley (2013). Stochastic variational inference. Journal of Machine Learning Research 14 , 1303–1347. Hulley, S., D. Grady, T. Bush, and et al. ("
3333,unknown,"prevention of coronary heart disease in postmenopausal women. JAMA 280, 605–613. Khan, M. and W. Lin (2017). Conjugate-computation variational inference : Converting variational inference in non-conjugate models to inferences in conjugate models. In A. Singh and J. Zhu (Eds.), Proceedings of the 20th International Conference on Artificial Intelligence and Statistics , Volume 54, pp. 878–887. PMLR."
3334,unknown,"Khan, M., D. Nielsen, V. Tangkaratt, W. Lin, Y. Gal, and A. Srivastava (2018). Fast and scalable Bayesian deep learning by weight-perturbation in Adam. In J. Dy and A. Krause (Eds.), Proceedings of the 35th International Conference on Machine Learning , Volume 80, pp. 2611–2620. PMLR. Khan, M. E. E., P. Baque, F. Fleuret, and P. Fua (2015). Kullback-leibler proximal variational infer- ence. In C. "
3335,unknown,"ence. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (Eds.), Advances in Neural Information Processing Systems, Volume 28. Curran Associates, Inc. Kim, K., K. Wu, J. Oh, Y. Ma, and J. R. Gardner (2023). Black-box variational inference converges. Kingma, D. P. and J. Ba (2015). Adam: A method for stochastic optimization. In Y. Bengio and Y. LeCun (Eds.), Proceedings of the 3rd Inter"
3336,unknown,"Y. LeCun (Eds.), Proceedings of the 3rd International Conference on Learning Representations . Kingma, D. P. and M. Welling (2014). Auto-encoding variational Bayes. In Y. Bengio and Y. LeCun Natural gradient updates in Gaussian variational approximation 27 (Eds.), Proceedings of the 2nd International Conference on Learning Representations . Kucukelbir, A., D. Tran, R. Ranganath, A. Gelman, and D. "
3337,unknown,"Kucukelbir, A., D. Tran, R. Ranganath, A. Gelman, and D. M. Blei (2017). Automatic differentiation variational inference. Journal of Machine Learning Research 18 , 1–45. Leoni, G. (2017). A First Course in Sobolev Spaces (Second ed.). Providence, Rhode Island: American Mathematical Society. Lin, W., M. E. Khan, and M. Schmidt (2019a). Fast and simple natural-gradient variational inference with mix"
3338,unknown,"with mixture of exponential-family approximations. In K. Chaudhuri and R. Salakhutdinov (Eds.), Proceedings of the 36th International Conference on Machine Learning , Volume 97, pp. 3992–4002. PMLR. Lin, W., M. E. Khan, and M. Schmidt (2019b). Stein’s lemma for the reparameterization trick with exponential family mixtures. https://github.com/yorkerlin/ vb-mixef/blob/master/report.pdf. Lin, W., F. "
3339,unknown,"Lin, W., F. Nielsen, K. M. Emtiyaz, and M. Schmidt (2021). Tractable structured natural-gradient descent using local parameterizations. In M. Meila and T. Zhang (Eds.), Proceedings of the 38th International Conference on Machine Learning , Volume 139 of Proceedings of Machine Learning Re- search, pp. 6680–6691. PMLR. Lin, W., M. Schmidt, and M. E. Khan (2020). Handling the positive-definite constr"
3340,unknown,"learning rule. In H. D. III and A. Singh (Eds.), Proceedings of the 37th International Conference on Machine Learning, Volume 119, pp. 6116–6126. PMLR. Magnus, J. R. and H. Neudecker (1980). The elimination matrix: Some lemmas and applications. SIAM Journal on Algebraic Discrete Methods 1 , 422–449. Magnus, J. R. and H. Neudecker (2019). Matrix Differential Calculus with Applications in Statistics"
3341,unknown,"Econometrics (Third ed.). John Wiley & Sons. Martens, J. (2020). New insights and perspectives on the natural gradient method. Journal of Machine Learning Research 21, 1–76. Martens, J. and R. Grosse (2015). Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning , pp. 2408–2417. PMLR. Nguyen, H., M. C. Aus´ ın, and P. Galeano (2020"
3342,unknown,"factor copulas. Computational Statistics and Data Analysis 151 , 107012. Ong, V. M. H., D. J. Nott, M.-N. Tran, S. A. Sisson, and C. C. Drovandi (2018). Variational Bayes with synthetic likelihood. Statistics and Computing 28 , 971–988. Opper, M. and C. Archambeau (2009). The variational Gaussian approximation revisited. Neural computation 21, 786–792. Osawa, K., S. Swaroop, M. E. Khan, A. Jain, R"
3343,unknown,"tical deep learning with bayesian principles. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´ e- Buc, E. Fox, and R. Garnett (Eds.), Advances in Neural Information Processing Systems, Volume 32. Curran Associates, Inc. Paisley, J., D. M. Blei, and M. I. Jordan (2012). Variational Bayesian inference with stochastic search. In Proceedings of the 29th International Coference on International"
3344,unknown,"pp. 1363–1370. Omnipress. Price, R. (1958). A useful theorem for nonlinear devices having Gaussian inputs. IRE Transactions on Information Theory 4 , 69–72. Ranganath, R., S. Gerrish, and D. Blei (2014). Black box variational inference. InProceedings of the 17th International Conference on Artificial Intelligence and Statistics , Volume 33, pp. 814–822. PMLR. Ranganath, R., D. Tran, and D. Blei (2"
3345,unknown,"on Machine Learning, pp. 324–333. PMLR. Rattray, M., D. Saad, and S. Amari (1998). Natural gradient descent for on-line learning. Physical Review Letters 81 , 5461–5464. Robbins, H. and S. Monro (1951). A stochastic approximation method. The Annals of Mathematical Statistics 22 , 400–407. Ruiz, F. J. R., M. K. Titsias, and D. M. Blei (2016). Overdispersed black-box variational inference. In A. Ihl"
3346,unknown,"In A. Ihler and D. Janzing (Eds.), Proceedings of the 32nd Conference on Uncertainty in Artificial Intelligence, pp. 647–656. AUAI Press. Salimans, T. and D. A. Knowles (2013). Fixed-form variational posterior approximation through stochas- tic linear regression. Bayesian Analysis 8 , 837–882. Salimbeni, H., S. Eleftheriadis, and J. Hensman (2018). Natural gradients in practice: Non-conjugate vari"
3347,unknown,"of the 21st International Conference on Artificial Intelligence and Statistics , Volume 84, pp. 689–697. PMLR. Sato, M.-A. (2001). Online model selection based on the variational Bayes. Neural computation 13 , 28 Linda Tan 1649–1681. Smith, M. S., R. Loaiza-Maya, and D. J. Nott (2020). High-dimensional copula variational approximation through transformation. Journal of Computational and Graphical "
3348,unknown,"through transformation. Journal of Computational and Graphical Statistics 29 , 729–743. Spall, J. C. (2003). Introduction to stochastic search and optimization: estimation, simulation and control. New Jersey: Wiley. Stein, C. M. (1981). Estimation of the Mean of a Multivariate Normal Distribution. The Annals of Statistics 9 (6), 1135 – 1151. Strack, B., J. P. DeShazo, C. Gennings, J. L. Olmo, S. V"
3349,unknown,"Strack, B., J. P. DeShazo, C. Gennings, J. L. Olmo, S. Ventura, K. J. Cios, and J. N. Clore (2014). Impact of hba1c measurement on hospital readmission rates: Analysis of 70,000 clinical database patient records. BioMed Research International 781670 . Tan, L. S. L. (2021). Use of model reparametrization to improve variational Bayes. Journal of the Royal Statistical Society: Series B (Statistical M"
3350,unknown,"Statistical Society: Series B (Statistical Methodology) 83 , 30–57. Tan, L. S. L. and N. Friel (2020). Bayesian variational inference for exponential random graph models. Journal of Computational and Graphical Statistics 29 , 910–928. Tan, L. S. L. and D. J. Nott (2013). Variational inference for generalized linear mixed models using partially non-centered parametrizations. Statistical Science 28 "
3351,unknown,"partially non-centered parametrizations. Statistical Science 28 , 168–188. Tan, L. S. L. and D. J. Nott (2018). Gaussian variational approximation with sparse precision matrices. Statistics and Computing 28 , 259–275. Tang, D. and R. Ranganath (2019). The variational predictive natural gradient. In K. Chaudhuri and R. Salakhutdinov (Eds.), Proceedings of the 36th International Conference on Machin"
3352,unknown,"Volume 97 of Proceedings of Machine Learning Research, pp. 6145–6154. PMLR. Thall, P. F. and S. C. Vail (1990). Some covariance models for longitudinal count data with overdisper- sion. Biometrics 46 , 657–671. Titsias, M. and M. L´ azaro-Gredilla (2014). Doubly stochastic variational Bayes for non-conjugate infer- ence. In E. P. Xing and T. Jebara (Eds.), Proceedings of the 31st International Con"
3353,unknown,"Learning, Volume 32, pp. 1971–1979. PMLR. Tran, M.-N., D. H. Nguyen, and D. Nguyen (2021). Variational Bayes on manifolds. Statistics and Computing 31 , 71. Tran, M.-N., N. Nguyen, D. Nott, and R. Kohn (2020). Bayesian deep net GLM and GLMM. Journal of Computational and Graphical Statistics 29 , 97–113. Wainwright, M. J. and M. I. Jordan (2008). Graphical models, exponential families, and variatio"
3354,unknown,"inference. Foundations and Trends in Machine Learning 1 , 1–305. Xu, M., M. Quiroz, R. Kohn, and S. A. Sisson (2019). Variance reduction properties of the reparameter- ization trick. In K. Chaudhuri and M. Sugiyama (Eds.), Proceedings of Machine Learning Research, Volume 89 of Proceedings of Machine Learning Research, pp. 2711–2720. PMLR. Yun, J., A. Lozano, and E. Yang (2021). Adaptive proximal g"
3355,unknown,"Yun, J., A. Lozano, and E. Yang (2021). Adaptive proximal gradient methods for structured neural networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems. Zeiler, M. D. (2012). Adadelta: An adaptive learning rate method. arXiv: 1212.5701. Zhang, G., S. Sun, D. Duvenaud, and R. Grosse (2018). Noisy natural gradient as variational"
3356,unknown,"In International Conference on Machine Learning , pp. 5852–5861. PMLR. Natural gradient updates in Gaussian variational approximation 29 Supplementary material S1. Natural gradient updates in terms of mean and covariance/precision matrix First we derive the natural gradient of L with respect to the natural parameter λ using e∇λL = ∇mL. For the Gaussian, m = E[ s(θ)] = ( m⊤ 1 , m⊤ 2 )⊤, where m1 = "
3357,unknown,"1 , m⊤ 2 )⊤, where m1 = µ, m2 = vech(Σ +µµ⊤). We introduce ζ = (ζ⊤ 1 , ζ⊤ 2 )⊤, where ζ1 = µ = m1, ζ 2 = vech(Σ) = m2 − vech(m1m⊤ 1 ). Then ∇mζ =  Id −2(Id ⊗ µ⊤)D+⊤ 0d(d+1)/2×d Id(d+1)/2  , where D+ = (D⊤D)−1D⊤ is the Moore-Penrose inverse of D, and D+vec(A) = vech(A) if A is symmetric. Applying chain rule, the natural gradient is e∇λL = ∇mL = ∇mζ∇ζL =  Id −2(Id ⊗ µ⊤)D+⊤ 0d(d+1)/2×d Id(d+1)/2 "
3358,unknown," ∇µL ∇vech(Σ)L  =  ∇µL −2(Id ⊗ µ⊤)vec(∇ΣL) D⊤vec(∇ΣL)  = ∇µL −2(∇ΣL)µ D⊤vec(∇ΣL)  . Note that ∇vech(Σ)L = D⊤∇vec(Σ)L = D⊤vec(∇ΣL). The update for the natural pa- rameter is thus given by λ(t+1) = λ(t) + ρt e∇λL(t), =⇒  Σ−1(t+1)µ(t+1) −1 2D⊤vec(Σ−1(t+1))  =  Σ−1(t)µ(t) −1 2D⊤vec(Σ−1(t))  + ρt  ∇µL(t) − 2(∇ΣL(t))µ(t) D⊤vec(∇ΣL(t))  . The second line simplifies to Σ−1(t+1) = Σ−1(t) − 2ρt"
3359,unknown,"For the first line, Σ−1(t+1)µ(t+1) = Σ−1(t)µ(t) + ρt∇µL(t) − 2ρt(∇ΣL(t))µ(t) = {Σ−1(t) − 2ρt∇ΣL(t)}µ(t) + ρt∇µL(t) = Σ−1(t+1)µ(t) + ρt∇µL(t) =⇒ µ(t+1) = µ(t) + ρtΣ(t+1)∇µL(t). As the update for µ(t+1) depends on Σ (t+1), we need to update Σ first before updating µ. 30 Linda Tan Next, consider the parametrization, κ = (µ⊤, vech(Σ)⊤)⊤. The Fisher information matrix and its inverse are respectively, "
3360,unknown,"Fκ = Σ−1 0 0 1 2D⊤(Σ−1 ⊗ Σ−1)D  , F −1 κ = Σ 0 0 2 D+(Σ ⊗ Σ)D+⊤  . Hence the natural gradient is e∇κL = Σ 0 0 2 D+(Σ ⊗ Σ)D+⊤  ∇µL ∇vech(Σ)L  =  Σ∇µL 2vech(Σ∇ΣLΣ)  . Alternatively, e∇κL = (∇λκ)⊤ e∇λL, which is equal to "" Σ 2Σ( µ⊤Σ−1 ⊗ Σ−1)DD+(Σ ⊗ Σ)D+⊤ 0 2 D+(Σ ⊗ Σ)D+⊤ #∇µL −2(∇ΣL)µ D⊤vec(∇ΣL)  =  Σ∇µL 2vech(Σ∇ΣLΣ)  . For the parametrization ξ = (µ⊤, vech(Σ−1)⊤)⊤, the Fisher informati"
3361,unknown,"verse are respectively, Fξ = Σ−1 0 0 1 2D⊤(Σ ⊗ Σ)D  , F −1 ξ = Σ 0 0 2 D+(Σ−1 ⊗ Σ−1)D+⊤  . Note that ∇vech(Σ−1)L = −D⊤(Σ ⊗ Σ)∇vec(Σ)L. Hence the natural gradient is e∇ξL = Σ 0 0 2 D+(Σ−1 ⊗ Σ−1)D+⊤  ∇µL −D⊤(Σ ⊗ Σ)∇vec(Σ)L  =  Σ∇µL −2vech(∇ΣL)  , Alternatively, e∇ξL = (∇λξ)⊤ e∇λL, which is equal to =  Σ 2Σ( µ⊤ ⊗ I)D+⊤ 0 −2(D⊤D)−1 ∇µL −2(∇ΣL)µ D⊤vec(∇ΣL)  =  Σ∇µL −2vech(∇ΣL)  . S2. Lo"
3362,unknown," . S2. Loglinear model Let y = (y1, . . . , yn)⊤ and X = (x1, . . . , xn)⊤. We have δi = exp(x⊤ i θ) for i = 1, . . . , n. The lower bound can be evaluated analytically and it is given by L(λ) = Eq{log p(y, θ) − log qλ(θ)} = Eq  y⊤Xθ − nX i=1 {exp(x⊤ i θ) + log(yi!)} −d 2 log(2π) − d 2 log(σ2 0) − θ⊤θ 2σ2 0  − Eq  − d 2 log(2π) − 1 2 log |Σ| −1 2(θ − µ)⊤Σ−1(θ − µ)  = y⊤Xµ − nX i=1 {wi + log(y"
3363,unknown," = y⊤Xµ − nX i=1 {wi + log(yi!)} −µ⊤µ + tr(Σ) 2σ2 0 + 1 2 log |Σ| + d 2{1 − log(σ2 0)}, Natural gradient updates in Gaussian variational approximation 31 where wi = exp(x⊤ i µ + 1 2x⊤ i Σxi). Let w = (w1, . . . , wn)⊤ and W = diag(w). Then the Euclidean gradients are given by ∇µL = X⊤(y − w) − µ/σ2 0, ∇vec(Σ)L = 1 2vec(Σ−1 − I/σ2 0 − X⊤W X), ∇vech(C)L = 2vech(∇ΣL C) = vech(C−⊤ − C/σ2 0 − X⊤W XC),"
3364,unknown,"0 − X⊤W XC), ∇vech(T)L = −2vech(Σ∇ΣLT−⊤) = vech({ΣX⊤W X+ Σ/σ2 0 − I}T−⊤). S3. Natural gradient updates in terms of mean and Cholesky factor First we present the proof of Lemma 1. As this proof requires several results from Magnus and Neudecker (1980) concerning the elimination matrix L, these are collected here in Lemma S1 for ease of reference. Lemma S1. If P and Q are lower triangular d×d matric"
3365,unknown,"(i) LL⊤ = Id(d+1)/2, (ii) (LNL⊤)−1 = 2Id(d+1)/2 − LKL⊤, (iii) N = DLN, (iv) L⊤L(P⊤ ⊗Q)L⊤ = (P⊤ ⊗Q)L⊤ and its transpose, L(P ⊗Q⊤)L⊤L = L(P ⊗Q⊤), (v) L(P⊤ ⊗Q)L⊤ = D⊤(P⊤ ⊗Q)L⊤ and its transpose, L(P ⊗Q⊤)L⊤ = L(P ⊗Q⊤)D. Proof (Lemma S1).The proofs can be found in Lemma 3.2 (ii), Lemma 3.4 (ii), Lemma 3.5 (ii) and Lemma 4.2 (i) and (iii) of Magnus and Neudecker (1980) respec- tively. S3.1. Proof of Lem"
3366,unknown,"tively. S3.1. Proof of Lemma 1 First, we prove (i): I(C) = L{K(C−⊤ ⊗ C−1) + Id ⊗ C−⊤C−1}L⊤ = L{K(C−⊤ ⊗ Id)(Id ⊗ C−1) + (Id ⊗ C−⊤)(Id ⊗ C−1)}L⊤ = L{(Id ⊗ C−⊤)K + (Id ⊗ C−⊤)}(Id ⊗ C−1)L⊤ = L(Id ⊗ C−⊤)(K + Id2 )(Id ⊗ C−1)L⊤ = 2L(Id ⊗ C−⊤)N(Id ⊗ C−1)L⊤. 32 Linda Tan Next we prove (ii) and (iii) by using the results in Lemma S1. The roman letters in square brackets on the right indicate which parts of "
3367,unknown,"{2L(Id ⊗ C−⊤)N(Id ⊗ C−1)L⊤} n 1 2L(Id ⊗ C)L⊤(LNL⊤)−1L(Id ⊗ C⊤)L⊤ o = L(Id ⊗ C−⊤)(DLN)(Id ⊗ C−1)(Id ⊗ C)L⊤(LNL⊤)−1L(Id ⊗ C⊤)L⊤ [(iii) & (iv)] = L(Id ⊗ C−⊤)L⊤(LNL⊤)(LNL⊤)−1L(Id ⊗ C⊤)L⊤ [(v)] = L(Id ⊗ C−⊤)L⊤L(Id ⊗ C⊤)L⊤ = L(Id ⊗ C−⊤)(Id ⊗ C⊤)L⊤ [(iv)] = LL⊤ = Id(d+1)/2. [(i)] For (iii), I(C)−1vech(G) = 1 2L(Id ⊗ C)L⊤(LNL⊤)−1L(Id ⊗ C⊤)L⊤vech( ¯G) = 1 2L(Id ⊗ C)L⊤(2Id(d+1)/2 − LKL⊤)L(Id ⊗ C⊤)vec( ¯G) ["
3368,unknown,= 1 2L(Id ⊗ C)(2Id2 − L⊤LK)L⊤Lvec(C⊤ ¯G) = 1 2L(Id ⊗ C)(2Id2 − L⊤LK)L⊤vech( ¯H) = 1 2L(Id ⊗ C)(2Id2 − L⊤LK)vec( ¯H) = L(Id ⊗ C)vec( ¯H) − 1 2L(Id ⊗ C)L⊤LKvec( ¯H) = Lvec(C ¯H) − 1 2L(Id ⊗ C)L⊤vech( ¯H⊤) = vech(C ¯H) − 1 2L(Id ⊗ C)vec(dg( ¯H)) = vech(C ¯H) − 1 2vech(Cdg( ¯H)) = vech(C ¯¯H). S3.2. Proof of Theorem 1 First we derive the Fisher information and its inverse for each of the two parametri
3369,unknown,"tions. We have ℓq = log qλ(θ) = −d 2 log(2π) − 1 2 log |Σ| −1 2(θ − µ)⊤Σ−1(θ − µ). For the first parametrization, λ = (µ⊤, vech(C)⊤)⊤, let z = C−1(θ − µ) to simplify expressions. The first order derivatives are ∇µℓq = Σ−1(θ − µ), ∇vech(C)ℓq = vech(C−⊤zz⊤ − C−⊤), and ∇2 λℓq is given by −  Σ−1 {(C−⊤ ⊗ z⊤C−1) + (z⊤ ⊗ Σ−1)}L⊤ · L[{(C−1 ⊗ C−⊤zz⊤) + (zz⊤C−1 ⊗ C−⊤) − (C−1 ⊗ C−⊤)}K + zz⊤ ⊗ Σ−1]L⊤  . Tak"
3370,unknown," . Taking the negative expectation of ∇2 λℓq and applying the fact that E( z) = 0 and E(zz⊤) = Id, we obtain Fλ = Σ−1 0 0 L{(C−1 ⊗ C−⊤)K + (Id ⊗ Σ−1)}L⊤  =  Σ−1 0 0 I(C)  . Natural gradient updates in Gaussian variational approximation 33 Thus F−1 λ = blockdiag(Σ, I(C)−1). For the second parametrization, λ = (µ⊤, vech(T)⊤)⊤, the first order derivative, ∇vech(T)ℓq = vech(T−⊤ − (θ − µ)(θ − µ)⊤T"
3371,unknown,"∇vech(T)ℓq = vech(T−⊤ − (θ − µ)(θ − µ)⊤T) and ∇2 λℓq = −  Σ−1 −{(θ − µ)⊤T ⊗ Id + T ⊗ (θ − µ)⊤}L⊤ · L{(T−1 ⊗ T−⊤)K + Id ⊗ (θ − µ)(θ − µ)⊤}L⊤  . Taking the negative expectation of ∇2 λℓq and applying the fact that E( θ) = µ and E[(θ − µ)(θ − µ)⊤] = Σ, we obtain Fλ = Σ−1 0 0 L{(T−1 ⊗ T−⊤)K + Id ⊗ Σ}L⊤  =  Σ−1 0 0 I(T)  . Thus F−1 λ = blockdiag(Σ, I(T)−1). Suppose ∇vech(C)L = vech(G). Then, the "
3372,unknown,"e∇λL = F−1 λ ∇λL =  Σ 0 0 I(C)−1  ∇µL vech(G)  =  Σ∇µL vech(C ¯¯H)  , where we have applied Lemma 1 (iii) in the last step. The proof is similar for the second parametrization. S3.3. Proof of Corollary 1 If ξ = ((T⊤µ)⊤, vech(T)⊤)⊤, then e∇ξL = (∇λξ)⊤ e∇λL =  T⊤ (I ⊗ µ⊤)L⊤ 0 I  Σ∇µL vech(T ¯¯H)  =  T−1∇µL + ¯¯H⊤T⊤µ vech(T ¯¯H)  . The natural gradient ascent update is T(t+1)⊤ µ(t+1) = T("
3373,unknown,"T(t+1)⊤ µ(t+1) = T(t)⊤ µ(t) + ρt{T(t)−1 ∇µL + ¯¯H(t)⊤ T(t)⊤ µ(t)}, T(t+1) = T(t) + ρtT(t) ¯¯H(t), The first line simplifies to T(t+1)⊤ µ(t+1) = {T(t) + ρtT(t) ¯¯H(t)}⊤µ(t) + ρtT(t)−1 ∇µL =⇒ µ(t+1) = µ(t) + ρtT(t+1)−⊤ T(t)−1 ∇µL. S3.4. Proof of Lemma 2 Define g(θ) = (θ − µ)⊤ejh(θ) to be a function from R d to R and ej to be a d × 1 vector with the jth element equal to one and zero elsewhere. Then ∇"
3374,unknown,"∇θg(θ) = h(θ)ej + (θ − µ)⊤ej∇θh(θ). 34 Linda Tan Replacing h(θ) by g(θ) in Stein’s Lemma, we obtain Eq[Σ−1(θ − µ)(θ − µ)⊤ejh(θ)] = Eq[h(θ)ej + (θ − µ)⊤ej∇θh(θ)]. This implies that for any 1 ≤ i, j≤ d, e⊤ i Eq[{Σ−1(θ − µ)(θ − µ)⊤ − Id}h(θ)]ej = e⊤ i Eq[∇θh(θ)(θ − µ)⊤]ej. Thus Eq[{Σ−1(θ −µ)(θ −µ)⊤ −Id}h(θ)] = Eq[∇θh(θ)(θ −µ)⊤] since every (i, j) element of these two matrices agree with each other. S"
3375,unknown,"S3.5. Proof of Theorem 2 Post-multiplying the identity in Lemma 2 by C−⊤, we obtain Eq[{Σ−1(θ − µ)(θ − µ)⊤C−⊤ − C−⊤}h(θ)] = Eq[∇θh(θ)(θ − µ)⊤C−⊤] = Eq(G1). ∴ ∇vech(C)L = Z ∇vech(C)qλ(θ)h(θ)dθ = Z qλ(θ)vech n Σ−1(θ − µ)(θ − µ)⊤C−⊤ − C−⊤ o h(θ)dθ = Eq h vech{Σ−1(θ − µ)(θ − µ)⊤C−⊤ − C−⊤}h(θ) i = Eqvech(G1), and the first part of the first identity in Theorem 2 is shown. For the second part of the ide"
3376,unknown,"identity, we have Eq[Σ−1(θ − µ)∇θh(θ)⊤] = Eq[∇2 θh(θ)] from Price’s Theorem. Taking the transpose and post-multiplying by C, we obtain Eq(G1) = Eq[∇θh(θ)(θ − µ)⊤C−⊤] = Eq[∇2 θh(θ)C] = Eq(F1). Taking the transpose of the identity in Lemma 2 and post-multiplying by T−⊤, Eq[{(θ − µ)(θ − µ)⊤T − T−⊤}h(θ)] = Eq[(θ − µ)∇θh(θ)⊤T−⊤] = −Eq(G2). ∴ ∇vech(T)L = Z ∇vech(T)qλ(θ)h(θ)dθ = Z qλ(θ)vech{T−⊤ − (θ − µ)"
3377,unknown,"= Z qλ(θ)vech{T−⊤ − (θ − µ)(θ − µ)⊤T}h(θ)dθ = Eq[vech{T−⊤ − (θ − µ)(θ − µ)⊤T}h(θ)] = Eqvech(G2), and the first part of the second identity in Theorem 2 is shown. For the second part of the identity, we have E q[Σ−1(θ − µ)∇θh(θ)⊤] = E q[∇2 θh(θ)] from Price’s Theorem. Pre-multiplying by Σ and post-multiplying by T−⊤, we obtain −Eq(G2) = Eq[(θ − µ)∇θh(θ)⊤T−⊤] = Eq[Σ∇2 θh(θ)T−⊤] = −Eq(F2). To obtain "
3378,unknown,"To obtain unbiased estimates in terms of the first order derivative, we can also apply the reparametrization trick directly, which will lead to the same results. For Natural gradient updates in Gaussian variational approximation 35 λ = (µ⊤, vech(C)⊤)⊤ where Σ = CC⊤, let θ = µ + Cz. For λ = (µ⊤, vech(T)⊤)⊤ where Σ−1 = T T⊤, let θ = µ + T−⊤z. ∇vech(C)L = ∇vech(C)θ∇θh(θ) = L(z ⊗ I)∇θh(θ) = Lvec(∇θh(θ"
3379,unknown,"= vech(∇θh(θ)z⊤), ∇vech(T)L = ∇vech(T)θ∇θh(θ) = −L(T−1 ⊗ T−⊤z)∇θh(θ) = −Lvec(T−⊤z∇θh(θ)T−⊤) = −vech(T−⊤z∇θh(θ)T−⊤). S4. Convergence of Snngm First we derive some intermediate results that are needed for the proof of Theorem 3. Let ⟨·, ·⟩ denote the inner product and ¯gt = egt/∥egt∥ so that ∥¯gt∥ = 1. Lemma S2. t−1X i=0 iβi ≤ β(1 − βt) (1 − β)2 . Proof. t−1X i=0 iβi = β{1 + 2β + . . .(t − 1)βt−2} ="
3380,unknown,"dβ (β + β2 + ··· + βt−1) = β d dβ β(1 − βt−1) 1 − β  = β{1 − βt − tβt−1(1 − β)} (1 − β)2 ≤ β(1 − βt) (1 − β)2 . S4.1. Bounds for norm of natural gradient We have ∥egt∥2 = bg⊤ t F−2 t bgt. By (A2), ∥bgt∥ ≤R and by (A4), R1 ≤ ev(Ft) ≤ R2. This implies that 1 /R2 ≤ ev(F−1 t ) ≤ 1/R1. Using the result in Pg. 18 of Magnus and Neudecker (2019), 1 R2 ≤ g⊤ t F−1 t gt g⊤ t gt ≤ 1 R1 =⇒ ∥gt∥2 R2 ≤ ⟨gt, F−"
3381,unknown,"t gt⟩ ≤∥gt∥2 R1 , 1 R2 2 ≤ bg⊤ t F−2 t bgt bg⊤ t bgt ≤ 1 R2 1 =⇒ ∥bgt∥ R2 ≤ ∥egt∥ ≤∥bgt∥ R1 ≤ R R1 . S4.2. Bound on momentum Since m0 = 0, mt = βmt−1 + (1 − β)¯gt = (1 − β) Pt−1 i=0 βi¯gt−i. Thus, ∥mt∥ ≤(1 − β) t−1X i=0 βi∥¯gt−i∥ = (1 − β) t−1X i=0 βi = 1 − βt. (S1) 36 Linda Tan S4.3. Inequality from L-Lipschitz smooth assumption Define G(t) = L(λ + t(λ′ − λ)). Then G′(t) = ∇λL(λ + t(λ′ − λ))⊤(λ′ "
3382,unknown,"G(1) = G(0) + G′(0) + Z 1 0 G′(t) − G′(0) dt. Therefore, by Cauchy-Schwarz inequality, |G(1) − G(0) − G′(0)| ≤ Z 1 0 |⟨∇λL(λ + t(λ′ − λ)) − ∇λL(λ), λ′ − λ⟩|dt ≤ L∥λ′ − λ∥2 Z 1 0 t dt = L∥λ′ − λ∥2/2. ∴ |L(λ′) − L(λ) − ⟨∇λL(λ), λ′ − λ⟩| ≤L∥λ′ − λ∥2/2 =⇒ −L(λ′) + L(λ) + ⟨∇λL(λ), λ′ − λ⟩ ≤L∥λ′ − λ∥2/2. (S2) S4.4. Proof of Theorem 3 Set λ = λ(t) and λ′ = λ(t+1) in (S2). Since λ(t+1) = λ(t) + αmt/(1 − β"
3383,unknown,"β) Pt−1 i=0 βi¯gt−i and ∥mt∥ ≤1 − βt from (S1), L(λ(t)) ≤ L(λ(t+1)) − ⟨∇λL(λ(t)), λ(t+1) − λ(t)⟩ + L∥λ(t+1) − λ(t)∥2/2 = L(λ(t+1)) − α 1 − βt ⟨∇L(λ(t)), mt⟩ + Lα2 2(1 − βt)2 ∥mt∥2 ≤ L(λ(t+1)) − α(1 − β) 1 − βt t−1X i=0 βi⟨∇λL(λ(t)), ¯gt−i⟩ + Lα2 2 . (S3) Write ⟨∇λL(λ(t)), ¯gt−i⟩ = ⟨∇λL(λ(t)) − ∇λL(λ(t−i)), ¯gt−i⟩ + ⟨∇λL(λ(t−i)), ¯gt−i⟩. For the first term, applying the Cauchy–Schwarz inequality an"
3384,unknown,"the first term, applying the Cauchy–Schwarz inequality andL-Lipschitz smooth assump- tion (A3), |⟨∇λL(λ(t)) − ∇λL(λ(t−i)), ¯gt−i⟩| ≤ ∥∇λL(λ(t)) − ∇λL(λ(t−i))∥∥¯gt−i∥ ≤L∥λ(t) − λ(t−i)∥, where λ(t) − λ(t−i) = Pt−1 j=t−i{λ(j+1) − λ(j)} = α Pt−1 j=t−i mj/(1 − βj). Hence ∥λ(t) − λ(t−i)∥ ≤α t−1X j=t−i ∥mj∥ 1 − βj ≤ αi. For the second term, ⟨∇λL(λ(t−i)), ¯gt−i⟩ = ⟨gt−i, F−1 t−ibgt−i⟩/∥egt−i∥ ≥R1 R ⟨gt−i,"
3385,unknown,"R ⟨gt−i, F−1 t−ibgt−i⟩. Substituting these back into (S3) and applying Lemma S2, L(λ(t)) ≤ L(λ(t+1)) + α(1 − β) 1 − βt t−1X i=0 βi  Lαi − R1 R ⟨gt−i, F−1 t−ibgt−i⟩  + Lα2 2 ≤ L(λ(t+1)) + Lα2β (1 − β) − R1α(1 − β) R(1 − βt) t−1X i=0 βi⟨gt−i, F−1 t−ibgt−i⟩ + Lα2 2 . Natural gradient updates in Gaussian variational approximation 37 Taking expectation, R1α(1 − β) R t−1X i=0 βi⟨gt−i, F−1 t−igt−i⟩ ≤R1"
3386,unknown,"R(1 − βt) t−1X i=0 βi⟨gt−i, F−1 t−igt−i⟩ ≤ L(λ(t+1)) − L(λ(t)) + Lα2β (1 − β) + Lα2 2 . Summing over t = 1 to t = N and applying L(λN+1) ≤ L∗ by (A1), R1α(1 − β) R NX t=1 t−1X i=0 βi⟨gt−i, F−1 t−igt−i⟩ ≤ L(λ(N+1)) − L(λ(1)) + NLα2β (1 − β) + NLα2 2 ≤ L∗ − L(λ(1)) + NLα2β (1 − β) + NLα2 2 . Since ⟨gt−i, F−1 t−igt−i⟩ ≥ ∥gt−i∥2/R2, R1α(1 − β) R2R NX t=1 t−1X i=0 βi∥gt−i∥2 ≤ L∗ − L(λ(1)) + NLα2β (1 − "
3387,unknown,"2 . Let j = t − i and interchanging the summation, NX t=1 t−1X i=0 βi∥gt−i∥2 = NX t=1 tX j=1 βt−j∥gj∥2 = NX j=1 NX t=j βt−j∥gj∥2 = PN j=1(1 − βN−j+1)∥gj∥2 1 − β . Therefore, E∥gτ ∥2 = NX j=1 1 − βN−j+1 C ∥gj∥2 ≤ 1 eN NX j=1 (1 − βN−j+1)∥gj∥2 = 1 − β eN NX t=1 t−1X i=0 βi∥gt−i∥2 ≤ RR2 eNR1α  L∗ − L(λ(1)) + NLα2β (1 − β) + NLα2 2  . S5. Methods for comparison In the experiments, we compare propose"
3388,unknown,"In the experiments, we compare proposed methods with Adam (Kingma and Ba, 2015), variational online Gauss-Newton (VOGN, Khan et al., 2018) and ProxGenAdam (Kim et al., 2023). The updates used in VOGN and ProxGenAdam are summarized below. S5.1. Variational online Gauss-Newton (VOGN) Khan et al. (2018) used the generalized Gauss-Newton approximation for the Hessian of the log-likelihood in the natur"
3389,unknown,"of the log-likelihood in the natural gradient update of the natural parameter for the 38 Linda Tan Gaussian density. They assumed a normal prior, θ ∼ N(0, σ2 0I), and a log-likelihood of the form log p(y|θ) = Pn i=1 log p(yi|θ). Suppose Bt contains the indices for a minibatch of observations sampled uniformly at random from {y1, . . . , yn} at iteration t. Let ˆgL t = n |Bt| X i∈Bt ∇θ log p(yi|θ) "
3390,unknown,"be the unbiased estimate of ∇θ log p(y|θ) and ˆHL t be the generalized Gauss-Newton estimate of −∇2 θ log p(y|θ) at iteration t. Then the ( j, k) element of ˆHL t is given by n |Bt| X i∈Bt {∇θj log p(yi|θ)}{∇θk log p(yi|θ)}. The remaining terms in the lower bound and their gradients are evaluated analytically: ∇µEq[log p(θ) + logqλ(θ)] = −µ/σ2 0, ∇ΣEq[log p(θ) + logqλ(θ)] = 1 2(Σ−1 − Id/σ2 0). Let"
3391,unknown,"0). Let S = Σ−1 denote the precision matrix. Applying the theorems of Bonnet and Price for the log-likelihood term, the natural gradient update at iteration t is S(t+1) = S(t) − 2ρt(−1 2 ˆHL t + 1 2S(t) − 1 2Id/σ2 0) = (1 − ρt)S(t) + ρt( ˆHL t + Id/σ2 0), µ(t+1) = µ(t) + ρtΣ(t+1)(ˆgL t − µ(t)/σ2 0). Khan et al. (2018) further incorporated momentum and introduced bias correction to arrive at Adam-l"
3392,unknown,"arrive at Adam-like updates. Let T T⊤ be the Cholesky factorization of S. • Initialize m0 = 0, µ1 and T(1). Set S(1) = T(1)T(1)⊤ . For t = 1, . . . , T, 1. Generate z ∼ N(0, Id) and compute θ(t) = T(t)−⊤ z + µ(t), ˆgL t and ˆHL t . 2. mt = β1mt−1 + (1 − β1)(ˆgL t − µ(t)/σ2 0), 3. ˆmt = mt/(1 − βt 1), 4. S(t+1) = β2S(t) + (1 − β2)( ˆHL t + Id/σ2 0), 5. µ(t+1) = µ(t) + αS(t+1)−1 ˆmt. 6. Find the Cho"
3393,unknown,"t + Id/σ2 0), 5. µ(t+1) = µ(t) + αS(t+1)−1 ˆmt. 6. Find the Cholesky factor, T(t+1), of S(t+1). Using the default values in Adam as reference, we setβ1 = 0.9, β2 = 0.999 and α = 0.001. We adjust α upwards by a factor of 10 each time if necessary. For the loglinear model where L is tractable and ∇ΣEq[log p(y|θ) is positive definite, we replace the estimates ˆ gL t and ˆHL t respectively by n |Bt| P"
3394,unknown,"P i∈Bt ∇µEq[log p(yi|θ)] and − 2n |Bt| P i∈Bt ∇ΣEq[log p(yi|θ)]. For GLMMs, θ = (θ⊤ 1 , . . . , θ⊤ n , θg)⊤ and the prior is not of the form θ ∼ N(0, σ2 0I). Moreover, Eq[log p(θi|θg)] cannot be evaluated analytically. Hence, we consider a gener- alized Gauss-Newton approximation of log p(y, θ) = nX i=1 fi(θ), where fi(θ) = log p(yi|θi, θg) + logp(θi|θG) + 1 n log p(θG), Natural gradient updates i"
3395,unknown,"where −∇2 θ log p(y, θ) ≈ J⊤J and J =   ∂f1 ∂θ1 0 . . . 0 ∂f1 ∂θg 0 ∂f2 ∂θ2 . . . 0 ∂f2 ∂θg ... ... ... ... ... 0 0 . . . ∂fn ∂θ1 ∂fn ∂θg   . Correspondingly, we replace (ˆgL t − µ(t)/σ2 0) in step (1) by an estimate of ∇log p(y, θ) at iteration t and ( ˆHL t + Id/σ2 0) in step (4) by J⊤J. S5.2. ProxGenAdam Let −L(λ) = −Eq[log p(y, θ)]| {z } L1(λ) + Eq[log qλ(θ)]| {z } L2(λ) . The stoc"
3396,unknown,"L2(λ) . The stochastic gradient descent (SGD) update, λ ← λ − γ(∇λL1(λ) +∇λL2(λ)), where γ is the stepsize, can be interpreted as minimizing a linear approximation of the negative lower bound with a quadratic penalty: s(v) = L1(λ) + L2(λ) + ∇λL1(λ)⊤(v − λ) + ∇λL2(λ)⊤(v − λ) + 1 2γ ∥v − λ∥2. Domke (2020) considered a Cholesky decomposition, Σ = CC⊤, and noted that L2(λ) is non-smooth. He proposed p"
3397,unknown,"is non-smooth. He proposed proximal optimization by using this term in the objective instead of its linear approximation. This led to minimization of s(v) = L1(λ) + ∇λL1(λ)⊤(v − λ) + L2(v) + 1 2γ ∥v − λ∥2 = L2(v) + 1 2γ ∥v − λ′∥2 + terms not inolving v, where λ′ = λ − γ∇λL1(λ). Since L2(λ) = −d 2 log(2π) − Pd i=1 log |Cii| −d 2 involves only the diagonal elements of C, the updates of all other ele"
3398,unknown,"we seek Cii that minimizes Li = −log |Cii| + 1 2γ (Cii − C′ ii)2, (i = 1, . . . , d), where C′ ii is the SGD update for Cii based on L1(λ) only. We have ∂Li ∂Cii = (Cii − C′ ii) γ − 1 Cii = 0 =⇒ Cii = C′ ii ± q C′ ii 2 + 4γ 2 , ∂2Li ∂C2 ii = 1 γ + 1 C2 ii > 0. 40 Linda Tan Hence, both solutions lead to minimum points. However, if C′ ii > 0, then the global minimum occurs at Cii = {C′ ii + (C′ ii 2"
3399,unknown,"ii < 0, then the global minimum occurs at Cii = {C′ ii − (C′ ii 2 + 4γ)1/2}/2. Thus, proximal gradient descent effectively keeps the diagonal entries of C further away from 0. We also consider Cholesky decomposition of the precision matrix, Σ−1 = T T⊤, where we minimize Li = log |Tii| + 1 2γ (Tii − T′ ii)2, (i = 1, . . . , d) with respect to Tii instead, and T′ ii is the SGD update for Tii based o"
3400,unknown,"have ∂Li ∂Tii = (Tii − T′ ii) γ + 1 Tii = 0 =⇒ Tii = T′ ii ± q T′ ii 2 − 4γ 2 , ∂2Li ∂T 2 ii = 1 γ − 1 T2 ii = T2 ii − γ γT 2 ii . We require T′2 ii > 4γ for the solution to be well-defined. If T′ ii > 0, then we have a minimum point at Tii = {T′ ii + (T′ ii 2 − 4γ)1/2}/2 and a maximum point at the other solution. On the other hand, if T′ ii < 0, then we have a minimum point at Tii = {T′ ii − (T′ "
3401,unknown,"{T′ ii − (T′ ii 2 − 4γ)1/2}/2 and a maximum point at the other solution. Thus proximal gradient descent effectively moves the diagonal entries of T closer to 0. In ProxGenAdam (Yun et al., 2021; Kim et al., 2023), momentum and adaptive stepsizes are combined with the proximal steps to yield Adam-like updates outlined below, although bias correction of estimated moments is not performed. • Initiali"
3402,unknown,"• Initialize m0 = 0, v0 = 0 and λ(1). For t = 1, 2, . . ., – Compute ˆgℓ t , an unbiased estimate of E q[log p(y, θ)]. – mt = β1mt−1 + (1 − β1)ˆgℓ t , – vt = β2vt−1 + (1 − β2)ˆgℓ 2 t , – λ(t+1) = λ(t) + α√vt+ϵmt. – For i = 1, . . . , d, update C(t+1) ii ← C(t+1) ii + sgn(C(t+1) ii ) q C(t+1) ii 2 + 4γt 2 , or T(t+1) ii ← T(t+1) ii + sgn(T(t+1) ii ) q T(t+1) ii 2 − 4γt 2 if T(t+1) ii 2 > 4γt. We se"
3403,unknown,"ii 2 > 4γt. We set α = 0.001 , β1 = 0.9, β2 = 0.999 and ϵ = 10−8 similar to Adam and γt = α√vt+ϵ. Natural gradient updates in Gaussian variational approximation 41 S6. Natural gradient for sparse precision matrix We derive the natural gradient where the precision matrix is sparse. In this case, ℓq = log qλ(θ) = −d 2 log(2π) + log|Tg| + NX i=1 log |Ti| −1 2 nX i=1 (θi − µi)⊤TiT⊤ i (θi − µi) − 1 2(θ"
3404,unknown,"− 1 2(θg − µg)⊤ nX i=1 TgiT⊤ gi + TgT⊤ g ! (θg − µg) − (θg − µg)⊤ nX i=1 TgiT⊤ i (θi − µi). S6.1. Proof of Theorem 4 If we integrate out all other variables from qλ(θ) = q(θg) Qn i=1 q(θi|θg) except θi and θg, then q(θi, θg) = q(θi|θg)q(θg), whose covariance matrix is  Ti 0 Tgi Tg −⊤  Ti 0 Tgi Tg −1 = T−⊤ i −T−⊤ i T⊤ gi T−⊤ g 0 T−⊤ g  T−1 i 0 −T−1 g TgiT−1 i T−1 g  = "" T−⊤ i T−1 i + T−⊤ i "
3405,unknown,"i + T−⊤ i T⊤ gi T−⊤ g T−1 g TgiT−1 i −T−⊤ i T⊤ gi T−⊤ g T−1 g −T−⊤ g T−1 g TgiT−1 i T−⊤ g T−1 g # . Hence Cov(θg) = E{(θg − µg)(θg − µg)⊤} = T−⊤ g T−1 g , Cov(θi) = E{(θi − µi)(θi − µi)⊤} = T−⊤ i T−1 i + T−⊤ i T⊤ gi T−⊤ g T−1 g TgiT−1 i , Cov(θi, θg) = E{(θi − µi)(θg − µg)⊤} = −T−⊤ i T⊤ gi T−⊤ g T−1 g . First, we find the elements in the Fisher information matrix. Differentiating ℓq with respect t"
3406,unknown,"respect to Ti and taking expectation with respect to qλ(θ), ∇vech(Ti)ℓq = vech{T−⊤ i − (θi − µi)(θi − µi)⊤Ti − (θi − µi)(θg − µg)⊤Tgi}, ∇2 vech(Ti)ℓq = −L{(T−1 i ⊗ T−⊤ i )K + I ⊗ (θi − µi)(θi − µi)⊤}L⊤, E[∇2 vech(Ti)ℓq] = −L{(T−1 i ⊗ T−⊤ i )K + I ⊗ T−⊤ i (I + T⊤ gi T−⊤ g T−1 g Tgi)T−1 i }L⊤ = −I(Ti) − L(I ⊗ T−⊤ i T⊤ gi T−⊤ g T−1 g TgiT−1 i )L⊤. Differentiating ∇vech(Ti)ℓq with respect to Tgi and t"
3407,unknown,"qλ(θ), ∇2 vech(Ti),vec(Tgi)ℓq = −L{I ⊗ (θi − µi)(θg − µg)⊤}, E[∇2 vech(Ti),vec(Tgi)ℓq] = L(I ⊗ T−⊤ i T⊤ gi T−⊤ g T−1 g ). Differentiating ℓq with respect to Tgi and taking expectation with respect to qλ(θ), ∇vec(Tgi)ℓq = −vec[(θg − µg)(θg − µg)⊤Tgi + (θg − µg)(θi − µi)⊤Ti]. ∇2 vec(Tgi)ℓq = −(I ⊗ (θg − µg)(θg − µg)⊤). E[∇2 vec(Tgi)ℓq] = −(I ⊗ T−⊤ g T−1 g ). 42 Linda Tan Differentiating ℓq with resp"
3408,unknown,"∇vech(Tg)ℓq = vech[T−⊤ g − (θg − µg)(θg − µg)⊤Tg]. ∇2 vech(Tg)ℓq = −L[(T−1 g ⊗ T−⊤ g )K + I ⊗ (θg − µg)(θg − µg)⊤]L⊤. E[∇2 vech(Tg)ℓq] = −L[(T−1 g ⊗ T−⊤ g )K + I ⊗ T−⊤ g T−1 g ]L⊤ = −I(Tg). Thus the Fisher information matrix is Fλ = blockdiag(Σ−1, F1, . . . , Fn, I(Tg)), where Fi = F11i F12i F⊤ 12i F22i  and F11i = I(Ti) + L(I ⊗ T−⊤ i T⊤ gi T−⊤ g T−1 g TgiT−1 i )L⊤, F12i = −L(I ⊗ T−⊤ i T⊤ gi T−⊤"
3409,unknown,"g T−1 g ), F22i = (I ⊗ T−⊤ g T−1 g ). Since F−1 22i = I ⊗ TgT⊤ g , and F12iF−1 22i = −L(I ⊗ T−⊤ i T⊤ gi ), F11i − F12iF−1 22i F⊤ 12i = I(Ti). Hence using block matrix inversion, F−1 i = "" I(Ti)−1 I(Ti)−1L(I ⊗ T−⊤ i T⊤ gi ) · (I ⊗ TgT⊤ g ) + (I ⊗ TgiT−1 i )L⊤I(Ti)−1L(I ⊗ T−⊤ i T⊤ gi ) # . Next, we derive the expression for the natural gradient. LetAi, Ggi, Gi for i = 1, . . . , n and Gg be as defin"
3410,unknown,"and Gg be as defined in theorem 4. In addition, let Hi = T⊤ i ¯Gi for i = 1, . . . , nand Hg = T⊤ g ¯Gg. For i = 1, . . . , n, "" e∇vech(Ti)L e∇vec(Tgi)L # = F−1 i  vech(Ai) vec(Ggi)  . Applying Lemma 1, e∇vech(Ti)L = I(Ti)−1vech(Ai) + I(Ti)−1L(I ⊗ T−⊤ i T⊤ gi )vec(Ggi) = I(Ti)−1vech(Ai + T−⊤ i T⊤ gi Ggi) = I(Ti)−1vech(Gi) = vech(Ti ¯¯Hi). e∇vec(Tgi)L = (I ⊗ TgT⊤ g )vec(Ggi) + (I ⊗ TgiT−1 i )L⊤ e"
3411,unknown,"i )L⊤ e∇vech(Ti)L = vec(TgT⊤ g Ggi) + (I ⊗ TgiT−1 i )L⊤vech(Ti ¯¯Hi) = vec(TgT⊤ g Ggi + Tgi ¯¯Hi). Finally, e∇vech(Tg)L = I(Tg)−1vech(Gg) = vech( Tg ¯¯Hg) from Lemma 1. Hence the full natural gradient is e∇λL = F−1 λ   ∇µL vech(Ai) vec(Ggi)  i=1:n vech(Gg)   =   Σ∇µL vech(Ti ¯¯Hi) vec(Tgi ¯¯Hi + TgT⊤ g Ggi)  i=1:n vech(Tg ¯¯Hg)  , Natural gradient updates in Gaussian variationa"
3412,unknown,"where [ai]i=1:n = (a⊤ 1 , . . . , a⊤ n )⊤. To compute the natural gradient, note that H = T⊤ d ¯G =   H1 . . . 0 0 ... ... ... ... 0 . . . H n 0 T⊤ g Gg1 . . . T⊤ g Ggn Hg  , ¯¯H =   ¯¯H1 . . . 0 0 ... ... ... ... 0 . . . ¯¯Hn 0 T⊤ g Gg1 . . . T⊤ g Ggn ¯¯Hg   and T ¯¯H =   T1 ¯¯H1 . . . 0 0 ... ... ... ... 0 . . . T n ¯¯Hn 0 Tg1 ¯¯H1 + TgT⊤ g Gg1 . . . Tgn ¯¯Hn + TgT⊤ g Gg"
3413,unknown,"g Ggn Tg ¯¯Hg  . Thus the natural gradient update for T is T ← T + ρtT ¯¯H. S6.2. Stochastic natural gradients Let θa = (θ⊤ 1 , . . . , θ⊤ n )⊤, µa = (µ⊤ 1 , . . . , µ⊤ n )⊤, va = (v⊤ 1 , . . . , v⊤ n )⊤ and T =  Ta 0 Tga Tg  , T −1 =  T−1 a 0 −T−1 g TgaT−1 a T−1 g  , where Ta = blockdiag(T1, . . . , Tn) and Tga = [Tg1 . . . Tgn]. Note that v =  T−1 a 0 −T−1 g TgaT−1 a T−1 g  ∇θah(θ) ∇"
3414,unknown," ∇θah(θ) ∇θg h(θ)  =  T−1 a ∇θah(θ) T−1 g {∇θg h(θ) − TgaT−1 a ∇θah(θ)}  =  va vg  , u = T−⊤ d T⊤(θ − µ) =  (θi − µi) + T−⊤ i Tgi(θg − µg)  i=1:n θg − µg  =  [ui]i=1:n ug  . First, we extract entries in G2 = −(θ − µ)v⊤ corresponding to nonzero entries in T. We have ∇vech(Ti)L = −Eqvech{(θi − µi)v⊤ i } = −Eqvech{(ui − T−⊤ i Tgiug)v⊤ i }, ∇vech(Tg)L = −Eqvech{(θg − µg)v⊤ g } = −Eqvech{u"
3415,unknown,"g } = −Eqvech{ugv⊤ g }, ∇vec(Tgi)L = −Eqvec{(θg − µg)v⊤ i } = −Eqvec{ugv⊤ i }. Next, we extract entries in F2 = −Σ∇2 θh(θ)T−⊤ corresponding to nonzero entries in T. If Σ g = T−⊤ g T−1 g , Σi = T−⊤ i Ti and Σa = T−⊤ a T−1 a , then Σ = T−⊤T−1 = T−⊤ a −T−⊤ a T⊤ gaT−⊤ g 0 T−⊤ g  T−1 a 0 −T−1 g TgaT−1 a T−1 g  = Σa + T−⊤ a T⊤ gaΣgTgaT−1 a −T−⊤ a T⊤ gaΣg −ΣgTgaT−1 a Σg  44 Linda Tan and ∇2 θh(θ)T−"
3416,unknown," ∇2 θa h(θ) ∇2 θa,θg h(θ) ∇2 θg,θa h(θ) ∇2 θg h(θ) T−⊤ a −T−⊤ a T⊤ gaT−⊤ g 0 T−⊤ g  = "" ∇2 θa h(θ)T−⊤ a {∇2 θa,θg h(θ) − ∇2 θa h(θ)T−⊤ a T⊤ ga}T−⊤ g ∇2 θg,θa h(θ)T−⊤ a {∇2 θg h(θ) − ∇2 θg,θa h(θ)T−⊤ a T⊤ ga}T−⊤ g # . Let Ugi = Σg{∇2 θg,θih(θ) − TgiT−1 i ∇2 θih(θ)}T−⊤ i (i = 1, . . . , n), Uga = Σg{∇2 θg,θah(θ) − TgaT−1 a ∇2 θah(θ)}T−⊤ a = Ug1 . . . Ugn  , Ugg = Σg{∇2 θg h(θ) − TgaT−1 a ∇2 θa"
3417,unknown,"g . Then F2,11 = T−⊤ a T⊤ gaUga − Σa∇2 θah(θ)T−⊤ a , F2,21 = −Uga, F2,22 = UgaT⊤ gaT−⊤ g − Ugg. Thus, we have ∇vech(Ti)L = Eqvech(T−⊤ i Tgiugv⊤ i − uiv⊤ i ) = Eqvech(T−⊤ i T⊤ gi Ugi − Σi∇2 θih(θ)T−⊤ i ), ∇vec(Tgi)L = −Eqvec(ugv⊤ i ) = −Eqvec(Ugi), ∇vech(Tg)L = −Eqvech(ugv⊤ g ) = Eqvech(UgaT⊤ gaT−⊤ g − Ugg). Using the above results, we can obtain unbiased estimates of the natural gradient in terms "
3418,unknown,"terms of the first derivative ∇θh(θ) by setting Gi = −uiv⊤ i , Ggi = −ugv⊤ i , Gg = −ugv⊤ g , for i = 1, . . . , n. Note that setting Ai = T−⊤ i Tgiugv⊤ i −uiv⊤ i = −T−⊤ i TgiGgi −uiv⊤ i leads to Gi = Ai + T−⊤ i T⊤ gi Ggi = −uiv⊤ i . Hence an unbiased estimate of G is   −u1v⊤ 1 0 · 0 ... ... ... ... 0 . . .−unv⊤ n 0 −ugv⊤ 1 . . . −ugv⊤ n −ugv⊤ g  , and an unbiased estimate of ¯G can be o"
3419,unknown,"correspond to zero entries in T to zero. On the other hand, unbiased estimates in terms of the second derivative ∇2 θh(θ) can be obtained by setting Gi = −Σi∇2 θih(θ)T−⊤ i , Ggi = −Ugi, Gg = UgaT⊤ gaT−⊤ g − Ugg, for i = 1, . . . , n. Note that setting Ai = T−⊤ i T⊤ gi Ugi − Σi∇2 θi h(θ)T−⊤ i = −T−⊤ i T⊤ gi Ggi − Natural gradient updates in Gaussian variational approximation 45 Σi∇2 θi h(θ)T−⊤ i le"
3420,unknown,"i T⊤ gi Ggi = −Σi∇2 θi h(θ)T−⊤ i . In addition, T−⊤ d T−1∇2 θh(θ)T−⊤ =  Σa 0 −ΣgTgaT−1 a Σg "" ∇2 θa h(θ)T−⊤ a {∇2 θa,θg h(θ) − ∇2 θa h(θ)T−⊤ a T⊤ ga}T−⊤ g ∇2 θg,θa h(θ)T−⊤ a {∇2 θg h(θ) − ∇2 θg,θa h(θ)T−⊤ a T⊤ ga}T−⊤ g # = Σa∇2 θa h(θ)T−⊤ a · Uga UgaT⊤ gaT−⊤ g − Ugg  =   Σ1∇2 θ1 h(θ)T−⊤ 1 . . . · · ... ... ... ... · . . .Σn∇2 θn h(θ)T−⊤ n · Ug1 . . . U gn Ugg − UgaT⊤ gaT−⊤ g   Hence "
3421,unknown,"gaT−⊤ g   Hence an unbiased estimate of ¯G can be obtained by setting elements in −T−⊤ d T−1∇2 θh(θ)T−⊤ which correspond to zero entries in T to zero. S7. Logistic regression Let y = ( y1, . . . , yn)⊤, X = ( x⊤ 1 , . . . , x⊤ n ) and w = ( w1, . . . , wn)⊤, where wi = exp(x⊤ i θ)/{1 + exp(x⊤ i θ)}. Let W be a diagonal matrix with the ith element given by wi(1 − wi) for i = 1, . . . , d. Then"
3422,unknown,"log p(y, θ) = y⊤Xθ − nX i=1 log{1 + exp(x⊤ i θ)} −d 2 log(2πσ2 0) − θ⊤θ 2σ2 0 , ∇θ log p(y, θ) = X⊤(y − w) − θ/σ2 0, ∇2 θ log p(y, θ) = −X⊤W X− Id/σ2 0, S8. Deep GLMs Consider yi following the Bernoulli or normal distribution. We first discuss treatment of the normal distribution as it has a dispersion parameter and is more complicated. The Bernoulli distribution can be treated similarly by removi"
3423,unknown,"The Bernoulli distribution can be treated similarly by removing terms associated to the dispersion parameter and replacing the likelihood function appropriately. S8.1. Response from normal distribution Suppose yi ∼ N(ηi, v) for i = 1 , . . . , nand g(·) is the identity link function. Let the prior for v be an inverse gamma density with shape and scale parameters a0 and b0 respectively. We have log"
3424,unknown,"log p(y, Θ|a0, b0, γ) = nX i=1 p(yi|θ, v) + logp(v|a0, b0) + logp(w|γw) + pX j=1 {log p(wxj |τj) + logp(τj|γj)} = −n 2 log(2π) − n 2 log v − 1 2v nX i=1 (yi − ηi)2 + a0 log b0 − log Γ(a0) − (a0 + 1) logv − b0 v 46 Linda Tan − dw 2 log(2π) + dw 2 log γw − γw 2 w⊤w − mp 2 log(2π) − p(m + 1) 2 log(2) − p log Γ(m+1 2 ) + pX j=1 ( (m + 1) logγj − w⊤ xj wxj 2τj − 1 2 log τj − γ2 j τj 2 ) . The optimal d"
3425,unknown,"q(v) ∝ exp E−v{log p(y, Θ|a0, b0, γ)} ∝ exp E−v ( −n 2 log v − 1 2v nX i=1 (yi − ηi)2 − (a0 + 1) logv − b0 v ) ∝ exp E−v  −  a0 + n 2 + 1  log v − b0 + Pn i=1(yi − ηi)2/2 v  . Hence q(v) is an inverse-gamma density with shape and scale parameters a = a0 + n 2 and b = b0 + Pn i=1 Eqλ(yi − ηi)2/2. Note that the update for a need only be applied once at the beginning. In addition, E q(log v) = lo"
3426,unknown,"the beginning. In addition, E q(log v) = log(b) − ψ(a) and E q(1/v) = a/b where ψ(·) is the digamma function. The optimal density of q(τj) for j = 1, . . . , pis q(τj) ∝ exp E−τj {log p(y, Θ|a0, b0, γ)} ∝ exp ( − Eqλ(w⊤ xj wxj ) 2τj − 1 2 log τj − γ2 j τj 2 ) ∝ exp ( − µ⊤ xj µxj + σ⊤ xj σxj 2τj − 1 2 log τj − γ2 j τj 2 ) , where µxj and σ2 xj are the variational means and variances of the weights "
3427,unknown,"j τj 2 ) , where µxj and σ2 xj are the variational means and variances of the weights that connect xj to hidden units in the first layer. The form of this density implies that the variational posterior of 1/τj is the inverse Gaussian density with parameters, ατj = γjq µ⊤xj µxj + σ⊤xj σxj , β τj = γ2 j . Note that Eq(1/τj) = ατj and Eq(τj) = 1/ατj + 1/βτj . The evidence lower bound is L = Eq{log p("
3428,unknown,pX j=1 Eq{log q(τj)} = d + p(1 − m) − n − dw 2 log(2π) + p + d 2 − p log Γ(m+1 2 ) − p(m + 1) 2 log(2) + a0 log b0 − log Γ(a0) + 1 2 log |Σ| +  a − a0 − n 2  {log(b) − ψ(a)} −a 2b nX i=1 Eqλ(yi − ηi)2 + dw 2 log γw − γw(µ⊤ wµw + σ⊤ w σw) 2 − a log b + log Γ(a) + a  1 − b0 b  Natural gradient updates in Gaussian variational approximation 47 + pX j=1 ( (m + 1) logγj − 1 2 log βτj − ατj (µ⊤ xj µx
3429,unknown,"2 log βτj − ατj (µ⊤ xj µxj + σ⊤ xj σxj ) 2 − γ2 j 2  1 ατj + 1 βτj ) . where µw and σ2 w are the variational means and variances corresponding to the weights w, which are not in the first hidden layer. Note that after performing the update a = a0 + n/2, the term   a − a0 − n 2  {log(b) − ψ(a)} = 0 and can be omitted from L. To determine appropriate values for the shrinkage parametersγ, we use t"
3430,unknown,"To determine appropriate values for the shrinkage parametersγ, we use the empirical Bayes approach proposed by Tran et al. (2020), which can also be interpreted as selecting γ to be the values that maximize the evidence lower bound. We have ∂L ∂γw = dw 2γw − µ⊤ wµw + σ⊤ w σw 2 = 0 =⇒ γw = dw µ⊤wµw + σ⊤w σw , ∂L ∂γj = m + 1 γj − γj  1 ατj + 1 βτj  = 0 =⇒ γj = s m + 1 1/ατj + 1/βτj . Note that all"
3431,unknown,"1/ατj + 1/βτj . Note that all the terms in the lower bound can be calculated analytically exceptPn i=1 Eqλ(yi − ηi)2 because ηi is the output of the deep neural network, which is a nonlinear function of θ, the weights and biases in the neural network. At each itera- tion, a minibatch B of observations is processed and we compute an unbiased estimate, n |B| P i∈B(yi − bmi)2, where ˆmi is the output"
3432,unknown,"n |B| P i∈B(yi − bmi)2, where ˆmi is the output of the neural network based on θ generated randomly from the existing variational approximation qλ(θ). An unbiased estimate of the gradient of Pn i=1 Eqλ(yi − ηi)2 is computed using automatic differentiation. S8.2. Response from Bernoulli distribution Let ηi be the output of the neural network. If yi ∼ Bernoulli(pi), then the lower bound is L = d + p"
3433,unknown,"2 log(2π) + p + d 2 − p log Γ(m+1 2 ) − p(m + 1) 2 log(2) + dw 2 log γw − γw(µ⊤ wµw + σ⊤ w σw) 2 + 1 2 log |Σ| + nX i=1 Eqλ{yiηi − log(1 + eηi)} + pX j=1 ( (m + 1) logγj − 1 2 log βτj − ατj (µ⊤ xj µxj + σ⊤ xj σxj ) 2 − γ2 j 2  1 ατj + 1 βτj ) . Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones? Lena Schmida,∗, Al"
3434,unknown,"Lena Schmida,∗, Alexander Gerharza, Andreas Grolla, Markus Paulya aDepartment of Statistics, TU Dortmund University, 44227 Dortmund, Germany Abstract Tree-based ensembles such as the Random Forest are modern classics among statistical learning methods. In particular, they are used for predicting univariate responses. In case of multiple outputs the question arises whether we separately ﬁt univaria"
3435,unknown,"possibilities exist that are, e.g. based on modiﬁed splitting or stopping rules for multi-output regression. In this work we compare these methods in extensive simulations to help in answering the primary question when to use multivariate ensemble techniques. Keywords: Machine Learning, Multi-Output Regression, Multivariate Trees 1. Introduction Multivariate data occur in a variety of disciplines,"
3436,unknown,"or econometrics. Data are said to be multivariate if the response not only consists of one variable, but of d≥2 output variables, say Y ∈Rd. Then, we are often interested in ﬁnding a functional relationship be- tween the output Y and some feature variables X ∈Rp, i.e. we want to perform a multivariate (also called multi-output) regression analysis. Unlike univariate multiple regression (with d = 1"
3437,unknown,"multiple features X ∈Rp, multivariate regression wants to specify the relationship of several outcome vari- ables with X simultaneously. The hope of such multivariate analyses is, that the consideration of possible dependencies between the outcomes may lead to procedures with better power (in case of inference) or accu- racy (in case of prediction) compared to separate univariate analyses. While t"
3438,unknown,"and use of valid and distributional robust or nonparametric multivariate methods has been recognized and addressed in inferential statistic (Dobler et al., 2020; Friedrich et al., 2019; Konietschke et al., 2015; Smaga, 2017; Vallejo and Ato, 2012; Zimmermann et al., 2020), there do not exist exhausting studies that exploit the potential of multivariate regression methods for prediction. Focussing "
3439,unknown,"Focussing on tree-based ensemble methods as the Random Forest, it is the aim of this manuscript to close this gap. In particular, we want to answer our research-motivating question: When should a holistic multivariate regression approach be preferred over separate univariate predictions? ∗Corresponding Author Email address: lena.schmid@tu-dortmund.de (Lena Schmid) Preprint submitted to Journal of "
3440,unknown,"arXiv:2201.05340v1 [stat.ML] 14 Jan 2022 The answer to this question is ad hoc not clear. In fact, univariate tree-based ensembles as the Random Forest (Breiman, 2001) or Extra Trees (Geurts et al., 2006) have been shown to be good predictive tools in various applications (Pauly, 2012; Gerke et al., 2018; Schauberger and Groll, 2018; Groll et al., 2019; Huang et al., 2020) and are applied far more"
3441,unknown,"could be that there are still only a few multivariate extensions, such as (De’ath, 2002; Larsen and Speckman, 2004; D’Ambrosio et al., 2017; Zhang, 1998; Zhang and Ye, 2008), where the impurity measure used in the tree construction was multivariatly extended. Zhang (1998) developed an impurity function based on a generalized entropy criterion to handle multiple binary outputs, this work was extend"
3442,unknown,"(2008) to ordinal outputs by transforming the outputs to binary-valued indicator functions. Siciliano and Mola (2000) used the weighted sum of Gini index reduction to construct trees for multiple binary outputs. One of the ﬁrst approaches for multi-output regression trees was proposed by De’ath (2002), which extended CART to multi-output regression by using the sum of squared errors over the multi"
3443,unknown,"impurity function of a node (De’ath (2002) also developed more general forms of distance-based impurity functions). This method is implemented in the R-package MVPART (De’ath, 2012) and was used by Segal and Xiao (2011) to develop multivariate Random Forests. Larsen and Speckman (2004) suggested a multi- variate regression tree that uses the Mahalanobis distance as node impurity function where the"
3444,unknown,"matrix is estimated from the whole data set. The Mahalanobis distance is also used in the R-packages MultivariateRanfomForest and IntegratedMRF (Rahman et al., 2017) to construct multivariate Random Forests. Furthermore, Rahman et al. (2017) have observed in several drug response predictions that multi- variate Random Forests provide higher accuracy than Random Forests when outputs are highly corr"
3445,unknown,"The Eart Mower and Mallows distances were used as impurity functions by D’Ambrosio et al. (2017) to develop regression trees for multivalued numerical outputs.There are also several approaches that extended the GUIDE algorithm (Loh, 2002) to multivariate and longitudinal outputs (Loh and Zheng, 2013; Hsiao and Shih, 2007). Lee (2005) developed a method that can be used with multivariate outputs of"
3446,unknown,"GEE techniques are used to ﬁnd the splits. Multivariate trees for a mixture of categorical and continuous outputs were presented by Dine et al. (2009), where the splits are derived from a likelihood-based approach for a general location model. For Extra Trees, there is also an extension for multi-task learning (Simm et al., 2014), where the split criterion has been adapted to handle multiple tasks"
3447,unknown,"the R-package extraTrees (Simm et al., 2014). To answer the central research question, we compare the predictive accuracy of separate univariate anal- yses with a simultaneous multivariate analysis by means of exhaustive simulations and in an illustrative data analysis. This work is structured as follows: Section 2 presents the univariate and multivariate ensemble methods. More precisely, univaria"
3448,unknown,"addition, the multi-task Extra Trees algorithm Simm et al. (2014) is described in more detail. The simulation design and framework are then presented in Section 3, while Section 4 summarizes the main simulation results. In Section 5, a small illustrative real-world data example is shown before the manuscript concludes with a discussion of our ﬁndings and an outlook for future research (Section 6)."
3449,unknown,"with a discussion of our ﬁndings and an outlook for future research (Section 6). 2 2. Methods In this section, we explain the univariate and multivariate tree-based methods under investigation. We thereby distinguish between three diﬀerent approaches: (i) univariate ensembles such as Random Forest and Extra Trees, (ii) their multivariate counterpart and (iii) multi-task Extra Trees. 2.1. Univariat"
3450,unknown,"2.1. Univariate Tree Ensemble Learner To explain the univariate methods, assume that one has access to a training data set Dn := {(x⊤ i ,yi)⊤∈ Rp+1 : i = 1 ,...,n }consisting of realisations of random vectors with real valued metric outcome Y and p-dimensional metric feature vector X. Random Forest. A Random Forest is a univariate machine learning method based on building ensembles of decision tre"
3451,unknown,"Regression Trees (CARTs) (Breiman et al., 2017). Random Forests consist of a large number of weak decision tree learners, which are grown in parallel to reduce the bias and variance of the model at the same time (Breiman, 2001). For training a Random Forest, N bootstrap samples are drawn from the training dataset. Each bootstrap sample is then used to grow an unpruned tree. Instead of using all av"
3452,unknown,"features in this step, only a small and ﬁxed number of randomly sampled mtry features are selected as split candidates. In the regression case, Breiman (2001) proposed to use mtry = ⌊p/3⌋, which is still the default choice in many software implementations. A split is chosen by the CART-split criterion for regression, i.e. by minimizing the sum of squared errors in both child nodes. These steps are"
3453,unknown,"trees are grown, and new data is predicted by taking the mean of all B tree predictions. The important hyperparameters for the Random Forest are: • B as the number of grown trees. Note that this parameter is usually not tuned since it is known that more trees are better. • The cardinality of the sample of features at every node is mtry. • The number of observations in each bootstrap sample for eac"
3454,unknown,"number of observations in the training dataset. • The minimum number of observations that each terminal node should contain (stopping criteria). Extra Trees. Extra Trees (also called Extremely Randomized Trees) is an ensemble method developed by Geurts et al. (2006) that aims at obtaining trees that are more decorrelated than in the Random Forest approach. Similar to Random Forest, Extra Trees con"
3455,unknown,"However, in contrast to Random Forest, the whole training dataset is used for constructing a tree. For the determination of the next split a random sample of features of size mtry is selected. Instead of computing the locally optimal cut-point for each feature in the random sample based on the CART-split criterion, random cut-points are selected. Then, of all the randomly generated splits, the spl"
3456,unknown,criterion is chosen to split the node. The idea of this extra level of randomness is to improve the process of decorrelation leading to smaller variance and a potentially better predictive accuracy in some situations. The important hyperparameters for the Extra Trees are: 3 • B as the number of grown trees. • The cardinality of the sample of features at every node is mtry. • The number of random s
3457,unknown,"• The number of random split values at each node. • The minimum number of observations that each terminal node should contain (stopping criteria). 2.2. Multivariate Tree Ensemble Learner For the multivariate case, we similarly assume that one has access to a training data set Dn := {(x⊤ i ,y⊤ i )⊤∈Rp+d : i = 1,...,n }which now consists of realisations of random vectors with d-dimensional metric ou"
3458,unknown,"metric outcome Y and p-dimensional X feature vector. To obtain the multivariate versions of Random Forests and Extra Trees, we need to modify the splitting criterion. Here, a natural extension of the CART splitting criteria to multiple metric outputs is to work with the multivariate L2-distance to measure the impurity of a node (Segal and Xiao, 2011). Hence, the impurity function i at node t is gi"
3459,unknown,"i(t) = ∑ yj∈t (yj −y(t))⊤(yj −y(t)), where y(t) is the sample mean of the output vector at node t, see (De’ath, 2002) for details. Thus, to construct a multivariate Random Forest, we simply proceed as in the univariate case and only change the splitting criterion to the multivariate CART-split extension. The same holds for a multivariate Extra Tree, where for each split, the multivariate impurity "
3460,unknown,"chosen splits. 2.3. Multi-Task Extra Trees Another extension of the Extra Trees algorithm was proposed by Simm et al. (2014) in order to handle multi-task learning. For multi-task learning, suppose we have given T supervised learning tasks and all data for the tasks come from the same space X ×Y, where X ⊂Rp and Y ⊂R. Moreover, for each task t we have access to a training data set Dt,n := { (x⊤ i,"
3461,unknown,"have access to a training data set Dt,n := { (x⊤ i,t,yi,t)⊤∈Rp+1 : i= 1,...n } , which is sampled from a distribution Pt on X ×Y. We assume that the Pt are diﬀerent for each task, but related. Simm et al. (2014) modiﬁed the split-criterion for Extra Trees in such a way that samples can now also be additionally split according to their tasks. These new splits will then create two child nodes and"
3462,unknown,"each node contains samples corresponding to separated task subsets. More precisely, when optimizing the split criterion for each node, not only the random feature splits are considered but also a random task split, which is determined as follows: In the regression case, for each task t, the task feature ft are computed by ft = ∑ v∈It yv + α 1 |I| ∑ w∈I yw |It|+ α , where I denotes the set of sampl"
3463,unknown,"the current node and αis a regularization parameter weighting the inﬂuence of the tasks with default value 4 1. Then, the task cut point is randomly selected from the intervall (min tft,maxtft). This modiﬁcation is implemented in the R-package extraTrees (Simm et al., 2014). To predict multivariate outputs with the multi-task Extra Tree algorithm, the data has to be transformed as follows: since e"
3464,unknown,"a multivariate regression problem { (x⊤ i ,y⊤ i )⊤∈Rp+d : i= 1,...,n } , is transformed to { (x⊤ i ,yj i) ∈Rp+1 : i= 1,...,n, j = 1,...,d } , where yi = (y1 i,...y d i)⊤. The multi-task implementation also requires the following task vector task = (1,..., 1,...,d,...,d )⊤∈Rnd, indicating to which component/task the output value yj i belongs. We note, however, that this multivariate extension is on"
3465,unknown,"extension is only feasible as long as the outputs are consumerate, i.e. measured on the same scale. 3. Simulation Set-up To give an answer to our central research question ‘when multivariate tree ensemble approaches should be preferred over separate univariate ones’ we compare the following machine learning approaches: 1. Univariate Random Forests, where for each component of the output vector a s"
3466,unknown,"Random Forest is built. 2. Univariate Extra Trees, where for each component of the output vector a separate univariate Extra Trees model is constructed. 3. Multivariate Random Forests based on the extended impurity function i. 4. Multivariate Extra Trees based on the extended impurity function i. 5. Multi-task Extra Trees as proposed in Simm et al. (2014). In extensive simulations we compare these"
3467,unknown,"simulations were conducted in the statistical computing software R (R Core Team, 2021). For the Multi-task Extra Trees approach we use the extraTrees package (Simm et al., 2014). For all other approaches, we implemented our own tree construction algorithm to allow a fair runtime comparison among them. The concrete simulation settings are described below. Following Loh (2002); Hsiao and Shih (2007)"
3468,unknown,"10 real-valued featuresX1,...,X 10 for which we speciﬁy diﬀerent distributions, dependencies and underlying models: Feature Dependencies and Distributions.Here, in addition to Loh (2002); Hsiao and Shih (2007),X6,...X 10 are iid and independent of X1,...X 5. Furthermore, three diﬀerent dependence structures among the fea- tures X1,...,X 5 are considered as summarized in Table 1. In particular, in "
3469,unknown,"5 we consider completely independent features, where X1 = Z ∼N(0,1) is standard normally distributed, X2 = W ∼exp(1) is standard exponentially distributed, X3 = T ∼t2 is t-distributed with df= 2 degrees of freedom, while X4 = C4 and X5 = C8 are uniformly distributed on [0,4] and [0,8], respectively. This covers symmetric as well as skewed and heavy-tailed distributions. In the other two settings, "
3470,unknown,"dependence between the ﬁrst three features and a strong dependence between the ﬁrst two, respectively, see the last two coloumns of Table 1. Features Independent (ind) Weakly dependent (wd) Strongly dependent (sd) X1 Z Z + W + T W + 0.1Z X2 W W W X3 T T T X4 C4 C2 C2 X5 C8 C8 C8 Table 1: Distributions of X1, . . . , X5 used in the simulation studies. Cm denotes a random variable that is uniformly "
3471,unknown,"on [0, m], T ∼t2, W ∼exp(1), Z ∼N(0, 1) and Cm, T, Wand Z are mutually independent. Models. Various relationships between the output and some of the features are considered as given in Table 2. The ﬁrst six dependent models are designed similarly to those in Loh (2002); Hsiao and Shih (2007). In addition, we also consider the MGAM and linear models. Thereby the error ε∼N(0,Σℓρ) is generated from a"
3472,unknown,"a multivariate normal distribution with covariance matrix Σ ℓρ =   1 ρ ρ ℓ ρ 1 ρ ρℓ ρ 1  . Here, we distinguish between independence ( ρ = 0) and moderate respectively strong correlations ρ ∈ {0.5,0.9}of adjacent components. The correlation between the ﬁrst and last component is either equal to ρ (ℓ = 1) or smaller (ℓ= 2) corresponding to a compound symmetry and autoregressive covariance str"
3473,unknown,"Jump Y = (U + 0.71(X3 >1)1 + ε Quadratic Y = 0.8X2 2 1 + ε Cubic Y = 0.02X3 2 1 + ε Additive Y = 0.71(X3 >1)1 + 0.125 ∑8 i=1 i1(i−1 ≤X5 <i)1 + ε Cross Y = 0.5sgn(X3 −1)X21 + ε Random jump (rjump) Y = sgn(X3 −1)U1 + ε Linear 1 Y = ∑5 i=1 Xi1 + ε Linear 2 Y = ∑10 i=1 Xi1 + ε MGAM1 Y = (X2 1 + log(X2) + cos(X3))1 + ε MGAM2 Y = (0.1 sin(X1),0.5 log(X2),U) + ε MGAM3 Y = (0.1 sin(X1),0.5 log(X2),U) + ∑1"
3474,unknown,"MGAM2 Y = (0.1 sin(X1),0.5 log(X2),U) + ε MGAM3 Y = (0.1 sin(X1),0.5 log(X2),U) + ∑10 i=4 Xi1 + ε Table 2: Diﬀerent dependent models between the output and some of the features. U is a uniform random variable on [0 , 1], 1 = (1, 1, 1)⊤. each setting, we generated samples of size nfrom the respective model with n∈{100,200,500}.In total, this results in 11(models)×5(errors)×3(feature dependencies)×3"
3475,unknown,"for each of the ﬁve ensemble approaches. 6 Choice of Ensemble Parameters.In order not to have to discuss the diﬀerent possibilities for hyperparameter tuning, we use the default values recommended in the literature (Breiman, 2001; Wright and Ziegler, 2017; Hastie et al., 2001). This has the additional advantage of a reduced runtime. Thus, each ensemble learner consists of 500 trees, the inner boot"
3476,unknown,"3 ⌋, the number of sample points N in the bagging step is equal to the number of sample size n. Each terminal node should at least contain ﬁve observations. Following the default values of the multi-task Extra Tree implementation (Simm et al., 2014), the number of random cuts is set to one. Performance Measures. As the three components of the output are computed on the same scale, we use the overa"
3477,unknown,"overall MSE for one setting we used 5 −fold cross-validation and repeated it 1,000 times. Additionally, we also consider the runtime of the algorithms. For each setting we repeated the runtime measurement 1 ,000 times. 4. Results In this section, we describe the results of the simulation study. In particular, we present the overall MSE and the runtime of the diﬀerent construction algorithms under "
3478,unknown,"4.1. Predictive Power For ease of presentation, we aggregated the overall MSE with respect to the 1 ,000 replications and 5 −fold cross-validation. Note that the simulation results of the methods in the setup of weakly dependent features and the MGAM 1 relationship are not presented in the graphics. This is because of their relatively poor performance regarding the predictive power. The methods re"
3479,unknown,"greater than 100,000 (see Figure A.6 of the Appendix). The average MSE for all methods separated by the sample size and the relationship between output vector and features are shown in Figure 1. Here, each boxplot represents 5(errors) ×3(dependencies)= 15 diﬀerent average MSE values. Note that, as MGAM 1 with weakly dependent features has been excluded as explained above, the boxplots contain just"
3480,unknown,"the performance of the methods depends on the relationship between output and features. Except in the MGAM 2 and random jump setup, either the multivariate Random Forest or the multi-task Extra Trees outperformed the other approaches regarding the predictive power. In the MGAM 2 setup, the two uni- variate approaches have the smallest MSE values. Similar results can be observed for MGAM 3, where t"
3481,unknown,"respective diﬀerences between univariate and multivariate approaches are small. In all other setups the two Random Forest approaches performed similarly (linear 1, linear 2, MGAM 1 and quadratic) or the multi- variate outperformed the univariate approach (additive, cross, cubic, jump, rjump). For the Extra Trees, on the other hand, no major diﬀerences in the performance of the univariate and multi"
3482,unknown,"are noticeable except in the case of MGAM2. As the sample size increases, the average MSEs for the two Random Forest approaches decrease in all setups. However, the improvement in the prediction power be- comes smaller with increasing sample size. The same observation can be made for the multivariate Extra Trees, but it is diﬀerent for the multivariate and univariate Extra Trees. While the average"
3483,unknown,7 Quadratic Rjump MGAM 1 MGAM 2 MGAM 3 Jump Linear 1 Linear 2 Additive Cross Cubic 100 200 500 100 200 500 100 200 500 1.2 1.3 1.4 10 20 30 40 50 2 3 4 1.2 1.3 1.4 10 20 30 40 50 1.00 1.05 1.10 1.15 1.10 1.15 1.20 1.25 1.04 1.06 1.08 1.10 1.12 1.14 1.12 1.14 1.16 1.18 1.20 5 10 15 20 25 5.0 7.5 10.0 12.5 Sample Size Average MSE Method et_mt et_multi et_univ rf_multi rf_univ Figure 1: Simulation re
3484,unknown,"(et multi), univariate Extra Trees (et univ), multivariate Random Forest (rf multi) and univariate Random Forest (rf univ) separated by the relation between output and feature and sample sizes. when the sample size is doubled from 100 to 200, the changes in average MSEs are either marginal or the average MSEs increase slightly when comparing sample sizes 200 and 500. Figure 2 summarizes the predic"
3485,unknown,"output vector and features and the dependency structure of the features. The prediction power of the weakly dependent features of the MGAM 1 setup is shown in Figure A.6 of the Appendix. When the features are weakly dependent, all methods perform poorly in the linear 1, linear 2 and MGAM 1 setup. Overall, the average MSEs are more than twice as large as for the other two feature dependence structu"
3486,unknown,"these settings, the feature dependency structure had only a slight eﬀect on the prediction power. In the MGAM 2 setup, it is noticeable that the average MSEs of the multivariate approaches decrease slightly 8 Quadratic Rjump MGAM 1 MGAM 2 MGAM 3 Jump Linear 1 Linear 2 Additive Cross Cubic et_mt et_multiet_univrf_multirf_univ et_mt et_multiet_univrf_multirf_univ et_mt et_multiet_univrf_multirf_univ"
3487,unknown,1.2 1.3 1.4 10 20 30 40 50 2 3 4 1.2 1.3 1.4 10 20 30 40 50 1.00 1.05 1.10 1.15 1.10 1.15 1.20 1.25 1.04 1.06 1.08 1.10 1.12 1.14 1.12 1.14 1.16 1.18 1.20 5 10 15 20 25 5.0 7.5 10.0 12.5 Method Average MSE Structure ind wd sd Figure 2: Simulation results on the average MSE for dependency structure of the features separated by the relation between output and feature and all methods multi-task Extra
3488,unknown,"Trees (et univ), multivariate Random Forest (rf multi) and univariate Random Forest (rf univ). Each boxplot represents 5(errors)×3(sample sizes)=15 diﬀerent average MSE values. with increasing feature dependence structure, but the average MSEs of the univariate methods increase. In all other setups, the multivariate and univariate approaches behave the same with increasing dependence structure in "
3489,unknown,"structure in the features. The inﬂuence of the diﬀerent errors on the prediction power can be seen in Figure 3. Generally, the diﬀerent errors had little impact on the prediction power. For setting linear 1, linear 2, MGAM 1, MGAM 3 and quadratic are no diﬀerences in the average MSE values between the errors for all methods. In all other setups, the average MSEs of the multivariate Random Forest i"
3490,unknown,"structure in the outputs. 4.2. Runtime The runtimes for all diﬀerent setups are given in Figures A.7 and A.8 of the Appendix. Studying the results in detail, we realize that the dependency structures of the features and the dependency structures of 9 Quadratic Rjump MGAM 1 MGAM 2 MGAM 3 Jump Linear 1 Linear 2 Additive Cross Cubic et_mt et_multiet_univrf_multirf_univ et_mt et_multiet_univrf_multirf"
3491,unknown,"et_mt et_multiet_univrf_multirf_univ 1.2 1.3 1.4 10 20 30 40 50 2 3 4 1.2 1.3 1.4 10 20 30 40 50 1.00 1.05 1.10 1.15 1.10 1.15 1.20 1.25 1.04 1.06 1.08 1.10 1.12 1.14 1.12 1.14 1.16 1.18 1.20 5 10 15 20 25 5.0 7.5 10.0 12.5 Method Average MSE Y cor=0,l=1 cor=0.5,l=1 cor=0.5,l=2 cor=0.9,l=1 cor=0.9,l=2 Figure 3: Simulation results on the average MSE for dependency structure of the outputs ated by t"
3492,unknown,"and feature and for all methods multi-task Extra Trees (et mt), multivariate Extra Trees (et multi), univariate Extra Trees (et univ), multivariate Random Forest (rf multi) and univariate Random Forest (rf univ). The boxplots in the MGAM 1 setup represent 6 diﬀerent average MSE values and all other boxplots represent 3(feature dependencies) ×3(sample sizes)=9 diﬀerent average MSE values. the outpu"
3493,unknown,"of the multiple simulation settings with regard to both dependency structures and the number of repetitions in Table 3. It summarizes the mean runtime for each construction method separated by the sample sizes and the relationship between output and features. Since the multi-task method was programmed runtime- eﬃciently in Java (Simm et al., 2014), unlike the other methods, this method is the fast"
3494,unknown,"runtimes are faster than 0.6 seconds for all settings. In addition, the runtime behaves approximately linear to the sample size with about 0.1 seconds for n= 100 up to 0 .6 seconds for n= 500. Considering our implementations, we ﬁrst note that the Extra Trees algorithms are faster than the Ran- dom Forest algorithms. The multivariate Random Forest requires on average (overall settings and sample"
3495,unknown,"sizes) 25 times as long as the multivariate Extra Trees. In the univariate case, the Random Forest takes on average 18 times as long as the Extra Trees algorithm. This time diﬀerence is probably due to the 10 n= 100 n= 200 n= 500 Setting et mt etmulti etuniv rfmulti rfuniv etmt etmulti etuniv rfmulti rfuniv etmt etmulti etuniv rfmulti rfuniv Linear 1 0.09 35.12 90.72 197.48 351.47 0.19 57.51 148.4"
3496,unknown,Linear 2 0.09 35.59 92.35 198.22 352.56 0.19 61.49 158.59 480.20 822.13 0.52 112.06 286.17 1485.69 2435.09 Additive 0.09 24.48 51.11 205.52 365.07 0.19 21.68 49.51 501.73 854.92 0.52 11.56 28.56 1511.66 2476.85 Cross 0.10 35.07 73.51 200.93 354.34 0.20 45.65 92.20 496.67 838.58 0.53 28.47 67.30 1574.91 2549.15 Cubic 0.10 34.59 70.13 202.80 354.38 0.20 43.17 83.76 512.42 852.50 0.53 21.75 53.67 169
3497,unknown,Jump 1 0.09 34.40 69.35 199.73 352.07 0.19 40.80 78.93 494.31 835.34 0.53 15.27 40.26 1603.17 2602.08 MGAM 1 0.09 31.47 79.42 207.01 361.96 0.20 42.71 106.84 515.78 862.85 0.53 40.20 99.86 1661.02 2639.98 MGAM 2 0.09 37.89 71.30 197.38 351.83 0.20 51.81 85.44 494.44 846.71 0.55 30.39 49.63 1541.62 2594.42 MGAM 3 0.09 35.77 86.17 185.40 332.39 0.19 62.06 145.50 447.06 773.65 0.51 117.51 264.01 1389
3498,unknown,"Quadratic 0.09 31.98 76.54 204.49 356.83 0.19 42.24 98.86 509.71 851.53 0.52 33.67 79.25 1625.67 2595.54 Rjump 0.09 34.15 69.65 197.58 348.84 0.20 40.77 80.19 486.75 822.54 0.53 16.09 41.99 1572.21 2545.54 Table 3: Average runtimes (in seconds) for the dependency models between the output and the features, aggregated over all dependency settings of the outputs and features and all repetitions for "
3499,unknown,"Extra Trees (et multi), univariate Extra Trees (et univ), multivariate Random Forest (rf multi) and univariate Random Forest (rf univ). diﬀerent choices of split values in the methods. Extra Trees randomly selects the split values, while Random Forest uses an exhaustive search to ﬁnd these values. More important for us is a comparison between the multivariate and the univariate approaches: the res"
3500,unknown,"require less runtime than the univariate approaches. In fact, the multivariate Extra Trees show a decrease in runtime between 38% and 62% compared to the univariate approach. Using the multivariate Random Forest approach decreases runtime by 37% to 44% compared to the corresponding univariate method. For both Random Forest approaches, average runtimes increase similarly with increasing sample size"
3501,unknown,"sample size doubles from 100 to 200, runtimes for both approaches increase by 127-153%, while runtimes increase by 190-231% when sample size changes from 200 to 500. The diﬀerent relations between output and features have a small eﬀect on runtimes, as runtimes for ﬁxed sample sizes (compared to the fastest set- ting MGAM 3) increase by 6-22% for multivariate Random Forests and 3-17% for univariate"
3502,unknown,"A diﬀerent observation can be made for the multivariate and univariate Extra Trees. When the sample sizes are increased, their runtime behaviour varies from setting to setting, see Figure 4. For the settings linear 1, linear 2 and MGAM 3, the runtimes increase for increasing sample sizes. Note that these are also the settings with the largest runtimes for a ﬁxed sample size. However, the setting a"
3503,unknown,"runtimes and decreases slightly with increasing sample size. The runtimes of the other settings increase by 13.81-36.75% when the sample size is doubled from 100 to 200, but decrease by 5.87-62.56% when increasing the sample size from 200 to 500. 5. Illustrative Real-World Data Example We consider the concrete slump test study (Yeh, 2007) taken from the UCI Machine Learning Repository (Dua and Gra"
3504,unknown,"are ingredients of concrete and measured in kg /m3): cement, slag, ﬂy ash, water, superplasticizer, coarse aggregate and ﬁne aggregate and three continuous output variables: slump (in cm), ﬂow (in cm) and 28-day compressive strength (cs; in Mpa). Slump is the vertical height by which a cone of wet concrete sinks while ﬂow is the horizontal distance by which the concrete spreads. Both variables are"
3505,unknown,11 et_multi et_univ 100 200 500 100 200 500 0 100 200 Sample Size Average Runtime Setting Additive Cross Cubic Jump Linear1 Linear2 MGAM1 MGAM2 MGAM3 Quadratic Rjump Figure 4: Average runtimes for multivariate (left) and univariate Extra Trees (right) aggregated over all dependency settings of the outputs and features and all repetitions. the viscosity of concrete. The dependencies within the outp
3506,unknown,"slump and ﬂow are highly positively correlated and the correlation between cs and slump/ﬂow is slightly negative. In the following analyses, the outputs were standardized to zero mean and unit variance. We now investigate our central research question ‘when multivariate tree ensembles approaches should be preferred over separate univariate ones’ for this speciﬁc dataset. Therefore, we construct th"
3507,unknown,"diﬀerent dimensions of outputs (uni-, bi- and trivariate) using all the methods presented in Section 2. To compare the overall MSE of the approaches, we applied 5 −fold cross-validation and repeated it 100 times. As reported in Table 4, the RF approaches have the smallest MSE values (between 0 .347 and 0.657) in all output dimension settings. When comparing the univariate and multivariate approach"
3508,unknown,"approaches show almost similar results with a slight advantage (at the second or third decimal position) for the univariate RF. For the Extra Trees the observation is vice versa, i.e. multivariate Extra Trees lead to a slightly lower MSEs than the univariate ones in all multivariate settings. univariate bivariate trivariate Method slump ﬂow cs slump and ﬂow slump and cs ﬂow and cs slump, ﬂow and c"
3509,unknown,RF uni 0.657 0.583 0.347 0.620 0.502 0.465 0.529 RF multi 0.657 0.583 0.347 0.619 0.523 0.497 0.563 ET uni 0.809 0.722 0.594 0.766 0.701 0.658 0.708 ET multi 0.809 0.722 0.594 0.761 0.674 0.634 0.683 ET mt 0.686 0.588 1.469 1.077 1.050 1.039 1.088 Table 4: Average MSE of the construction methods presented in Section 2 using concrete dataset based on 5-fold crossvalidation and 100 repetations. 12 C
3510,unknown,"and 100 repetations. 12 Corr: 0.906 Corr: −0.223 Corr: −0.124 slump flow cs slumpflowcs 0 10 20 30 20 40 60 80 20 30 40 50 60 0.00 0.02 0.04 0.06 20 40 60 80 20 30 40 50 60 Figure 5: Plots of the output vector (using the unscaled data): Empirical Pearson correlation coeﬃcients (upper triangle), density plots (diagonal) and scatterplots (lower triangle). To compare the prediction accuracy of our ap"
3511,unknown,"To compare the prediction accuracy of our approaches to MVPART (De’ath, 2012), univariate and multivariate GUIDE (Loh and Zheng, 2013), we follow Loh and Zheng (2013) and apply leave-one-out cross- validation to estimate the sum of MSEs of the trees, where the sum is over the three output variables. In contrast to Loh and Zheng (2013), we do not prune the trees in our approaches. The results are s"
3512,unknown,"in Table 5. Note that we include the results of Loh and Zheng (2013) in the table without performing their experiments ourselves. While the multivariate Extra Trees outperformed their univariate method by decreasing the sum of MSE by 5.941%, the univariate RF and GUIDE slightly decreased the sum of MSEs by 5.458% to 7.15%s compared with their multivariate approaches. In general, the Random Forest "
3513,unknown,have the smallest sum of MSEs (1.594/1.681). Methods RF uni RF multi ET uni Et multi ET mt MVPART GUIDE uni GUIDE multi Sum of MSE 1.594 1.681 2.12 2.02 3.42 2.096 1.957 2.097 Table 5: Sum of average MSEs of methods considered in Section 2 and Loh and Zheng (2013) using concrete data set based on leave-one-out crossvalidation. 13 6. Conclusion and Outlook The main purpose of this simulation study 
3514,unknown,"Forests and Extra Tree algorithms with multivariate approaches in case of multivariate outputs. In most of the simulation settings it was clearly shown that either the Random Forest or the Extra Tree approaches have yielded better performances than the other methods. However, when comparing the multi- variate approaches with their univariate counterparts, then in two of the simulation settings (MG"
3515,unknown,"MGAM 3) advantages for the univariate approaches considering the performance (average MSE) could be found. In all other simulation settings the multivariate approaches have shown at least similar or even better performances than the univariate approaches. Especially, when comparing univariate and multivariate Ran- dom Forest approaches, in some of the considered settings the performances of the mu"
3516,unknown,"were substantially better. Moreover, for all methods, the diﬀerent dependency structures within the covari- ates did have an impact on the methods’ performance. However, it was virtually the same for both, the univariate approaches and the multivariate approaches meaning not one approach had an advantage over the others. Another interesting ﬁnding was that correlation within the outputs only showe"
3517,unknown,"impacts on the performance of the multivariate Random Forests in some simulation settings, while for all other approaches this had little to no impact at all. While the diﬀerences of the univariate and multivariate approaches’ predictive performance were small to moderate, a huge diﬀerence in the runtime could be noted. Here, the multi-task Extra Tree method was the fastest approach. However, it w"
3518,unknown,"tion study. For a fair runtime comparison the other approaches were implemented in a comparable way and it was shown that the multivariate approaches have a huge runtime advantage over the univariate approaches. Last but not least, with a real data example it was shown that the multivariate approaches can im- prove the performance when considering multivariate outputs, in this case especially for "
3519,unknown,"approaches. However, for the Random Forest approaches the multivariate counterpart could only improve the performance in one of the bivariate cases. As only regression problems with numeric outputs were considered, future simulation studies should investigate whether the same potential for improvement can also be found for multivariate classiﬁcation problems. Also, mixed problems with numeric and "
3520,unknown,"tigated. Moreover, as in this study only rather low sample sizes were investigated, the behavior of these approaches in big data settings with larger sample sizes should also be further researched. References Breiman, L., 2001. Random Forests. Machine Learning 45, 5 – 32. doi: https://doi.org/10.1023/A:1010933404324. Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J., 2017. Classiﬁcation and R"
3521,unknown,"D’Ambrosio, A., Aria, M., Iorio, C., Siciliano, R., 2017. Regression trees for multivalued numerical response variables. Expert Systems with Applications 69, 21–28. URL: https://www.sciencedirect.com/science/article/pii/S0957417416305528, doi:https://doi.org/10.1016/j.eswa.2016.10.021. 14 De’ath, G., 2002. Multivariate Regression Trees: A New Technique for Modeling Species-Environment Relationship"
3522,unknown,"83, 1105–1117. De’ath, G., 2012. MVPART: Multivariate Partitioning. R package version 1.6-0. Dine, A., Larocque, D., Bellavance, F., 2009. Multivariate trees for mixed outcomes. Computational Statistics & Data Analysis 53, 3795–3804. URL: https://www.sciencedirect.com/science/article/pii/S0167947309001376, doi: https://doi.org/ 10.1016/j.csda.2009.04.003. Dobler, D., Friedrich, S., Pauly, M., 2020"
3523,unknown,"Mathematics 72, 997–1022. Dua, D., Graﬀ, C., 2017. UCI machine learning repository. URL: http://archive.ics.uci.edu/ml. Friedrich, S., Konietschke, F., Pauly, M., 2019. Resampling-based analysis of multivariate data and repeated measures designs with the R package MANOVA.RM. R J. 11, 380. Gerke, J., Koenig, A.M., Conrad, D., Doyen-Waldecker, C., Pauly, M., G¨ undel, H., Wilker, S., Kolassa, I.T., "
3524,unknown,"maltreatment as risk factor for lifetime depression: The role of diﬀerent types of experiences and sensitive periods. Mental Health & Prevention 10, 56–65. Geurts, P., Ernst, D., Wehenkel, L., 2006. Extremely randomized trees. Machine Learning 63, 3–42. URL: https://doi.org/ 10.1007/s10994-006-6226-1 . Groll, A., Ley, C., Schauberger, G., Van Eetvelde, H., 2019. A hybrid random forest to predict s"
3525,unknown,"tournaments. Journal of Quantitative Analysis in Sports 15, 271–287. Hastie, T., Tibshirani, R., Friedman, J., 2001. The Elements of Statistical Learning. Springer Series in Statistics, Springer New York Inc. Hsiao, W.C., Shih, Y.S., 2007. Splitting variable selection for multivariate regression trees. Statistics & Probability Letters 77, 265–271. Huang, H., Pouls, M., Meyer, A., Pauly, M., 2020. "
3526,unknown,"Conference on Computational Logistics, Springer. pp. 412–427. Konietschke, F., Bathke, A.C., Harrar, S.W., Pauly, M., 2015. Parametric and nonparametric bootstrap methods for general MANOVA. Journal of Multivariate Analysis 140, 291–301. Larsen, D.R., Speckman, P.L., 2004. Multivariate regression trees for analysis of abundance data. Biometrics 60, 543–549. Lee, S.K., 2005. On generalized multivar"
3527,unknown,"1105–1119. Loh, W.Y., 2002. Regression trees with unbiased variable selection and interaction detection. Statistica Sinica 12, 361–386. Loh, W.Y., Zheng, W., 2013. Regression trees for longitudinal and multiresponse data. The Annals of Applied Statistics 7, 495 – 522. doi: 10.1214/12-AOAS596. Pauly, O., 2012. Random forests for medical applications. Ph.D. thesis. Technische Universit¨ at M¨ unchen"
3528,unknown,"R Core Team, 2021. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. Vienna, Austria. URL: https://www.R-project.org/. Rahman, R., Otridge, J., Pal, R., 2017. IntegratedMRF: random forest-based framework for integrating prediction from diﬀerent data types. Bioinformatics 33, 1407–1410. doi: 10.1093/bioinformatics/btw765. Schauberger, G., Groll, A., 20"
3529,unknown,"Modelling 18, 460–482. Segal, M., Xiao, Y., 2011. Multivariate random forests. Wiley interdisciplinary reviews: Data mining and knowledge discovery 1, 80–87. Siciliano, R., Mola, F., 2000. Multivariate data analysis and modeling through classiﬁcation and regression trees. Computational Statistics & Data Analysis 32, 285–301. URL: https://www.sciencedirect.com/science/article/pii/S0167947399000821,"
3530,unknown,"doi:https://doi.org/10.1016/S0167-9473(99)00082-1. Simm, J., De Abril, I.M., Sugiyama, M., 2014. Tree-based ensemble multi-task learning method for classiﬁcation and regression. IEICE TRANSACTIONS on Information and Systems 97, 1677–1681. Smaga, L., 2017. Bootstrap methods for multivariate hypothesis testing. Communications in Statistics-Simulation and Com- putation 46, 7654–7667."
3531,unknown,"putation 46, 7654–7667. Vallejo, G., Ato, M., 2012. Robust tests for multivariate factorial designs under heteroscedasticity. Behavior Research Methods 44, 471–489. Wright, M.N., Ziegler, A., 2017. ranger: A fast implementation of random forests for high dimensional data in C++ and R. Journal of Statistical Software 77, 1–17. doi: 10.18637/jss.v077.i01. 15 Yeh, I.C., 2007. Modeling slump ﬂow of co"
3532,unknown,"Concrete Composites 29, 474–480. doi: 10.1016/0003-4916(63)90068-X. Zhang, H., 1998. Classiﬁcation Trees for Multiple Binary Responses. Journal of the American Statistical Association 93, 180–193. Zhang, H., Ye, Y., 2008. A tree-based method for modeling a multivariate ordinal response. Statistics and its Interface 1, 169–178. doi: 10.4310/sii.2008.v1.n1.a14. Zimmermann, G., Pauly, M., Bathke, A.C"
3533,unknown,"matrices and non-normal responses. Journal of Multivariate Analysis 177, 104594. 16 Appendix A. Additional Simulation Results 100 200 500 et_mt et_multiet_univrf_multirf_univ et_mt et_multiet_univrf_multirf_univ et_mt et_multiet_univrf_multirf_univ 3500000 3550000 3600000 1070000 1080000 1090000 1100000 292000 296000 300000 304000 Method Average MSE Figure A.6: Average MSE for the setup of weakly "
3534,unknown,"separated by the sample size and the methods multi-task Extra Trees (et mt), multivariate Extra Trees (et multi), univariate Extra Trees (et univ), multivariate Random Forest (rf multi) and univariate Random Forest (rf univ). Each boxplot represents 5 average MSE values. 17 Quadratic Rjump MGAM1 MGAM2 MGAM3 Jump Linear1 Linear2 Additive Cross Cubic et_mt et_multiet_univrf_multirf_univ et_mt et_mul"
3535,unknown,et_mt et_multiet_univrf_multirf_univ 0 1000 2000 3000 0 1000 2000 3000 0 1000 2000 0 1000 2000 3000 0 1000 2000 3000 0 3000 6000 9000 0 1000 2000 3000 0 1000 2000 3000 0 1000 2000 3000 0 1000 2000 3000 0 1000 2000 3000 Method Runtime Figure A.7: Simulation results on the runtime for the feature dependence settings separated by the relation between output and feature and all methods multi-task Extr
3536,unknown,"multivariate Random Forest (rf multi) and univariate Random Forest (rfuniv). The arranged boxplots in each method segment correspond to the following feature dependence structures: (from left to right) independent (red), weakly dependent (green), and strongly dependent (blue). Each boxplot contains 15,000 runtime measurements. 18 Quadratic Rjump MGAM1 MGAM2 MGAM3 Jump Linear1 Linear2 Additive Cros"
3537,unknown,Additive Cross Cubic et_mt et_multiet_univrf_multirf_univ et_mt et_multiet_univrf_multirf_univ et_mt et_multiet_univrf_multirf_univ 0 1000 2000 3000 0 1000 2000 3000 0 1000 2000 0 1000 2000 3000 0 1000 2000 3000 0 3000 6000 9000 0 1000 2000 3000 0 1000 2000 3000 0 1000 2000 3000 0 1000 2000 3000 0 1000 2000 3000 Method Runtime Figure A.8: Simulation results on the runtime for the output dependence
3538,unknown,"feature and all methods multi-task Extra Trees (et mt), multivariate Extra Trees (et multi), univariate Extra Trees (et univ), multivariate Random Forest (rf multi) and univariate Random Forest (rfuniv). The arranged boxplots in each method segment correspond to the following dependence structures: (from left to right) cor=0 and ℓ = 1, cor=0.5 and ℓ = 1, cor=0.5 and ℓ = 2, cor=0.9 and ℓ = 1 and co"
3539,unknown,"19 _h. ,%'+, _. 1994 Universal Cokriging Under Intrinsic Coregionalization ~ Jeffrey D. Helterbrand 2""+ and Noel Cressie 2 [ lhlPF l/l(' illll'ill.*i( corr IIIodi'/. i~ J~ol]l prlllkll~ lllld ~c~olld~lrx nl('(l~Hll'ttl('lll.~ lit(' ,:1~111~11~1~' ill ~1// ',lllllll/l' IOi lllitql~. Ilk' ~~JIl~'llll~*llill k'~'~&lll~lRlll Ilixdottl i~ I11r i ~l'ik, lllk, 111~lldd~ Ilhl]\ Iht' StlltlU ~ohlliqql ~1~ "
3540,unknown,", ~tllll[)l{'~ hllVC h('UIt l,,ix'l'll iihClk"" 111911Z~'ll; sdloncltll~V l+lklik, itlk r Ildl,ghls hdvl' (~[(lll'rt'd Illld('l 1111~ ,/~dlllgl dt.pt'lldl'lll(' Slrllc llll(' ]ills Iiii1(' Jdt'lllillls Ihr ('tllldJlJotls IIIId{'l"" II]lJ( ]l st'i llllddr~ [?lfllt'l?ltlllllll e ~ ll*<'/il/ IIIldl't"" f/IU r ~lllllllf]Oll Q/ inlrinsi~ I'ordk'il~lllllizr ..|n il/llXllr is k, ixUII IlSllll~ iI ~J~Aldl~l'"
3541,unknown,"'1 I)]llloIlllllll (11111 ~mlcrt( ittm ( +~ll('Ctlll'dllOll~ co/]c('tcd./r,.n a rd.k'ioll ~)1 tit(' ,'x,'l'x'(/(h/ ]'('~t ,%'ild. KEY %VORI)S: mulli\ ariatc spaltal dep(_'nduncc rnndcls, partial cmariancc, restricted linear models. INTRODUCTION Cokriging produces predictors that use nc)l only inlomlation from direct mea- ',urenlents of the spatial component process being considered, but also the i"
3542,unknown,"Ionllation frona nlcasurements of a secondary component process. Let s~. s,, 9 . , % be sample locations xvherc |neasurcments on both component processes ,re avuilablc. For the time being, v,e shall LISF, UlllC no missing data at the sample Locations. Let Z,(sa) be the measuremenl for the ith component at location s a (k I ..... n: i = I, 2). and Z(st) denote the bivariate vector at location s a. "
3543,unknown,"\Vc shall retcr to the first component process, ZL(. ) us the primar) process and /,(') as the secondary' process. Define the data vector Z =- (Z(sl)/. Z(s2) l, .... Z(s,,)/) ~. Suppose we arc interested in predicting the primary process Z L iS41) at unsanlpled location s,. The cokriging predictor for Z I(s,} given the sawtple infl+m+mtion is t+t the It + FI]I, ZI (s,,) \' ~"" = ~ ~ X,aZ,(s a) (1) "
3544,unknown,"ZI (s,,) \' ~"" = ~ ~ X,aZ,(s a) (1) Rccci~cd II Noxcmhcr It,)~)2: accepted f~ .hmtlar,, It4""43 I)cparllncnt ol ,~[allSlids. Io'.'.a Y, lalc Unixcrsil 3 . Ames. ]ox~a 5{RJl I I'rcscnl addrcsn: Jcflrc\ l) Hcltcrhrund. Senior ScicnliM, l.tlb, Research l.ahorahu-lc.., [ndtanap ,,h',. Indiunu 41~2,~5. 205 ~F~,=,2 "",1.!1 'J~ 1~2041 ~l.'ql+~ql - ~'4~ I HI'*4 hlqum4h.uknl \,~,,, udn,,u h,t \141hcm,ntn~ d "
3545,unknown,"206 Helterbrand and Cressie where the cokriging weights { X/k: k = 1 ..... n; i = 1, 2} satisfy the uniform unbiasedness constraint, E(E~= i E~= i XikZi(sD) = E(Z~ (so)). These cokriging weights are selected to minimize E Zl(so) - kikZi(sk) (2) i=1 ""= subject to the unbiasedness constraint. To obtain the optimal cokriging weights requires knowledge of models of the spatial correlation for both the"
3546,unknown,"the spatial correlation for both the primary and the secondary components as well as for the cross-correlation between the two processes. The bivariate spatial dependence model is said to be intrinsically coregionalized if the assumed spatial dependence structure is I E 1 C (s,, s,,,) --- = I_C21(Sk, Sin) C22(Sk, Sm)J 7, ~ ~-3 where c0(s k, Sin) = COV [Zi(s~), Zj(Sm)], c(s,, S,,,) is a valid covar"
3547,unknown,"3' > 0, and I ol -< I. Under the intrinsic coregionalization model, if both primary and secondary measurements are available at all sample data locations, the conventional geo- statistical wisdom is that cokriging provides exactly the same solution as uni- variate kriging on the primary component alone. Specifically, all secondary cokriging weights are zero. In the Summer 1992 issue of Geostatisti"
3548,unknown,"hlterdisciplinar3., Geostatistics Newsletter, an example was given where, under the intrinsic coregionalization assumption with all sample data available, the secondary cokriging weights were nonzero. The question was posed regarding when secondary information is useful under the assumption of intrinsic core- gionalization. In this note we clarify when the secondary cokriging weights may be non- z"
3549,unknown,"zero. In particular, we investigate properties of the cokriging model under in- trinsic coregionalization and determine conditions where cokriging provides the same solution as univariate kriging. We also investigate what happens to the secondary cokriging weights when an additional primary or secondary obser- vation is available at a location s,, + ~. The results of this paper extend imme- diatel"
3550,unknown,"diately to the m-variate cokriging case (cf. Myers, 1982) if an m-variate intrinsic coregionalization model is assumed. Finally, cokriging under intrinsic core- gionalization is illustrated using a dataset of plutonium and americium concen- trations collected from a nuclear testing area on the Nevada Test Site. But first, we argue that the intrinsic coregionalization model is very restrictive and "
3551,unknown,inappropriate. This is done in order to dispel any thoughts one might have that multivariate spatial statistics is an unnecessary complication in which the in- corporation of covariate infomlation is not useful.
3552,unknown,"Universal Cokriging 207 The Intrinsic Coregionalization Model is Restrictive The intrinsic coregionalization assumption for a bivariate spatial depen- dence model is very restrictive and is often not appropriate. Should microscale variation and/or measurement error variation be present in a bivariate spatial dependence model, the model is no longer intrinsically coregionalized. Even when a small n"
3553,unknown,"when a small nugget effect due to microscale variation is present, the secondary cokriging weights may be nonzero. For a simple example in the plane, suppose that there are two sample locations, s~, and s2, one on either side of the point So where the primary value Z~ (So) is to be predicted. The closer location, s~, is 50 meters due west to So, and s2 is 150 meters due east. We wish to predict Z~"
3554,unknown,"Z~ (so) using information Z(s~) and Z(s2). Suppose the bivariate autocovariance function is an intrinsically coregionalized exponential model given by (3) with 3' = 1, 0 = 0.5, and spatial-dependence parameter -3/200. That is, the bivariate spatial dependence model can be expressed as, [, 0;] (3 ) C(s~, Sin) = exp [Isk - stall (4) o.5 so that the univariate covariogram is isotropic. The resulting "
3555,unknown,"o.5 so that the univariate covariogram is isotropic. The resulting simple cokriging weights (simple cokriging will be discussed momentarily) are Xll = .468, ~u2 = .082, X2~ = 0, and X22 = 0. We see that the secondary cokriging weights are zero. Now incorporate microscale variation into the bivariate spatial de- pendence structure. Specifically, consider C (sk, s,,) = 1 exp 0.5 EOo' ol ) + exp -- s"
3556,unknown,"C (sk, s,,) = 1 exp 0.5 EOo' ol ) + exp -- s.,ll (5) 0.25 Under this modified spatial dependence model we obtain the simple cokriging weights, ;ktl = .289, Xt2 = .056, ~,21 = .072, and X22 = .011. Here, the secondary information is used in the simple cokriging predictor. A second remark is that the assumption of intrinsic coregionalization carries with it the assumption of a symmetric spatial cros"
3557,unknown,"with it the assumption of a symmetric spatial cross-covariance function. That is, cO(s k, s,n) = cji(sk, Sin). This symmetry need not be tree in general (e.g., Ver Hoef and Cressie, 1993). Third, it isoften necessary to estimate unknown parameters present in the valid univariate covariance function c(- ,-) given in Eq. (3). In this case, sec- ondary information should be used in conjunction with p"
3558,unknown,"ondary information should be used in conjunction with primary information to obtain the most efficient estimators. Thus, the secondary information should not be ignored. For these reasons, one should not view multivariate spatial prediction as"
3559,unknown,"208 Helterbrand and Cressie an unneccessary complication to a univariate problem. As in standard multivari- ate analysis, multivariate spatial prediction allows covariate knowledge to yield more precise predictions. SIMPLE COKRIGING We can determine the conditions under which the intrinsically coregion- alized cokriging model produces zero secondary weights by exploring its prop- erties in the sim"
3560,unknown,"erties in the simple cokriging environment. Consider the following decompo- sition for the component processes. Let Zi(s) = ~i(s) + 5i(s); s ~ D, i = 1, 2 (6) where D is the spatial domain of interest, E(Z,(s)) = p.i(s), t~i(s) is referred to as the large-scale variation term, and 6~(s) is a zero-mean process including smooth small-scale variation, microscale variation, and measurement error (see "
3561,unknown,"Cressie, 1991, p. 112 for further details). The process 5i is referred to as the small-scale variation for the ith component. The large-scale structure for a component process is usually unknown. Often we pavameterize #,(') in a linear manner. That is. we assume ~i(s) = Xi(s)r~/ (7) where X,(s) is most generally a (pi  l) vector of fixed ""explanatory"" variables for the ith process and [~, is a (p"
3562,unknown,"for the ith process and [~, is a (p,  1) vector of corresponding unknown parameters. The parameters [~, are referred to as the large-scale parameters for the ith process; i = 1, 2. In simple cokriging, it is assumed that the p~(') processes are known in full. For example, in Eq. (7), all p = Pl + P2 large-scale parameters, p = (p~, p~)r, are fixed and known, say equal to pc, = ([~,,r ~OT)T. The l"
3563,unknown,"parameter space for simple cokriging is thus f2 = { I~""} (8) a single point in p-dimensional space. Since p.j(-) is known for i = 1,2, simple cokriging lbcuses on the prediction of 5t (so) based on 5i(sk) = Zi(sk) - ~i(sk); k = 1, 2 ..... n, i = 1, 2 (9) We now establish that the secondary simple cokriging weights will always be zero under the intrinsic coregionalization assumption. We arrive at t"
3564,unknown,"Universal Cokriging 209 Simple Cokriging Equations Consider the simple cokriging equations for predicting 8~ (so) based on 6 = (6 (sl) T, 6(s2) T, .... 6(s~)r) r, which are r {X,,.c,,(sk, s,,,) + X2,,,c,2(sk, s,,,)} = c~(sk, So), rn=l n {)KImCI2(S k, Sin) q- )k2mC22(Sk, Sin)} = CI2(Sk, S0), m= I k= 1,2 ..... n k= 1,2 ..... n (10) By the intrinsic coregionalization assumption, we know cl~(sk, s,,,)"
3565,unknown,"(10) By the intrinsic coregionalization assumption, we know cl~(sk, s,,,) = c(sk, s,,,), cl2(s~, s,,,) = c21(s ~, s,,,) = ""ypc(sk, s,,,), and C2e(S k, Sin) = 3'2C'(Sk. S,,,), which reduces the simple cokriging equations to Pl {X,,,, + 3,o;',z,,,}c(s~, s,,,) = c(sk, So), m = I tl {3,pX.,, + ""r2X2,,,}c(s~, s,,,) = 3,pc(sk, so), rr 1 k = 1,2 ..... n k = 1.2 ..... n (11) The simple cokriging equations"
3566,unknown,"{X.,, + 7pX2,,,}c(sk, s,,,) = c(sk, so), k = I, 2 ..... n m = ] I 1 ~] )""Jm + 3' X2,, ' C(Sk, %.) = C(S~, SO), k = 1, 2 ..... n (12) m = 1 p If we define X,,* -= XI,,, + "",/OX2,,, and k,,** --- X~,~ + (3'/P) X2,,, (m = 1,2 ..... n), we observe that each set of equations represents exactly the equations for standard univariate simple kriging. Furthermore, since in general lal ~ 1 and we must have X"
3567,unknown,"we must have X,* = X**; m = 1, 2 ..... n, we see that the secondary weights {X2m: m = 1, 2 ..... n} must be zero. (If ]Pl = 1, Eq. (12) does not have a unique solution). Least Squares Solution for k Recall that 6 = (8(s0 r, ~i(s2) r ..... ~5(s.)7) r. Denote the corresponding variance-covariance matrix for ti as T =- cov (6). Under the intrinsic coregion- alization assumption, T takes on a special "
3568,unknown,"210 Helterbrand and Cressie [ c(s~, sO c(s~, s2) c(st, s2) c(s2, s~) T = LC(St, S,) ... p-y -y- j ""'"" c(s,, s,,)l P3' 3 `2 (13) = ((a,jB)),,,t ,  where | denotes the Kronecker product [recall A ..... | B,  q Next, let c --- cov (6, 6~ (so)), which is a 2n x 1 vector. Note that, under the intrinsic coregionalization assumption, c can be expressed as e = ([c(sl, so), c(s2, So) ..... c(s,,, so) ] |"
3569,unknown,"e = ([c(sl, so), c(s2, So) ..... c(s,,, so) ] | [1, pT]) T -= co | [I. pT] 7. (I4) Minimizing Eq. (2) with respect to {hit: k = 1, 2 .... , n; i = 1. 2} results in the following least squares solution for k --- (X~, h~2, X2~, X22 ..... X,,t, X,,Dr: (E' ~.=T-'c = (2 | (Co| [1, p~,l r) P3' 3 `2 J / [ 1 0""7 -i = C-' * ] ) (c0 @ [1, p'),] T) P'Y 312 J / I :1' 1 p = C ~co | [1, p.y]T (15) P3' ""Y- J Thu"
3570,unknown,"I :1' 1 p = C ~co | [1, p.y]T (15) P3' ""Y- J Thus, we have ~. = C Ic o| [1,017. (16) Again we see that the secondary simple cokriging weights are zero under the assumption of intrinsic coregionalization. Partial Covariance To understand the mechanism generating the null secondary weights under intrinsic coregionalization, it is fruitful to consider the partial covariance of 6~(s0) and 62(s D given"
3571,unknown,"and 62(s D given all other measurements. For example, consider the partial co- variance between 6t(so) and 62(s ~) given all other observations. We first calculate the partial covariance without conditioning on 6~(s~), and then we calculate the partial covariance including conditioning on 6~(s~). Consider the vector 60 -= [b~(So), 6(sl) r, 6(s2) T, .... 6(s,,)r] T. Decompose 6 o into three parts, "
3572,unknown,"6 o into three parts, namely 6 0 ~ (hi(S0) , 6(SI) T, 6TI) T, where 8111 ~ (6(S2) T, .... 6(S,,)7) T. The corresponding partitioned covariance matrix is"
3573,unknown,"Universal Cokriging 211 I var (hi(So)) cov (6(s0, 51(So)) r coy (6m, 51(so)) r-] l ~1 -= |cov (6(s0. 51(so)) var (6(SI)) COV (611D 6(Sl)) T ] (17) 1 A LCOV (6m. 51(s~)) cov (fro. 6(s,)) var (6m) Now, under the assumption of intrinsic coregionalization we have --c(s~. so) ('(S O, Sl) @ [1, .07] r C(SI' ~Jl) @ 12 = C(,~I, Sill ) @ [[, p~]l C(Sl ' Sill ) I where ill P7 03' 2 C(SIll, SIll) ""@ ;:1 C(So"
3574,unknown,"where ill P7 03' 2 C(SIll, SIll) ""@ ;:1 C(So, Snl ) ~ COV (51(S0) , [61(S2) ..... (~l(Sn)]T) T C(sl, sin) -= coy (5ds0, Ill(s2) ..... 5ds,,)]r) r C(sIH, s~H) ----- coy ([61(s2) ..... 6ds,,)] r, [hds~) ..... 6r(sD] r) (18) ~,,,, - ~,,h ~ ~, t Eh,, I c(s,~, so) C(So, s,) | [1, pT] [ c(s~ C(Sl' sl) @ I IL P')/ P')/] 17]/2 3] - .. / 1 P7 C(s I, Sill) 7 @ C(Sll 1, snl) @ L P""Y 3 fl IC(s(I, SIIIIT | [l,"
3575,unknown,"., | [-1 P~l/ C(Si' LP~ 2 J-J Sin) / I Io; 71] (20) Y~aa - J, = The other entries of the matrix are given by symmetry. First calculate the partial covariance matrix for 5.(So). 5~(s~). and 5._(s,). given all other observations, which we will denote 12.. b- Consider the partition I2 =- (19) 9 12b. 2:bhJ where 22.,, is a (3  3) matrix of covariance terms corresponding to 5~(so), 6t(sl), and 5z(sl)."
3576,unknown,"212 If we further decompose Helterbrand and Cressie I Ogll ('012 r 1 I2,.,.b-- 1o921 0)22 o9231 (21) L_ o931 (.-o32 o933d then ( E' 71) O911 = C(So, So) -- (C(So, Sill) T @ [1, p'y]) C(S[n, snl ) @ 03' 9 (C(so, SIll) r @ [1, pT]) T = C(So, So) -- IC(so, SIu)TC(Sul, SlIl)-IC(So, Sill) F 1 1 p~ [1, | [1, p-y] p3'] T 03, 3,- J = C(So, sl)) -- {C(so, Slu)rC(siil, Sul)-IC(so, sul) } (22) Similarly, [O9"
3577,unknown,"Similarly, [O921"" O931] T = (C(So"" Sl) -- C(SI, Sill)re(sill , Sill) It(so, Sill)) @ [1, p-y]T and (23) [ O9~'~ 0323 ] -- = (C(Sl, Sl) -- C(Sl ' Sill)TC(siii, snl)-IC(sl, Sill)) L ~ O933 [ 1 @ (24) 03, ~y- j We can therefore piece together .Y..,,. h- We note that O931 = (C(So"" SI) -- C(SI, Slll)/C(SlI[, SIIl)-lC(so, sIII))P-Y (25) which need not be zero in general. That is, the partial covariance "
3578,unknown,"which need not be zero in general. That is, the partial covariance between 8~(So) and 82(s,), conditioned on all other measurements except 81(Sl), may not be ZerO. Now consider what happens when we also condition on 8~(s,). Let R = coy (8,(So), 82(st)lfl(si), 6(s2) T, 6(s3) T, .... fi(S,,)T), where coy (d, elf) denotes the partial covariance of d and e given f. Then,"
3579,unknown,"Universal Cokriging 213 09210)32 R =- 0)31 - -- (26) 0)22 From our results in the preceding text, 0)31 = (c(so, sl) - C(sl, sm)rc(sH[, stH)-IC(so, s.0)p3' 0)_~1 = (c(s~, sl) - C(sl, siH)rc(sm, siH) IC(so, stH)) 0)32 = (C(SI, SI) -- C(Sl, Slll)/C(S[ll , Sl[l)-IC(Sl , sIII))P'Y 0)22 = (C(Sl, st) - C(si, SIll)TC(sIII 9 SIIl)-If(st , Sill)) and hence R = 0. Thus, by also conditioning on 6~(s~), the pa"
3580,unknown,"and hence R = 0. Thus, by also conditioning on 6~(s~), the partial covariance becomes zero. This appears analogous to the screen effect encountered in uni- variate spatial prediction (see, e.g., Journel and Huijbregts, 1978, p. 346, or Cressie, 1991, p. 133). The screen effect says that the influence of a datum is reduced when it is hidden by another datum. Under our current assumptions, we see th"
3581,unknown,"we see that the secondary datum at a sample location is completely hidden by the corresponding primary datum. UNIVERSAL COKRIGING Recall the decomposition Zi(s) = #i(s) + 6i(s); s ~ D, i = 1, 2 (27) where #i(s) = Xi(s)I~i (28) [5s is a (p~  1) vector of unknown large-scale variation parameters, and 6i(s) represents small-scale variation; i = 1, 2. We have shown in a variety of ways that when [5o "
3582,unknown,"that when [5o is fixed and known, simple cokriging on {6s(sk): k = 1, 2 ..... n: i = 1, 2} yields a predictor for 3j(so) that gives null weightings to the secondary information. Then, the simple cokriging predictor for Z~(so) is Zl(s o) = /zl(s o) + ~l(so) (29) where #l(So) = Xl(so)/P~l ~. The simple cokriging predictor is always unbiased. Now consider the more general universal cokriging case (My"
3583,unknown,"Clark et al., 1987) when [5 is unknown. The large-scale parameter space can be written as f2 = {[5 =- ([5~-, [5~)r: [5 ~ Rt,}, where p = Pl + P2. Recall that we are currently assuming that p~ are large-scale parameters exclusive to the large- scale structure in Z~(-), and I~_~ are exclusive to the large-scale structure in Z2('). That is, there are no existing relationships or restrictions between "
3584,unknown,"To examine the generalized least-squares estimator of [5 concisely, we redefine Z = [Z(, Z~] r, where Z i =- (Z,(sl) ..... Zi(s,,))r; i = 1, 2. This is"
3585,unknown,"214 Helterbrand and Cressie simply a rearrangement of the elements of the original Z vector. We note that the optimal universal cokriging predictor for the vector Z (So) at location So can be written, Z(so) = CTT-'(Z - X~,.~) + X(so)~t~ (30) where Co is the (2n x 2) matrix cov(Z, Z(so) ), T is the (2n x 2n) matrix var (Z), X is a matrix of ""explanatory"" variables, X(so) is the (2 x p) matrix of ex"
3586,unknown,"of explanatory variables at location So, and J]~l.,- = (XTT-IX) -~XrT-IZ is the generalized least-squares estimator of the large-scale parameter vector. (For details, see VerHoef and Cressie, 1993). Under the model given in (3), we have Co = @ c o (31) PT T- 3 and we can rewrite T = | C (32) p'Y ""Y- 3 We note that CTT-~(Z - X~ls) is equivalent to simple cokriging on 6 --- Z - XfIx, Is, where the u"
3587,unknown,"Z - XfIx, Is, where the unknown large-scale parameters are replaced by their generalized least-squares estimators. We have shown that the secondary simple cokriging weights must be zero under the intrinsic coregionalization assumption with all measurements available at the n sample locations. Hence, it is only possible for nonzero secondary cokriging weights to occur when the estimation of large-s"
3588,unknown,"of large-scale parameters is necessary, such as in ordinary and universal co- kriging. To determine when secondary information is used to estimate Pt we con- sider two situations. First assume all explanatory variables are common to both component processes, i.e., Xi(s) = X2(s) -= X*(s); s e D and Pl = P2 = P*. This model is often encountered in geostatistical problems. In this situation, we note "
3589,unknown,"note that the matrix of explanatory variables can be written as X = (I2 | X*), where I k denotes the (k x k) identity matrix and X* is the (n x p*) matrix with i-th row X*(si). The generalized least squares estimator for p can then be written E (I )' ]' ~L, = (I 2 @ X*)r 1 p @ C (I 2 @ X*) PT T- A 9 (I_~ | X*) T ~) C Z (33) PT T"" 3 which reduces to ~gt.,- = [I2 | (X*rC-IX*) Ix*Tc-I]Z (34)"
3590,unknown,"Universal Cokriging 215 Thus we see only the primary observations are used to estimate p~ and only the secondary observations are used to estimate ~2- However, should a restriction be placed on the parameter space that involves parameters from I~ and ~2 jointly, all observations, both primary and secondary, are used to obtain the optimal linear large-scale parameter estimator of ~j. From (30), the"
3591,unknown,"predictor for the primary process is where A T is the first row of CorT -I, and Xt(so) T is the first row of X(so). The first term involves simple cokriging weights which we have shown are zero for the secondary component. The second term involves O~.~/.,- and it is here that nonzero weights on Z2 may enter. For example, if ~l(s) = ~ and ~2(s) = g2, and we have the restriction g~ = ~2, we note tha"
3592,unknown,"= ~2, we note that the full-rank reparameterization will lead to a model with one large-scale parameter, ~ =- ~ = ~2- This restriction might arise if the two processes represent two kinds of unbiased measurements of the same variable. The restricted generalized-least-squares estimator ~ will be a linear weighting of all 2n observations. Thus, in this situation, the secondary information will be us"
3593,unknown,"be used to estimate the mean of the primary process, ~, and so will receive nonzero universal cokriging weights in predicting Z~(s0). Now assume that some of the explanatory variables are unique to one of the component processes and that no restriction is placed on the large-scale parameter space that involves parameters from 1~ and p, jointly. In this case X~(s) ~ X2(s); s ~ D, but it is still tr"
3594,unknown,"estimator will reduce to the form = Z (36) D2 where D t and D2 are n x n matrices. Hence, secondary information will not be used to estimate I]~ and therefore the secondary universal cokriging weights will be zero. We can therefore conclude that when restrictions are placed on the large- scale parameter space that jointly involve parameters in I]~ and ~2, the secondary information will be used to "
3595,unknown,"information will be used to estimate the mean of the primary process and will thus receive non-zero universal cokriging weights in predicting Z~(so). Other- wise, the secondary cokriging weights will be zero. An Additional Primary Observation It is of interest to observe the behavior of the secondary cokriging weights when an additional primary observation, say Z~(s,,+~), is available without Z2(s"
3596,unknown,"216 Helterbrand and Cressie weights can occur when particular restrictions are placed on the large-scale parameter space, we may consider the simple cokriging problem and determine if nonzero secondary cokriging weights can occur when no restrictions are placed on the large-scale parameter space. Again, consider the simple cokriging equations vt Z m=l n Z m = ] {~lmCtl(Sk, Sm) -F ~k2mCi2(Sk, Sin) "
3597,unknown,"= Cll(Sk, SO), k = 1, 2 ..... n + 1 {XlmCI2(Sk, Sin) + ~k2mC22(Sk, Sin) } ""F"" ~'kl. n + I C21(Sk, S n + I) = c21(Sk, SO). k = 1, 2 ..... n (37) By the intrinsic coregionalization assumption, we can express these equations as n ~] h*c(st, s,,) + Xi.n+lc(sk, s,,+ I) = c(s k, So), k = 1, 2 ..... n + 1 m-- 1 n ~,, X,,**c(sk, s,,,) + h I .... iC(Sk, S,,+l) = C(Sk, S0), k = 1,2 ..... n (38) m-- I Again,"
3598,unknown,"m-- I Again, we see that two sets of equations are identical for k = 1, 2 ..... n, which implies h,,* = ~. .... m = 1, 2 ..... n, and hence the secondary cokriging weights are still zero despite the addition of an additional primary' observation. It can also be shown that the partial correlation between 6i(so) and 62(s~) remains zero when we condition additionally on tS~(s,, ~ t). An Additional Se"
3599,unknown,"We may also want to consider what happens to the secondary cokriging weights when an additional secondary observation Z2(s,, + ~) is available without ZI(s,, ~ I). Again, consider the simple cokriging equations n Z {~,l,,,cll(sk, s,,,) + ~,2,,,c~_(sk. s,,,)} + ~2.,,+lcl:(s~, s,,+l) m = I = c II(sk, So), k = 1. 2 ..... n tl {~kl,,,CI2(Sk, Sin) + ~2,,,C22(Sk, Sin)} + ~2 .... IC22(Sk, Sn+l) tit- [ = "
3600,unknown,"Universal Cokriging 217 By the intrinsic coregionalization assumption, we have n ~k~C(Sk, Sin) ""~- ~[p~k2,n+lC(Sk, Sn+l) : C(Sk, S0) , k = 1, 2 ..... n I/t= I II ** 3' X,~ C(Sk, Sm) +--X2,,+IC(Sk, S,,+I) = c(s~,so),k = 1,2 ..... n + 1 m=l ,O (40) We see that, with the inclusion of Z2(s . + i), it is no longer necessary for X,* = ** X,, , m = 1,2 ..... n. Thus, it is no longer necessary for the sec"
3601,unknown,"cokriging weights to be zero. It can also be shown that the partial correlation between 6~(So) and 62(st) need not be zero when we condition additionally on 62(Sn + I)"" AN EXAMPLE--THE NEVADA TEST SITE DATA In 1957, a device containing plutonium was blown apart by chemical ex- plosives at the Area 13 ""safety-shot"" location on the Nevada Test Site (NTS). This experiment was performed partly to test"
3602,unknown,"This experiment was performed partly to test for ""safety"" against fission re- actions in an accident situation involving an atomic weapon. A consequence of the test was the contamination of the immediate surrounding desert soil and vegetation with plutonium (Pu) and americium (Am). In 1971, the Nevada Applied Ecology Group (NAEG) began conducting environmental transuranic studies in this area by t"
3603,unknown,"studies in this area by taking field instrument surveys and collecting soil, veg- etation, and animal tissue samples. One goal of these studies was to predict the total amount and spatial distribution of 239'24~ and 24~Am in surface soil. The Pu concentrations (in/~Ci/m 2) were determined by wet chemistry on surface (top 5 cm) soil samples taken at random locations. The Am concentra- tions in surf"
3604,unknown,"tions in surface soil was obtained from Field Instrument for the Detection of Low Energy Radiation (FIDLER) readings (in 10 3 counts per minute (cpm)) at one foot above the surface. In this paper, we use a subset of the data accumulated in Area 13, considering 104 sample locations where measurements are available on both components (Gilbert, 1978). A map of these sample locations and ground zero i"
3605,unknown,"ground zero is displayed in Fig. 1. Previous analyses on the Area 13 data have concluded that there is a good overall correlation, on the log scale, between wet chemistry Pu analyses and Am FIDLER measurements (Church et al., 1975; Gilbert and Simpson, 1985). To consider the effects of cokriging with an in- trinsic coregionalization model, a restricted large-scale structure between com- ponents, a"
3606,unknown,"ponents, and measurements on both components at all sample locations, we consider the prediction of log Pu at unsampled locations based on the measure- ments at the locations displayed in Fig. 1. A prediction is made for each of 120 unsampled grid sites in the interior of the rectangular region outlined in Fig. 1."
3607,unknown,218 Helterbrand and Cressie : ~ .... g 720000 720500 721000 721500 722000 722500 723000 Easl (Feet) Fig. 1. The sample locations in the inner-fence region of Area 13 where measurements on both Pu and Am are available. The axes are in Nevada feet coordinates and the cross (X) denotes ground zero. Cokriging is very useful in the under-sampled problem where there are relatively few primary measuremen
3608,unknown,"relatively few primary measurements as compared to secondary measurements (see, e.g., Stein and Corsten, 1991; and Zhang et al., 1992). Indeed, in the NTS study, many more FIDLER readings were taken because the cost of a FIDLER reading was approximately 50 times less than that of a Pu analysis on a soil sample (Gilbert, 1978). However, in this intrinsic coregionalization study, we are only interes"
3609,unknown,"we are only interested in the effects of cokriging when measurements are avail- able for both components at all sample locations. Figure 2 displays a bivariate ray-glyph map of the raw log Pu and log Am measurements (Carr et al., 1992). The rays pointing to the right represent log Pu concentration trends and rays pointing to the left represent log Am concen- tration trends. The bivariate rap map i"
3610,unknown,"tration trends. The bivariate rap map is an effective technique for showing bivariate associations. The two rays at each location in Fig. 2 generally point down or up together, clearly indicating that the components are positively cor- related. To examine the effects of cokriging under intrinsic coregionalization and a restricted large-scale structure, the following spatial model will be considere"
3611,unknown,"for Zt -= log Pu, and ~ -= log Am, at spatial locations s E D. Assume Z,(s) = #, + /3 exp <-IIs - s~.ll/0) + a,(s) KZ2(s) = #2 + /3 exp (-IIs - s~=ll/0) + a2(s) (41) where sg: denotes ground zero, K and 0 are parameters that are assumed known, #l, #2, and ~ are unknown large-scale parameters, and ~51(s) and 6=(s) are as-"
3612,unknown,"Universal Cokriging 219 8 Q Observed Log Data Log Am Lo~Pu ~"" 6 85 '~ 970 5 22 ""e"" 708 ~,- 360 -8- 445 197 ~. 1 83 0 34 Q -80 -~ 4)- J& Ground Zero Q i i i i i i [ 720000 720500 721000 721500 722000 722500 723000 East (Nevada Feet Coordinales) Fig. 2. A bivariate ray-glyph map of the log Pu and Am measurements. Pu is measured in /,tCi/m-', and Am is measured in 10 3 counts per minute (cpm). sumed "
3613,unknown,"(cpm). sumed to be second-order stationary processes that are spatially cross-correlated; i.e., the vector 6(s) =- [61(s), 62(s)] T is a bivariate second-order stationary process. The large-scale structure for both components in Eq. (41) was selected to model the observed exponential decline away from ground zero. The parameter [3 is assumed to be shared by the two large-scale models. The paramete"
3614,unknown,"to transform the log Am data to an equivalent scale with log Pu. Though we assume this parameter is known, we actually estimated K externally by regressing log Pu on log Am, based on all 104 observations. Similarly, the exponential scale parameter 0 was estimated externally (for fixed K) using non-linear least squares. The sample variance of these estimates were relatively small and thus we consid"
3615,unknown,we consider K = 1.25 and 0 = 1880 fixed for the following analysis. The ordinary-least-squares residuals from the large-scale models above were used to assist in modeling an isotropic small-scale spatial dependence structure. The experimental (cross-) semivariograms were estimated using the robust fourth- root estimator proposed by Cressie and Hawkins (1980). (The derivation of the fourth-root cro
3616,unknown,fourth-root cross-variogram estimator is exactly as that for the variogram esti- mator under proper standardization.) In this example we chose to fit an isotropic exponential intrinsic coregion- alization model to the experimental (cross-) semivariograms. A bivariate gen- eralization of the univariate non-linear weighted-least-squares criterion of Cres- sie (1985) was used to obtain estimates for 
3617,unknown,"sie (1985) was used to obtain estimates for the parameters of the exponential variogram model. Since variograms and cross-variograms are interrelated, pa- rameter estimation requires simultaneous consideration of the experimental var-"
3618,unknown,"221111 Helterbrand and ('ressie iograms and cross-variograms (Heherhrand and Cressie, 1994). The multivariate spatial dependence stnwture (in tenns of covariograms) was estimated as 2.079 1.614 ( 1 ) C(sa, s,,,) = exp - Ils, - s,,,ll (42) 1.614 2.290 ~7~ The fit of this ',Mid model to the experimental (cross-) semivadograms is dis- played (standardized) in a matrix display in Fig. 3. The semivario"
3619,unknown,"played (standardized) in a matrix display in Fig. 3. The semivariograms for Pu and Am appear on the diagonal, with the cross-semivariogram displayed off the diagonal. Notice that the intrinsic coregionalizalion model does not have a nug- gcI c11~'c11 and flails to lit the cxperimental variogram for the primary component (Pu) at the shorter lags. When a nugget cfllect parameter is included for the "
3620,unknown,"variogram model, the estimalled bivariate spatial dependence model is 0.834 1.330 1.330 2.290 1.928 1.330 1.330 2.29(/ (' ) e -340 - its~ s,,, i|"" S,{ : Sm (43) This model is n~ longer of an intrinsic coregionalization Ionn. but provides a satisfactory fit to the experimental variograms (Fig. 4). To compare the gain in :~recision of cokriging relatiw; to kriging, the pre- I C, "". : .~-.7 ~ . ..~'"
3621,unknown,"q, ! -- 0 0 Z 2~i 4qO C,C') i)Ch FO~A 0 200 400 600 800 Feel --i i 1 ,, G 5 ' . 9 "" I] , ,,I ,Ii10 i_(;O i-;( Ii / C, 5 .---,.... r~ ,. Ii. r, _ ,o .m() ~300 8oo Fc,_.t Feel Fi~. 3. ]'he lit Lfl the cslhnutcd c~,poncnlial inlrmsh.' cnrugnmalilafinn model Io the r JcIOs~. - ) SCIlli~.;.IrioIZRIIll~."
3622,unknown,"Universal Cokrigin~ 221 oo -- 9 I O5 ""' 05 t O0 0 200 400 600 800 Feet B 0 200 400 600 800 Feet J O0 .~ ... ~ ] J n 200 400 600 800 0 200 400 600 800 Feel Feet Fig. 4. The fit of the exponential scnfi,.,ariogram model, ~i111 a nugget cfl~""..'l Ik~r the prima D component, to the expenmcnlal Icros~ I semi'.ario gr~tms. diction error variance was calculated fl~r the 120 prediction locations based on"
3623,unknown,"gr~tms. diction error variance was calculated fl~r the 120 prediction locations based on the universal kriging and cokriging predictors, Z'b,u~: and Zl,~,{v,-, respectively. The relative efficiency is calculated as var (Zl(sll) - 2i.u~-(S.)) (4-4.) var (Zl(s .) - Zj,u(-,~-(s.)) A kriging neighborhood with a search window of radius 560 feet was used for prediction. I[ more than 12 sample locations "
3624,unknown,"prediction. I[ more than 12 sample locations fell in the search window, only the measurements from the 12 nearest locations were used. If fewer than ten loca- tions fell in the search window, the search window was expanded so that each prediction was based on measurements from its nearest ten neighbors (e.g., Harper et al., 1988). Kriging neighborhoods are primarily used to reduce the computationa"
3625,unknown,"computational burden demanded by kriging, although Joumel and Rossi (1989) also point out that kriging neighborhoods can be used to protect the user lrom local large-scale model misspecification. Figures 5 through 7 display the kriging, cokriging, and cokriging variance maps, respectively, based on the fitted exponential intrinsic coregionalization model given in Eq. (42). Though non-zero secondar"
3626,unknown,"model given in Eq. (42). Though non-zero secondary weights occur and the kriging and cokriging predictions differ, the reduction in the prediction error variance is minimal. The mean relative efficiency for the 120 prediction locations is 1.004, with a maximum of 1.031 lk-~r one prediction location. This minimal reduction is not surprising when one notes that the prediction error variance"
3627,unknown,"222 Helterbrand and Cressie ~ o z ~ / s I ~ff 9 ~ . 9 - "" ~ i f , J , i i 720400 720600 720800 721000 721200 721400 721600 East (Feel) Fig. 5. The kriging map based on the exponential intrinsic coregionalization model. Contour units are in log gCi/m 2. The cross (X) denotes ground zero, u ~ o z / i i i i , i 720400 720600 720800 721000 721200 721400 721600 Easl (Feel) Fig. 6. The cokriging map bas"
3628,unknown,"Easl (Feel) Fig. 6. The cokriging map based on the exponential intrinsic coregional- ization model. Contour units are in log /~Ci/m 2. The cross (X) denotes ground zero. matrix corresponding to Eq. (30) can be decomposed into the sum of two terms, var (Z(so) - CorT-'Z), and vat [(X(so) - CoTT-IX)~gt}. (For this example, P =- (tzl, ~2, ilL) Under an intrinsic coregionalization model with measuremen"
3629,unknown,"on both components at all sample locations, CorT -' = 12 | corC-', and the first variance term is the same for both the kriging and cokriging predictor of"
3630,unknown,Universal Cokriging 223 A LL v g i i i i t i i 720400 720600 720800 721000 721200 721400 721600 Easl (Feel) Fig. 7. The cokriging variance map based on the exponential intrinsic co- regJonalizatJon model. Contour units are in (log /~Ci/m2) 2. The cross (X) denotes ground zero. u_ o ~ o z m i i J i i ~ i 720400 720600 720800 721000 721200 721400 721600 Easl (Feel) Fig. 8. The kriging variance map b
3631,unknown,"Easl (Feel) Fig. 8. The kriging variance map based on the exponential variogram model with a nugget effect for Pu. Contour units are in (log #Ci/m2f. The cross (X) denotes ground zero. the primary [secondary] component. Thus, the reduction in the prediction error variance due to cokriging is from the second variance term, so that any additional efficiency attained by cokriging is due to the additi"
3632,unknown,224 Helterbrand and Cressie Y. o i i i i i ~ i 720400 720600 720800 721000 721200 721400 721600 Easl (Feet) Fig. 9. The cokriging variance map based on the exponential variogram model with a nugget effect for Pu. Contour units are in dog/~Ci/m-')-'. The cross (X) denotes ground zero. Recall that the intrinsic coregionalization model fails to fit the experimental variogram for the primary component
3633,unknown,"variogram for the primary component (Pu) at the shorter lags. Figures 8 and 9 display, respectively, the kriging variance and cokriging variance maps based on the fitted exponential variogram model with a nugget effect for the Pu var- iogram. For this model, the first term of the prediction error variance is not the same for kriging and cokriging. The mean relative efficiency for the 120 pre- dict"
3634,unknown,"diction locations under this more appropriate spatial dependence model is 1.072, with a minimum of 1.013 and maximum of 1.214. Thus, for this example, a reduction of up to 17.6% in the prediction error variance is obtained using cokriging even when measurements are available on both components at all sample locations. CONCLUSION We have shown that, under the assumption of intrinsic coregionalizati"
3635,unknown,"with observations available on both components at each sample location, the large-scale parameter space for uniform unbiasedness determines the allowable values for the secondary cokriging weights. If we require uniform unbiasedness on a parameter space where the large-scale parameters for the primary mean and the large-scale parameters for the secondary mean are restricted to depend on each other"
3636,unknown,"on each other in some manner, the secondary information will be used in esti- mating primary large-scale parameters and thus nonzero secondary cokriging weights will occur. However, the reduction in the prediction error variance may"
3637,unknown,"Universal Cokriging 225 be minimal. The results here also extend to the m-variate (where m > 2) cokriging problem when we assume m-variate intrinsic coregionalization. We began by arguing that the intrinsic coregionalization assumption is restrictive and often inappropriate. Realistic multivariate spatial statistical models are typically more complex than intrinsic coregionalization, resulting in "
3638,unknown,"multivariate universal cokriging predictors (Eq. 30). In general, secondary in- formation is crucial in determining optimal predictors. ACKNOWLEDGMENTS This research was supported by the National Science Foundation (DMS- 9001862 and DMS-9204521), the National Security Agency (MDA904-92-H- 3021), and an Iowa State University Research Grant (Carver Grant). The authors are grateful to Richard O. Gilb"
3639,unknown,"are grateful to Richard O. Gilbert of Battelle, Pacific Northwest Laboratories and the Nevada Field Office of the U.S. Department of Energy for use of the Nevada Test Site data. REFERENCES Carr, D., OIsen, A., and White, D., 1992, Hexagon mosaic maps for display of univariate and bivariate geographical data: Cartogr. Geogr. Inf. Syst., v. 19, p. 228-236. Church, B., Medling, E., and Brady, D., 197"
3640,unknown,"Church, B., Medling, E., and Brady, D., 1975, A different look at area 13 [=IDLER survey data, in M. White and P, Dunaway (Eds.), The Radioecology of Plutonium and Other Transuranies in Desert Environments: USERDA Report, NVO-153, p. 231-235~ Clark, 1., Basinger, K., and Harper, W., 1987, MUCK--A novel approach to cokriging, in B. Buxton (Ed.), Proceedings of the Conference on Geostatstical, Sensi"
3641,unknown,"Buxton (Ed.), Proceedings of the Conference on Geostatstical, Sensitivit3,, and Uncertainty Methods for Ground-Water Flow and Radionuclide Transport Modeling: Battelle Press, Co- lumbus, p. 473~493. Cressie, N., 1991, Statistics for Spatial Data: Wiley, New York. Cressie, N., 1985, Fitting variogram models by weighted least squares: Math. Geol., v. 17, p. 563-586. Cressie, N., and Hawkins, D., 198"
3642,unknown,"p. 115-125. Gilbert, R., 1978, On the estimation of spatial pattern for environment contaminants, in M. White and P. Dunaway (Eds.), Selected Environmental Plutonium Research Reports of the NAEG: Nevada Applied Ecology Group, U.S. Department of Energy, Las Vegas, Nevada, p. 319- 360. Gilbert, R., and Simpson, J., 1985. Kriging for estimating spatial pattern of contaminants: Potential and problems:"
3643,unknown,"and problems: Env. Monitor. Assess., v. 5, p. 113-135. Harper, W., Basinger, K., and Fun-, J., 1988, Geostatistical analysis of potentiometric data in the Pennsylvanian aquifer of the Palo Duro Basin, Texas: Tech. Rep BMI/ONW1-680, Office of Nuclear Waste Isolation, Battelle Memorial Institute, Hereford, Texas. Helterbrand, J., and Cressie, N., 1994, Models and inference for multivariate spatial p"
3644,unknown,"preparation. Joumel, A., and Huijbregts, C., 1978, Mining Geostatistics: Academic Press, London. Joumel, A., and Rossi, M., 1989, When do we need a trend model in kriging? Math. Geol., v. 21, p. 715-739."
3645,unknown,"226 Helterbrand and Cressie Myers, D., 1982, Matrix formulation of co-kriging: Math. Geol., v. 14, p. 249-257. Stein. A., and Corsten, L., 1991, Universal kriging and cokriging as a regression procedure: Biometrics, v. 47, p, 575-587. Ver Hoef, J., and Cressie, N., 1993. Multivariable spatial prediction: Math. Geol,. v. 25, p. 219- 240. Zhang. R., Yates, S., and Shouse, P,, 1992, Prediction of soi"
3646,unknown,"Zhang. R., Yates, S., and Shouse, P,, 1992, Prediction of soil salinity using cokriging with non- symmetric pseudo-cross-variograms, in First Conference of the Working Group on Pedometrics of the International Society of Soil Science: Pedometrics-92: Developments in Spatial Statistics for Soil Science, International Agricultural Centre, Wageningen, The Netherlands, p. 145- 166."
3647,unknown,"arXiv:1106.6251v2 [stat.ML] 16 Apr 2012 Kernels for V ector-V alued Functions: a Review Mauricio A. ´Alvarez+, Lorenzo Rosasco ♯,†, Neil D. Lawrence ⋆,⋄, ‡ - School of Computer Science, University of Manchester Manc hester , UK, M13 9PL. + Department of Electrical Engineering, Universidad T ecnol ´ ogica de Pereira, Colombia, 660003 ♯- CBCL, McGovern Institute, Massachusetts Institute of T ec hnol"
3648,unknown,"† -IIT@MIT Lab, Istituto Italiano di T ecnologia, Genova, Ita ly ⋆- Department of Computer Science, University of Shefﬁeld, U K ⋄ The Shefﬁeld Institute for T ranslational Neuroscience, Sh efﬁeld, UK. malvarez@utp.edu.co, lrosasco@mit.edu, n.lawrence@sheffield.ac.uk November 26, 2024 Abstract Kernel methods are among the most popular techniques in mach ine learning. From a regularization perspec-"
3649,unknown,"tive they play a central role in regularization theory as the y provide a natural choice for the hypotheses space and the regularization functional through the notion of reprod ucing kernel Hilbert spaces. From a probabilistic per- spective they are the key in the context of Gaussian processe s, where the kernel function is known as the covariance function. T raditionally , kernel methods have bee"
3650,unknown,"indeed there has been a considerable amount of work devoted t o designing and learning kernels. More recently there has been an increasing interest in methods that deal wi th multiple outputs, motivated partly by frameworks like multitask learning. In this paper , we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the conne ct"
3651,unknown,1 Contents 1 Introduction 3 2 Learning Scalar Outputs with Kernel Methods 3 2.1 A Regularization Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 A Bayesian Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3 A Connection Between Bayesian and Regularization Point of Views . . . . . . . . . . . 
3652,unknown,3 Learning Multiple Outputs with Kernels Methods 7 3.1 Multi-output Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.2 Reproducing Kernel for V ector V alued Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3.3 Gaussian Processes for V ector V alued Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4 Se
3653,unknown,4 Separable Kernels and Sum of Separable Kernels 10 4.1 Kernels and Regularizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.2 Coregionalization Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.2.1 The Linear Model of Coregionalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3654,unknown,4.2.2 Intrinsic Coregionalization Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.2.3 Comparison Between ICM and LMC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.2.4 Linear Model of Coregionalization in Machine Learnin g and Statistics . . . . . . . . . . . . . 15 4.3 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3655,unknown,4.3.1 Extensions Within the Regularization Framework . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.3.2 Extensions from the Gaussian Processes Perspective . . . . . . . . . . . . . . . . . . . . . . . 20 5 Beyond Separable Kernels 20 5.1 Invariant Kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 5.2 Further Extensions of the LMC . . .
3656,unknown,5.3 Process Convolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 5.3.1 Comparison Between Process Convolutions and LMC . . . . . . . . . . . . . . . . . . . . . . 23 5.3.2 Other Approaches Related to Process Convolutions . . . . . . . . . . . . . . . . . . . . . . . . 23 6 Inference and Computational Considerations 26 6.1 Estimation of Parameter
3657,unknown,6.2 Parameters Estimation for Gaussian Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 7 Applications of Multivariate Kernels 29 8 Discussion 30 2 1 Introduction Many modern applications of machine learning require solvi ng several decision making or prediction problems and exploiting dependencies between the problems is often t he key to obtain better results and coping
3658,unknown,"of data (to solve a problem we can borrow strength from a distinct but related problem). In sensor networks , for example, missing signals from certain sensors may be pr edicted by exploiting their cor- relation with observed signals acquired from other sensors [72]. In geostatistics, predicting the concentration of heavy pollutant metals, which are expensive to measure, can be done using inexpens"
3659,unknown,"ables as a proxy [37]. In computer graphics , a common theme is the animation and simulation of physicall y plausible humanoid motion. Given a set of poses that delineate a partic ular movement (for example, walking), we are faced with the task of completing a sequence by ﬁlling in the missin g frames with natural-looking poses. Human move- ment exhibits a high-degree of correlation. Consider , fo"
3660,unknown,"forward, we unconsciously prepare the left leg, which is cur rently touching the ground, to start moving as soon as the right leg reaches the ﬂoor . At the same time, our hands m ove synchronously with our legs. W e can exploit these implicit correlations for predicting new poses and fo r generating new natural-looking walking sequences [106]. In text categorization , one document can be assigned "
3661,unknown,"the examples above, the simplest approach ignores the poten tial correlation among the different output compo- nents of the problem and employ models that make predictions individually for each output. However , these examples suggest a different approach through a joint predi ction exploiting the interaction between the different components to improve on individual predictions. Within th e machin"
3662,unknown,"is often broadly referred to to as multitask learning . Again the key idea is that information shared between diffe rent tasks can lead to improved performance in comparison to lear ning the same tasks individually . These ideas are related to transfer learning [97, 20, 12, 74], a term which refers to systems that learn by t ransferring knowledge between different domains, for example: “what can w"
3663,unknown,"between different domains, for example: “what can we learn a bout running through seeing walking?” More formally , the classical supervised learning problem r equires estimating the output for any given input x∗; an estimator f∗(x∗) is built on the basis of a training set consisting of N input-output pairs S = ( X,Y) = (x1,y1),..., (xN,yN). The input space X is usually a space of vectors, while th"
3664,unknown,"In multiple output learning (MOL) the output space is a space of vectors; the estimator is now a vector valued function f. Indeed, this situation can also be described as the problem of solving D distinct classical supervised problems, where each problem is described by one of the compo nents f1,...,f D of f. As mentioned before, the key idea is to work under the assumption that the problems are i"
3665,unknown,"the relation among the problems to improve upon solving each problem separately . The goal of this survey is twofold. First, we aim at discussin g recent results in multi-output/multi-task learning based on kernel methods and Gaussian processes providing an account of the state of the art in the ﬁeld. Second, we analyze systematically the connections between Bayesia n and regularization (frequenti"
3666,unknown,"related techniques have been proposed from different persp ectives and drawing clearer connections can boost advances in the ﬁeld, while fostering collaborations betwe en different communities. The plan of the paper follows. In chapter 2 we give a brief revi ew of the main ideas underlying kernel methods for scalar learning, introducing the concepts of regulariz ation in reproducing kernel Hilbert"
3667,unknown,"processes. In chapter 3 we describe how similar concepts ext end to the context of vector valued functions and discuss different settings that can be considered. In chapt ers 4 and 5 we discuss approaches to constructing mul- tiple output kernels, drawing connections between the Baye sian and regularization frameworks. The parameter estimation problem and the computational complexity probl em are "
3668,unknown,"discuss some potential applications that can be seen as mult i-output learning. Finally we conclude in chapter 8 with some remarks and discussion. 2 Learning Scalar Outputs with Kernel Methods T o make the paper self contained, we will start our study revi ewing the classical problem of learning a scalar valued function, see for example [100, 40, 10, 82]. This will also serve as an opportunity to "
3669,unknown,"between Bayesian and regularization methods. As we mentioned above, in the classical setting of supervise d learning, we have to build an estimator (e.g. a classiﬁcation rule or a regression function) on the basis o f a training set S = ( X,Y) = ( x1,y1),..., (xN,yN). Given a symmetric and positive bivariate function k(·,·), namely a kernel, one of the most popular estimators in machine learning i"
3670,unknown,"machine learning is deﬁned as f∗(x∗) = k⊤ x∗ (k(X,X) + λNI)−1Y, (1) 3 where k(X,X) has entries k(xi,xj), Y = [ y1,...,y N]⊤ and kx∗ = [ k(x1,x∗),...,k (xN,x∗)]⊤, where x∗ is a new input point. Interestingly , such an estimator can be derive d from two different, though, related perspectives. 2.1 A Regularization Perspective W e will ﬁrst describe a regularization (frequentist) persp ective (see [3"
3671,unknown,"is that the function of interest is assumed to belong to a repr oducing kernel Hilbert space (RKHS), f∗ ∈ H k. Then the estimator is derived as the minimizer of a regulariz ed functional 1 N N∑ i=1 (f(xi) − yi)2 + λ∥f∥2 k. (2) The ﬁrst term in the functional is the so called empirical ris k and it is the sum of the squared errors. It is a measure of the price we pay when predicting f(x) in place o"
3672,unknown,"in a RKHS. This latter concept plays a key role, so we review a f ew essential concepts (see [87, 6, 105, 25]). A RKHS Hk is a Hilbert space of functions and can be deﬁned by a reproduc ing kernel 1 k : X × X → R, which is a symmetric, positive deﬁnite function. The latter assum ption amounts to requiring the matrix with entries k(xi,xj) to be positive for any (ﬁnite) sequence (xi). Given a kernel"
3673,unknown,"the function k(x,·) belongs to belongs to Hk for all x ∈ X and f(x) = ⟨f,k(x,·)⟩k, ∀ f ∈ H k, where ⟨·,·⟩k is the inner product in Hk. The latter property , known as the reproducing property , giv es the name to the space. T wo further properties make RKHS appealing: • functions in a RKHS are in the closure of the linear combinati ons of the kernel at given points, f(x) =∑ ik(xi,x)ci. This allows "
3674,unknown,"generalized linear models; • the norm in a RKHS can be written as ∑ i,jk(xi,xj)cicj and is a natural measure of how complex is a function. Speciﬁc examples are given by the shrinkage point of view tak en in ridge regression with linear models [40] or the regularity expressed in terms of magnitude of derivat ives, as is done in spline models [105]. In this setting the functional (2) can be derived "
3675,unknown,"theory of empirical risk minimization (ERM) [100]. In the fo rmer , one observes that, if the space Hk is large enough, the minimization of the empirical error is ill-pose d, and in particular it responds in an unstable manner to noise, or when the number of samples is low Adding the squar ed norm stabilizes the problem. The latter point of view , starts from the analysis of ERM showing that gener"
3676,unknown,"tradeoff between ﬁtting and complexity 2 of the estimator . The functional (2) can be seen as an instanc e of such a trade-off. The explicit form of the estimator is derived in two steps. Fi rst, one can show that the minimizer of (2) can always be written as a linear combination of the kernels cent ered at the training set points, f∗(x∗) = N∑ i=1 k(x∗,xi)ci = k⊤ x∗ c, see for example [65, 19]. Th"
3677,unknown,"[88] and [26] for recent results and further references). Th e explicit form of the coefﬁcients c = [ c1,...,c N]⊤ can be then derived by substituting for f∗(x∗) in (2). 1 In the following we will simply write kernel rather than repr oducing kernel. 2 For example, a measure of complexity is the V apnik–Chervonenkis dimension [86] 4 2.2 A Bayesian Perspective A Gaussian process (GP) is a stochastic"
3678,unknown,"variables, taken from a realization of the GP , follows a join t Gaussian distribution. A GP is usually used as a prior distribution for functions [82]. If the function f follows a Gaussian process we write f ∼ GP (m,k), where m is the mean function and k the covariance or kernel function. The mean function and the covariance function completely specify the Gaussian process. In other words the abo"
3679,unknown,"set X = {xn}N n=1 if we let f(X) = [ f(x1),...,f (xN)]⊤ then f(X) ∼ N (m(X),k(X,X)), where m(X) = [ m(x1),...,m (xN)]⊤ and k(X,X) is the kernel matrix. In the following, unless otherwise sta ted, we assume that the mean vector is zero. From a Bayesian point of view , the Gaussian process speciﬁes our prior belie fs about the properties of the func- tion we are modeling. Our beliefs are updated in "
3680,unknown,"our prior assumptions to the actual observations. This lead s to an updated distribution, the posterior distribution , that can be used, for example, for predicting test cases. In a regression context, the likelihood function is usually Gaussian and expresses a linear relation between the observations and a given model for the data that is corrupted with a zero mean Gaussian noise, p(y|f,x,σ2) = N"
3681,unknown,"p(y|f,x,σ2) = N (f(x),σ2), where σ2 corresponds to the variance of the noise. Noise is assumed to be independent and identically distributed. In this way , the likelihood function factorizes over data po ints, given the set of inputs X and σ2. The posterior distribution can be computed analytically . For a test input vector x∗, given the training data S = {X,Y}, this posterior distribution is give"
3682,unknown,"posterior distribution is given by , p(f(x∗)|S,x∗,φ ) = N (f∗(x∗),k∗(x∗,x∗)), where φ denotes the set of parameters which include the variance of t he noise, σ2, and any parameters from the covariance function k(x,x′). Here we have f∗(x∗) = k⊤ x∗ (k(X,X) + σ2I)−1Y, k∗(x∗,x∗) = k(x∗,x∗) − k⊤ x∗ (k(X,X) + σ2I)−1kx∗ and ﬁnally we note that if we are interested into the distribu tion of the noisy pred"
3683,unknown,"easy to see that we simply have to add σ2 to the expression for the predictive variance (see [82]). Figure 1 represents a posterior predictive distribution fo r a data vector Y with N = 4 . Data points are repre- sented as dots in the ﬁgure. The solid line represents the mea n function predicted, f∗(x∗), while the shaded region corresponds to two standard deviations away from the mean. T his shade"
3684,unknown,"covariance function, k∗(x∗,x∗). Notice how the uncertainty in the prediction increases as w e move away from the data points. Equations for f∗(x∗) and k∗(x∗,x∗) are obtained under the assumption of a Gaussian likelihood, common in regression setups. For non-Gaussian likelihoods, for exam ple in classiﬁcation problems, closed form solutions are not longer possible. In this case, one can resort to d"
3685,unknown,"and variational methods [82]. 2.3 A Connection Between Bayesian and Regularization Point of Views Connections between regularization theory and Gaussian pr ocess prediction or Bayesian models for prediction have been pointed out elsewhere [78, 105, 82]. Here we just gi ve a very brief sketch of the argument. W e restrict ourselves to ﬁnite dimensional RKHS. Under this assumption one can show that "
3686,unknown,"terms of a feature map [100], that is a map Φ : X → Rp, such that k(x,x′) = p∑ j=1 Φ j(x)Φ j(x′). 5 −0.2 0 0.2 0.4 0.6 0.8 1 1.2−2.5 −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 x f (x) Figure 1: Example of a predictive posterior distribution in ferred with N = 4. The solid line corresponds to the predictive mean, the shaded region corresponds to two stand ard deviations of the prediction. Dots are values of"
3687,unknown,"the output function Y. W e have also included some samples from the posterior distr ibution, shown as dashed lines. In fact in this case one can show that functions in the RKHS wit h kernel kcan be written as fw(x) = p∑ j=1 wjΦ j(x) = ⟨w,Φ( x)⟩, and ∥fw∥k = ∥w∥. Then we can build a Gaussian process by assuming the coefﬁcie nt w= w1,...,w p to be distributed according to a multivariate Gaussian dis"
3688,unknown,"w ∼ N (0,Ip) ∝ e−∥ w∥ 2 . As we noted before if we assume a Gaussian likelihood we have P(Y|X,f) = N (f(X),σ2ID) ∝ e− 1 σ 2 ∥ fw(X)−Y∥ 2 n , where fw(X) = ( ⟨w,Φ( x1)⟩,..., ⟨w,Φ( xn)⟩) and ∥fw(X) − Y∥2 n = ∑ n i=1(⟨w,Φ( xi)⟩ − yi)2. Then the posterior distribution is proportional to e−( 1 σ 2 ∥ fw(X)−Y∥ 2 n+∥ w∥ 2), and we see that a maximum a posteriori estimate will in turn gi ve the minimizatio"
3689,unknown,"regularization [98], where the regularization parameter i s now related to the noise variance. 6 W e note that in regularization the squared error is often rep laced by a more general error term 1 N ∑ N i=1 ℓ(f(xi),yi). In a regularization perspective, the loss function ℓ: R × R → R+ measure the error we incur when predicting f(x) in place of y. The choice of the loss function is problem dependent"
3690,unknown,"logistic loss or the hinge loss used in support vector machin es (see [86]). The choice of a loss function in a regularization setting can be contrasted to the choice of the likelihood in a Bayesian setting. In this context, the likelihood function models how the observations deviate from the assumed true model in the generative process. The notion of a loss functio n is philosophically different."
3691,unknown,"cost we pay for making errors. In Bayesian modeling decision making is separated from inference. In the inference stage the posterior distributions are computed evaluating the uncertainty in the model. The loss function appears only at the second stage of the analysis, known as the decision stage, and weighs how incorrect decisions are penalized given the current uncertainty . However , whilst t h"
3692,unknown,"can see that, due to the formulation of the frameworks, the lo ss function and the log likelihood provide the same role mathematically . The discussion in the previous sections shows that the notio n of a kernel plays a crucial role in statistical modeling both in the Bayesian perspective (as the covarianc e function of a GP) and the regularization perspective (as a reproducing kernel). Indeed, fo"
3693,unknown,"(see for example [86, 90, 82] and references therein). In the next sections we show how the concept of a kernel can be used in multi-output learning problems. Before doing tha t, we describe how the concepts of RKHSs and GPs translate to the setting of vector valued learning. 3 Learning Multiple Outputs with Kernels Methods In this chapter we discuss the basic setting for learning vec tor valued f"
3694,unknown,"multilabel) and then describe how the concept of kernels (re producing kernels and covariance function for GP) translate to this setting. 3.1 Multi-output Learning The problem we are interested in is that of learning an unknow n functional relationship f between an input space X , for example X = Rp, and an output space RD. In the following we will see that the problem can be tackled"
3695,unknown,"either assuming that f belongs to reproducing kernel Hilbert space of vector value d functions or assuming that f is drawn from a vector valued Gaussian process. Before doing this we describe several related settings all falling under the framework of multi-output learning. The natural extension of the traditional (scalar) supervis ed learning problem is the one we discussed in the introduction, w"
3696,unknown,"for problems such as motion/velocity ﬁelds estimation. A sp ecial case is that of multi-category classiﬁcation problem or multi-label problems, where if we have D classes each input point can be associated to a (binary) coding vector where, for example 1 stands for presence ( 0 for absence) of a class instance.The simplest example is the so called one vs all approach to multiclass classiﬁcation wh"
3697,unknown,"coding i→ ei, where (ei) is the canonical basis of RD. A more general situation is that where different outputs mig ht have different training set cardinalities, different input points or in the extreme case even different input spac es. More formally , in this case we have a training set Sd = ( Xd,Yd) = ( xd,1,yd,1),..., (xd,Nd ,yd,Nd ) for each component fd, with d = 1 ,...,D , where the number "
3698,unknown,"data associated with each output, (Nd) might be different and the input for a component might belong to different input space (Xd). The terminology used in machine learning often does not dist inguish the different settings above and the term multitask learning is often used. In this paper we use the ter m multi-output learning or vector valued learning to deﬁne the general class of problems and u"
3699,unknown,"inputs. Indeed in this very general situation each componen t can be thought of as a distinct task possibly related to other tasks (components). In the geostatistics literatu re, if each output has the same set of inputs the model is called isotopic and heterotopic if each output to be associated with a different set of inputs [104]. Heterotopic data is further classiﬁed into entirely heterotopic "
3700,unknown,"and partially heterotopic data , where the variables share some sample locations. In machin e learning, the partially heterotopic case is sometimes referred to as asymmetric multitask learning [112, 21]. The notation in the multitask learning scenario (heterotop ic case) is a bit more involved. T o simplify the nota- tion we assume that the number of data for each output is the sa me. Moreover , f"
3701,unknown,"7 we restrict the presentation to the isotopic setting, thoug h the models can usually readily be extended to the more general setting. W e will use the notation X to indicate the collection of all the training input points, {Xj}N j=1, and S to denote the collection of all the training data. Also we wil l use the notation f(X) to indicate a vector valued function evaluated at different training po"
3702,unknown,"the input points are sampled. If the input to all the componen ts are the same then X = x1,..., xN and f(X) = f1(x1),...,f D(xN). If the input for the different components are different the n X = {Xd}D d=1 = X1,..., XD, where Xd = {xd,n}N n=1 and f(X) = ( f1(x1,1),...,f 1(x1,N)),..., (fD(xD,1),...,f D(xD,N)). 3.2 Reproducing Kernel for V ector V alued Function The deﬁnition of RKHS for vector valu"
3703,unknown,"reproducing kernel is now matrix valued, see for example [65, 19] . A reproducing kernel is a sy mmetric function K : X × X → RD×D, such that for any x,x′ K(x,x′) is a positive semi-deﬁnite matrix. A vector valued RKHS is a Hilbert space H of functions f : X → RD, such that for very c ∈ RD, and x ∈ X , K(x,x′)c, as a function of x′ belongs to H and moreover K has the reproducing property ⟨f,K(·,x)"
3704,unknown,"⟨f,K(·,x)c⟩K = f(x)⊤c, where ⟨·,·⟩K is the inner product in H. Again, the choice of the kernel corresponds to the choice of t he representation (parameterization) for the func- tion of interest. In fact any function in the RKHS is in the clo sure of the set of linear combinations f(x) = p∑ i=1 K(xi,x)cj, cj ∈ RD, where we note that in the above equation each term K(xi,x) is a matrix acting on a ve"
3705,unknown,"RKHS typically provides a measure of the complexity of a func tion and this will be the subject of the next sections. Note that the deﬁnition of vector valued RKHS can be describe d in a component-wise fashion in the following sense. The kernel K can be described by a scalar kernel Racting jointly on input examples and task indices, that is (K(x,x′))d,d′ = R((x,d),(x′,d′)), (3) where R is a scalar"
3706,unknown,"dealing with multitask learning, see [28] for a discussion. Provided with the above concepts we can follow a regularizat ion approach to deﬁne an estimator by minimiz- ing the regularized empirical error (2), which in this case c an be written as D∑ j=1 1 N N∑ i=1 (fj(xi) − yj,i)2 + λ∥f∥2 K, (4) where f = ( f1,...,f D). Once again the solution is given by the representer theorem [65] f(x) = N∑ i=1"
3707,unknown,"f(x) = N∑ i=1 K(xi,x)ci, and the coefﬁcient satisﬁes the linear system c = ( K(X,X) + λNI)−1y, (5) where c,y are ND vectors obtained concatenating the coefﬁcients and the out put vectors, and K(X,X) is an ND × ND with entries (K(xi,xj))d,d′ , for i,j = 1 ,...,N and d,d′ = 1 ,...,D (see for example [65]). More explicitly K(X,X) =      (K(X1,X1))1,1 · · · (K(X1,XD))1,D (K(X2,X1))2,1 · · · (K(X2"
3708,unknown,". . . · · · . . . (K(XD,X1))D,1 · · · (K(XD,XD))D,D      (6) 8 where each block (K(Xi,Xj))i,j is an N by N matrix (here we make the simplifying assumption that each ou tput has same number of training data). Note that given a new point x∗ the corresponding prediction is given by f(x∗) = K⊤ x∗ c, where Kx∗ ∈ RD×ND has entries (K(x∗,xj))d,d′ for j = 1 ,...,N and d,d′ = 1 ,...,D . 3.3 Gaussian P"
3709,unknown,"3.3 Gaussian Processes for V ector V alued Functions Gaussian process methods for modeling vector-valued funct ions follow the same approach as in the single output case. Recall that a Gaussian process is deﬁned as a collectio n of random variables, such that any ﬁnite number of them follows a joint Gaussian distribution. In the single output case, the random variables are associated to"
3710,unknown,"a single process f evaluated at different values of x while in the multiple output case, the random variables are associated to different processes {fd}D d=1, evaluated at different values of x [24, 37, 102]. The vector-valued function f is assumed to follow a Gaussian process f ∼ GP (m,K), (7) where m ∈ RD is a vector which components are the mean functions {md(x)}D d=1 of each output and K is a"
3711,unknown,"d=1 of each output and K is a positive matrix valued function as in section 3.2. The entries (K(x,x′))d,d′ in the matrix K(x,x′) correspond to the covariances between the outputs fd(x) and fd′ (x′) and express the degree of correlation or similarity between them. For a set of inputs X, the prior distribution over the vector f(X) is given by f(X) ∼ N (m(X),K(X,X)), where m(X) is a vector that conca"
3712,unknown,"K(X,X) is the block partitioned matrix in (6). Without loss of gener ality , we assume the mean vector to be zero. In a regression context, the likelihood function for the out puts is often taken to be Gaussian distribution, so that p(y|f,x,Σ) = N (f(x),Σ) , where Σ ∈ RD×D is a diagonal matrix with elements 3 {σ2 d}D d=1. For a Gaussian likelihood, the predictive distribution and the marginal like"
3713,unknown,"The predictive distribution for a new vector x∗ is [82] p(f(x∗)|S,f,x∗,φ ) = N (f∗(x∗),K∗(x∗,x∗)) , (8) with f∗(x∗) = K⊤ x∗ (K(X,X) + Σ )−1 y, K∗(x∗,x∗) = K(x∗,x∗) − Kx∗ (K(X,X) + Σ )−1K⊤ x∗ , where Σ = Σ ⊗ IN, Kx∗ ∈ RD×ND has entries (K(x∗,xj))d,d′ for j = 1 ,...,N and d,d′ = 1 ,...,D , and φ denotes a possible set of hyperparameters of the covariance function K(x,x′) used to compute K(X,X) and t"
3714,unknown,variances of the noise for each output {σ2 d}D d=1. Again we note that if we are interested into the distributio n of the noisy predictions it is easy to see that we simply have to a dd P Σ to the expression of the prediction variance. The above expression for the mean prediction coincides agai n with the prediction of the estimator derived in the regularization framework. In the following chapter
3715,unknown,"problems. W e start in the next chapter with kernel functions that clearly separate the contributions of input and output. W e will see later alternative ways to construct kern el functions that interleave both contributions in a non trivial way . 3 This relation derives from yd(x) = fd(x) +ǫd(x), for each d, where {ǫd(x)}D d=1 are independent white Gaussian noise processes with variance σ2 d. 9 4"
3716,unknown,"variance σ2 d. 9 4 Separable Kernels and Sum of Separable Kernels In this chapter we review a special class of multi-output ker nel functions that can be formulated as a sum of products between a kernel function for the input space alone , and a kernel function that encodes the interactions among the outputs. W e refer to this type of multi-output kern el functions as separable kernels and sum of "
3717,unknown,"kernels (SoS kernels). W e consider a class of kernels of the form (K(x,x′))d,d′ = k(x,x′)kT(d,d′), where k,kT are scalar kernels on X × X and {1,...,D } × { 1,...,D }. Equivalently one can consider the matrix expression K(x,x′) = k(x,x′)B, (9) where B is a D× D symmetric and positive semi-deﬁnite matrix. W e call this cl ass of kernels separable since, comparing to (3), we see that the contributi"
3718,unknown,"comparing to (3), we see that the contribution of input and ou tput is decoupled. In the same spirit a more general class of kernels is given by K(x,x′) = Q∑ q=1 kq(x,x′)Bq. For this class of kernels, the kernel matrix associated to a d ata set X has a simpler form and can be written as K(X,X) = Q∑ q=1 Bq ⊗ kq(X,X), (10) where ⊗ represents the Kronecker product between matrices. W e call this clas"
3719,unknown,"(SoS kernels). The simplest example of separable kernel is given by setting kT(d,d′) = δd,d′ , where δd,d′ is the Kronecker delta. In this case B = IN, that is all the outputs are treated as being unrelated. In th is case the kernel matrix K(X,X), associated to some set of data X, becomes block diagonal. Since the off diagonal terms encod e output relatedness. W e can see that the matrix B encodes"
3720,unknown,"The key question is how to choose the scalar kernels {kq}Q q=1 and especially how to design, or learn, the matri- ces {Bq}Q q=1. This is the subject we discuss in the next few sections. W e wi ll see that one can approach the problem from a regularization point of view , where kernels will be de ﬁned by the choice of suitable regularizers, or , from a Bayesian point of view , constructing covarian"
3721,unknown,"components. As it turns out these two points of view are equiv alent and allow for two different interpretations of the same class of models. 4.1 Kernels and Regularizers In this section we largely follow the results in [64, 65, 27] a nd [7]. A possible way to design multi-output kernels of the form (9) is given by the following result. If K is given by (9) then is possible to prove that the norm "
3722,unknown,"function in the corresponding RKHS can be written as ∥f∥2 K = D∑ d,d′=1 B† d,d′ ⟨fd,fd′ ⟩k, (11) where B† is the pseudoinverse of B and f = ( f1,...,f D). The above expression gives another way to see why the matrix B encodes the relation among the components. In fact, we can in terpret the right hand side in the above expression as a regularizer inducing speciﬁc coupling amon g different tasks ⟨f"
3723,unknown,"by B† d,d′ . This result says that any such regularizer induces a kernel of the form (9). W e illustrate the above idea with a few examples. 10 Mixed Effect Regularizer Consider the regularizer given by R(f) = Aω ( Cω D∑ ℓ=1 ∥fℓ∥2 k + ωD D∑ ℓ=1 ∥fℓ − 1 D D∑ q=1 fq∥2 k ) (12) where Aω = 1 2(1−ω)(1−ω+ωD) and Cω = (2 − 2ω + ωD).The above regularizer is composed of two terms: the ﬁrst is a standard re"
3724,unknown,"to be close to the mean estimator across the components, f = 1 D ∑ D q=1 fq. The corresponding kernel imposes a common similarity structure between all the output compone nts and the strength of the similarity is controlled by a parameter ω, Kω(x,x′) = k(x,x′)(ω1 + (1 − ω)ID) (13) where 1 is the D × D matrix whose entries are all equal to 1, and k is a scalar kernel on the input space X ."
3725,unknown,"Setting ω = 0 corresponds to treating all components independently and t he possible similarity among them is not exploited. Conversely , ω= 1 is equivalent to assuming that all components are identical and are explained by the same function. By tuning the parameter ωthe above kernel interpolates between this two opposites ca ses. W e note that from a Bayesian perspective Bis a correlation matrix "
3726,unknown,"that the output of the Gaussian process are exchangeable. Cluster Based Regularizer . Another example of regularizer , proposed in [28], is based o n the idea of grouping the components into rclusters and enforcing the components in each cluster to be s imilar . Following [47], let us deﬁne the matrix E as the D× r matrix, where ris the number of clusters, such that Eℓ,c = 1 if the component l bel"
3727,unknown,"l belongs to cluster cand 0 otherwise. Then we can compute the D× D matrix M = E(E⊤E)−1E⊤ such that Mℓ,q = 1 mc if components l and q belong to the same cluster c, and mc is its cardinality , Mℓ,q = 0 otherwise. Furthermore let I(c) be the index set of the components that belong to cluster c. Then we can consider the following regularizer that forces components belonging to the same cl uster to be"
3728,unknown,"R(f) = ǫ1 r∑ c=1 ∑ ℓ∈I(c) ∥fℓ − fc∥2 k + ǫ2 r∑ c=1 mc∥ fc∥2 k, (14) where fc is the mean of the components in cluster cand ǫ1,ǫ2 are parameters balancing the two terms. Straight- forward calculations show that the previous regularizer ca n be rewritten as R(f) = ∑ ℓ,qGℓ,q⟨fℓ,fq⟩k, where Gℓ,q = ǫ1δlq + (ǫ2 − ǫ1)Mℓ,q. (15) Therefore the corresponding matrix valued kernel is K(x,x′) = k(x,x′)G†."
3729,unknown,"Therefore the corresponding matrix valued kernel is K(x,x′) = k(x,x′)G†. Graph Regularizer . Following [64, 91], we can deﬁne a regularizer that, in addit ion to a standard regulariza- tion on the single components, forces stronger or weaker sim ilarity between them through a given D× Dpositive weight matrix M, R(f) = 1 2 D∑ ℓ,q=1 ∥fℓ − fq∥2 kMℓq + D∑ ℓ=1 ∥fℓ∥2 kMℓ,ℓ. (16) The regularizer J(f) can"
3730,unknown,"D∑ ℓ,q=1 ( ∥fℓ∥2 kMℓ,q − ⟨fℓ,fq⟩kMℓ,q ) + D∑ ℓ=1 ∥fℓ∥2 kMℓ,ℓ = D∑ ℓ=1 ∥fℓ∥2 k D∑ q=1 (1 + δℓ,q)Mℓ,q − D∑ ℓ,q=1 ⟨fℓ,fq⟩kMℓ,q = D∑ ℓ,q=1 ⟨fℓ,fq⟩kLℓ,q (17) where L = D − M, with Dℓ,q = δℓ,q (∑ D h=1 Mℓ,h + Mℓ,q ) . Therefore the resulting kernel will be K(x,x′) = k(x,x′)L†, with k(x,x′) a scalar kernel to be chosen according to the problem at hand. In the next section we will see how models related t"
3731,unknown,"generative models. 11 4.2 Coregionalization Models The use of probabilistic models and Gaussian processes for m ulti-output learning was pioneered and largely de- veloped in the context of geostatistics, where prediction o ver vector-valued output data is known as cokriging. Geostatistical approaches to multivariate modelling are m ostly formulated around the “linear model of coregion-"
3732,unknown,"alization” (LMC) [49, 37], that can be considered as a genera tive approach for developing valid covariance func- tions. Covariance functions obtained under the LMC assumpt ion follow the form of a sum of separable kernels. W e will start considering this model and then discuss how sev eral models recently proposed in the machine learn- ing literature are special cases of the LMC. 4.2.1 The Linear"
3733,unknown,"ing literature are special cases of the LMC. 4.2.1 The Linear Model of Coregionalization In the linear model of coregionalization, the outputs are ex pressed as linear combinations of independent random functions. This is done in a way that ensures that the resulti ng covariance function (expressed jointly over all the outputs and the inputs) is a valid positive semideﬁnite func tion. Consider a s"
3734,unknown,"d=1 with x ∈ Rp. In the LMC, each component fd is expressed as [49] fd(x) = Q∑ q=1 ad,quq(x), where the latent functions uq(x), have mean zero and covariance cov[uq(x),uq′ (x′)] = kq(x,x′) if q= q′, and ad,q are scalar coefﬁcients. The processes {uq(x)}Q q=1 are independent for q ̸= q′. The independence assumption can be relaxed and such relaxation is presented as an extension i n section 4.3. Som"
3735,unknown,"uq′ (x′) can have the same covariance kq(x,x′), while remaining independent. A similar expression for {fd(x)}D d=1 can be written grouping the functions uq(x) which share the same covari- ance [49, 37] fd(x) = Q∑ q=1 Rq∑ i=1 ai d,qui q(x), (18) where the functions ui q(x), with q = 1 ,...,Q and i = 1 ,...,R q, have mean equal to zero and covariance cov[ui q(x),ui′ q′ (x′)] = kq(x,x′) if i = i′ and"
3736,unknown,"ui q(x) and that the functions ui q(x) within each group share the same covariance, but are indepen dent. The cross covariance between any two functions fd(x) and fd′ (x) is given in terms of the covariance functions for ui q(x) cov[fd(x),fd′ (x′)] = Q∑ q=1 Q∑ q′=1 Rq∑ i=1 Rq∑ i′=1 ai d,qai′ d′,q′ cov[ui q(x),ui′ q′ (x′)]. The covariance cov[fd(x),fd′ (x′)] is given by (K(x,x′))d,d′ . Due to the i"
3737,unknown,"q(x), the above expression reduces to (K(x,x′))d,d′ = Q∑ q=1 Rq∑ i=1 ai d,qai d′,qkq(x,x′) = Q∑ q=1 bq d,d′ kq(x,x′), (19) with bq d,d′ = ∑ Rq i=1 ai d,qai d′,q. The kernel K(x,x′) can now be expressed as K(x,x′) = Q∑ q=1 Bqkq(x,x′), (20) where each Bq ∈ RD×D is known as a coregionalization matrix . The elements of each Bq are the coefﬁcients bq d,d′ appearing in equation (19). The rank for each m"
3738,unknown,"the same covariance function kq(x,x′), that is, by the coefﬁcient Rq. Equation (18) can be interpreted as a nested structure [104] in which the outputs fd(x) are ﬁrst expressed as a linear combination of spatially uncorrelated processes fd(x) = ∑ Q q=1 fq d(x),with E[fq d(x)] = 0 and cov[fq d(x),fq′ d′ (x′)] = bq d,d′ kq(x,x′) if q= q′, otherwise it is equal to zero. At the same time, each process"
3739,unknown,"d(x) can be represented as a set 12 of uncorrelated functions weighted by the coefﬁcients ai d,q, fq d(x) = ∑ Rq i=1 ai d,qui q(x) where again, the covariance function for ui q(x) is kq(x,x′). Therefore, starting from a generative model for the outputs , the linear model of coregionalization leads to a sum of separable kernels that represents the covariance functi on as the sum of the products of "
3740,unknown,"one that models the dependence between the outputs, indepen dently of the input vector x (the coregionalization matrix Bq), and one that models the input dependence, independently o f the particular set of functions {fd(x)} (the covariance function kq(x,x′)). The covariance matrix for f(X) is given by (10). 4.2.2 Intrinsic Coregionalization Model A simpliﬁed version of the LMC, known as the intrin"
3741,unknown,"elements bq d,d′ of the coregionalization matrix Bq can be written as bq d,d′ = υd,d′ bq, for some suitable coefﬁcients υd,d′ . With this form for bq d,d′ , we have cov[fd(x),fd′ (x′)] = Q∑ q=1 υd,d′ bqkq(x,x′),= υd,d′ Q∑ q=1 bqkq(x,x′) = υd,d′ k(x,x′), where k(x,x′) = ∑ Q q=1 bqkq(x,x′). The above expression can be seen as a particular case of the k ernel function ob- tained from the linear model"
3742,unknown,"i=1 ai d,1ai d′,1 = b1 d,d′ , and the kernel matrix for multiple outputs becomes K(x,x′) = k(x,x′)B as in (9). The kernel matrix corresponding to a dataset X takes the form K(X,X) = B ⊗ k(X,X). (21) One can see that the intrinsic coregionalization model corr esponds to the special separable kernel often used in the context of regularization. Notice that the value of R1 for the coefﬁcients υd,d′ = "
3743,unknown,"i=1 ai d,1ai d′,1 = b1 d,d′ , determines the rank of the matrix B. As pointed out by [37], the ICM is much more restrictive than t he LMC since it assumes that each basic covari- ance kq(x,x′) contributes equally to the construction of the autocovaria nces and cross covariances for the outputs. However , the computations required for the corresponding i nference are greatly simpliﬁed, essentially "
3744,unknown,"the properties of the Kronecker product. This latter point i s discussed in detail in Section 6. It can be shown that if the outputs are considered to be noise- free, prediction using the intrinsic coregional- ization model under an isotopic data case is equivalent to in dependent prediction over each output [41]. This circumstance is also known as autokrigeability [104]. 4.2.3 Comparison Between "
3745,unknown,circumstance is also known as autokrigeability [104]. 4.2.3 Comparison Between ICM and LMC W e have seen before that the intrinsic coregionalization mo del is a particular case of the linear model of core- gionalization for Q = 1 (with Rq ̸= 1 ) in equation 19. Here we contrast these two models. Note that a different particular case of the linear model of coregionalization is assuming Rq = 1 (with
3746,unknown,"the machine learning literature as the semiparametric late nt factor model (SLFM) [96], will be introduced in the next subsection. T o compare the two models we have sampled from a multi-output Gaussian process with two outputs ( D= 2 ), a one-dimensional input space ( x∈ R) and a LMC with different values for Rq and Q. As basic kernels kq(x,x′) we have used the exponentiated quadratic (EQ) kernel"
3747,unknown,"kq(x,x′) = exp ( − ∥x − x′∥2 ℓ2 q ) , where ∥·∥ represents the Euclidian norm and ℓq is known as the characteristic length-scale. The exponenti ated quadratic is variously referred to as the Gaussian, the radi al basis function or the squared exponential kernel. Figure 2 shows samples from the intrinsic coregionalizatio n model for Rq = 1 , meaning a coregionalization matrix B1 of rank one. Sample"
3748,unknown,"though. Each sample may be considered as a scaled version of t he latent function, as it can be seen from equation 18 with Q= 1 and Rq = 1 , f1(x) = a1 1,1u1 1(x), f 2(x) = a1 2,1u1 1(x), 13 0 1 2 3 4 5−1 0 1 2 ICM Rq = 1, f1(x) 0 1 2 3 4 5−5 0 5 10 ICM Rq = 1, f2(x) Figure 2: T wo samples from the intrinsic coregionalization model with rank one, this is Rq = 1. Solid lines represent one of the sa"
3749,unknown,"where we have used xinstead of x for the one-dimensional input space. Figure 3 shows samples from an ICM of rank two. From equation 1 8, we have for Q= 1 and Rq = 2 , f1(x) = a1 1,1u1 1(x) + a2 1,1u2 1(x), f 2(x) = a1 2,1u1 1(x) + a2 2,1u2 1(x), where u1 1(x) and u2 1(x) are sampled from the same Gaussian process. Outputs are weig hted sums of two different latent functions that share the same cov"
3750,unknown,"outputs have different forms, although they share the same l ength-scale. Figure 4 displays outputs sampled from a LMC with Rq = 1 and two latent functions ( Q = 2 ) with different length-scales. Notice that both samples are combinations o f two terms, a long length-scale term and a short length- scale term. According to equation 18, outputs are given as f1(x) = a1 1,1u1 1(x) + a1 1,2u1 2(x), f 2("
3751,unknown,"f1(x) = a1 1,1u1 1(x) + a1 1,2u1 2(x), f 2(x) = a1 2,1u1 1(x) + a1 2,2u1 2(x), where u1 1(x) and u1 2(x) are samples from two Gaussian processes with different cova riance functions. In a similar way to the ICM of rank one (see ﬁgure 2), samples from both outp uts have the same form, this is, they are aligned. W e have the additional case for a LMC with Rq = 2 and Q = 2 in ﬁgure 5. According to eq"
3752,unknown,"outputs are give as f1(x) = a1 1,1u1 1(x) + a2 1,1u2 1(x) + a1 1,2u1 2(x) + a2 1,2u2 2(x), f2(x) = a1 2,1u1 1(x) + a2 2,1u2 1(x) + a1 2,2u1 2(x) + a2 2,2u2 2(x), 14 0 1 2 3 4 5−2 −1 0 1 2 ICM Rq = 2, f1(x) 0 1 2 3 4 5−4 −2 0 2 ICM Rq = 2, f2(x) Figure 3: T wo samples from the intrinsic coregionalization model with rank two, Rq = 2. Solid lines and dashed lines represent different samples. Although"
3753,unknown,"different and are not simply scaled versions of one another . where the pair of latent functions u1 1(x) and u2 1(x) share their covariance function and the pair of latent funct ions u1 2(x) and u2 2(x) also share their covariance function. As in the case of the LM C with Rq = 1 and Q= 2 in ﬁgure 4, the outputs are combinations of a term with a long length-s cale and a term with a short length-sca"
3754,unknown,"difference however , is that, for Rq = 2 and Q= 2 , samples from different outputs have different shapes. 4 4.2.4 Linear Model of Coregionalization in Machine Learnin g and Statistics The linear model of coregionalization has already been used in machine learning in the context of Gaussian pro- cesses for multivariate regression and in statistics for co mputer emulation of expensive multivariate c"
3755,unknown,"codes. As we have seen before, the linear model of coregionalizatio n imposes the correlation of the outputs explicitly through the set of coregionalization matrices. A simple ide a used in the early papers of multi-output GPs for machine learning was based on the intrinsic coregionalizat ion model and assumed B = ID. In other words, the outputs were considered to be conditionally independent gi v"
3756,unknown,"was assumed to exist implicitly by imposing the same set of hy perparameters φ for all outputs and estimating those parameters, or the kernel matrix k(X,X) directly , using data from all the outputs [66, 55, 113]. 4 Notice that samples from each output are not synchronized, m eaning that the maximums and minimus do not always occur at th e same input points. 15 0 1 2 3 4 5−2 0 2 4 LMC with Rq = 1 "
3757,unknown,"0 2 4 LMC with Rq = 1 and Q = 2, f1(x) 0 1 2 3 4 5−2 0 2 4 LMC with Rq = 1 and Q = 2, f2(x) Figure 4: T wo samples from a linear model of coregionalizati on with Rq = 1and Q = 2. The solid lines represent one of the samples. The dashed lines represent the other samp le. Samples are the weigthed sums of latent functions with different length-scales. In this section, we review more recent approaches"
3758,unknown,"the linear model of coregionalization. Semiparametric latent factor model. The semiparametric latent factor model (SLFM) proposed by [ 96] turns out to be a simpliﬁed version of the LMC. In fact it correspond s to setting Rq = 1 in (18) so that we can rewrite equation (10) as K(X,X) = Q∑ q=1 aqa⊤ q ⊗ kq(X,X), where aq ∈ RD×1 with elements {ad,q}D d=1 and q ﬁxed. With some algebraic manipulations, "
3759,unknown,"properties of the Kronecker product, we can write K(X,X) = Q∑ q=1 (aq ⊗ IN)kq(X,X)(a⊤ q ⊗ IN) = ( ˜A ⊗ IN) ˜K( ˜A⊤ ⊗ IN), where ˜A ∈ RD×Q is a matrix with columns aq and ˜K ∈ RQN×QN is a block diagonal matrix with blocks given by kq(X,X). 16 0 1 2 3 4 5−5 0 5 LMC with Rq = 2 and Q = 2, f1(x) 0 1 2 3 4 5−5 0 5 LMC with Rq = 2 and Q = 2, f2(x) Figure 5: T wo samples from a linear model of coregional"
3760,unknown,"one of the samples. The dashed lines represent the other samp le. Samples are the weigthed sums of four latent functions, two of them share a covariance with a long length- scale and the other two share a covariance with a shorter length-scale. The functions uq(x) are considered to be latent factors and the semiparametric n ame comes from the fact that it is combining a nonparametric model, that i"
3761,unknown,"functions uq(x). The kernels kq, for each basic process is assumed to be exponentiated quadr atic with a different characteristic length-scale for each input dimension. The informative vector machine (IVM) [57] is employed to speed up computations. Gaussian processes for Multi-task, Multi-output and Multi -class The intrinsic coregionalization model is considered by [12] in the context of multitas"
3762,unknown,"analysis (PPCA) model to represent the matrix B. The spectral factorization in the PPCA model is replaced by an incomplete Cholesky decomposition to keep numerical sta bility . The authors also refer to the autokrigeability effect as the cancellation of inter-task transfer [12], and discuss the similarities between the multi-task GP and the ICM, and its relationship to the SLFM and the LMC. The in"
3763,unknown,"spherical parametrization, B = diag( e)S⊤S diag(e), where e gives a description for the scale length of each output variable and S is an upper triangular matrix whose i-th column is associated with particular spherical coordin ates of points in Ri (for details see sec. 3.4 [71]). The scalar kernel k is represented through a Mat ´ ern kernel, where different parameterizations allow the expression o"
3764,unknown,"is obtained using an IVM style approach. In a classiﬁcation context, Gaussian processes methodolog y has been mostly restricted to the case where the 17 outputs are conditionally independent given the hyperpara meters φ [66, 110, 55, 89, 113, 82]. Therefore, the kernel matrix K(X,X) takes a block-diagonal form, with blocks given by (K(Xd,Xd))d,d. Correlation between the out- puts is assumed to exi"
3765,unknown,"those parameters, or directly the kernel matrices (K(Xd,Xd))d,d, using data from all the outputs [66, 55, 113, 82]. Alternatively , it is also possible to have parameters φ d associated to each output [110, 89]. Only recently , the intrinsic coregionalization model has b een used in the multiclass scenario. In [93], the authors use the intrinsic coregionalization model for classiﬁcati on, by intro"
3766,unknown,"Since the posterior distribution is no longer analytically tractable, the authors use Gibbs sampling, Expectation- Propagation (EP) and variational Bayes 5 to approximate the distribution. Computer emulation. A computer emulator is a statistical model used as a surrogat e for a computationally expensive deterministic model or computer code, also known as a simulator . Gaussian processes have becom"
3767,unknown,"the preferred statistical model among computer emulation p ractitioners (for a review see [70]). Different Gaussian process emulators have been recently proposed to deal with s everal outputs [42, 23, 83, 63, 9, 79]. In [42], the linear model of coregionalization is used to mod el images representing the evolution of the implo- sion of steel cylinders after using TNT and obtained employi ng the so"
3768,unknown,"[42] for further details). The input variable x represents parameters of the simulation model, while the ou tput is an image of the radius of the inner shell of the cylinder over a ﬁxed grid of times and angles. In the version of the LMC that the authors employed, Rq = 1 and the Qvectors aq were obtained as the eigenvectors of a PCA decomposition of the set of training images. In [23], the intrins"
3769,unknown,"called the Shefﬁeld Dynamic Global V egetation Model (SDGVM ) [111]. Authors refer to the ICM as the Multiple- Output (MO) emulator . The inputs to the model are ten ( p = 10 ) variables related to broad soil, vegetation and climate data, while the outputs are time series of the net bio me productivity (NBP) index measured at a particular site in a forest area of Harwood, UK. The NBP index account"
3770,unknown,"site after some natural processes have taken place. In the pa per , the authors assume that the outputs correspond to the different sampling time points, so that D = T, being T the number of time points, while each observation corresponds to speciﬁc values of the ten input variables. V a lues of the input variables are chosen according to a maxi-min Latin hypercube design. Rougier [83] introduces "
3771,unknown,"seen as a single variable while augmenting the input space wi th an additional index over the outputs. In other words, it considers the output variable as an input variable . [23], refers to the model in [83] as the Time Input (TI) emulator and discussed how the TI model turns out to be a p articular case of the MO model that assumes a particular exponentiated quadratic kernel (see chapter 4 [ 82]"
3772,unknown,"McFarland et al. [63] consider a multiple-output problem as a single output o ne. The setup is similar to the one used in [23], where the number of outputs are associated t o different time points, this is, D = T. The outputs correspond to the time evolutions of the temperature of cert ain location of a container with decomposing foam, as function of ﬁve different calibration variables (input vari"
3773,unknown,"index as an input (akin to [83]) and apply a greedy-like algor ithm to select the training points for the Gaussian process. Greedy approximations like this one have also been used in the machine learning literature (for details, see [82], page 174). Similar to [83] and [63], Bayarri et al. [9] use the time index as an input for a computer emulator that evaluates the accuracy of CRASH, a computer m"
3774,unknown,"of barriers. Quian et al. [79] propose a computer emulator based on Gaussian processe s that supports quantitative and qualitative inputs. The covariance function in this comput er emulator is related to the ICM in the case of one qualitative factor: the qualitative factor is considered t o be the index of the output, and the covariance function takes again the form k(x,x′)kT(d,d′). In the case of"
3775,unknown,"be considered a multiple output GP in which each output index would correspond to a particular combination of the possible values taken by the qualitative factors. In thi s case, the matrix B in ICM would have a block diagonal form, each block determining the covariance between the val ues taken by a particular qualitative input. 5 Mathematical treatment for each of these inference methods can be "
3776,unknown,"18 4.3 Extensions In this section we describe further developments related to the setting of separable kernels or SoS kernels, both from a regularization and a Bayesian perspective. 4.3.1 Extensions Within the Regularization Framework When we consider kernels of the form K(x,x′) = k(x,x′)B, a natural question is whether the matrix B can be learned from data. In a regression setting, one idea is to"
3777,unknown,matrix of the output vectors in the training set and this is st andard in the geostatistics literature [104]. A further question is whether we can learn both B and an estimator within a unique inference step. This is the q uestion tackled in [48]. The authors consider a variation of the regu larizer in (14) and try to learn the cluster matrix as a part of the optimization process. More precisely t
3778,unknown,"R(f) = ǫ1∥ f∥k + ǫ2 r∑ c=1 mc∥fc − f∥2 k + ǫ3 r∑ c=1 ∑ l∈I(c) ∥fl − fc∥2 k, (22) where we recall that ris the number of clusters. The three terms in the functional c an be seen as: a global penalty , a term penalizing between cluster variance and a term penalizing within cluster variance. As in the case of the regularizer in (14), the above regularizer is completely ch aracterized by a cluster mat"
3779,unknown,"(note that the corresponding matrix B will be slightly different from (15)). The idea is then to consider a regularized functional D∑ i=1 1 N N∑ i=1 (fj(xi) − yj,i)2 + λRM(f) (23) to be minimized jointly over f and M (see [48] for details). This problem is typically non tracta ble from a com- putational point of view , so the authors in [48] propose a rel axation of the problem which can be shown "
3780,unknown,"convex. A different approach is taken in [4] and [5]. In this case the i dea is that only a a small subset of features is useful to learn all the components/tasks. In the simplest case the a uthors propose to minimize a functional of the form D∑ d=1 { 1 N N∑ i=1 (w⊤ d U⊤xi − yd,i)2 + λw⊤ d wd } . over w1,..., wD ∈ Rp,U ∈ RD×D under the constraint T r (U⊤ t Ut) ≤ γ. Note that the minimization over "
3781,unknown,"t Ut) ≤ γ. Note that the minimization over the matrix U couples the otherwise disjoint component-wise problems. T he authors of [4] discuss how the above model is equivalent to considering a kernel of the form K(x,x′) = kD(x,x′)ID, k D(x,x′) = x⊤Dx′ where D is a positive deﬁnite matrix and a model which can be describe d components wise as fd(x) = p∑ i=1 ad,ixj = a⊤ dx, making apparent the connect"
3782,unknown,"problem is equivalent to minimizing D∑ d=1 1 N N∑ i=1 (a⊤ dxi − yd,i)2 + λ D∑ d=1 a⊤ dDad, (24) over a′ 1,..., a′ D ∈ Rp and T r (D) ≤ 1, where the last restriction is a convex approximation of the low rank re- quirement. Note that from a Bayesian perspective the above s cheme can be interpreted as learning a covariance matrix for the response variables which is optimal for all th e tasks. In [4],"
3783,unknown,"setting where D is replaced by F(D) and show that if the matrix valued function F is matrix concave, then the induced minimization problem is jointly convex in (ai) and D. Moreover , the authors discuss how to extend the 19 above framework to the case of more general kernel functions . Note that an approach similar to the one we just described is at the basis of recent work exploiting the conce pt"
3784,unknown,methods cannot in general be cast in the framework of kernel m ethods and we refer the interested reader to [69] and references therein. For the reasoning above the key assumption is that a response variable is either important for all the tasks or not. In practice it is probably often the case that only certa in subgroups of tasks share the same variables. This idea is at the basis of the study i
3785,unknown,"the best set of variables for each groups of tasks. Let G = ( Gt)⊤ t=1 be a partition of the set of components/tasks, where Gt denotes a group of tasks and |Gt| ≤ D. Then the author propose to consider a functional of the form min G ∑ Gt∈G min ad,d∈Gt,Ut ∑ d∈Gt { 1 N N∑ i=1 (a⊤ dU⊤ t xi − yd,i)2 + λw⊤ d w⊤ d + γT r(U⊤ t Ut) } , where U1,...U T is a sequence of pby pmatrices. The authors show that "
3786,unknown,"is not convex, stochastic gradient descent can be used to ﬁnd local minimizers which seems to perform well in practice. 4.3.2 Extensions from the Gaussian Processes Perspective A recent extension of the linear model of coregionalization expresses the output covariance function through a linear combination of nonorthogonal latent functions [39] . In particular , the basic processes ui q(x) are assu"
3787,unknown,"be nonorthogonal, leading to the following covariance func tion cov[f(x),f(x′)] = Q∑ q=1 Q∑ q′=1 Bq,q′ kq,q′ (x,x′), where Bq,q′ are cross-coregionalization matrices. Cross-covariances kq,q′ (x,x′) can be negative (while keeping pos- itive semideﬁniteness for cov[f(x),f(x′)]), allowing negative cross-covariances in the linear model of coregional- ization. The authors argue that, in some real scena"
3788,unknown,"combined to represent a single model and give examples in min ing, hydrology and oil industry [39]. 5 Beyond Separable Kernels W orking with separable kernels or SoS kernels is appealing f or their simplicity , but can be limiting in several applications. Next we review different types of kernels tha t go beyond the separable case or SoS case. 5.1 Invariant Kernels Divergence free and curl free ﬁe"
3789,unknown,"(EQ) kernels [68] and can be used to estimate divergence-fre e or curl-free vector ﬁelds [60] when the input and output space have the same dimension. These kernels induce a similarity between the vector ﬁeld components that depends on the input points, and therefore cannot be red uced to the form K(x,x′) = k(x,x′)B. W e consider the case of vector ﬁelds with D = p, where X = Rp. The divergence-fr"
3790,unknown,"can be deﬁned via a translation invariant matrix-valued EQ k ernel Φ( u) = ( ∇∇⊤ − ∇⊤∇I)φ(u) = Hφ(u) − tr(Hφ(u))ID , where His the Hessian operator and φa scalar EQ kernel, so that K(x,x′) := Φ( x − x′). The columns of the matrix valued EQ kernel, Φ , are divergence-free. In fact, computing the divergence of a linear combination of its columns, ∇⊤(Φ( u)c), with c∈ Rp, it is possible to show that ["
3791,unknown,"∇⊤(Φ( u)c) = ( ∇⊤∇∇⊤φ(u))c− (∇⊤∇⊤∇φ(u))c= 0 , 20 where the last equality follows applying the product rule of the gradient, the fact that the coefﬁcient vector cdoes not depend upon uand the equality a⊤aa⊤ = a⊤a⊤a,∀a∈ Rp. Choosing a exponentiated quadratic, we obtain the divergen ce-free kernel K(x,x′) = 1 σ2 e− ∥ x−x′∥ 2 2σ 2 Ax,x′ , (25) where Ax,x′ = ((x − x′ σ )(x − x′ σ )⊤ + ( (p− 1) − ∥x − x"
3792,unknown,"σ )⊤ + ( (p− 1) − ∥x − x′∥2 σ2 ) Ip ) . The curl-free matrix valued kernels are obtained as K(x,x′) := Ψ( x − x′) = −∇∇⊤φ(x − x′) = −Hφ(x − x′) , where φis a scalar RBF . It is easy to show that the columns of Ψ are curl-free. The j-th column of Ψ is given by Ψ ej, where ej is the standard basis vector with a one in the j-th position. This gives us Φ cfej = −∇∇⊤Φ cfej = ∇(−∇⊤Φ cfej) = ∇g ,"
3793,unknown,"Φ cfej = −∇∇⊤Φ cfej = ∇(−∇⊤Φ cfej) = ∇g , where g = −∂φ/∂xj. The function gis a scalar function and the curl of the gradient of a scalar fu nction is always zero. Choosing a exponentiated quadratic, we obtain the fol lowing curl-free kernel Γ cf(x,x′) = 1 σ2 e− ∥ x−x′∥ 2 2σ 2 ( ID − (x − x′ σ )(x − x′ σ )⊤ ) . (26) It is possible to consider a convex linear combination of the se two kernels to obt"
3794,unknown,"of vector ﬁeld, while at the same time allowing reconstructi on of the divergence-free and curl-free parts separately (see [60]). The interested reader can refer to [68, 59, 32] fo r further details on matrix-valued RBF and the properties of divergence-free and curl-free kernels. T ransformable kernels. Another example of invariant kernels is discussed in [18] an d is given by kernels deﬁned by tr"
3795,unknown,"family of maps (not necessarily linear) from X to X0 for d= {1,...,D } . Then, given a continuous scalar kernel k : X0 × X 0 → R, it is possible to deﬁne the following matrix valued kernel for any x,x′ ∈ X ( K(x,x′) ) d,d′ = k(Tdx,Td′ x′). A speciﬁc instance of the above example is described by [101] in the context of system identiﬁcation, see also [18] for further details. 5.2 Further Extensions "
3796,unknown,"for further details. 5.2 Further Extensions of the LMC In [34], the authors introduced a nonstationary version of t he LMC, in which the coregionalization matrices are allowed to vary as functions of the input variables. In parti cular , Bq now depends on the input variable x, this is, Bq(x,x′). The authors refer to this model as the spatially varying LMC (SVLMC). As a model for the varying core-"
3797,unknown,"gionalization matrix Bq(x,x′), the authors employ two alternatives. In one of them, they as sume that Bq(x,x′) = (α (x,x′))ψBq, where α (x) is a covariate of the input variable, and ψ is a variable that follows a uniform prior . In the other alternative, Bq(x,x′) follows a Wishart spatial process, which is constructed usi ng the deﬁnition of a Wishart distribution, as follows. Suppose Z ∈ RD×P is "
3798,unknown,"dently and identically distributed, for d= 1 ,...,D and p= 1 ,...,P . Deﬁne the matrix Υ = ΓZ , with Γ ∈ RD×D. The matrix Ω = ΥΥ ⊤ = ΓZZ ⊤Γ ⊤ ∈ RD×D follows a Wishart distribution W(P,ΓΓ ⊤), where P is known as the number of degrees of freedom of the distribution. The spatial Wishart process is constru cted assuming that zd,p depends on the input x, this is, zd,p(x,x′), with zd,p(x,x′) ∼ N (0,ρd(x"
3799,unknown,"d=1 are correlation functions. Matrix Υ (x,x′) = ΓZ (x,x′) and Ω (x,x′) = Υ (x,x′)Υ ⊤(x,x′) = ΓZ (x,x′)Z⊤(x,x′)Γ ⊤ ∈ RD×D follows a spatial Wishart process SW (P,ΓΓ ⊤,{ρd(x,x′)}D d=1). In [34], authors assume Γ = ID and ρd(x,x′) is the same for all values of d. Inference in this model is accomplished using Markov chain Monte Carlo. For details about the particular inference procedure, the reader i"
3800,unknown,21 5.3 Process Convolutions More general non-separable kernels can also be constructed from a generative point of view . W e saw in section 4.2.1 that the linear model of coregionalization involves i nstantaneous mixing through a linear weighted sum of independent processes to construct correlated processe s. By instantaneous mixing we mean that the output function f(x) evaluated at the input poin
3801,unknown,"q=1 at the same input x. This instantaneous mixing leads to a kernel function for ve ctor-valued functions that has a separable form. A non-trivial way to mix the latent functions is through conv olving a base process with a smoothing kernel. 6 If the base process is a Gaussian process, it turns out that the c onvolved process is also a Gaussian process. W e can therefore exploit convolutions to c"
3802,unknown,"therefore exploit convolutions to construct covariance fu nctions [8, 102, 43, 44, 14, 56, 2]. In a similar way to the linear model of coregionalization, we consider Qgroups of functions, where a particular group qhas elements ui q(z), for i= 1 ,...,R q. Each member of the group has the same covariance kq(x,x′), but is sampled independently . Any output fd(x) is described by fd(x) = Q∑ q=1 Rq∑ i=1"
3803,unknown,"Rq∑ i=1 ∫ X Gi d,q(x − z)ui q(z)dz + wd(x) = Q∑ q=1 fq d(x) + wd(x), where fq d(x) = Rq∑ i=1 ∫ X Gi d,q(x − z)ui q(z)dz, (27) and {wd(x)}D d=1 are independent Gaussian processes with zero mean and covar iance kwd (x,x′). For the integrals in equation (27) to exist, it is assumed that each kernel Gi d,q(x) is a continuous function with compact support [46] or square-integrable [102, 44]. The kernel"
3804,unknown,"d,q(x) is also known as the moving average function [102] or the smoothing kernel [44]. W e have included the superscript qfor fq d(x) in (27) to emphasize the fact that the function depends on the set of latent processes {ui q(x)} Rq i=1. The latent functions ui q(z) are Gaussian processes with general covariances kq(x,x′). Under the same independence assumptions used in the linear m odel of core"
3805,unknown,"tween fd(x) and fd′ (x′) follows (K(x,x′))d,d′ = Q∑ q=1 kfq d ,fq d′ (x,x′) + kwd (x,x′)δd,d′ , (28) where kfq d ,fq d′ (x,x′) = Rq∑ i=1 ∫ X Gi d,q(x − z) ∫ X Gi d′,q(x′ − z′)kq(z,z′)dz′dz. (29) Specifying Gi d,q(x − z) and kq(z,z′) in the equation above, the covariance for the outputs fd(x) can be constructed indirectly . Notice that if the smoothing kernels are taken t o be the Dirac delta funct"
3806,unknown,"that Gi d,q(x − z) = ai d,qδ(x − z),7 the double integral is easily solved and the linear model of c oregionalization is recovered. In this respect, process convolutions could a lso be seen as a dynamic version of the linear model of coregionalization in the sense that the latent functions ar e dynamically transformed with the help of the kernel smoothing functions, as opposed to a static mapping "
3807,unknown,"a comparison between the process convolution and the LMC. A recent review of several extensions of this approach for th e single output case is presented in [17]. Some of those extensions include the construction of nonstation ary covariances [43, 45, 30, 31, 73] and spatiotemporal covariances [109, 107, 108]. The idea of using convolutions for constructing multiple ou tput covariances was origina"
3808,unknown,"They assumed that Q = 1 , Rq = 1 , that the process u(x) was white Gaussian noise and that the input space was 6 W e use kernel to refer to both reproducing kernels and smooth ing kernels. Smoothing kernels are functions which are conv olved with a signal to create a smoothed version of that signal. 7 W e have slightly abused of the delta notation to indicate the Kronecker delta for discrete argum"
3809,unknown,"arguments. The particular meaning should be understood fro m the context. 22 X = Rp. [44] depicted a similar construction to the one introduced by [102], but partitioned the input space into disjoint subsets X = ⋂D d=0 Xd, allowing dependence between the outputs only in certain su bsets of the input space where the latent process was common to all convolutions. 8 Higdon [44] coined the general mov"
3810,unknown,"as a process convolution . Boyle and Frean [14, 15] introduced the process convolution approach for multiple outputs to the machine learning community with the name of “dependent Gaussian pro cesses” (DGP), further developed in [13]. They allow the number of latent functions to be greater than one ( Q≥ 1). In [56] and [2], the latent processes {uq(x)}Q q=1 followed a more general Gaussian process "
3811,unknown,"5.3.1 Comparison Between Process Convolutions and LMC Figure 6 shows an example of the instantaneous mixing effect obtained in the ICM and the LMC, and the non- instantaneous mixing effect due to the process convolution framework. W e sampled twice from a two-output Gaussian process with an ICM covariance with Rq = 1 (ﬁrst column), an LMC covariance with Rq = 2 (second column) and a process convol"
3812,unknown,"the LMC, we use EQ kernels for the basic kernels kq(x,x′). W e also use an exponentiated quadraticform for the smoothing kernel functions G1 1,1(x − x′) and G1 2,1(x − x′) and assume that the latent function is white Gaussian noise. Notice from Figure 6 that samples from the ICM share the same l ength-scale. Samples from the LMC are weighted sums of functions with different length-scales (a long l"
3813,unknown,"In both models, ICM and LMC, the two outputs share the same len gth-scale or the same combination of length- scales. Samples from the PC show that the contribution from t he latent function is different over each output. Output f1(x) has a long length-scale behavior , while output f2(x) has a short length-scale behavior . It would be possible to get similar samples to the PC ones usin g a LMC. W e"
3814,unknown,"that some covariances in a particular coregionalization ma trix Bq are zero. In Figure 7, we display samples from a LMC with Rq = 2 and Q= 2 . W e have forced b2 1,1 = b2 1,2 = b2 2,1 = 0 . T o generate these samples we use an ICM with Rq = 2 and a latent function with long length-scale, and then add a s ample from an independent Gaussian process with a short length-scale to output f2(x). It is de"
3815,unknown,"GP) would capture the relevant correlations between the out put functions. T o summarize, the choice of a kernel corresponds to specifyi ng dependencies among inputs and outputs. In the linear model of co-regionalization this is done conside ring separately inputs, via the kernels kq, and outputs, via the coregionalization matrices Bq, for q = 1 ,...,Q . Having a large large value of Qallows for a"
3816,unknown,"pressive power of the model. For example if the output compon ents are substantially different functions (different smoothness or length scale), we might be able to capture thei r variability by choosing a sufﬁciently large Q. This is at the expense of a larger computational burden. On the other hand, the process convolution framework attemp ts to model the variance of the set of outputs by"
3817,unknown,"the direct association of a different smoothing kernel Gd(x) to each output fd(x). By specifying Gd(x), one can model, for example, the degree of smoothness and the length- scale that characterizes each output. If each output happens to have more than one degree of variation (marginall y , it is a sum of functions of varied smoothness) one is faced with the same situation as in LMC, namely , the n"
3818,unknown,"required precision. However , due to the local description o f each output that the process convolution performs, it is likely that the parameter space for the process convoluti on approach grows slower than the parameter space for LMC. 5.3.2 Other Approaches Related to Process Convolutions In [61], a different moving average construction for the cov ariance of multiple outputs was introduced. It "
3819,unknown,"obtained as a convolution over covariance functions in cont rast to the process convolution approach where the convolution is performed over processes. Assuming that the covariances involved are isotropic and the only latent function u(x) is a white Gaussian noise, [61] show that the cross-covarian ce obtained from cov [fd(x + h),fd′ (x)] = ∫ X kd(h − z)kd′ (z)dz, 8 The latent process u(x) was ass"
3820,unknown,"23 0 5−1 0 1 2 3 ICM, f1(x) 0 5−5 0 5 LMC, f1(x) 0 5−2 0 2 4 PC, f1(x) 0 5−5 0 5 10 ICM, f2(x) 0 5−5 0 5 LMC, f2(x) 0 5−5 0 5 10 PC, f2(x) Figure 6: T wo samples from three kernel matrices obtained us ing the intrinsic coregionalization model with Rq = 1 (ﬁrst column), the linear model of coregionalization with Rq = 2 (second column) and the process convolution formalism with Rq = 1and Q = 1(third"
3821,unknown,"the other sample. There are two outputs, one row per output. N otice that for the ICM and the LMC, the outputs have the same length-scale (in the ICM case) or combined leng th-scales (in the LMC case). The outputs generated from the process convolution covariance differ in their rel ative length-scale. 24 0 1 2 3 4 5−4 −2 0 2 4 LMC with Rq = 2 and Q = 2, f1(x) 0 1 2 3 4 5−5 0 5 LMC with Rq = 2 and"
3822,unknown,"−2 0 2 4 LMC with Rq = 2 and Q = 2, f1(x) 0 1 2 3 4 5−5 0 5 LMC with Rq = 2 and Q = 2, f2(x) Figure 7: T wo samples from a linear model of coregionalizati on with Rq = 2and Q = 2. The solid lines represent one of the samples. The dashed lines represent the other samp le. Samples share a long length-scale behavior . An added short length-scale term appears only in output two. where kd(h) and kd′ (h"
3823,unknown,"the outputs {fd(x)}D d=1. If we assume that the smoothing kernels are not only square i ntegrable, but also posi- tive deﬁnite functions, then the covariance convolution ap proach turns out to be a particular case of the process convolution approach (square-integrability might be easi er to satisfy than positive deﬁniteness). [67] introduced the idea of transforming a Gaussian process prior using "
3824,unknown,"fd = Gdu, where Gd ∈ RN×M is a so called design matrix with elements {Gd(xn,zm)}N,M n=1,m=1 and u⊤ = [u(x1),...,u (xM)]. Such a transformation could be applied for the purposes of f using the information from mul- tiple sensors, for solving inverse problems in reconstruct ion of images or for reducing computational complexity working with the ﬁltered data in the transformed space [92]. Convolution"
3825,unknown,"modelling single outputs, were also proposed by [30, 31], bu t instead of the continuous convolution, [30, 31] used a discrete convolution. The purpose in [30, 31] was to develo p a spatially varying covariance for single outputs, by allowing the parameters of the covariance of a base proces s to change as a function of the input domain. Process convolutions are closely related to the Bayesian ke "
3826,unknown,"Hilbert spaces (RKHS) by assigning priors to signed measure s and mapping these measures through integral operators. In particular , deﬁne the following space of func tions, F = { f ⏐ ⏐ ⏐f(x) = ∫ X G(x,z)γ(dz), γ∈ Γ } , for some space Γ ⊆ B (X ) of signed Borel measures. In [77, proposition 1], the author s show that for Γ = B(X ), the space of all signed Borel measures, F corresponds to a RKHS. E"
3827,unknown,"25 form of stochastic processes include Gaussian processes, D irichlet processes and L ´ evy processes. In principle, we can extend this framework for the multiple output case, expr essing each output as fd(x) = ∫ X Gd(x,z)γ(dz). 6 Inference and Computational Considerations Practical use of multiple-output kernel functions require the tuning of the hyperparameters, and dealing with the issue of co"
3828,unknown,"validation and maximization of the log-marginal likelihoo d are alternatives for parameter tuning, while matrix diagonalization and reduced-rank approximations are choi ces for overcoming computational complexity of the matrix inversion. In this section we refer to the parameter estimation problem for the models presented in section 4 and 5 and also to the computational complexity when using those"
3829,unknown,"6.1 Estimation of Parameters in Regularization Theory From a regularization perspective, once the kernel is ﬁxed, to ﬁnd a solution we need to solve the linear system deﬁned in (5). The regularization parameter as well as the po ssible kernel parameters are typically tuned via cross- validation. The kernel free-parameters are usually reduce d to one or two scalars (e.g. the width of a scalar kerne"
3830,unknown,"While considering for example separable kernels the matrix B is ﬁxed by design, rather than learned, and the only free parameters are those of the scalar kernel. Solving problem (5), this is c = ( K(X,X) + λNI)−1y, is in general a costly operation both in terms of memory and time. When we have to solve the problem for a single value o f λ Cholesky decomposition is the method of choice, while when "
3831,unknown,"validation) singular valued decomposition (SVD) is the met hod of choice. In both case the complexity in the worst case is O(D3N3) (with a larger constant for the SVD) and the associated stora ge requirement is O(D2N2) As observed in [7], this computational burden can be greatly reduced for separable kernels. For example, if we consider the kernel K(x,x′) = k(x,x′)I the kernel matrix K(X,X) become"
3832,unknown,"input points are the same, all the blocks are equal and the pro blem reduces to inverting an N by N matrix. The simple example above serves as a prototype for the more gener al case of a kernel of the form K(x,x′) = k(x,x′)B. The point is that for this class of kernels, we can use the eige n-system of the matrix B to deﬁne a new coordinate system where the kernel matrix becomes block diagonal."
3833,unknown,"system where the kernel matrix becomes block diagonal. W e start observing that if we denote with (σ1,u1),..., (σD,uD) the eigenvalues and eigenvectors of B we can write the matrix C = ( c1,..., cN), with ci ∈ RD, as C = ∑ D d=1 ˜cd ⊗ ud,where ˜cd = ( ⟨c1,ud⟩D,..., ⟨cN,ud⟩D) and ⊗ is the tensor product and similarly ˜Y = ∑ D d=1 ˜yd ⊗ ud,with ˜yd = ( ⟨y1,ud⟩D,..., ⟨yN,ud⟩D). The above"
3834,unknown,"d=1 ˜yd ⊗ ud,with ˜yd = ( ⟨y1,ud⟩D,..., ⟨yN,ud⟩D). The above transformations are simply rotations in the output space. M oreover , for the considered class of kernels, the kernel matrix K(X,X) is given by the tensor product of the N×Nscalar kernel matrix k(X,X) and B, that is K(X,X) = B ⊗ k(X,X). Then we have the following equalities C = ( K(X,X) + NλNI)−1Y = D∑ d=1 (B ⊗ k(X,X) + NλNI)−1 ˜yd ⊗ ud "
3835,unknown,"= D∑ d=1 (σdk(X,X) + NλNI)−1 ˜yd ⊗ ud. Since the eigenvectors uj are orthonormal, it follows that: ˜cd = ( σdk(X,X) + NλNI)−1 ˜yj = ( k(X,X) + λN σd I )−1 ˜yd σd , (30) for d = 1 ,...,D . The above equation shows that in the new coordinate system w e have to solve D essentially independent problems after rescaling each kernel matrix by σd or equivalently rescaling the regularization param-"
3836,unknown,"eter (and the outputs). The above calculation shows that all kernels of this form allow for a simple implementation at the price of the eigen-decomposition of the matrix B. Then we see that the computational cost is now essentially 26 O(D3) + O(N3) as opposed to O(D3N3) in the general case. Also, it shows that the coupling among th e different tasks can be seen as a rotation and rescaling of the o"
3837,unknown,"context of ﬁtting matrix variate Gaussian models with spher ical noise. 6.2 Parameters Estimation for Gaussian Processes In machine learning parameter estimation for Gaussian proc esses is often approached through maximization of the marginal likelihood. The method also goes by the names of evi dence approximation, type II maximum likelihood, empirical Bayes, among others [10]. With a Gaussian lik"
3838,unknown,"by p(y|X,φ ) = N (y|0,K(X,X) + Σ ), (31) where φ are the hyperparameters. The objective function is the logarithm of the marginal like lihood log p(y|X,φ ) = − 1 2 y⊤(K(X,X)+Σ )−1y − 1 2 log |K(X,X) + Σ | − ND 2 log 2π. (32) The parameters φ are obtained by maximizing log p(y|X,φ ) with respect to each element in φ . Maximization is performed using a numerical optimization algorithm, for ex ample,"
3839,unknown,"∂log p(y|X,φ ) ∂φi = 1 2 y⊤K(X,X) −1 ∂K(X,X) ∂φi K(X,X) −1 y − 1 2 trace ( K(X,X) −1 ∂K(X,X) ∂φi ) , (33) where φi is an element of the vector φ and K(X,X) = K(X,X) + Σ . In the case of the LMC, in which the core- gionalization matrices must be positive semideﬁnite, it is possible to use an incomplete Cholesky decomposition Bq = ˜Lq˜L⊤ q , with ˜Lq ∈ RD×Rq , as suggested in [12]. The elements of t"
3840,unknown,"vector φ . Another method used for parameter estimation, more common i n the geostatistics literature, consists of op- timizing an objective function which involves some empiric al measure of the correlation between the functions fd(x), ˆK(x,x′), and the multivariate covariance obtained using a particul ar model, K(x,x′) [38, 53, 76]. Assum- ing stationary covariances, this criteria reduces to WSS"
3841,unknown,"WSS = N∑ i=1 w(hi) trace {[( ˆK(hi) − K(hi) )]2} , (34) where hi = xi − x′ i is a lag vector , w(hi) is a weight coefﬁcient, ˆK(hi) is an experimental covariance matrix with entries obtained by different estimators for cross-covari ance functions [75, 102], and K(hi) is the covariance matrix obtained, for example, using the linear model of coregional ization.9 One of the ﬁrst algorithms for estima"
3842,unknown,"parameter vector φ in LMC was proposed by [38]. It assumed that the parameters of the basic covariance functions kq(x,x′) had been determined a priori and then used a weighted least sq uares method to ﬁt the coregionalization matrices. In [76] the efﬁciency of other least squares proce dures was evaluated experimentally , including ordinary least squares and generalized least squares. Other more g"
3843,unknown,"mated simultaneously include simulated annealing [54] and the EM algorithm [114]. V er Hoef and Barry [102] also proposed the use of an objective function like (34), to estim ate the parameters in the covariance obtained from a process convolution. 9 Note that the common practice in geostatistics is to work wit h variograms instead of covariances. A variogram character izes a general"
3844,unknown,"class of random functions known as intrinsic random functio ns [62], which are random processes whose increments follow a stationary second- order process. For clarity of exposition, we will avoid the i ntroduction of the variogram and its properties. The intere sted reader can follow the original paper by [62] for a motivation of their existenc e, [36] for a comparison between variograms and cova"
3845,unknown,"a deﬁnition of the linear model of coregionalization in term s of variograms. 27 Both methods described above, the evidence approximation o r the least-square method, give point estimates of the parameter vector φ . Several authors have employed full Bayesian inference by a ssigning priors to φ and computing the posterior distribution through some samplin g procedure. Examples include [42] and [23"
3846,unknown,"the LMC framework or [14] and [99] under the process convolut ion approach. As mentioned before, for non-Gaussian likelihoods, there i s not a closed form solution for the posterior distri- bution nor for the marginal likelihood. However , the margin al likelihood can be approximated under a Laplace, variational Bayes or expectation propagation (EP) approxi mation frameworks for multiple output cl"
3847,unknown,"[93, 11], and used to ﬁnd estimates for the hyperparameters. Hence, the error function is replaced for log q(y|X,φ ), where q(y|X,φ ) is the approximated marginal likelihood. Parameters are ag ain estimated using a gradient based methods. The problem of computational complexity for Gaussian proce sses in the multiple output context has been studied by different authors [83, 103, 96, 13, 2, 1]. Fun"
3848,unknown,"the one appearing in regularization theory , that is, the inv ersion of the matrix K(X,X) = K(X,X) + Σ for solv- ing equation (5). This step is necessary for computing the ma rginal likelihood and its derivatives (for estimating the hyperparameters as explained before) or for computing t he predictive distribution. With the exception of the method by [83], the approximation methods proposed in [10"
3849,unknown,"tional complexity , whichever covariance function (LMC or p rocess convolution, for example) is used to compute the multi-output covariance matrix. In other words, the com putational efﬁciency gained is independent of the particular method employed to compute the covariance matri x. Before looking with some detail at the different approximat ion methods employed in the Gaussian processes"
3850,unknown,"literature for multiple outputs, it is worth mentioning tha t computing the kernel function through process con- volutions in equation (29) implies solving a double integra l, which is not always feasible for any choice of the smoothing kernels Gi d,q(·) and covariance functions kq(x,x′). An example of an analytically tractable covariance function occurs when both the smoothing kernel and the cova"
3851,unknown,"kernels [2], or when the smoothing kernels have an exponenti ated quadratic form and the latent functions are Gaussian white noise processes [73, 14]. An alternative wou ld be to consider discrete process convolutions [44] instead of the continuous process convolution of equations (28) and (29), avoiding in this way the need to solve double integrals. W e now brieﬂy summarize different methods for"
3852,unknown,"processes. As we mentioned before, Rougier [83] assumes that the multip le output problem can be seen as a single output problem considering the output index as another variable of the input space. The predicted output, f(x∗) is expressed as a weighted sum of Q deterministic regressors that explain the mean of the outpu t process plus a Gaussian error term that explains the variance in the output "
3853,unknown,"the error are assumed to be separable in the input space. The c ovariance takes the form k(x,x′)kT(d,d′), as in the introduction of section 4. For isotopic models ([83] r efers to this condition as regular outputs, meaning outputs that are evaluated at the same set of inputs X), the mean and covariance for the output, can be obtained through Kronecker products for the regressors and the covar ianc"
3854,unknown,"the inversion of the necessary terms is accomplished using p roperties of the Kronecker product. For example, if K(X,X′) = B ⊗ k(X,X′), then K−1(X,X′) = B−1 ⊗ k−1(X,X′). Computational complexity is reduced to O(D3) + O(N3), similar to the eigendecomposition method in section 6.1. V er Hoef and Barry [103] present a simulation example with D= 2 . Prediction over one of the variables is per-"
3855,unknown,"formed using cokriging. In cokriging scenarios, usually one has access to a few meas urements of a primary variable, but plenty of observations for a secondary variable. In geos tatistics, for example, predicting the concentration of heavy pollutant metals (say Cadmium or Lead), which are expe nsive to measure, can be done using inexpensive and oversampled variables as a proxy (say pH levels) [37]"
3856,unknown,"partition the secondary observations into subgroups of obs ervations and assume the likelihood function is the sum of the partial likelihood functions of several systems t hat include the primary observations and each of the subgroups of the secondary observations. In other words, th e joint probability distribution p(f1(X1),f2(X2)) is factorised as p(f1(X1),f2(X2)) = ∏ J j=1 p(f1(X1),f(j) 2 (X(j)"
3857,unknown,"j=1 p(f1(X1),f(j) 2 (X(j) 2 )), where f(j) 2 (X(j) 2 ) indicates the observations in the subgroup jout of J subgroups of observations, for the secondary variable. Inv ersion of the particular covariance matrix derived from these assumptions grows as O(JN3), where N is the number of input points per secondary variable. Also, the authors use a fast Fourier transform for computing the autocovariance "
3858,unknown,"cross-covariance matrices (K(Xd,Xd′ ))d,d′ . Boyle [13] proposed an extension of the reduced rank approximation method presented by [80], to be applied 28 to the dependent Gaussian process construction. The author outlined the generalization of the methodology for D= 2 . The outputs f1(X1) and f2(X2) are deﬁned as [ f1(X1) f2(X2) ] = [ (K(X1,X1))1,1 (K(X1,X2))1,2 (K(X2,X1))2,1 (K(X2,X2))2,2 ][ ˜w1"
3859,unknown,"[ f1(X1) f2(X2) ] = [ (K(X1,X1))1,1 (K(X1,X2))1,2 (K(X2,X1))2,1 (K(X2,X2))2,2 ][ ˜w1 ˜w2 ] , where ˜wd are vectors of weights associated to each output including a dditional weights corresponding to the test inputs, one for each output. Based on this likelihood, a pred ictive distribution for the joint prediction of f1(X) and f2(X) can be obtained, with the characteristic that the variance f or th"
3860,unknown,"of the full predictive distribution of the Gaussian process , even for test points away from the training data. The elements in the matrices (K(Xd,Xd′ ))d,d′ are computed using the covariances and cross-covariances d eveloped in sections 4 and 5. Computational complexity reduces to O(DNM2), where N is the number of sample points per output and M is an user speciﬁed value that accounts for the rank"
3861,unknown,"per output and M is an user speciﬁed value that accounts for the rank of the app roximation. In [2], the authors show how through making speciﬁc conditio nal independence assumptions, inspired by the model structure in the process convolution formulation (fo r which the LMC is a special case), it is possible to arrive at a series of efﬁcient approximations that represen t the covariance matrix K(X"
3862,unknown,"approximation Q plus a matrix D, where D has a speciﬁc structure that depends on the particular indep endence assumption made to obtain the approximation. Approximatio ns can reduce the computational complexity to O(NDM2) with M representing a user speciﬁed value that determines the rank of Q. Approximations obtained in this way , have similarities with the conditional approxi mations summarized f"
3863,unknown,"Finally , the informative vector machine (IVM) [57] has also been extended to Gaussian processes using kernel matrices derived from particular versions of the linear mod el of coregionalization, including [96] and [55]. In the IVM, only a smaller subset of size M of the data points is chosen for constructing the GP predicto r . The data points selected are the ones that maximize a differential en "
3864,unknown,"Computational complexity for this approximation is again O(NDM2). For the computational complexities shown above, we assumed Rq = 1 and Q= 1 . 7 Applications of Multivariate Kernels In this chapter we further describe in more detail some of the applications of kernel approaches to multi-output learning from the statistics and machine learning communit ies. One of the main application areas of mult"
3865,unknown,"LMC is used as the covariance function for a Gaussian process emulator of a ﬁnite-element method that solves for frequency response functions obtained from a structure. Th e outputs correspond to pairs of masses and stiffnesses for several structural modes of vibration for an aircraft mo del. The input space is made of variables related to physical properties, such as T ail tip mass or Wingtip mas"
3866,unknown,"physical properties, such as T ail tip mass or Wingtip mass, a mong others. Multivariate computer emulators are also frequently used f or modelling time series. W e mentioned this type of application in section 4.2.4. Mostly , the number of time p oints in the time series are matched to the number of outputs (we expressed this as D = T before), and different time series correspond to different inp"
3867,unknown,"the emulation. The particular input values employed are obt ained from different ranges that the input variables can take (given by an expert), and are chosen according to som e space-ﬁlling criteria (Latin hypercube design, for example) [84]. In [23], the time series correspond to the evo lution of the net biome productivity (NBP) index, which in turn is the output of the Shefﬁeld dynamic global "
3868,unknown,"a particular location of a container with decomposing foam. The simulation model is a ﬁnite element model and simulates the transfer of heat through decomposing foam. In machine learning the range of applications for multivari ate kernels is increasing. In [72], the ICM is used to model the dependencies of multivariate time series in a se nsor network. Sensors located in the south coast of"
3869,unknown,"England measure different environmental variables such as temperature, wind speed, tide height, among others. Sensors located close to each other make similar readings. I f there are faulty sensors, their missing readings could be interpolated using the healthy ones. In [22], the authors use the ICM for obtaining the inverse dyn amics of a robotic manipulator . The inverse dynamics problem consist"
3870,unknown,"angle velocity and angle acceleration for the different joi nts. Computed torques are necessary to drive the robotic arm along a particular trajectory . Furthermore, the author s consider several contexts, this is, different dynamics due to different loadings at the end effector . Joints are modell ed independently using an ICM for each of them, being the outputs the different contexts and being t"
3871,unknown,"Besides interpolation, the model is also used for extrapola tion of novel contexts. 29 The authors of [11] use the ICM for preference elicitation, w here a user is prompted to solve simple queries in order to receive a recommendation. The ICM is used as a covari ance function for a GP that captures dependencies between users (through the matrix B), and dependencies between items (through the covar"
3872,unknown,"In [56] and [33], the authors use a process convolution to mod el the interaction between several genes and a transcription factor protein, in a gene regulatory network . Each output corresponds to a gene, and each latent function corresponds to a transcription factor protein. It is assummed that transcription factors regulate the rate at which particular genes produce primary RNA. The output func"
3873,unknown,"The smoothing kernel functions Gi d,q(·) correspond to the impulse response obtained from an ordinar y differential equation of ﬁrst order . Given gene expression data, the prob lem is to infer the time evolution of the transcription factor . In [3], the authors use a process convolution to model the dep endencies between different body parts of an actor that performs modern dancing movements. Thi"
3874,unknown,"data. The outputs correspond to time courses of angles refer enced to a root node, for each body part modelled. The smoothing kernel used corresponds to a Green’s function arising from a second order ordinary differential equation. In [67], the authors use a discretized process convolution f or solving an inverse problem in reconstruction of images, and for fusing the information from multiple sen"
3875,unknown,"images, and for fusing the information from multiple sensor s. In [16], two particulate matter (PM) levels measured in the a ir (10 µm in diameter and 2.5 µm in diameter), at different spatial locations, are modeled as the added inﬂue nce of coarse and ﬁne particles. In turn, these coarse and ﬁne particles are modeled as random walks and then transf ormed by discrete convolutions to represent the"
3876,unknown,"levels of PM at 10 µm and 2.5 µm. The objective is to extract information about PM at 2.5 µm from the abundant readings of PM at 10 µm. 8 Discussion W e have presented a survey of multiple output kernel functio ns to be used in kernel methods including regular- ization theory and Gaussian processes. From the regulariza tion theory point of view , the multiple output problem can be seen as a regula"
3877,unknown,"of the learned vector-valued function. In a Gaussian proces s framework, from a machine learning context, the multiple output problem is equivalent to formulate a genera tive model for each output that expresses correlations as functions of the output function index and the input space , using a set of common latent functions. W e presented two general families of kernels for vector-val ued functi"
3878,unknown,"cluding the SoS kernels) and different instantiations of wh at we would call the nonseparable family . The separable family represent the kernel function as the product of a kern el for the outputs, independently of the value that the input can have, and a kernel function for the input space, ind ependently of the output index. The most general model is the linear model of coregionalization, with "
3879,unknown,"ing literature as particular cases. Within the family of non separable kernels, the process convolution construction has proved useful for several theoretical and applied probl ems and as we have seen before, it can be considered as a generalization of the linear model of coregionalization. Model selection establishes a path for future research in mu ltiple-output kernels related problems. From a"
3880,unknown,"Bayesian perspective, in the setup of LMC and process convol utions, model selection includes principled mech- anisms to ﬁnd the number of latent functions and/or the rank o f the coregionalization matrices. More general model selection problems involve the ability to test if give n some data, the outputs are really correlated or inﬂu- ence each other , compared to the simpler assumption of indep "
3881,unknown,"the inﬂuence of the input space conﬁguration (isotopic agai nst heterotopic) towards the sharing of strengths be- tween outputs. Although such problems have been studied to s ome extent in the geostatistics literature, there remain open issues. Acknowledgements MA and NL are very grateful for support from a Google Research A ward “Mechanistically Inspired Convolution Processes for Learning” and th"
3882,unknown,"with Applications in Systems Biology”. MA also acknowledge s the support from the Overseas Research Student A ward Scheme (ORSAS), from the School of Computer Science of the University of Manchester and from the Universidad T ecnol ´ ogica de Pereira, Colombia. LR would li ke to thank Luca Baldassarre and T omaso Poggio for 30 many useful discussions. LR is assistant professor at DISI, University "
3883,unknown,"W e also thank to two anonymous reviewers for their helpful co mments. This work was supported in part by the IST Programme of the Eur opean Community , under the P ASCAL2 Net- work of Excellence, IST-2007-216886. Thanks to P ASCAL 2 sup port the authors of this paper organized two work- shops: Statistics and Machine Learning Interface Meeting (see http://intranet.cs.man.ac.uk/mlo/slim09/),"
3884,unknown,"across 23-24 of July , 2009 at Manchester , UK and Kernels for Multiple Outputs and Multi-task Learning: Freq uentist and Bayesian Points of V iew (see http://intranet.cs.man.ac.uk/mlo/mock09/) held on December 12 at Whistler , Canada as part as one of the W orkshops of NIPS 2009. This publication only reﬂects the authors’ views, but they beneﬁted greatly by interactions with other resear chers at"
3885,unknown,"gyriou, David Higdon, T om Fricker , Sayan Mukherjee, T ony O’ Hagan, Ian V ernon, Hans W ackernagel, Richard Wilkinson, and Chris Williams. 31 Notation Generalities p dimensionality of the input space D number of outputs N,Nd number of data points for output d Q number of latent functions (for generative models) X input space Xd input training data for output d, Xd = {xd,n}Nd n=1 X input training"
3886,unknown,"n=1 X input training data for all outputs, X = {Xd}D d=1 Functions k(x,x′) general scalar kernel K(x,x′) general kernel valued matrix with entries (K(x,x′))d,d′ with d,d = 1 ,...,D kq(x,x′) scalar kernel for the q−th latent function fd(x) d-th output evaluated at x f(x) , vector-valued function, f(x) = [ f1(x),...,f D(x)]⊤ δk,k′ Kronecker delta for discrete arguments δ(x) Dirac delta for continuou"
3887,unknown,"V ectors and matrices kq(X,X) kernel matrix with entries kq(x,x′) evaluated at X fd(Xd) fd(x) evaluated at Xd, fd = [ fd(xd,1),...,f d(xd,Nd )]⊤ f(X) vectors {fd}D d=1, stacked in a column vector (K(Xd,Xd′ ))d,d′ kernel matrix with entries (K(xd,n,xd′,m))d,d′ with xd,n ∈ Xd and xd′,m ∈ Xd′ K(X,X) kernel matrix with blocks (K(Xd,Xd′ ))d,d′ with d,d′ = 1 ,...,D IN identity matrix of size N 32 Refere"
3888,unknown,"IN identity matrix of size N 32 References [1] Mauricio A. ´Alvarez. Convolved Gaussian Process Priors for Multivariate Regres sion with Applications to Dynam- ical Systems . PhD thesis, School of Computer Science, University of Manc hester , Manchester , UK, 2011. [2] Mauricio A. ´Alvarez and Neil D. Lawrence. Sparse convolved Gaussian pro cesses for multi-output regres- sion. In Koller et al. [5"
3889,unknown,"sion. In Koller et al. [52], pages 57–64. [3] Mauricio A. ´Alvarez, David Luengo, and Neil D. Lawrence. Latent Force Mo dels. In David van Dyk and Max W elling, editors, Proceedings of the T welfth International Conference on Art iﬁcial Intelligence and Statistics , pages 9–16, Clearwater Beach, Florida, 16-18 April 2009. JM LR W&CP 5. [4] Andreas Argyriou, Theodoros Evgeniou, and Massimilian o Po"
3890,unknown,"Machine Learning , 73(3):243–272, 2008. [5] Andreas Argyriou, Andreas Maurer , and Massimiliano Pon til. An algorithm for transfer learning in a het- erogeneous environment. In ECML/PKDD (1) , pages 71–85, 2008. [6] N. Aronszajn. Theory of reproducing kernels. T rans. Amer . Math. Soc. , 68:337–404, 1950. [7] L. Baldassarre, L. Rosasco, A. Barla, and A. V erri. Multi -output learning via spectral "
3891,unknown,"report, Massachusetts Institute of T echnology , 2011. MIT- CSAIL-TR-2011-004, CBCL-296. [8] Ronald Paul Barry and Jay M. V er Hoef. Blackbox kriging: s patial prediction without specifying variogram models. Journal of Agricultural, Biological and Environmental Sta tistics, 1(3):297–322, 1996. [9] M. J. Bayarri, James O. Berger , Marc C. Kennedy , Athanasi os Kottas, Rui Paulo, Jerry Sacks, John A"
3892,unknown,"Chin-Hsu Lin, and Jian T u. Predicting vehicle crashworthin ess: V alidation of computer models for func- tional and hierarchical data. Journal of the American Statistical Association , 104(487):929–943, 2009. [10] Christopher M. Bishop. Pattern Recognition and Machine Learning . Information Science and Statistics. Springer , 2006. [11] Edwin Bonilla, Shengbo Guo, and Scott Sanner . Gaussian proce"
3893,unknown,"pages 262–270, Cambridge, MA, 2011. MIT Press. [12] Edwin V . Bonilla, Kian Ming Chai, and Christopher K. I. W illiams. Multi-task Gaussian process prediction. In John C. Platt, Daphne Koller , Y oram Singer , and Sam Roweis , editors, NIPS, volume 20, Cambridge, MA, 2008. MIT Press. [13] Phillip Boyle. Gaussian Processes for Regression and Optimisation . PhD thesis, Victoria University of W ellin"
3894,unknown,"W ellington, New Zealand, 2007. [14] Phillip Boyle and Marcus Frean. Dependent Gaussian pro cesses. In Saul et al. [85], pages 217–224. [15] Phillip Boyle and Marcus Frean. Multiple output Gaussi an process regression. T echnical Report CS-TR-05/2, School of Mathematical and Computing Sciences, Victoria Un iversity , New Zealand, 2005. [16] Catherine A. Calder . A dynamic process convolution app r"
3895,unknown,"concentrations. Environmetrics, 19:39–48, 2008. [17] Catherine A. Calder and Noel Cressie. Some topics in con volution-based spatial modeling. In Proceedings of the 56th Session of the International Statistics Institute , August 2007. [18] A. Caponnetto, C.A. Micchelli, M. Pontil, and Y . Ying. U niversal kernels for multi-task learning. Journal of Machine Learning Research , 9:1615–1646, 2008."
3896,unknown,"Machine Learning Research , 9:1615–1646, 2008. [19] C. Carmeli, E. De Vito, and A. T oigo. V ector valued repro ducing kernel Hilbert spaces of integrable functions and Mercer theorem. Anal. Appl. (Singap.) , 4(4):377–408, 2006. [20] Rich Caruana. Multitask learning. Machine Learning , 28:41–75, 1997. [21] Kian Ming Chai. Generalization errors and learning cur ves for regression with multi-task Ga"
3897,unknown,"cesses. In Y oshua Bengio, Dale Schuurmans, John Laferty , Ch ris Williams, and Aron Culotta, editors, NIPS, volume 22, pages 279–287, Cambridge, MA, 2010. MIT Press. [22] Kian Ming A. Chai, Christopher K. I. Williams, Stefan Kl anke, and Sethu Vijayakumar . Multi-task Gaussian process learning of robot inverse dynamics. In Koller et al. [52], pages 265–272. [23] Stefano Conti and Anthony O’Hagan."
3898,unknown,"models. Journal of Statistical Planning and Inference , 140(3):640–651, 2010. 33 [24] Noel A. C. Cressie. Statistics for Spatial Data . John Wiley & Sons (Revised edition), USA, 1993. [25] F . Cucker and S. Smale. On the mathematical foundations of learning. Bull. Amer . Math. Soc. (N.S.) , 39(1):1–49 (electronic), 2002. [26] E. De Vito, L. Rosasco, A. Caponnetto, M. Piana, and A. V e rri. Some pr"
3899,unknown,"ods. Journal of Machine Learning Research , 5:1363–1390, 2004. [27] T . Evgeniou, C. A. Micchelli, and M. Pontil. Learning mu ltiple tasks with kernel methods. Journal of Machine Learning Research , 6:615–637, 2005. [28] Theodoros Evgeniou, Charles A. Micchelli, and Massimi liano Pontil. Learning multiple tasks with kernel methods. Journal of Machine Learning Research , 6:615–637, 2005."
3900,unknown,"methods. Journal of Machine Learning Research , 6:615–637, 2005. [29] Thomas E. Fricker , Jeremy E. Oakley , Neil D. Sims, and Ke ith W orden. Probabilistic uncertainty analysis of an frf of a structure using a Gaussian process emulator . Mechanical Systems and Signal Processing , 25(8):2962– 2975, 2011. [30] Montserrat Fuentes. Interpolation of nonstationary a ir pollution processes: a spatial sp"
3901,unknown,"Statistical Modelling , 2:281–298, 2002. [31] Montserrat Fuentes. Spectral methods for nonstationa ry spatial processes. Biometrika, 89(1):197–210, 2002. [32] E.J. Fuselier Jr . Reﬁned error estimates for matrix-valued radial basis func tions. PhD thesis, T exas A&M University , 2006. [33] Pei Gao, Antti Honkela, Magnus Rattray , and Neil D. Lawr ence. Gaussian process modelling of latent"
3902,unknown,"chemical species: Applications to inferring transcriptio n factor activities. Bioinformatics, 24:i70–i75, 2008. [34] Alan E. Gelfand, Alexandra M. Schmidt, Sudipto Banerje e, and C.F . Sirmans. Nonstationary multivariate process modeling through spatially varying coregionaliza tion. TEST, 13(2):263–312, 2004. [35] F . Girosi and T . Poggio. Networks and the best approxima tion property . Biologica"
3903,unknown,"1989. [36] Tilmann Gneiting, Zolt ´ an Sasv ´ ari, and Martin Schlat her . Analogies and correspondences between vari- ograms and covariance functions. Advances in Applied Probability , 33(3):617–630, 2001. [37] Pierre Goovaerts. Geostatistics For Natural Resources Evaluation . Oxford University Press, USA, 1997. [38] Michel Goulard and Marc V oltz. Linear coregionalizati on model: T ools for esti"
3904,unknown,"variogram matrix. Mathematical Geology , 24(3):269–286, 1992. [39] J.A. V argas Guzm ´ an, A.W . W arrick, and D.E. Myers. Core gionalization by linear combination of nonorthog- onal components. Mathematical Geology , 34(4):405–419, 2002. [40] T revor Hastie, Robert Tibshirani, and Jerome Friedman . The Elements of Statistical Learning . Springer , second edition, 2009. [41] Jeffrey D. Helterbrand"
3905,unknown,"Geology, 26(2):205–226, 1994. [42] Dave Higdon, Jim Gattiker , Brian Williams, and Maria Ri ghtley . Computer model calibration using high dimensional output. Journal of the American Statistical Association , 103(482):570–583, 2008. [43] David M. Higdon. A process-convolution approach to mod eling temperatures in the north atlantic ocean. Journal of Ecological and Environmental Statistics , 5:173–"
3906,unknown,"Journal of Ecological and Environmental Statistics , 5:173–190, 1998. [44] David M. Higdon. Space and space-time modelling using p rocess convolutions. In C. Anderson, V . Barnett, P . Chatwin, and A. El-Shaarawi, editors, Quantitative methods for current environmental issues , pages 37–56. Springer-V erlag, 2002. [45] David M. Higdon, Jenise Swall, and John Kern. Non-stati onary spatial modeling."
3907,unknown,"Berger , A. P . Dawid, and A. F . M. Smith, editors, Bayesian Statistics 6 , pages 761–768. Oxford University Press, 1998. [46] Lars H ¨ ormander . The analysis of Linear Partial Differential Operators I . Springer-V erlag, Berlin Hiedelberg, ﬁrst edition, 1983. [47] L. Jacob, F . Bach, and J.P . V ert. Clustered multi-task l earning: A convex formulation. In Advances in Neural Information Process"
3908,unknown,"Information Processing Systems (NIPS) . Curran Associates, Inc, 2008. 34 [48] Laurent Jacob, Francis Bach, and Jean-Philippe V ert. C lustered multi-task learning: A convex formulation. In NIPS 21 , pages 745–752, 2008. [49] Andre G. Journel and Charles J. Huijbregts. Mining Geostatistics . Academic Press, London, 1978. [50] Hideto Kazawa, T omonori Izumitani, Hirotoshi T aira, a nd Eisaku Maeda. "
3909,unknown,"multi-topic text categorization. In Saul et al. [85], pages 649–656. [51] G. Kimeldorf and G. W ahba. A correspondence between Bay esian estimation of stochastic processes and smoothing by splines. Ann. Math. Stat. , 41:495–502, 1970. [52] Daphne Koller , Dale Schuurmans, Y oshua Bengio, and L ´ e on Bottou, editors. NIPS, volume 21, Cambridge, MA, 2009. MIT Press. [53] H.R. K ¨ unsch, A. Papritz,"
3910,unknown,"Geology, 29(6):779–799, 1997. [54] R.M. Lark and A. Papritz. Fitting a linear model of coreg ionalization for soil properties using simulated annealing. Geoderma, 115:245–260, 2003. [55] Neil D. Lawrence and John C. Platt. Learning to learn wit h the informative vector machine. In Proceedings of the 21st International Conference on Machine Learning (I CML 2004) , pages 512–519, 2004. [56] Neil D. "
3911,unknown,"Gaussian processes. In Bernhard Sch ¨ olkopf, John C. Platt, and Thomas Hofmann, editors, NIPS, volume 19, pages 785–792, Cambridge, MA, 2007. MIT Press. [57] Neil D. Lawrence, Matthias Seeger , and Ralf Herbrich. F ast sparse Gaussian process methods: The informa- tive vector machine. In Sue Becker , Sebastian Thrun, and Kla us Obermayer , editors, NIPS, volume 15, pages 625–632, Cambridge, MA, 2"
3912,unknown,"625–632, Cambridge, MA, 2003. MIT Press. [58] Feng Liang, Kai Mao, Ming Liao, Sayan Mukherjee, and Mik e W est. Non-parametric Bayesian kernel mod- els. Department of Statistical Science, Duke University , D iscussion Paper 07-10. (Submitted for publication), 2009. [59] S. Lowitzsch. A density theorem for matrix-valued radi al basis functions. Numerical Algorithms , 39(1):253– 256, 2005."
3913,unknown,"256, 2005. [60] I. Mac ˆ edo and R. Castro. Learning divergence-free and curl-free vector ﬁelds with matrix-valued kernels. T echnical report, Instituto Nacional de Matematica Pura e A plicada, 2008. [61] Anandamayee Majumdar and Alan E. Gelfand. Multivariat e spatial modeling for geostatistical data using convolved covariance functions. Mathematical Geology , 39(2):225–244, 2007. [62] Georges Mat"
3914,unknown,"5(3):439–468, 1973. [63] John McFarland, Sankaran Mahadevan, Vicente Romero, a nd Laura Swiler . Calibration and Uncertainty Analysis for Computer Simulations with Multivariate Outpu t. AIAA Journal , 46(5):1253–1265, 2008. [64] C. A. Micchelli and M. Pontil. Kernels for multi-task le arning. In Advances in Neural Information Processing Systems (NIPS) . MIT Press, 2004. [65] C.A. Micchelli and M. "
3915,unknown,"[66] Thomas P . Minka and Rosalind W . Picard. Learning how to l earn is learning with point sets, 1999. Revised version 1999 avai lable at http://research.microsoft.com/en-us/um/people/minka/papers/point-sets.html. [67] Roderick Murray-Smith and Barak A. Pearlmutter . T rans formation of Gaussian process priors. In Joab Win- kler , Mahesan Niranjan, and Neil Lawrence, editors, Deterministic and S"
3916,unknown,"pages 110–123. LNAI 3635, Springer-V erlag, 2005. [68] F .J. Narcowich and J.D. W ard. Generalized hermite inte rpolation via matrix-valued conditionally positive deﬁnite functions. Mathematics of Computation , 63(208):661–687, 1994. [69] G. Obozinski, B. T askar , and M.I. Jordan. Joint covaria te selection and joint subspace selection for multiple classiﬁcation problems. Statistics and Computing"
3917,unknown,"classiﬁcation problems. Statistics and Computing , 20(2):231–252, 2010. [70] Anthony O’Hagan. Bayesian analysis of computer code ou tputs: A tutorial. Reliability Engineering and System Safety, 91:1290–1300, 2006. 35 [71] Michael A. Osborne and Stephen J. Roberts. Gaussian pro cesses for prediction. T echnical report, Department of Engineering Science, University of Oxford, 2007. [72] Michael A. O"
3918,unknown,"T owards real-time information processing of sensor networ k data using computationally efﬁcient multi- output Gaussian processes. In Proceedings of the International Conference on Informatio n Processing in Sensor Networks (IPSN 2008) , 2008. [73] Christopher J. Paciorek and Mark J. Schervish. Nonstat ionary covariance functions for Gaussian process regression. In Sebastian Thrun, Lawrence Saul, "
3919,unknown,"mation Processing Systems 16 . MIT Press, Cambridge, MA, 2004. [74] Sinno Jialin Pan and Qiang Y ang. A survey on transfer lea rning. Knowledge and Data Engineering, IEEE T ransactions on, 22(10):1345 –1359, oct. 2010. [75] A. Papritz, H.R. K ¨ unsch, and R. W ebster . On the pseudo c ross-variogram. Mathematical Geology , 25(8):1015– 1026, 1993. [76] Bernard Pelletier , Pierre Dutilleul, Guillaum"
3920,unknown,"coregionalization by generalized least squares. Mathematical Geology , 36(3):323–343, 2004. [77] Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee , and Robert L. W olpert. Characterizing the func- tion space for Bayesian kernel models. Journal of Machine Learning Research , 8:1769–1797, 2007. [78] T omaso Poggio and Federico Girosi. Networks for approx imation and learning. Proceedings of t"
3921,unknown,"78(9):1481–1497, 1990. [79] Peter Z. G Qian, Huaiqing Wu, and C. F . Jeff Wu. Gaussian p rocess models for computer experiments with qualitative and quantitative factors. T echnometrics, 50(3):383–396, 2008. [80] Joaquin Qui ˜ nonero-Candela and Carl Edward Rasmussen . Analysis of some methods for reduced rank Gaussian process regression. In R. Murray-Smith and R. Shor ten, editors, Lecture Notes "
3922,unknown,"volume 3355, pages 98–127. Springer , 2005. [81] Joaquin Qui ˜ nonero-Candela and Carl Edward Rasmussen . A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research , 6:1939–1959, 2005. [82] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning . MIT Press, Cambridge, MA, 2006. [83] Jonathan Rougier . Efﬁcient emu"
3923,unknown,"Graphical Statistics , 17(4):827–834, 2008. [84] Thomas J. Santner , Brian J. Williams, and William I. Not z. The Design and Analysis of Computer Experiments . Springer , ﬁrst edition, 2003. [85] Lawrence Saul, Y air W eiss, and L ´ eon Bouttou, editors. NIPS, volume 17, Cambridge, MA, 2005. MIT Press. [86] Bernhard Sch ¨ olkopf and Alexander J. Smola. Learning with Kernels: Support V ector Machin"
3924,unknown,"Optimization and Beyond. The MIT Press, USA, 2002. [87] L. Schwartz. Sous-espaces hilbertiens d’espaces vect oriels topologiques et noyaux associ ´ es (noyaux repro- duisants). J. Analyse Math. , 13:115–256, 1964. [88] Bernhard Schˇ slkopf, Ralf Herbrich, and Alex J. Smola. A generalized representer theorem. In In Proceedings of the Annual Conference on Computational Learning Theory , pages 416–42"
3925,unknown,"of the Annual Conference on Computational Learning Theory , pages 416–426, 2001. [89] Matthias Seeger and Michael I. Jordan. Sparse Gaussian Process Classiﬁcation With Multiple Classes. T ech- nical Report 661, Department of Statistics, University of C alifornia at Berkeley , 2004. [90] John Shawe-T aylor and Nello Cristianini. Kernel Methods for Pattern Analysis . Cambridge University Press, New "
3926,unknown,"New Y ork, NY , USA, 2004. [91] D. Sheldon. Graphical multi-task learning. T echnical report, Cornell University , 2008. Preprint. [92] Jian Qing Shi, Roderick Murray-Smith, D.M. Titteringt on, and Barak Pearlmutter . Learning with large data sets using ﬁltered Gaussian process priors. In R. Murray-Sm ith and R. Shorten, editors, Proceedings of the Hamilton Summer School on Switching and Learning "
3927,unknown,"V erlag, 2005. [93] Grigorios Skolidis and Guido Sanguinetti. Bayesian mu ltitask classiﬁcation with Gaussian process priors. IEEE T ransactions on Neural Networks , 22(12):2011 – 2021, 2011. 36 [94] Oliver Stegle, Christoph Lippert, Joris Mooij, Neil La wrence, and Karsten Borgwardt. Learning sparse in- verse covariance matrices in the presence of confounders. I n Neural Information Processing Sy"
3928,unknown,"[95] Michael L. Stein. Interpolation of Spatial Data . Springer-V erlag New Y ork, Inc., ﬁrst edition, 1999. [96] Y ee Whye T eh, Matthias Seeger , and Michael I. Jordan. Se miparametric latent factor models. In Robert G. Cowell and Zoubin Ghahramani, editors, AIST ATS 10, pages 333–340, Barbados, 6-8 January 2005. Society for Artiﬁcial Intelligence and Statistics. [97] Sebastian Thrun. Is learnin"
3929,unknown,"Mozer , and Michael E. Hasselmo, editors, NIPS, volume 08, pages 640–646, Cambridge, MA, 1996. MIT Press. [98] A.N. Tikhonov and V .Y . Arsenin. Solutions of Ill Posed Problems . W . H. Winston, W ashington, D.C., 1977. [99] Michalis Titsias, Neil D Lawrence, and Magnus Rattray . Efﬁcient sampling for Gaussian process inference using control variables. In Koller et al. [52], pages 1681–1 688."
3930,unknown,"using control variables. In Koller et al. [52], pages 1681–1 688. [100] V . N. V apnik. Statistical learning theory . Adaptive and Learning Systems for Signal Processing, Comm unica- tions, and Control. John Wiley & Sons Inc., New Y ork, 1998. A W iley-Interscience Publication. [101] E. V azquez and E. W alter . Multi-output support vector regression. 13th IF AC Symposium on System Identiﬁ- cation"
3931,unknown,"cation, 2003. [102] Jay M. V er Hoef and Ronald Paul Barry . Constructing and ﬁtting models for cokriging and multivariable spatial prediction. Journal of Statistical Plannig and Inference , 69:275–294, 1998. [103] Jay M. V er Hoef, Noel Cressie, and Ronald Paul Barry . Fl exible spatial models for kriging and cokriging using moving averages and the Fast Fourier Transform (FFT). Journal of Computa"
3932,unknown,"13(2):265–282, 2004. [104] Hans W ackernagel. Multivariate Geostatistics . Springer-V erlag Heidelberg New york, 2003. [105] Grace W ahba. Spline Models for Observational Data . SIAM, ﬁrst edition, 1990. [106] Jack M. W ang, David J. Fleet, and Aaron Hertzmann. Gaus sian process dynamical models for human mo- tion. IEEE T ransactions on Pattern Analysis and Machine Intellig ence, 30(2):283–298, 20"
3933,unknown,"tion. IEEE T ransactions on Pattern Analysis and Machine Intellig ence, 30(2):283–298, 2008. [107] Christopher K. Wikle. A kernel-based spectral model f or non-Gaussian spatio-temporal processes. Statistical Modelling, 2:299–314, 2002. [108] Christopher K. Wikle. Hierarchical Bayesian models f or predicting the spread of ecological processes. Ecol- ogy, 84(6):1382–1394, 2003. [109] Christopher K. "
3934,unknown,"ronmental and Ecological Statistics , 5:117–154, 1998. [110] Christopher K.I. Williams and David Barber . Bayesian Classiﬁcation with Gaussian processes. IEEE T rans- actions on Pattern Analysis and Machine Intelligence , 20(12):1342–1351, 1998. [111] Ian W oodward, Mark R. Lomas, and Richard A. Betts. V ege tation-climate feedbacks in a greenhouse world. Philosophical T ransactions: Biological Sc"
3935,unknown,"Philosophical T ransactions: Biological Sciences , 353(1365):29–39, 1998. [112] Y a Xue, Xuejun Liao, and Lawrence Carin. Multi-task le arning for classiﬁcation with Dirichlet process priors. Journal of Machine Learning Research , 8:35–63, 2007. [113] Kai Y u, V olker T resp, and Anton Schwaighofer . Learnin g Gaussian processes from multiple tasks. In Pro- ceedings of the 22nd International Confe"
3936,unknown,"[114] Hao Zhang. Maximum-likelihood estimation for multiv ariate spatial linear coregionalization models. Envi- ronmetrics, 18:125–139, 2007. 37 COMPUTATIONAL CONSIDERATIONS FOR THE LINEAR MODEL OF COREGIONALIZATION Renaud Alie, David A. Stephens Department of Mathematics and Statistics McGill University Montreal Alexandra M. Schmidt Department of Epidemiology, Biostatistics and Occupational Healt"
3937,unknown,"McGill University Montreal ABSTRACT In the last two decades, the linear model of coregionalization (LMC) has been widely used to model multivariate spatial processes. However, it can be a challenging task to conduct likelihood-based inference for such models because of the cubic cost associated with Gaussian likelihood evaluations. Starting from an analogy with matrix normal models, we propose a r"
3938,unknown,"Starting from an analogy with matrix normal models, we propose a reformulation of the LMC likelihood that highlights the linear, rather than cubic, computational complexity as a function of the dimension of the response vector. We describe how those simplifications can be exploited in Gaussian hierarchical models. In addition, we propose a new sparsity-inducing approach to the LMC that introduces "
3939,unknown,"parameters in a principled and data-driven way. Our reformulation of the LMC likelihood ensures that our sparse approach comes at virtually no additional cost when included in a Markov chain Monte Carlo (MCMC) algorithm. It is shown, on synthetic data, to significantly improve predictive performance. We also apply our methodology to a dataset comprised of air pollutant measurements from the state "
3940,unknown,from the state of California. We investigate the strength of the correlation among the measurements by providing new insights from our sparse method. 1 Introduction The literature concerning computational scalability for spatial Gaussian models has for the most part been concerned with applications where the number of spatial locations n is large. Such situations usually forbid likelihood-based
3941,unknown,"inference because of the O(n3) computational cost associated with evaluating the joint normal density. Some of the most prominent approximations used in the applied literature include predictive processes [Banerjee et al., 2008, Finley et al., 2009], the stochastic partial differential equation approach pioneered in Lindgren et al. [2011] or the nearest-neighbor methodology [Vecchia, 1988, Datta e"
3942,unknown,"study of those methods and more. In multivariate spatial models where an output of dimension p is modeled, the cost associated with Gaussian process likelihood evaluations is, in general, O(p3n3). While the approximations mentioned above are expressed in multivariate generality, likelihood computations still scale with the number p of dimensions cubed. Multivariate spatial models necessitate the s"
3943,unknown,"describe the necessary conditions under which such a function will lead to a valid model. They also review some of the most widespread approaches. A lot of those fall under the category of constructive methods in which cross-covariance functions are built from univariate models. Those include the covariance convolution framework [Majumdar and Gelfand, 2007] or the approach based on latent dimensio"
3944,unknown,"important example is the class of Matérn cross-covariance functions [Gneiting et al., 2010, Apanasovich et al., 2012] which offers, under some constraints, multivariate spatial models where the marginal distribution of each process has univariate Matérn covariance structure. None of the models mentioned above benefit from the type of computational simplifications we describe in this paper."
3945,unknown,"simplifications we describe in this paper. Perhaps the simplest (at least conceptually) of multivariate models, the linear model of coregionalization (LMC), is constructed as a linear combination of independent processes. From r orthogonal base processes, we obtain p new arXiv:2402.08877v2 [stat.ME] 2 Dec 2024 Computational Considerations for the LMC components that have cross-dependence. In its i"
3946,unknown,"V oltz, 1992], the LMC was conceptualized as a mechanism allowing for dimensionality reduction. This is accomplished by specifying a transformation matrix of dimension p × r with r < p. It essentially shrinks an observed p-dimensional process to r latent and independent ones. In this work, we only discuss the r = p case. We exclusively consider the LMC as an approach to construct a p-variate proce"
3947,unknown,"structures as it is described in Schmidt and Gelfand [2003], Gelfand et al. [2004, 2005]. The LMC is still used to this day in the applied sciences (see for example Ji et al. [2021], Carter et al. [2024], Heydari et al. [2023]). It is moreover an excellent candidate to incorporate into more involved methodology, such as spatiotemporal models [Mastrantonio et al., 2019, Cappello et al., 2022] or ne"
3948,unknown,"The LMC implies a likelihood that can be evaluated in a number of operations that is linear in p. This offers immense speed-ups in cases where the number of dimensions is even moderately large. This is not the result of an approximation. Instead, we achieve those computational shortcuts by exploiting the cross-covariance structure implied by the LMC. Hence, there are no drawbacks to utilizing the "
3949,unknown,"shortcuts can be viewed as a natural extension of separable models. Work on so-called highly multivariate processes concerns spatial statistical applications where the number p of dimensions and the number n of locations are both fairly large, with the former being at least a significant fraction of the latter. Taylor-Rodriguez et al. [2019] and Zhang and Banerjee [2022] describe similar approache"
3950,unknown,"factor model is compounded with a nearest-neighbor approximation to handle high-dimensional processes observed at numerous locations. In the context of areal data, Bradley et al. [2015] proposed to combine a dimension reduction mechanism with a specific basis function expansion of the spatial process, creating a model that handles both large p and n. In this work, we focus on the advantages of wor"
3951,unknown,simplifications associated with this model could be implemented in any of the frameworks described above as an alternative to dimension reduction. Other approaches to highly multivariate data reduce the computations in terms of p by exploiting a sparser depen- dence structure among processes. Dey et al. [2022] propose a stitching construction that conforms to a conditional independence graph among
3952,unknown,"decomposable graphs lead to easier evaluated likelihood functions with fewer parameters. Whenever the conditional independence graph is not known in advance, the authors propose a reversible jump MCMC algorithm on the edges of the conditional independence graph. In Krock et al. [2023], the authors assume a basis expansion of the spatial Gaussian process. They obtain a sparse dependence structure b"
3953,unknown,"basis coefficients as a graphical lasso optimization problem. In contrast, we show that the LMC, as a multivariate model, implies likelihood computations that scale linearly in terms of the number p of components even when allowing complete dependence among processes. Additionally, we propose a sparse version that aims to reduce the size of the parameter space associated with the regular LMC. As a"
3954,unknown,"structures without increasing the computing time when compared to the regular LMC. We first review the equivalence between separable models and matrix normal distributions in Section 2. We then extend this matrix normal analogy to the non-separable LMC. In the context of MCMC-based Bayesian inference, the structure of the LMC can be advantageous beyond the obvious computational speed-ups. We intro"
3955,unknown,"coregionalization matrix in Section 3. This introduces sparsity both in the parameter space and the resulting marginal covariance of the p components. Because of our matrix normal formulation of the LMC likelihood, this new approach comes at virtually no additional cost from a compuational standpoint. It is also shown to significantly improve the accuracy of out-of-sample predictions. 2 Linear Mod"
3956,unknown,"2 Linear Model of Coregionalization 2.1 Separable Specification In this section, we explore the link between multivariate Gaussian processes with a separable cross-covariance function and the matrix normal distribution. This will serve as an analogy for the computational results we present in Section 2.2 concerning the non-separable LMC. Definition 1 (Matrix Normal Distribution [Dawid, 1981]) . A "
3957,unknown,"distribution if its probability density function is given by p(X|M, Σ, R) = exp[−1 2 tr{R−1(X − M)⊤Σ−1(X − M)}] (2π)pn/2det(R)p/2det(Σ)n/2 , (1) where M is a real valued p × n matrix and Σ, R are respectively p × p and n × n positive definite matrices. 2 Computational Considerations for the LMC Equivalently, X ∼ MN(M, Σ, R) if the vector vec(X) obtained by stacking the columns x1, . . . ,xn of X h"
3958,unknown,"multivariate normal distribution with mean vec(M) and covariance R ⊗ Σ where ⊗ represents the Kronecker product. In the previous definition, M is the location parameter while Σ (resp. R) corresponds to the marginal column (resp. row) covariance. A realization of theMN(M, Σ, R) distribution can be obtained from ap×n matrix Z of independent standard normal by setting X = AZB⊤ + M where A, B are such"
3959,unknown,"the lower triangular Cholesky factors of Σ and R for example). Importantly, simulation from such a matrix normal distribution can be accomplished without factorizing the full np × np matrix R ⊗ Σ. The structure of the model allows us to work with individual matrices R and Σ. This will be a common thread in the remainder of this article. In the spatial modeling context, the equivalent would be to d"
3960,unknown,"a p × p full rank matrix and w(s) = (w1(s), . . . , wp(s))⊤ consists of p i.i.d. Gaussian random fields. This is known as the LMC with separable covariance structure [Gelfand et al., 2004]. We assume that eachwj(·), j= 1, . . . , phas 0 mean and a marginal variance of 1. Scaling is handled by A which is referred to as the coregionalization matrix in the following. Let V = (v(s1), . . . ,v(sn)) den"
3961,unknown,"of this multivariate process at distinct locations s1, . . . ,sn contained in some domain S. Commonly, we consider S ⊂R2 such as in the case of geographically indexed observations. Under the separable specification, the distribution of V is MN(0, AA⊤, R) where R is the common correlation matrix associated with the wj(·) processes at locations s1. . . . ,sn. It depends on the coregionalization matr"
3962,unknown,"cross-covariance function of the form C(s, t) = ρ(s, t)AAT , where ρ(·, ·) is the common correlation function of processes wj(·), j= 1, . . . , p. The term separable stems from the fact that the covariance structure factorizes in a spatial component times the marginal (at a single location) covariance among the p dimensions. In this case, inference can be conducted directly on the positive definit"
3963,unknown,"parameters are those contained in the common spatial correlation function defining the processes wj(·), j= 1, . . . , p. We assume the correlation between two locations s, t to be of the form ρ(φ|s − t|) where ρ(·) is some positive definite function. For ease of exposition, the spatial dependence is encapsulated by a single range parameter φ >0. More elaborate families could also be envisioned."
3964,unknown,elaborate families could also be envisioned. We focus on a Bayesian formulation of inference and consider an MCMC approach that updates parameters Σ and φ in turn at each iteration. This usually entails evaluating the likelihood at each step of the Markov chain which is known to be costly for multivariate Gaussian densities even with a moderate sample size. If we were to naively consider
3965,unknown,"the multivariate normal distribution of vec(V), evaluating the likelihood would require computing the inverse and determinant of the covariance matrix R ⊗ Σ. This computation has overall O(p3n3) asymptotic complexity. However, basic algebra results tell us that (R ⊗ Σ)−1 = R−1 ⊗ Σ−1 and det(R ⊗ Σ) = det( R)pdet(Σ)n. Exploiting the structure of this model allows us to conduct our calculation direct"
3966,unknown,"advantage over a general multivariate model for a sample size of np. An update to the single spatial range parameter φ requires us to compute the inverse and determinant of the n × n matrix R which is the crux of the computation since we consider the number n of samples to greatly exceed the dimension p of each observation, i.e. p ≪ n. Moreover, we can rewrite the quadratic product vec(V)⊤(R−1 ⊗ Σ"
3967,unknown,"similar to equation (1), thus avoiding the need to ever construct or store any np × np matrix. What is also worth mentioning is that we obtain full-conditional conjugacy by assigning an inverse-Wishart prior toΣ. Those computational considerations about the separable model are exposed in Banerjee and Gelfand [2002] among others. We solely review them here to provide some context for the more gener"
3968,unknown,"perspective, the separable specification is too restrictive for most applications as the components of v(·) have the same marginal distribution. In particular, it would imply that all the variables in the model have the same practical range. This assumption is pretty strong and rather unreasonable when modeling the spatial covariation of different environmental variables, for example. 2.2 Non-Sepa"
3969,unknown,"variables, for example. 2.2 Non-Separable LMC Allowing the independent base processes wj(·), j= 1, . . . , pto have distinct covariance functions solves the issue of having identically distributed components vj(·). In light of Section 2.1, a natural question that arises is how much, if any, of the computational advantages can be retained from the separable case. Some of the computational results"
3970,unknown,"we employ were discussed in the Ph.D. thesis of Kyzyurova [2017]. To summarize, the covariance structure of the LMC implies Gaussian likelihood evaluations that scale linearly in p rather than the O(p3n3) associated with general multivariate models. It positions the LMC as a computationally advantageous alternative to other multivariate processes. In our opinion, this fact has failed to gain the t"
3971,unknown,"3 Computational Considerations for the LMC The setup is as before, with a multivariate process v(s) defined from p independent processes wj(s), j= 1, . . . , p through the linear transformation v(s) = Aw(s). This time however, the base processes wj(·) do not have the same distribution. As before, we consider the processes to have a mean of 0 and a variance of 1, but we make them distinct by assign"
3972,unknown,"ourselves to one family of isotropic and stationary correlation functions indexed by distinct range parameters so that Cov(wj(s), wj(t)) = ρj(s, t) = ρ(φj|s − t|). Gaussian distribution properties under linear transformations tell us that the cross-covariance function of the multivariate process v(·) is given by C(s, t) = Pp j=1 ρj(s, t)aja⊤ j where aj is the jth column of A. For a finite sample o"
3973,unknown,"j=1 ρj(s, t)aja⊤ j where aj is the jth column of A. For a finite sample of size n organized in a column vector (v(s1)⊤, . . . ,v(sn)⊤)⊤ of length np, this equates to a multivariate normal distribution of mean 0 and covariance matrix Pp j=1 Rj ⊗ aja⊤ j [Gelfand et al., 2004] with Rj being the spatial correlation matrix associated with (wj(s1), . . . , wj(sn))⊤. Neither the covariance function nor m"
3974,unknown,"structure of the model could be exploited. On the other hand, we know intuitively that generating LMC-distributed random variables does not require the Cholesky factor of the np × np covariance matrix. We can simulate the base point processes wj(·) at locations s1, . . . ,sn from the individual Cholesky factors (Bj, say) of the spatial correlation matrices Rj, j= 1 . . . , pand then apply the line"
3975,unknown,"generate a p × n LMC distributed matrix V with rows (resp. columns) corresponding to processes (resp. locations) by setting V = pX j=1 ajzjB⊤ j , where zj are row vectors of independent and standard normal random variables. Just as we can circumvent the factorization of the np × np covariance matrix when simulating the LMC, we can also avoid computing the inverse and determinant directly when eval"
3976,unknown,"determinant directly when evaluating the likelihood. In Appendix A, we detail the following two results which have appeared in Kyzyurova [2017] under some slightly different notation. The first one tells us how to compute the precision matrix: (Pp j=1 Rj ⊗ aja⊤ j )−1 = Pp j=1 R−1 j ⊗ a−⊤ j a−1 j , where a−1 j is the jth row of the inverse matrix A−1. The second one concerns the determinant and say"
3977,unknown,"det(Pp j=1 Rj ⊗ aja⊤ j ) = det(A)2n Qp j=1 det(Rj). We go further and express the LMC likelihood in terms of the p × n matrix V. This facilitates the calculation beyond what is shown in Kyzyurova [2017] by simplifying what would be a np × np quadratic product under the vectorized form. Proposition 1 (The LMC Density). Let wj ind. ∼ N(0, Rj), j= 1, . . . , pbe independent n-vectors forming the rows"
3978,unknown,"ind. ∼ N(0, Rj), j= 1, . . . , pbe independent n-vectors forming the rows of the p × n matrix W = (w1, . . . ,wp)⊤. The random matrix V = AW (for some p × p full-rank matrix A) has probability density function p(V|A, Rp j=1) = exp[−1 2 Pp j=1 a−1 j VR−1 j V⊤a−⊤ j ] (2π)np/2|det(A)|n Qp j=1 det(Rj)1/2 . (2) Equivalently, we write V ∼ LMC(A, Rp j=1) if vec(V) has the multivariate normal distribution"
3979,unknown,"covariance Pp j=1 Rj ⊗ aja⊤ j with A = (a1, . . . ,ap). Expression (2) is analogous (in the case of the general LMC) to the matrix normal density in equation (1). We can see that it is again possible to evaluate the likelihood of V without computing the determinant or inverse of any np × np matrix neither is it necessary to store any such matrix. Computation can be carried out directly on the core"
3980,unknown,"matrix A and the p spatial correlation matrices Rj obtained by applying the functions ρ(φj| · |) to all pairs of locations in s1, . . . ,sn. In Figure 1, we empirically compare how likelihood computations scale: we can see the polynomial trend (as a function of p) of the naive vectorized form compared to the likelihood formulation in (2) which is linear in p. Marginally, the covariance matrix of t"
3981,unknown,"a variance of 1. However, unlike in the separable case, the finite sample distribution is not the same in general for two different coregionalization matrices A1, A2 even if A1A⊤ 1 = A2A⊤ 2 (see Appendix B). There are still however identifiability issues with the matrix A in the covariance matrix Pp j=1 Rj ⊗ aja⊤ j . The model is equivalent even if we apply a change of sign to any column aj. It is"
3982,unknown,"general LMC implies a covariance among pairs of scalar observations that is of the form Cov(vi(s), vj(t)) = Cij(s, t) = pX k=1 aikajkρ(φk|s − t|), (3) 4 Computational Considerations for the LMC 2 3 4 5 6 7 8 0 20 40 60 n=100 2 3 4 5 6 7 8 0 100 200 n=200 2 3 4 5 6 7 8 0 200 400 600 800 n=300 2 3 4 5 6 7 8 0 250 500 750 1000 1250 n=400 p Time (sec) Vector Matrix Figure 1: Time elapsed after 500 lik"
3983,unknown,"the one described in Proposition 1 while the vector form is the naive implementation that computes both the determinant and inverse of Pp j=1 Rj ⊗ aja⊤ j . where the quantities aij are the individual entries in the coregionalization matrix A. Next, we discuss predicting the multivariate process implied by the LMC at a set sPRED of locations conditional on its values observed at locations sOBS. We "
3984,unknown,"assume V is a p × n matrix, only this time the matrix is split along the horizontal into two sub-matrices VOBS and VPRED such that V = (VOBS, VPRED). In that case, the spatial covariance matrices Rj are each partitioned between the observed and prediction locations as Rj = R(j) OBS R(j) OBS,PRED R(j) PRED,OBS R(j) PRED ! . (4) We are interested in the conditional distribution of VPRED given VOBS. "
3985,unknown,"conditional on VOBS preserves the LMC covariance structure. This is detailed in the following result. Proposition 2 (Conditional Distributions of the LMC) . The distribution implied by the LMC at locations sPRED conditional on its values VOBS at locations sOBS is also an LMC with coregionalization matrix A and spatial correlation matrices given by R(j) PRED − R(j) PRED,OBSR(j)−1 OBS R(j) OBS,PRED."
3986,unknown,"OBS,PRED. It is centered around the matrix pX j=1 aja−1 j VOBSR(j)−1 OBS R(j) OBS,PRED. (5) This is equivalent to applying the inverse transformation A−1VOBS = WOBS, computing the conditional WPRED|WOBS across the p independent rows and transforming back as VPRED = AWPRED. The proof can be found in Appendix D. Proposition 2 has implications for Vecchia type of approximations [Vecchia,"
3987,unknown,"1988, Datta et al., 2016, Katzfuss and Guinness, 2021] which are described by conditional independence assumptions. This computational approximation offers likelihood evaluations that are linear in the number n of locations. It also has been shown empirically to be accurate when compared with alternative methods [Heaton et al., 2019]. That the LMC covariance structure is preserved under conditioni"
3988,unknown,"principle be used in conjunction with such approximations when both n and p are fairly large. 2.3 The triangular specification A common thread among the statistical literature concerning the LMC (see Schmidt and Gelfand [2003], Foley and Fuentes [2008] for example) is to use a lower triangular parametrization for the coregionalization matrix A where 5 Computational Considerations for the LMC"
3989,unknown,"5 Computational Considerations for the LMC diagonal elements are restricted to positive values. It does solve the identifiability issue described in the previous section. Columns cannot change signs anymore because of the positivity of the diagonal. They cannot be reordered either since that would break the triangular structure. Doing so, however, imposes an asymmetry on the components of the mode"
3990,unknown,"under a reordering of dimension indices {1, . . . , p} even though we usually think of such labels as superfluous. It is perhaps better appreciated by looking at the marginal covariance function implied by the lower triangular structure. For each process vj(·), j= 1, . . . , p, we have from (3) that Cj(s, t) = Pj k=1 a2 jkρ(φk|s − t|). Marginal models go up in complexity as indices increase. This "
3991,unknown,"complexity as indices increase. This limitation is also outlined in Carter et al. [2024]. As discussed in Schmidt and Gelfand [2003] and Gelfand et al. [2004], the lower triangular specification for A admits a computational simplification when describing the processes conditionally as vj(·)|v1(·), . . . , vj−1(·). This arises because we can write A−1v(s) = w(s) and A−1 is also lower triangular. He"
3992,unknown,"vj(s) = Pj−1 ℓ=1 αjℓvℓ(s) + σjw(s) with αjℓ = −a−1 jℓ /a−1 jj and σj = 1/a−1 jj (a−1 jℓ are elements of the inverse matrix A−1). Each value vj(s) is a linear combination of the previous v1(s), . . . , vj−1(s) and the jth underlying process wj(s). The covariance matrix of process vj(·) at locations s1, . . . ,sn conditional on previous vℓ(·), ℓ= 1, . . . , j− 1 simplifies to that of wj(·) at said l"
3993,unknown,"simplifies to that of wj(·) at said locations. To summarize, the likelihood can be computed in conditional factorization from the inverses and determinants of individual matrices A, R1, . . . ,Rp. This is an attractive formulation from a computational standpoint. Generalizing this, we demonstrated in Section 2.2 that any LMC parametrization benefits from a similar computational simplification."
3994,unknown,"This is fortunate as the triangular approach is limiting and hard to justify. It is a fundamental (rather than circumstantial) property of the LMC to have likelihood evaluations that require a number of operations that is linear inp. Computational simplicity is perhaps its most attractive feature. 2.4 Hierarchical Models In this section, we employ the LMC as the latent component in a hierarchical "
3995,unknown,"a standard problem when discussing computational scalability in any Gaussian process model. Suppose you observe a process Y(s) = v(s) + ϵ(s) where v(·) is some zero mean process accounting for spatial dependence in S ⊂Rd space while ϵ(·) is some form of white noise. The multivariate N(0, Σ) distribution implied by process v(·) observed at locations s1, . . . ,sn might benefit from some useful comp"
3996,unknown,"could be the result of employing an approximation such as the nearest neighbor approach of Datta et al. [2016] that provides a factorization (detailed in Finley et al. [2019]) of the precision matrix Σ−1. It could also be because the structure of the covariance matrix Σ can be exploited, as is the case for the LMC. In either case, we face the same problem: the finite-dimensional distribution of Y("
3997,unknown,"diagonal matrix D. Since D is of full rank, there is no simple way to evaluate the Gaussian likelihood without inverting or computing the determinant of any np × np matrix (in the multivariate setting). In other words, working with the distribution of the collapsed Gaussian process Y(·) forces you to relinquish any computational schemes that applied to v(·). In a Bayesian context, there is always "
3998,unknown,"turn in a Gibbs sampling approach. However, this often leads to Markov chains that exhibit high autocorrelation [Papaspiliopoulos et al., 2007, Murray and Adams, 2010, Yu and Meng, 2011]. We explore strategies to alleviate this problem for the specific case of the LMC in Appendix E. The distribution of v conditional on y is multivariate normal with covariance matrix (Σ−1 + D−1)−1 which is also cum"
3999,unknown,"conditionals of the np-dimensional distribution of v|y can be accomplished without the need to invert or factorize any np × np matrix. Details are given in Appendix F. In the context of the LMC, the finite sample equation has the form Y = V + ϵ where V ∼ LMC(A, Rp j=1) and ϵ ∼ MN(0, D, I). The p × p matrix D = diag{τj, j= 1 , . . . , p} contains the distinct observational variances"
4000,unknown,"associated with each of the p processes. We omit the usual mean structure to focus our discussion on the sampling of covariance parameters. The LMC structure of V cannot be exploited in order to derive the covariance ofY even though ϵ is itself an LMC distributed matrix normal. In general, the sum of LMC-distributed matrices is not itself distributed according to the LMC. The same is true for the "
4001,unknown,"The 2np-dimensional normal density of the complete data Y, V is of the form: p(Y, V|A, Rp j=1, D) = exp[−1 2 Pp j=1 a−1 j VR−1 j V⊤a−⊤ j − 1 2 tr{(V − Y)⊤D−1(V − Y)}] (2π)np|det(A)|n Qp j=1 det(Rj)1/2 Qp j=1 τn/2 j . (6) 6 Computational Considerations for the LMC Because we used the alternative formulation described in Section 2.2 for the latent LMC V, expression (6) above can be evaluated without"
4002,unknown,"be evaluated without the need to carry out computations on any np × np matrices. The complete data likelihood in (6) describes a hierarchical model for Y that is centered at V with independent error terms ϵ. Alternatively, we could also write Y = AW + ϵ where W ∼ LMC(I, Rp j=1). The rows w1, w2, . . . ,wp of W are the p independent spatial processes underlying the LMC. In this context, the latent "
4003,unknown,"independent of the coregionalization matrix A. In essence, since the latent process V is not observed, we can equivalently think of the complete data as (Y, W). This leads to a joint likelihood of the form p(Y, W|A, Rp j=1, D) = exp[−1 2 Pp j=1 w⊤ j R−1 j wj − 1 2 tr{(AW − Y)⊤D−1(AW − Y)}] (2π)np Qp j=1 det(Rj)1/2 Qp j=1 τn/2 j . (7) Expression (7), same as (6), factorizes in individual terms cont"
4004,unknown,"correlation matrices Rj. Those represent the crux of the computation when evaluating the likelihood. It seems we can benefit from similar computational advantages to those described in Section 2.2 by simply changing the parametrization. However, in the context where the independent variance terms τj along the diagonal of D are small, updates to A conditional on fixed W will tend to be very concent"
4005,unknown,"updates to the latent variables W and model parameters A, φp j=1, τp j=1 are likely to exhibit high autocorrelation in the components of A for reasons that are well outlined in Murray and Adams [2010]. In other words, such a formulation of the problem is unlikely to perform very well when the signal-to-noise (StN) ratio is relatively high. In return, it is likely to perform better than the centere"
4006,unknown,"scenario is not auspicious for statistical analysis in general. In Appendix E, we explore interweaving strategies that can, in principle, benefit from the advantages of both parametrizations. 3 Sparse LMC As a function of dimensionality, the LMC introduces p2 parameters. It makes for a model that is not parsimonious and this might cause overfitting (in the sense of deteriorating predictions at out"
4007,unknown,moderately large. There could also exist independencies among the processes; it would perhaps be expected in high dimensional settings. We address those issues by designing a prior that puts probability mass on coregionalization matrices that have structural zeros. We introduce the concept of a mask M which is a binary matrix of the same size as A. The true coregionalization matrix is then the ele
4008,unknown,"to be of full rank such that none of the p processes end up being deterministic linear combinations of the others. The triangular specification of the LMC discussed in Section 2.3 provides an example of a coregionalization matrix with structural zeros (everywhere over the main diagonal in this case). We argued that a lower triangular A is a model hypothesis that is hard to justify. In comparison, "
4009,unknown,"by learning the structure of the coregionalization matrix in a principled and data-driven approach. Same as before, we put i.i.d. normal priors on the elements of A which guarantees that det(A) ̸= 0 with probability 1. However, this might not hold when setting some elements to zero. For example, setting an entire row or column of M to zero will always lead to A ◦ M being singular. For any given bi"
4010,unknown,"to the elements of A that is absolutely continuous with respect to the p2-dimensional Lebesgue measure, we have that P(det(A ◦ M) = 0) is either 0 or 1. We detail below the process by which one can verify if a model, defined by the binary masking matrix M, is admissible in the sense that the matrix A ◦ M will be of full rank with probability 1. We start with the Laplace expansion along the ith row"
4011,unknown,"det(A ◦ M) = pX j=1 aijmijCij (8) where Cij is the (i, j) cofactor: the determinant of the matrix obtained by removing the ith row and jth column of A ◦ M multiplied by (−1)i+j. Equation (8) will be equal to 0 if and only if every summand is identically 0. The determinant can be expanded from any row or column. If any of those consists entirely of 0s, then expression (8) will be equal to 0 with pr"
4012,unknown,"the admissibility of a model M. The other stopping state is when there are globally less than p zeros. In that case, expression (8) is almost surely not equal to 0. Indeed, this is trivially true for p = 1 and can be shown to be true for any p by induction using the Laplace expansion. Equation (8) allows us to transform a statement about a p × p matrix into verifying the same condition on smaller "
4013,unknown,only if all the determinants of the smaller matrices are identically 0 (those corresponding to the non-zero entries of 7 Computational Considerations for the LMC the row or column we chose to expand from). A good strategy is thus to always expand the determinant by the row or column that contains the most 0s to minimize the number of recursive calls. This allows us to formulate a hierarchical prio
4014,unknown,"admissible ones. The respective entries of the coregionalization matrix are assigned at the later stage. The prior on the masking matrix is assumed to take the form p(M) ∝ π Pmij (1 − π)p2−Pmij , (9) where Pmij is the number of 1s contained in M and π ∈ [0, 1] is a hyperparameter controlling prior belief on the degree of sparsity. It is essentially the prior probability for an entry of M to be non"
4015,unknown,"described above. We use the framework of reversible jump MCMC [Green, 1995] to conceive proposals to the coregionalization matrix A ◦ M that allow trans-dimensional updates. At every step, we either propose to replace a non-zero entry with a 0 or to assign a new value to a previously null entry. We chainp of those proposals at each iteration of the Markov chain. The computational overhead is negli"
4016,unknown,"φp j=1, the updates to which are orders of magnitude more costly (see Appendix G for more details). The model parameters A, M, φp j=1, τp j=1 and the unobserved random fields V are updated successively in a Gibbs sampler. Next, we conduct an experiment designed to evaluate the effect on predictive accuracy of our sparsity-inducing approach. 3.1 Simulation Study Model parameters for the sparse LMC "
4017,unknown,"j=1, τp j=1. The prior distribution on the submodel defined by the p × p binary matrix M is described by equation (9). We use π = 1/p throughout as our prior belief on the degree of sparsity. We put independent Gaussian priors on the elements of the coregionalization matrixA. We use the exponential correlation function ρ(φj|s − t|) = exp( −φj|s − t|) for the underlying process wj(·) and the range "
4018,unknown,"φ1, . . . , φp are assigned uniform priors on [3, 30]. Roughly speaking, this corresponds to a practical range at distances between 0.1 and 1 (points are observed in the unit square). We use this informative prior on the range parameters because they cannot be consistently estimated along with the marginal variances contained in the coregionalization matrix [Zhang, 2004]. Finally, we assign indepe"
4019,unknown,"τ1, . . . , τp to obtain conjugate full conditional updates. We employ the complete data likelihood of Y, V described by equation (6). Additional details on the Bayesian setup for this study are provided in Appendix H. We start by simulating a set of n = 100 irregularly spaced locations from the uniform distribution on the unit square. We keep those locations fixed throughout the study. For each o"
4020,unknown,"of the multivariate GP. The model is simulated according to the LMC for each of two examples. In the first, the coregionalization matrix A ◦ M is full: it has no structural zeros. All processes are interdependent. In the second example, every entry of M is zero except the main diagonal hence the p resulting processes are mutually independent. In both cases, the rows of the matrix A are scaled so t"
4021,unknown,"full M example, the coregionalization matrix is filled with the value 1/√p with entries above the diagonal being negative. For the diagonal example, A ◦ M is simply the identity matrix. We add an observational noise of variance τj = 0.25, j= 1, . . . , pto the p components at each location. For the range parameters φp j=1, we split the [5, 25] interval with equal gaps on the logarithmic scale so f"
4022,unknown,"to ensure the p underlying processes are as distinguishable as possible while having range inside the [3,30] support. We run two MCMC inference algorithms on each LMC repetition. The first fits a regular LMC (without model selection) and the second fits the sparse LMC using reversible jumps. Both algorithms are described in Appendix G. We run each algorithm for 2000 iterations, keeping only the la"
4023,unknown,"LMC parameters A, M, φp j=1, we interpolate the processes to a regular 10 × 10 grid covering the domain. We compute the root mean square error (averaged over the grid locations and also across posterior samples) between the predictions and the held-out true values. We look at the difference in prediction error between the sparse and regular LMC fit. The distribution (across LMC repetitions) of thi"
4024,unknown,"Figures 2 and 3. In lower dimension p, there is little difference in using either of the two estimation methods. However, as dimensions increase, predictions from the sparse model appear more accurate than those under the standard approach with no structural zeros. This is certainly expected for the diagonal example as there are only p non-zero entries in the coregionalization matrix. It is perhap"
4025,unknown,better generalizes to out-of-sample predictions. This is because the number p2 of coregionalization parameters to estimate rapidly becomes unwieldy when compared to the number of observed locations. Our sparse approach facilitates the use of the LMC in such scenarios. We can also see on the right side of Figures 2 and 3 that our method manages to curb the growth of parameters especially in the dia
4026,unknown,"8 Computational Considerations for the LMC 2 3 4 5 6 p 0.10 0.08 0.06 0.04 0.02 0.00 0.02 0.04 Difference in RMSE 2 3 4 5 6 p 5 10 15 20 25 Number of Non-Zero Components Figure 2: Comparison of our sparse method with the standard approach for the case of a full (no zeros) coregionalization matrix. On the left, the difference (sparse minus standard) in RMSE computed on each realization. On the righ"
4027,unknown,"illustrate the posterior-mean number of non-zero components in the matrix M. The median (point) and [0.05,0.95] quantiles are computed across the 100 simulated LMC realizations. 2 3 4 5 6 p 0.10 0.05 0.00 0.05 0.10 Difference in RMSE 2 3 4 5 6 p 2 3 4 5 6 7 8 9 Number of Non-Zero Components Figure 3: Comparison of our sparse method with the standard approach for the case of a diagonal coregionaliz"
4028,unknown,"matrix (independent components). On the left, the difference (sparse minus standard) in RMSE computed on each realization. On the right, we illustrate the posterior-mean number of non-zero components in the matrix M. The median (point) and [0.05,0.95] quantiles are computed across the 100 simulated LMC realizations. p Method 2 3 4 5 6 Sparse 15.687 18.658 21.607 24.706 27.869 Standard 15.741 18.47"
4029,unknown,"Sparse 15.687 18.658 21.607 24.706 27.869 Standard 15.741 18.477 21.205 24.038 26.906 Table 1: Mean time (in seconds) elapsed per model fitting for the different scenarios. In Table 1, we compare the computing time of the sparse and standard specifications. Our approach adds very little computing time when compared to the traditional LMC. This is because recomputing the likelihood upon a modificat"
4030,unknown,"of the coregionalization matrix is negligible under our formulation (2) of the LMC density (see appendix G for more details). In the next section, we use our sparse methodology on a real dataset to showcase the inferential properties of our model. 3.2 Analysis of California Air Data In this section, we analyze a data set consisting of measurements in the concentration of four pollutants: carbon"
4031,unknown,"monoxide (CO), nitric oxide (NO), nitrogen dioxide (NO2) and ozone (O3). There are no covariates included. Our 9 Computational Considerations for the LMC interest is mainly in inferring the cross-covariance structure. We use the reversible jump sampler for the coregionalization matrix A ◦ M. This will help us identify potential independencies among the measured quantities. We analyze the"
4032,unknown,"data on the log scale to achieve approximate normality. The measurements are made at monitoring sites spread across the state of California. Those locations, projected to the unit square, are illustrated in Figure 4. The data is available publicly on the California Air Resources Board website. We chose the date of 26/04/2002 as it had the highest number of sites where all 4 components were measure"
4033,unknown,"least one measurement missing) for a total number of 131. The proportion of missing values for the components CO, NO, NO2 and O3 were respectively 0.25, 0.10, 0.09 and 0.05. Data from this database was also analyzed in Schmidt and Gelfand [2003] although in the summer rather than the spring and without the ozone component. Nevertheless, the resulting inference we present here, while not identical,"
4034,unknown,"magnitude of correlations among the three components CO, NO and NO2. 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 1.0 Monitoring Sites Figure 4: Projected locations of the 131 monitoring stations used in the analysis of the California air data. We assign independent Gaussian priors to the non-zero components of A. The range components φp j=1 are constrained to the uniform [3,30] distribution, a priori."
4035,unknown,"j=1 are given inverse gamma prior distributions. The log concentrations of each of the four pollutants do not appear to share a common mean hence we add a component-specific mean structure to the LMC. The p-dimensional mean vector µ is assigned a Gaussian distribution with diagonal covariance to obtain conjugate normal full conditional updates. Finally, we use 1/p = 0.25 for the hyperparameter π t"
4036,unknown,"Appendix H. We run the algorithm for 200,000 iterations, discarding the first 50,000 as burn-in. The posterior samples of the covariance and correlation matrices are summarized in Figure 5. We can see some of the credible intervals have boundaries that are exactly 0. This is a consequence of assigning prior probability mass at 0 for the elements of the coregionalization matrix A ◦ M. Another inter"
4037,unknown,"to the LMC is that we can evaluate the posterior probability of two processes to be independent. Those probabilities are illustrated in Figure 6 along with the distribution of non-zero elements in the matrix M. It seems to potentially indicate that the CO pollutant could be uncorrelated with the other three. Further evidence would be needed before reaching this conclusion, but this is not the obje"
4038,unknown,"seems to be worth it since they have probability 0 of being independent. In Figure 7, we show the posterior probabilities of independence in an alternate scenario where we simulate an independent fifth component (D). We can see that our model appropriately captures this independency. 4 Discussion We revisited the LMC as a constructive approach to specifying cross-covariance functions, deriving som"
4039,unknown,"characteristics in the process. Starting with the separable specification of the LMC and its link with the matrix normal distribution, we were able to make an analogous connection between the more general LMC and a new kind of Gaussian distribution for matrices. Doing so exposed a covariance structure that can be exploited to obtain likelihood evaluations in a number of operations that is linear i"
4040,unknown,10 Computational Considerations for the LMC CO NO NO 2 O3 CO 0.13 (0.00;0.41) NO 0.17 (0.00;0.41) 0.65 (0.36;1.05) NO2 0.09 (0.00;0.30) 0.46 (0.28;0.75) 0.52 (0.32;0.92) O3 -0.02 (-0.07;0.00) -0.10 (-0.17;-0.06) -0.09 (-0.16;-0.05) 0.03 (0.01;0.05) Figure 5: Posterior median of the marginal (at each location) covariance matrix along with the 0.05 and 0.95 quantiles in parenthesis. CO NO NO 2 O3 CO
4041,unknown,CO NO NO 2 O3 CO 0.0000 NO 0.2243 0.0000 NO2 0.3785 0.0000 0.0000 O3 0.3804 0.0000 0.0000 0.0000 (a) Pairwise Probability of Independence 6 7 8 9 10 11 12 0 10000 20000 30000 40000 50000 60000 (b) Number of ones in M(out of p2 = 16entries) Figure 6 CO NO NO 2 O3 D CO 0.0000 NO 0.1187 0.0000 NO2 0.2673 0.0000 0.0000 O3 0.3776 0.0000 0.0000 0.0000 D 0.6037 0.6066 0.8207 0.8623 0.0000 Figure 7: Pairw
4042,unknown,"mean -3.92, unit variance and range parameter equal to 10. suit of the LMC, a fact that has not been taken advantage of in the literature about multivariate Gaussian processes. Since this is not an approximation, there are only benefits to using the formulation we describe in Section 2.2. Beyond the plain computational speedups, there are other advantages to the matrix normal representation of the"
4043,unknown,"Foremost, the coregionalization matrix A is not entangled with the spatial correlation matrices R1, . . . ,Rp in the likelihood. In particular, an update to A does not require computing the inverse and determinant of any np × np sized matrix. Even though it is generally small in comparison with the spatial correlation matrices, the matrix A contains more parameters and can become difficult to esti"
4044,unknown,cheap so we can incorporate more involved samplers for it without much of an effect on the total computing time. We exploit this fact in a new model specification for the LMC where probability mass is assigned to coregionalization matrices with structural zeros. Doing so allows us to curb the quadratic growth (as a function of p) of the number of parameters associated with the LMC. This type of re
4045,unknown,"on out-of-sample data in Section 3. Moreover, our sparse approach to the LMC can provide interesting insights by discovering potential independencies among the p modeled components. In terms of future developments and applications, we can think of a few. First, since the computational structure is preserved under conditioning, we would like to see it exploited and fully integrated into the framewo"
4046,unknown,"11 Computational Considerations for the LMC approximations like the nearest neighbor approach. This would in principle be a straightforward application of Propositions 1 and 2. It would form a class of highly scalable (as a function of both n and p) multivariate Gaussian models. Also, we used the flexibility of the matrix normal formulation to propose a sparse version of the LMC along with the app"
4047,unknown,"design samplers that facilitate estimation in particular contexts and applications. Finally, while we focus on spatial covariance functions throughout this paper, any structure on the underlying independent processes can be employed. For example, the dependence in time series applications can often be modeled using Markovian assumptions which induce a likelihood that can be decomposed into a serie"
4048,unknown,"through a linear transformation (applied at each time point) of independent processes (à la LMC) would, in principle, imply a matrix normal distribution with likelihood evaluations that scale linearly in both dimensions. Funding Alie, Stephens and Schmidt acknowledge support from the Natural Sciences and Engineering Research Council of Canada (NSERC). Alie also acknowledges the support from the Fo"
4049,unknown,"(FRQNT). References Tatiyana V Apanasovich and Marc G Genton. Cross-covariance functions for multivariate random fields based on latent dimensions. Biometrika, 97(1):15–30, 2010. Tatiyana V Apanasovich, Marc G Genton, and Ying Sun. A valid Matérn class of cross-covariance functions for multivariate random fields with any number of components. Journal of the American Statistical Association, 107 (4"
4050,unknown,"(497):180–193, 2012. S Banerjee and AE Gelfand. Prediction, interpolation and regression for spatially misaligned data. Sankhy¯a: The Indian Journal of Statistics, Series A, pages 227–245, 2002. Sudipto Banerjee, Alan E Gelfand, Andrew O Finley, and Huiyan Sang. Gaussian predictive process models for large spatial data sets. Journal of the Royal Statistical Society Series B: Statistical Methodolog"
4051,unknown,"Gilles Bourgault and Denis Marcotte. Multivariable variogram and its application to the linear model of coregionalization. Mathematical Geology, 23:899–928, 1991. Jonathan R. Bradley, Scott H. Holan, and Christopher K. Wikle. Multivariate spatio-temporal models for high- dimensional areal data with application to Longitudinal Employer-Household Dynamics. The Annals of Applied Statistics, 9(4):1761"
4052,unknown,"Claudia Cappello, Sandra De Iaco, and Monica Palma. Computational advances for spatio-temporal multivariate environmental models. Computational Statistics, pages 1–20, 2022. J Brandon Carter, Christopher R Browning, Bethany Boettner, Nicolo Pinchak, and Catherine A Calder. Land-use filtering for nonstationary spatial prediction of collective efficacy in an urban environment. The annals of applied "
4053,unknown,"statistics, 18(1):794, 2024. N Cressie. Statistics for spatial data. Wiley Location New York, NY , 1993. Abhirup Datta, Sudipto Banerjee, Andrew O Finley, and Alan E Gelfand. Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets. Journal of the American Statistical Association, 111(514):800–812, 2016. A Philip Dawid. Some matrix-variate distribution theory: notati"
4054,unknown,"Biometrika, 68(1):265–274, 1981. Debangan Dey, Abhirup Datta, and Sudipto Banerjee. Graphical Gaussian process models for highly multivariate spatial data. Biometrika, 109(4):993–1014, 2022. Paul Erdös. On sets of distances of n points. The American Mathematical Monthly, 53(5):248–250, 1946. Andrew O Finley, Huiyan Sang, Sudipto Banerjee, and Alan E Gelfand. Improving the performance of predictive"
4055,unknown,"process modeling for large datasets. Computational statistics & data analysis, 53(8):2873–2884, 2009. Andrew O Finley, Abhirup Datta, Bruce D Cook, Douglas C Morton, Hans E Andersen, and Sudipto Banerjee. Efficient algorithms for bayesian nearest neighbor Gaussian processes. Journal of Computational and Graphical Statistics, 28 (2):401–414, 2019. Kristen M Foley and Montserrat Fuentes. A statistic"
4056,unknown,"models for hurricane surface wind prediction. Journal of agricultural, biological, and environmental statistics, pages 37–59, 2008. 12 Computational Considerations for the LMC Alan E Gelfand, Alexandra M Schmidt, Sudipto Banerjee, and CF Sirmans. Nonstationary multivariate process modeling through spatially varying coregionalization. Test, 13:263–312, 2004. Alan E Gelfand, Sudipto Banerjee, and Da"
4057,unknown,"dynamic spatial data. Environmetrics: The official journal of the International Environmetrics Society, 16(5):465–479, 2005. Marc G. Genton and William Kleiber. Cross-Covariance Functions for Multivariate Geostatistics. Statistical Science, 30(2):147 – 163, 2015. doi: 10.1214/14-STS487. URL https://doi.org/10.1214/14-STS487. Tilmann Gneiting. Nonseparable, stationary covariance functions for space"
4058,unknown,"Statistical Association, 97(458):590–600, 2002. Tilmann Gneiting, William Kleiber, and Martin Schlather. Matérn cross-covariance functions for multivariate random fields. Journal of the American Statistical Association, 105(491):1167–1177, 2010. Michel Goulard and Marc V oltz. Linear coregionalization model: tools for estimation and choice of cross-variogram matrix. Mathematical Geology, 24:269–28"
4059,unknown,"matrix. Mathematical Geology, 24:269–286, 1992. Peter J Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination.Biometrika, 82(4):711–732, 1995. Peter Guttorp and Tilmann Gneiting. Studies in the history of probability and statistics XLIX on the Matérn correlation family. Biometrika, 93(4):989–995, 2006. Matthew J Heaton, Abhirup Datta, Andrew O Finley, Reinhar"
4060,unknown,"Gerber, Robert B Gramacy, Dorit Hammerling, Matthias Katzfuss, et al. A case study competition among methods for analyzing large spatial data. Journal of Agricultural, Biological and Environmental Statistics, 24:398–425, 2019. Ladan Heydari, Hossein Bayat, and Annamaria Castrignanò. Scale-dependent geostatistical modelling of crop-soil relationships in view of precision agriculture. Precision Agri"
4061,unknown,"relationships in view of precision agriculture. Precision Agriculture, pages 1–27, 2023. Shujuan Ji, Yuanqing Wang, and Yao Wang. Geographically weighted Poisson regression under linear model of coregionalization assistance: Application to a bicycle crash study. Accident Analysis & Prevention, 159:106230, 2021. Xiaoping Jin, Sudipto Banerjee, and Bradley P Carlin. Order-free co-regionalized areal "
4062,unknown,"multiple-disease mapping. Journal of the Royal Statistical Society Series B: Statistical Methodology, 69(5):817–838, 2007. Matthias Katzfuss and Joseph Guinness. A general framework for Vecchia approximations of Gaussian processes. Statistical Science, 36(1):124 – 141, 2021. Mitchell L. Krock, William Kleiber, Dorit Hammerling, and Stephen Becker. Modeling massive highly multivariate"
4063,unknown,"nonstationary spatial data with the basis graphical lasso. Journal of Computational and Graphical Statistics, 32(4): 1472–1487, 2023. doi: 10.1080/10618600.2023.2174126. Ksenia N Kyzyurova. On uncertainty quantification for systems of computer models. PhD thesis, Duke University, 2017. E. Laguerre. Sur quelques points de la théorie des équations numériques. Acta Mathematica, 4:97 – 120, 1884."
4064,unknown,"Finn Lindgren, Håvard Rue, and Johan Lindström. An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach. Journal of the Royal Statistical Society Series B: Statistical Methodology, 73(4):423–498, 2011. Haitao Liu, Jiaqi Ding, Xinyu Xie, Xiaomo Jiang, Yusong Zhao, and Xiaofang Wang. Scalable multi-task Gaussian processes with"
4065,unknown,"processes with neural embedding of coregionalization. Knowledge-Based Systems, 247:108775, 2022. Anandamayee Majumdar and Alan E Gelfand. Multivariate spatial modeling for geostatistical data using convolved covariance functions. Mathematical Geology, 39:225–245, 2007. Gianluca Mastrantonio, Giovanna Jona Lasinio, Alessio Pollice, Giulia Capotorti, Lorenzo Teodonio, Giulio Genova,"
4066,unknown,"and Carlo Blasi. A hierarchical multivariate spatio-temporal model for clustered climate data with annual cycles. The Annals of Applied Statistics, 13(2):797–823, 2019. Bertil Matérn. Spatial variation. Medd. Statns Skogsforskinst., 49(5), 1960. G Matheron. Pour une analyse krigeante des données régionalisées. Centre de Géostatistique, Report N-732, Fontainebleau, page 22, 1982. Iain Murray and Ry"
4067,unknown,"neural information processing systems, 23, 2010. 13 Computational Considerations for the LMC Radford M Neal. Slice sampling. The annals of statistics, 31(3):705–767, 2003. Omiros Papaspiliopoulos, Gareth O Roberts, and Martin Sköld. A general framework for the parametrization of hierarchical models. Statistical Science, pages 59–73, 2007. Martyn Plummer, Nicky Best, Kate Cowles, and Karen Vines. C"
4068,unknown,"mcmc. R News, 6(1):7–11, 2006. URL https://journal.r-project.org/archive/. Alexandra M Schmidt and Alan E Gelfand. A Bayesian coregionalization approach for multivariate pollutant data. Journal of Geophysical Research: Atmospheres, 108(D24), 2003. Daniel Taylor-Rodriguez, Andrew O Finley, Abhirup Datta, Chad Babcock, Hans-Erik Andersen, Bruce D Cook, Douglas C Morton, and Sudipto Banerjee. Spatial"
4069,unknown,"application in forest variable mapping. Statistica Sinica, 29:1155, 2019. David A Van Dyk and Taeyoung Park. Partially collapsed Gibbs samplers: Theory and methods. Journal of the American Statistical Association, 103(482):790–796, 2008. Aldo V Vecchia. Estimation and model identification for continuous spatial processes. Journal of the Royal Statistical Society Series B: Statistical Methodology, "
4070,unknown,"Society Series B: Statistical Methodology, 50(2):297–312, 1988. Shihong Wang, Xueying Zhang, Yichen Meng, and Wei W Xing. E-lmc: Extended linear model of coregionalization for spatial field prediction. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2022. Yaming Yu and Xiao-Li Meng. To center or not to center: That is not the question—an ancillarity–sufficiency"
4071,unknown,"interweaving strategy (asis) for boosting mcmc efficiency. Journal of Computational and Graphical Statistics, 20(3): 531–570, 2011. Hao Zhang. Inconsistent estimation and asymptotically equal interpolations in model-based geostatistics. Journal of the American Statistical Association, 99(465):250–261, 2004. Lu Zhang and Sudipto Banerjee. Spatial factor modeling: A bayesian matrix-normal approach f"
4072,unknown,"Biometrics, 78(2):560–573, 2022. A Proof of Proposition 1 We can think of a vectorized formw′ = (w1, w2, . . . ,wp)⊤ for the base processes where we stack thep n-dimensional processes instead of ordering by locations. This is how it is presented in Jin et al. [2007] in the context of multivariate conditional auto-regressive models rather than continuously indexed spatial data. The authors get fair"
4073,unknown,"argument we use here. Since we start with independent processes, thenp×np covariance matrix of w′ is block diagonal with blocks consisting of the n × n correlation matrices R1, . . . ,Rp. In the usual representation (ordered by locations), we would consider the np-dimensional vector w = (w(s1)⊤, w(s2)⊤, . . . ,w(sn)⊤)⊤. Of course, w is obtained by a permutation of the elements of w′ so there exist"
4074,unknown,"The multivariate process v = (v(s1)⊤, v(s2)⊤, . . . ,v(sn)⊤)⊤ with cross-dependence is obtained from w by trans- forming at each location s according to v(s) = Aw(s). For the vectorized forms, this means that v is obtained by multiplying a block diagonal matrix consisting of n A matrices along the diagonal with w. Putting all the above together, we can write the covariance of the vectorized v as V"
4075,unknown,"Var(v) =   A 0 . . . 0 0 A . . . 0 ... ... ... ... 0 0 . . . A  P   R1 0 . . . 0 0 R 2 . . . 0 ... ... ... ... 0 0 . . . Rp  P⊤   A⊤ 0 . . . 0 0 A ⊤ . . . 0 ... ... ... ... 0 0 . . . A⊤  . (10) Lemma 1. The inverse for the covariance matrix of the vectorized form (v(s1)⊤, . . . ,v(sn)⊤)⊤ of the LMC can be computed as   pX j=1 Rj ⊗ aja⊤ j   −1 = pX j=1 R−1 j ⊗ a−⊤ j a−"
4076,unknown,"j   −1 = pX j=1 R−1 j ⊗ a−⊤ j a−1 j , where a−1 j is the jth row of the inverse matrix A−1. 14 Computational Considerations for the LMC Proof. This can be shown by direct verification. Alternatively, from equation (10) we can guess the inverse   A−⊤ 0 . . . 0 0 A −⊤ . . . 0 ... ... ... ... 0 0 . . . A−⊤  P   R−1 1 0 . . . 0 0 R −1 2 . . . 0 ... ... ... ... 0 0 . . . R−1 p  P⊤"
4077,unknown,"0 0 . . . R−1 p  P⊤   A−1 0 . . . 0 0 A −1 . . . 0 ... ... ... ... 0 0 . . . A−1   which is exactly the claimed solution. Lemma 2. The determinant for the covariance matrix of the vectorized form (v(s1)⊤, . . . ,v(sn)⊤)⊤ of the LMC can be computed as det   pX j=1 Rj ⊗ aja⊤ j   = det(A)2n pY j=1 det(Rj). Proof. Evident from equation (10) since the determinant of the permutation m"
4078,unknown,The final step to prove Proposition 1 is to express the p quadratic products v⊤   R−1 j ⊗ a−⊤ j a−1 j  v using the matrix form V of the LMC. The np-vector v is obtained by stacking the columns of V hence we write v = vec(V). We can then use the well known identities (C⊤ ⊗ A)vec(B) = vec(ABC) and vec(A)⊤vec(B) = tr(A⊤B) in this order to obtain vec(V)⊤   R−1 j ⊗ a−⊤ j a−1 j  vec(V) = tr(V⊤a−⊤ j a−
4079,unknown,"j a−1 j VR−1 j ) and use the cyclic property of the trace to express it as the scalar a−1 j VR−1 j V⊤a−⊤ j . The argument can be recycled for models where the coregionalization matrix varies over locations. Such is the case for the non-stationary LMC proposed by Gelfand et al. [2004] where the matrix A varies smoothly across space. In that case, we can index the coregionalization matrix Ai with lo"
4080,unknown,sequence in place of the singular A on the diagonal in equation (10). This implies that the inverse and determinant of the covariance matrix implied by the nonstationary version of the LMC can also be computed from those of the individual matrices; this time the n p× p coregionalization matrices Ai and the p n× n spatial correlation matrices Rj. B Identifiability In the case where exponential corr
4081,unknown,"wj(·), j= 1, . . . , p, we will show that one can identify both the full rank coregionalization matrix A along with range parameters φ1, . . . , φp. We will assume without loss of generality that the range parameters are in increasing order such that φ1 < ··· < φp. To be clear, we mean that provided there is a sufficient numbern of points (to be clarified below), parameters A, φp j=1 and B, ψp j=1"
4082,unknown,"j=1 and B, ψp j=1 will lead to the same model if and only if φj = ψj for all j and A = B up to column sign switches. This is unlike the separable case ( φ1 = φ2 = ··· = φp) where the model is a function of the product AA⊤. There exists an infinity of equivalent such factorizations. Standard examples include the lower triangular Cholesky decomposition or the positive definite square root."
4083,unknown,"triangular Cholesky decomposition or the positive definite square root. To establish this result, we first look at the cross-covariance function of components i and j evaluated at distance d: Cij(d) = pX k=1 aikajk exp(−φkd). We postulate for a contradiction that another set of parametersB, ψp j=1 leads to the same model. This leads to equations fij(d) := pX k=1 aikajk exp(−φkd) − pX ℓ=1 biℓbjℓ ex"
4084,unknown,"fij(d) := pX k=1 aikajk exp(−φkd) − pX ℓ=1 biℓbjℓ exp(−ψℓd) = 0, i, j = 1, . . . , p. (11) According to Laguerre’s generalization [Laguerre, 1884] of Descartes’ rule of signs, each fij above either has at most 2p − 1 positive roots or is identically 0 everywhere. We debunk the latter first. For fij to be zero everywhere, it needs to be the case that φk = ψk for all k. Otherwise, if there is a φk t"
4085,unknown,"to any of the alternative range parameters, then it means in particular that the coefficients a2 ik = 0 for each i = 1, . . . , p. 15 Computational Considerations for the LMC This signifies a column of 0s in A which is not allowed (A is of full rank). So we can assume φk = ψk, k= 1, . . . , p since both sets are ordered. Consequently, for the fii(d) to be 0 everywhere we now need the coefficients "
4086,unknown,"ik − b2 ik to be equal to zero for all i, k= 1, . . . , p; every entry of B is equal to the corresponding entry in A up to a change of sign. We also need aikajk − bikbjk = 0 for every j = 1, . . . , pso if an entry aik = −bik, then the whole column needs to be sign flipped. For all fij(d) to be zero everywhere, the range parameters need to be the same, and A and B are equal up to column sign chang"
4087,unknown,"sign changes. However, it may be the case that the 2p − 1 possible roots of the fij functions are enough to obtain 2 equivalent models at least at the locations observed. However, if there are more than 2p − 1 distinct distances among the points observed, then there wouldn’t be enough roots for the 2 models to agree on all of those. For example, let’s say the multivariate process v(s) is observed "
4088,unknown,"distinct distances problem [Erdös, 1946] that there are at the very least p n − 3/4 − 1/2 different pairwise distances. It means that if there is a minimum of (2p − 1/2)2 + 3/4 points, then the LMC is guaranteed to be identifiable in this context. Most often, points come in no regular pattern and hence present n(n − 1)/2 distinct distances (as many as there are pairs of points) and such a number u"
4089,unknown,"C Cross-Covariance Functions There is a vast literature on cross-covariance functions for multivariate Gaussian processes. We review some of the modeling advantages and drawbacks, specifically when compared to the LMC. In the approach of Apanasovich and Genton [2010] based on latent dimensions, a p-variate cross-covariance function is defined from a single univariate one over Rd+k (1 ≤ k ≤ p) by C"
4090,unknown,"Cij(s, t) = C{(s, ξi), (t, ξj)}, with s, t ∈ Rd; ξi, ξj ∈ Rk. (12) The cross-covariance above is valid by construction. The authors use this idea to generalize the class of space-time covariance functions described in Gneiting [2002] to multivariate data. Ignoring the time component for simplicity, the framework described by equation (12) is employed to construct a cross-covariance function of the"
4091,unknown,"Cij(|s − t|) = σ2 |ξi − ξj| + 1 exp −α|s − t| (|ξi − ξj| + 1)β/2 ! . (13) The interesting part of expression (13) is the presence of a single parameter β ∈ [0, 1] controlling separability. Another notable fact is how parsimonious model (13) is in terms of parameters. It involves p locations ξ1, . . . ,ξp, each one in Rk, along with the parameters of the single covariance function over Rd+k. In com"
4092,unknown,"components in its coregionalization matrix A plus the individual parameters contained in the p distinct univariate correlation functions ρj(·). We propose an approach to curb this potential over-parametrization problem in Section 3. An inconvenience of using (12), or the specific instance (13) of it, is that it leads to identical marginal models for each of the p processes. Perhaps less apparent i"
4093,unknown,"themselves valid univariate covariance functions. For general d and stationary-isotropic covariances that are a function of distance |s − t|, that restricts cross-covariances to the class of completely monotone functions (see Cressie [1993, Section 2.5.1]). In particular, negative correlation across processes is prohibited at any distance. [Apanasovich and Genton, 2010] propose to circumvent those"
4094,unknown,"that is very similar in spirit to what the LMC does. In terms of univariate correlation functions, there are perhaps none more celebrated than those of the Matérn class [Matérn, 1960] defined by M(s − t|ν, φ) = 21−ν Γ(ν) (φ|s − t|)νKν(φ|s − t|), (14) where Kν is the modified Bessel function and φ is a range parameter. Smoothness is handled by ν in the sense that any stationary random field with co"
4095,unknown,"sense [Guttorp and Gneiting, 2006]. The Matérn class includes the exponential correlation function, among others, as a special case when ν = 1/2. The objective of a multivariate Matérn covariance function is to have Matérn marginal covariances for thep processes while still allowing cross-dependence among them. Gneiting et al. [2010] specified a stationary cross-covariance function of the form, Ci"
4096,unknown,"function of the form, Cij(|s − t|) = σijM(s − t|νij, φij), i, j = 1, . . . , p. (15) 16 Computational Considerations for the LMC Conditions need to be imposed on parameters νij, φij to ensure a positive definite covariance. The authors first propose a parsimonious model obtained by applying the covariance convolution approach of Majumdar and Gelfand [2007] (which is itself a rich conceptual framew"
4097,unknown,approach imposes strong restrictions on the parameters of (15). It requires every range parameter φij to have the same value but still allows each process to have its own differentiability parameterνii > 0. Gneiting et al. [2010] also described a more general formulation for p = 2 by establishing a sufficient condition on σij under which (15) constitutes a valid model. This method was extended to 
4098,unknown,"[2012] although the conditions therein are a bit more involved. The authors do provide some helpful specific case examples. A multivariate Matérn model has a specific smoothness parameter for each of the p processes. For the LMC, each component is defined as the linear combination vi(s) = Pp j=1 aijwj(s) and therefore will have sample paths that are as differentiable as the roughest of underlying "
4099,unknown,"triangular case). However, multivariate cross-covariance functions of the form described in (15) for i ̸= j are restricted to monotone functions, so either Cij is positive everywhere or it is negative everywhere. Cross correlation functions implied by the LMC can be positive at close range and negative elsewhere. Finally, we highlighted in Section 2.2 how the structure of the LMC can be exploited "
4100,unknown,"likelihood in linear computational complexity when it comes to the number of components p. For the alternatives we discussed, computations still scale with p3 and therefore quickly become prohibitive for high dimensional processes. D Proof of Proposition 2 Proposition 2 can be verified by direct and standard calculations. Let Rj be defined as in equation (4), the covariance matrix of the vector (v"
4101,unknown,"OBS, v⊤ PRED)⊤ (with vOBS = vec( VOBS) and vPRED = vec( VPRED)) is of the formPp j=1 Rj ⊗ aja⊤ j . Standard conditional results concerning multivariate Gaussians can be applied. We start by first deriving the conditional mean: E[vPRED|vOBS] = pX i=1 R(i) PRED,OBS ⊗ aia⊤ i !  pX j=1 R(j)−1 OBS ⊗ a−⊤ j a−1 j  vOBS. We apply the mixed product property to simplify the first matrix multiplication t"
4102,unknown,"pX j=1 R(j) PRED,OBSR(j)−1 OBS ⊗ aja−1 j . Note that the double sum vanishes because a⊤ i a−⊤ j is equal to zero whenever i ̸= j and 1 otherwise. The conditional mean, in matrix form, is pX j=1 aja−1 j VOBSR(j)−1 OBS R(j) OBS,PRED. For the conditional variance, we have Var(vPRED|vOBS) = pX j=1 R(j) PRED ⊗ aja⊤ j −   pX j=1 R(j) PRED,OBS ⊗ aja⊤ j     pX j=1 R(j)−1 OBS ⊗ a−⊤ j a−1 j     pX"
4103,unknown,"j     pX j=1 R(j)−1 OBS ⊗ a−⊤ j a−1 j     pX j=1 R(j) OBS,PRED ⊗ aja⊤ j  . Using the same arguments as for the conditional mean, we can simplify to Var(vPRED|vOBS) = pX j=1 {R(j) PRED − R(j) PRED,OBSR(j)−1 OBS R(j) OBS,PRED} ⊗aja⊤ j . The LMC covariance structure is preserved under conditioning. 17 Computational Considerations for the LMC E Interweaving Components of the model are V, W, "
4104,unknown,"j=1, τp j=1. Model parameters are A, φp j=1, τp j=1 while V, W are equivalent representations of the latent LMC linked by the 1 to 1 transformation V = AW. In the example we present below, we put independent Gaussian priors on the elements of the coregionalization matrix A. We use the exponential correlation function ρ(φj|s − t|) = exp(−φj|s − t|) for the underlying process wj(·) and the range par"
4105,unknown,"assigned uniform priors on [3, 30]. Roughly speaking, this corresponds to a practical range at distances between 0.1 and 1 (points are observed in the unit square). Finally, we assign independent inverse Gamma priors with high variance to the parameters τ1, . . . , τp to obtain conjugate full conditional updates. The global MCMC sampling algorithm we employ is described below. It interweaves updat"
4106,unknown,"are conditioned on in turn. Algorithm 1 (Interweaving MCMC for A). help me 1. Update V, W conditional on A, φp j=1, τp j=1 such that the transition leaves the full conditional invariant: • Update V (with A, φp j=1, τp j=1 fixed) from full conditionals as detailed in Appendix F, • Update W conditional on V, A, φp j=1, τp j=1 using the deterministic relation W = A−1V. 2. Update A, W conditional on V"
4107,unknown,"j=1, τp j=1 such that the transition leaves the full conditional invariant: • Update A (with V, φp j=1, τp j=1 fixed) using a slice sampling step [Neal, 2003, Murray and Adams, 2010], • Update W conditional on V, A, φp j=1, τp j=1 using the deterministic relation W = A−1V. 3. Update A, V conditional on W, φp j=1, τp j=1 from its exact distribution: • Update A (with W, φp j=1, τp j=1 fixed) from it"
4108,unknown,"• Update V conditional on W, A, φp j=1, τp j=1 using the deterministic relation V = AW. 4. Update φp j=1 conditional on V, W, A, τp j=1 using a Metropolis-Hastings move centered at the current values. 5. Update τp j=1 conditional on V, W, A, φp j=1 from their conjugate inverse gamma distributions. In Algorithm 1, components V, W and A are sampled at multiple steps of the algorithm. This is referre"
4109,unknown,"marginalization in collapsed Gibbs samplers theory [Van Dyk and Park, 2008]. We could omit either of steps 2 or 3 and still have an MCMC algorithm that leaves the joint posterior invariant. If we remove step 2, A is only updated conditional on W and if we remove step 3, A is only updated conditional on V. Hence, the two situations correspond respectively to the whitened and centered parametrizatio"
4110,unknown,"respectively to the whitened and centered parametrizations. We are interested in comparing the interweaving approach with the more standard whitened ( W parametrization) and centered (V parametrization) updates in terms of autocorrelation in the posterior samples relating to the matrix A. As detailed in Section 2.2, the elements of A are prone to both label and sign switching. We instead look at"
4111,unknown,"identifiable quantities: the cross-covariances C12(0) = a11a21 + a12a22 and C12(0.1) = a11a21 exp(−0.1φ1) + a12a22 exp(−0.1φ2) at distances 0 and 0.1 respectively. The first is a function of A while the second also depends on range parameters φ1 and φ2. We chose a set of n = 200 locations randomly and uniformly on the unit square [0, 1]2. Importantly, this is done only once and before performing a"
4112,unknown,"StN ratio value, the following is repeated 100 times. We instantiate a p = 2 dimensional random field at said locations. We use A = [[−1.5, 1.0]⊤, [1.1, 2.0]⊤] and [φ1, φ2] = [5, 20]. For the case StN = 1, we add independent observational noise of variance τ1 = τ2 = 1 at every location; for StN = 0.1 and StN = 10 we multiply the noise variables ϵ(s) by√ 10 and √ 0.1 respectively. We run Algorithm "
4113,unknown,"for 2000 iterations, discarding the first 1000 as burn-in. Finally, we evaluate the auto-correlation in the Markov chain of both quantities of interest C12(0) and C12(0.1) using the effective sample size metric. This is computed using the CODA package in R [Plummer et al., 2006]. Results are presented in Figure 8. As expected, the whitened (resp. centered) approach performs poorly when the StN rat"
4114,unknown,"interweaving approach seems to always perform at least slightly better than the most suitable of the other two methods. In practice, we do not know in advance how process level and observational variances compare. Interweaving, as was the objective in its conceptualization [Yu and Meng, 2011], can benefit from the advantages of both worlds, eliminating the need to decide upon an appropriate parame"
4115,unknown,18 Computational Considerations for the LMC 0 100 200 300 400 C_12(0) (StN=0.1) 0 20 40 60 80 100 120 140 160 C_12(0.1) (StN=0.1) 0 50 100 150 200 250 C_12(0) (StN=1) 10 20 30 40 50 60 70 80 C_12(0.1) (StN=1) Centered Interweaved Whitened 0 20 40 60 80 100 120 140 C_12(0) (StN=10) Centered Interweaved Whitened 10 20 30 40 50 60 70 C_12(0.1) (StN=10) Effective Sample Size Figure 8: Results of the s
4116,unknown,"conducting inference on a new set of observations each time. The effective sample size is calculated for the two Markov chains of length 1000 corresponding to quantities of interest C12(0) and C12(0.1). The average computing time for the centered, interweaved and whitened algorithms were respectively 33.46, 33.60 and 31.77 seconds. It is important to emphasize that interweaving samples come at no "
4117,unknown,"computing time is identical across the three methods (about 30 seconds per chain of length 2000). This is because the computation is dominated by steps 1 and 4 of Algorithm 1 which require respectively O(p2n2) (see Appendix F) and O(pn3) floating point operations (flops) (inverse and determinant of the n × n matrices Rj, j= 1, . . . , p). In the vectorized interpretation of the LMC, updating the m"
4118,unknown,"the inverse and determinant of the np × np covariance matrix Pp j=1 Rj ⊗ aja⊤ j . A consequence of Proposition 1 is that computations on the coregionalization matrix A and the spatial correlation matrices R1, . . . ,Rp are disentangled when evaluating the likelihood. Updates to A are negligible when compared to updates of spatial ranges φ1, . . . , φp. Nevertheless, there are p2 coregionalization "
4119,unknown,"19 Computational Considerations for the LMC parameters, hence being able to use more sophisticated samplers on A (at no additional cost) improves the convergence rate of the MCMC algorithm. F Instantiating the Latent GP In Section 2.4, we described the common issue regarding computational scalability in latent Gaussian process models. We first do a high-level recap here. Suppose you have an additi"
4120,unknown,"is observed, v has a multivariate normal distribution accounting for dependence among observations (such as spatial correlation accounting for proximity) and ϵ represents observational noise that is independent across observations. We assumev to have then-dimensional normal distribution with mean 0 and covariance matrixΣ. In general, evaluating the density of v requires O(n3) flops to compute both"
4121,unknown,"an approximate model instead or impose some structure on Σ such that both Σ−1 and det(Σ) are easier to compute. The issue is that the marginal model for y has covariance matrix Σ + D where D is the diagonal covariance matrix associated with ϵ. Even if we employ a scalable model for Σ, there is no simple way to extend such a strategy for computing the inverse and determinant of the covariance matri"
4122,unknown,"between the easier computed Σ−1 and det(Σ) and the analogous for Σ + D because D is of full rank. Even though v cannot easily be marginalized, there is always the option to rely on data augmentation and instantiate this latent vector at every step of the Markov chain. In this case, we work with the complete data likelihood given by p(y, v) = exp(−1 2 [v⊤Σ−1v + (y − v)⊤D−1(y − v)]) (2π)n/2det(Σ)1/2"
4123,unknown,"p(y, v) = exp(−1 2 [v⊤Σ−1v + (y − v)⊤D−1(y − v)]) (2π)n/2det(Σ)1/2det(D)1/2 (16) which is more amenable to likelihood-based inference since it only involves the inverse and determinant of the well-behaved Σ matrix and the diagonal D. We are left with only the matter of instantiating v conditional on y. From standard manipulations of equation (16), which is proportional to the conditional, we concl"
4124,unknown,"and variance (Σ−1 + D−1)−1. We again deal with matrices of size n × n which we need to invert and, in this case, factorize to simulate this n-dimensional Gaussian. In terms of asymptotic complexity, it renders pointless our attempt to use a scalable model for v as we would still need to perform O(n3) computations. We have the option of updating v using some variant of Metropolis-Hastings. Acceptan"
4125,unknown,"using (16). However, v is unlikely to move a lot in high-dimensional settings unless careful attention is put into tailoring and tuning the proposals for the specific problem at hand. A more universal approach is to simulate the values of v one at a time from the full conditionals described by Var(vi|v−i, y) = 1 {Σ−1}ii + D−1 ii , (17) E[vi|v−i, y] = Var(vi|v−i, y)  D−1 ii yi − X j̸=i {Σ−1}ijvj "
4126,unknown,"{Σ−1}ii + D−1 ii , (17) E[vi|v−i, y] = Var(vi|v−i, y)  D−1 ii yi − X j̸=i {Σ−1}ijvj  . (18) Variance is directly obtained from Σ−1 and D−1 while the mean is obtained from a computation linear in n. Repeat that for every i = 1, . . . , nand v is updated in O(n2) flops. In the case of the LMC, we can explicitly compute the precision matrix Pp j=1 R−1 j ⊗ a−⊤ j a−1 j of the np-dimensional"
4127,unknown,"j=1 R−1 j ⊗ a−⊤ j a−1 j of the np-dimensional vector v = vec(V) and then perform the update from full conditionals in O(p2n2) complexity. However, we do not require computing the inverse of the np × np covariance matrix to evaluate the likelihood (equation (2)). Doing so entails O(p3n2) operations and therefore would dominate the spatial latent effect update. We instead update the latentp×n matrix"
4128,unknown,"full conditionals. From the joint likelihood of V and Y in equation (6), we can derive Var(v(si)|V−i) =   pX j=1  R−1 j  ii a−⊤ j a−1 j + D−1   −1 , (19) E[v(si)|V−i] = Var(v(si)|V−i)  D−1y(si) − X k̸=i pX j=1  R−1 j  ik a−⊤ j a−1 j v(sk)  , (20) 20 Computational Considerations for the LMC where D is the p × p diagonal matrix with elements τi, i= 1 , . . . , p. The p-dimensional full c"
4129,unknown,"described by equations (19) and (20) above is accomplished in O(p2n) operations provided we start by computing the inner products a−1 j v(sk) in the mean structure. Consequently, the global column-by-column update to the matrix V has O(p2n2) complexity. A common occurrence in multivariate spatial modeling is the presence of missing entries in the matrixY of observations. In this case, we can still"
4130,unknown,"adaptation of the procedure outlined above. From a numerical standpoint, it will be simpler to consider the matrix Y to be full of scalar values. We shall input arbitrary values in place of the missing entries. We employ a p × n binary masking matrix M = [m(s1), . . . ,m(sn)] to encode (with 0s) the position of missing values. In the complete likelihood of Y, V in equation (6), the influence of Y "
4131,unknown,"tr{(V − Y)⊤D−1(V − Y)} = nX i=1 (v(si) − y(si))⊤D−1(v(si) − y(si)). (21) To discount elements that would involve a valueyj(si) that is missing for some j in 1, . . . , p, we can simply replace the p×p matrix D−1 on the RHS of equation (21) by D−1 ◦diag{m(si)} where ◦ stands for the element-wise (Hadamard) product of two matrices. It means that we can simply replace D−1 by D−1 ◦ diag{m(si)} in equa"
4132,unknown,"and otherwise carry out the updates to each vector v(si) unchanged. G Algorithms In this section, we present the structure of the MCMC algorithms we use for both the regular LMC and our sparse version. Each algorithm can be decomposed in four steps corresponding to the sampling of the latent spatial matrix V, covariance parameters A, φp j=1 and the observational variances τp j=1. We first present "
4133,unknown,"sampling quantities of interest in a zero-mean standard LMC. Algorithm 2 (Standard MCMC for LMC distributed random fields). help me 1. Update V conditional on Y, A, φp j=1, τp j=1 from full conditionals as detailed in Appendix F. 2. Update A conditional on Y, V, φp j=1, τp j=1 using a slice sampling step [Neal, 2003, Murray and Adams, 2010]. 3. Update φp j=1 conditional on Y, V, A, τp j=1 using a "
4134,unknown,"j=1 using a Metropolis-Hastings move centered at the current values. 4. Update τp j=1 conditional on Y, V, A, φp j=1 from their conjugate inverse gamma distributions. For the sparse LMC, we need to be able to perform updates that explore admissible models. We consider two types of reversible jump proposals: one that removes a structural zero and one that inserts a structural zero. The latter move"
4135,unknown,"needs to be valid in the sense that it does not render the coregionalization matrix A singular with probability one (see Section 3). Algorithm 3 (MCMC for the sparse LMC model). help me 1. Update V conditional on Y, A, φp j=1, τp j=1 from full conditionals as detailed in Appendix F. 2. Update A conditional on Y, V, φp j=1, τp j=1: • Perform p updates to the structure of A (with V, φp j=1, τp j=1 f"
4136,unknown,"j=1 fixed) using reversible jump proposals, • Update the non-zero values of A (with V, φp j=1, τp j=1 fixed) using a slice sampling step [Neal, 2003, Murray and Adams, 2010]. 3. Update φp j=1 conditional on Y, V, A, τp j=1 using a Metropolis-Hastings move centered at the current values. 4. Update τp j=1 conditional on Y, V, A, φp j=1 from their conjugate inverse gamma distributions."
4137,unknown,"4. Update τp j=1 conditional on Y, V, A, φp j=1 from their conjugate inverse gamma distributions. Under the complete data likelihood of Y, V described by equation (6), both algorithms run at about the same speed. This is assuming the number n of observations is considerably larger than the number p of components. First, let us consider the form of the likelihood ratio upon an update to the range p"
4138,unknown,"j=1. The move proposes to replace 21 Computational Considerations for the LMC each n × n spatial correlation matrix Rj with a new R′ j for j = 1, . . . , p. The computational bottleneck of this update is the following likelihood ratio which is necessary for determining the accept rule: L(R1, . . . ,Rp; V) L(R′ 1, . . . ,R′ p; V) = exp  −1 2 pX j=1 a−1 j V(R−1 j − R′ j −1 )V⊤a−⊤ j   pY j=1 "" de"
4139,unknown," −1 2 pX j=1 a−1 j V(R−1 j − R′ j −1 )V⊤a−⊤ j   pY j=1 "" det(R′ j) det(Rj) #1 2 . (22) The update requires to invert and compute the determinant of the p n× n correlation matrices R′ j, j= 1, . . . , pfor a calculation that scales as O(pn3). On the other hand, an update to A requires the likelihood ratio described by L(A; V) L(A′; V) = etr  −1 2 pX j=1 (a−⊤ j a−1 j − a′ j −⊤a′ j −1)VR−1 j V⊤"
4140,unknown," −1 2 pX j=1 (a−⊤ j a−1 j − a′ j −⊤a′ j −1)VR−1 j V⊤   det(A′) det(A) n , (23) where etr(·) := exp(tr(·)). When we calculate the quantity above, the inverse matrices R−1 j are already computed and stored. Empirically, the fact that we perform multiple updates to the matrix A, both in the slice sampling step and the reversible jumps proposals, has little effect on the global computation. This i"
4141,unknown,"compute the likelihood ratio upon an update to A (equation (23)) is negligible when compared with the likelihood ratio in (22). If we were to consider the np-dimensional Gaussian distribution of the vector vec(V) as is typically the case in the multivariate spatial literature, then an update to A or the range parameters φp j=1 would entail a change in the np × np covariance matrix Pp j=1 Rj ⊗ aja⊤"
4142,unknown,"j and its inverse and determinant would need to be recomputed, which is very expensive. H MCMC Setup In this section, we describe in more detail the prior distributions employed in the analysis of Sections 3.1 and 3.2. We also provide the likelihood functions used and the target posterior distribution in each case. Section 3.1: Simulation Study on the Sparse LMC Prior Distributions • Elements of t"
4143,unknown,"equal to 1: p(A) ∝ Qp j=1 Qp i=1 exp(−a2 ij/2). • The range parameters φp j=1 are assigned independent uniform priors on the [3,30] interval: p(φp j=1) ∝Qp j=1 1[3,30](φj). • The observational variances τp j=1 are assigned independent inverse gamma prior distributions with shape and scale parameters equal to 1: p(τp j=1) ∝ Qp j=1 τ−2 j exp(−1/τj). • For the masking matrix M (sparse LMC), we employ"
4144,unknown,"Section 3: p(M) ∝ (1/p) Pmij (1 − 1/p)p2−Pmij . Likelihood We make use of the complete data likelihood consisting of the matrix Y of observations and the matrix V of latent spatial effects: p(Y, V|A, Rp j=1, D) = exp[−1 2 Pp j=1 a−1 j VR−1 j V⊤a−⊤ j − 1 2 tr{(V − Y)⊤D−1(V − Y)}] (2π)np|det(A)|n Qp j=1 det(Rj)1/2 Qp j=1 τn/2 j . The vectors a−⊤ j , j= 1 , . . . , pare the rows of the inverse matrix"
4145,unknown,"correlation matrices computed respectively from each range parameter φp j=1 using the exponential correlation function. Finally, the matrix D is p × p diagonal with each term corresponding to the observational variances τp j=1. In the case of the sparse LMC, we replace the coregionalization matrix A by the element-wise product A ◦ M. 22 Computational Considerations for the LMC Posterior Distributi"
4146,unknown,"22 Computational Considerations for the LMC Posterior Distribution The target posterior distribution is proportional to the product of priors with the complete data density: p(V, A, φp j=1, τp j=1|Y) ∝ p(Y, V|A, Rp j=1, D)p(A)p(φp j=1)p(τp j=1). For the sparse LMC, we also sample among admissible models described by the masking matrix M: p(V, A, M, φp j=1, τp j=1|Y) ∝ p(Y, V|A ◦ M, Rp j=1, D)p(A)p"
4147,unknown,"j=1)p(τp j=1)p(M). Section 3.2: Analysis of the California Air Data Prior Distributions • Elements of the coregionalization matrix A are assigned independent normal priors with mean 0 and variance equal to 1: p(A) ∝ Qp j=1 Qp i=1 exp(−a2 ij/2). • The range parameters φp j=1 are assigned independent uniform priors on the [3,30] interval: p(φp j=1) ∝Qp j=1 1[3,30](φj). • The observational variances "
4148,unknown,"j=1 are assigned independent inverse gamma prior distributions with shape and scale parameters equal to 1: p(τp j=1) ∝ Qp j=1 τ−2 j exp(−1/τj). • Values of component-specific mean vectorµ are each assigned independent normal priors with mean 0 and variance equal to 10: p(µ) = Qp j=1 exp(−µ2 j/20). • For the masking matrix M, we employ the prior distribution (we use π = 1 /p) described in Section 3"
4149,unknown,"p(M) ∝ (1/p) Pmij (1 − 1/p)p2−Pmij . Likelihood We make use of the complete data likelihood consisting of the matrix Y of observations and the matrix V of latent spatial effects: p(Y, V|µ, A, Rp j=1, D) = exp[−1 2 Pp j=1 a−1 j (V − µ1⊤ n )R−1 j (V − µ1⊤ n )⊤a−⊤ j − 1 2 tr{(V − Y)⊤D−1(V − Y)}] (2π)np|det(A)|n Qp j=1 det(Rj)1/2 Qp j=1 τn/2 j . The vectors a−⊤ j , j= 1 , . . . , pare the rows of the "
4150,unknown,"correlation matrices computed respectively from each range parameter φp j=1 using the exponential correlation function. Finally, the matrix D is p × p diagonal with each term corresponding to the observational variances τp j=1. We replace the coregionalization matrix A by the element-wise product A ◦ M. The jth row of p × n matrix µ1⊤ n is filled with the value µj, j= 1, . . . , p. Posterior Distr"
4151,unknown,"n is filled with the value µj, j= 1, . . . , p. Posterior Distribution The target posterior distribution is proportional to the product of priors with the complete data density: p(V, µ, A, M, φp j=1, τp j=1|Y) ∝ p(Y, V|A ◦ M, Rp j=1, D)p(A)p(φp j=1)p(τp j=1)p(µ)p(M). 23 Machine Learning and Neural Networks Dr. Hailiang Du 2023-06-01 2 Contents W elcome 7 1 Introduction 9 1.1 What is Machine Learni"
4152,unknown,1.1 What is Machine Learning? . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.3 What do we need? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.4 A Note on These Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.5 Key References . . . . . . . . . . . . . 
4153,unknown,2 Definitions and Basic Concepts 17 2.1 Types of data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.2 Supervised and Unsupervised Learning . . . . . . . . . . . . . . . . . . . . . 17 2.3 Classification and Regression . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.4 Parametric vs non-parametric models . . . . . . . . . . . . . . . . . . . . . . 19 2.5 Uncerta
4154,unknown,2.5 Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.6 Correlation and Causation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 3 Linear Regression 25 3.1 Simple Linear Regression: Introduction . . . . . . . . . . . . . . . . . . . . . 25 3.2 Simple Regression: Coeﬀicient of Determination . . . . . . . . . . . . . . . . 29 3.3 Simple Linear Regre
4155,unknown,3.3 Simple Linear Regression: Assumptions . . . . . . . . . . . . . . . . . . . . . 35 3.4 Simple Linear Regression: Inference . . . . . . . . . . . . . . . . . . . . . . . 46 3.5 Simple Linear Regression: Confidence and Prediction intervals . . . . . . . . 53 3.6 Multiple Linear Regression: Introduction . . . . . . . . . . . . . . . . . . . . 58 4 Model Assessment and Selection 69 4.1 In-sample v
4156,unknown,4.1 In-sample vs out-of-sample . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 4.2 Cross-V alidation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 4.3 Overfitting vs Underfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4.4 Bias V ariance T rade-off . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 4.5 Model selection for Linear regr
4157,unknown,4.5 Model selection for Linear regression . . . . . . . . . . . . . . . . . . . . . . 76 4.6 Model Selection Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 5 Model Search Methods 81 5.1 Best Subset Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 5.2 F orwards Stepwise Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 3 4 CONTENTS 5
4158,unknown,4 CONTENTS 5.3 Backwards Stepwise Selection . . . . . . . . . . . . . . . . . . . . . . . . . . 83 5.4 Practical Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 6 Shrinkage Methods 103 6.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 6.2 Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 6.3 
4159,unknown,6.3 Lasso Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 6.4 Choosing 𝜆 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 7 Principal Component Analysis 135 7.1 Reminder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 7.2 Linear approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 7
4160,unknown,7.3 Decomposing variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 7.4 Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 7.5 Data compression (Dimension reduction) . . . . . . . . . . . . . . . . . . . . 142 7.6 Principal Component Regression . . . . . . . . . . . . . . . . . . . . . . . . 147 8 Polynomial Regression 155 8.1 The Assumpt
4161,unknown,8.1 The Assumption of Linearity . . . . . . . . . . . . . . . . . . . . . . . . . . 155 8.2 Polynomial Regression Models . . . . . . . . . . . . . . . . . . . . . . . . . . 156 8.3 Practical Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159 9 Step F unctions 169 9.1 Step F unctions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169 9.2 Practical Demon
4162,unknown,10 Splines 175 10.1 Regression Splines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175 10.2 Practical Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180 10.3 Natural Splines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184 10.4 Practical Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 10.5 Smoothing Spl
4163,unknown,10.5 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188 10.6 Practical Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 11 Generalised Additive Models 195 11.1 Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195 11.2 GAMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
4164,unknown,11.3 Practical Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 12 Logistic Regression 201 12.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 12.2 Simple Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206 12.3 Logistic regression with several variables . . . . . . . . . . . . . . . . . . . . 209 12.4
4165,unknown,12.4 Interpreting coeﬀicients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 12.5 Significance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 13 T ree-based Models 211 13.1 Classification and Regression T rees . . . . . . . . . . . . . . . . . . . . . . . 212 CONTENTS 5 13.2 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
4166,unknown,13.2 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 13.3 Random F orests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226 13.4 Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 13.5 Practical Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 I Problem Sheets 237 Problem 
4167,unknown,I Problem Sheets 237 Problem Sheets 239 Problem Sheet 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239 Problem Sheet 1 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241 Problem Sheet 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244 Problem Sheet 2 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
4168,unknown,Problem Sheet 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251 Problem Sheet 3 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253 Problem Sheet 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258 Problem Sheet 4 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260 II Practical Classes 265 Practical
4169,unknown,II Practical Classes 265 Practical Class Sheets 1 267 The faithful data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 Fitting the simple linear regression model . . . . . . . . . . . . . . . . . . . . . . 268 Inference on the coeﬀicicents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272 Estimation and prediction . . . . . . . . . . . . . . . . . . . . . .
4170,unknown,Estimation and prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274 Residual Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 Practical Class Sheets 2 279 Body F at and Body Measurements Data . . . . . . . . . . . . . . . . . . . . . . . 279 Simple Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285 Multi
4171,unknown,Multiple Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288 V ariable Selection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293 Practical Class Sheets 3 299 Initial Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299 Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
4172,unknown,Lasso Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303 Principal Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307 Principal Component Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . 310 Predictive Performance of Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 311 Practical Class Sheets 4 315 Polynomi
4173,unknown,Polynomial and Step-wise F unction Regression . . . . . . . . . . . . . . . . . . . . 315 Splines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320 GAMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322 Boston Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324 Logistic Regression . . . .
4174,unknown,"6 CONTENTS References 331 W elcome W elcome to the material for the first half of the Machine Learning and Neural Networks module (MA TH3431) at Durham University . These pages consist of relevant lecture notes will be updated as the course progresses. I would recommend that you use the html version of these notes (they have been designed for use in this way), however, there is also a pdf version "
4175,unknown,"for use in this way), however, there is also a pdf version of these notes. In this first half of the module (Michaelmas T erm), we will be focusing on “Machine Learning” rather than “Neural Networks” . W e will i) go over the fundamental concepts of Machine (Statistical) Learning, ii) study the classic linear models, iii) explore models beyond Linearity and finally iv) look into the simpler yet po"
4176,unknown,"and finally iv) look into the simpler yet powerful tree-based models. If you would like to contact me regarding any of the material in this course, then my email address is hailiang.du@durham.ac.uk Credits: Thanks to Dr T ahani Coolen-Maturi, Dr Samuel Jackson and Dr Emmanuel Ogundimu for their kindly providing the materials for this module contents. 7 8 CONTENTS Chapter 1 Introduction 1.1 What is"
4177,unknown,"7 8 CONTENTS Chapter 1 Introduction 1.1 What is Machine Learning? Is ML the same as Statistics? Is it Data Science? Is it its own thing? Is it just a buzzword which is good for employment? Just what is Machine Learning? W ell, I don’t think there’s a single concrete definition for Machine Learning (ML), but let’s have a look at several different ones…. A popular definition: “A computer program is "
4178,unknown,"A popular definition: “A computer program is said to learn from experience E with respect to some class of tasks T, and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. ”. Mitchell [1997] This means there are many different kinds of machine learning, depending on the nature of the task T we wish the system to learn, the nature of the performanc"
4179,unknown,"the task T we wish the system to learn, the nature of the performance measure P we use to evaluate the system, and the nature of the training signal or experience E we give it. Murphy [2022] Another definition: “Machine learning is a branch of artificial intel ligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradual ly improvin"
4180,unknown,"gradual ly improving its accuracy”. Hurwitz and Kirsch [2018] And one more definition: “A set of methods that can automatical ly detect patterns in data, and then use the uncovered patterns to predict future data, or to perform other kinds of decision making under uncertainty (such as planning how to col lect more data!). ” . Murphy [2012] W ell, it seems that the general consensus is that ML is a"
4181,unknown,"W ell, it seems that the general consensus is that ML is about getting the machine (most likely a computer in some sense) to learn as much as possible, with the least human interaction possible. This may be an admirable aim, however, I would say that there are elements of human interaction and interpretation of results that machines can’t do. Machines and humans (bringing an analytical mind as wel"
4182,unknown,"humans (bringing an analytical mind as well as the know-how to use the machines) must together perform Statistics, Data Science and Machine Learning in my opinion. With regards to this course specifically , my half of the course could be seen as Statistical Learning, we view machine learning from a probabilistic perspective and apply statistical tools to machine learning. 9 10 CHAPTER 1. INTRODUCT"
4183,unknown,"I will leave you to decide for yourself about these various names for our science as the course progresses! What I do know is that we are going to cover some important statistical tools if you want to get into “Machine Learning”! 1.2 Motivation W e are currently living in a data-driven world, where the amount of digitally-collected data grows exponentially . Being a powerful tool, machine learning"
4184,unknown,"tremendous amount of data and helps solve many practical problems. People who are exploring machine learning as a field to shift their careers or people who are just curious about why there is such a buzz of machine learning all over places often have one burning question in their mind – what all possible things can one achieve with machine learning? W ell, the short answer is – that the possibili"
4185,unknown,"only limit. Here are a few machine learning examples that are in popular use in today’s world. Machine Learning is an integral part of our daily life Google search engine, now processes over 40,000 searches per second on average and billions of searches daily and yet throws relevant search results within seconds. Did you ever freak out at how Google gives such accurate autocomplete prompts in the "
4186,unknown,"reading our minds about search context? Google maps predict which route will be faster and shows such an accurate ET A of reaching the destination. Ever noticed that spam/junk folder in your university mailbox or many other email providers, for example Gmail, and did you ever open it to see so many spam mails indeed. V arious language translators and automatic subtitles are becoming better and bet"
4187,unknown,"V arious language translators and automatic subtitles are becoming better and better with time. Y outube and Tiktok suggestions, which keep people hooked for endless hours. Amazon and Netflix are so good at recommending what you may like. Y es, I think you are getting my point now, it is machine learning that is behind all the magic. Machine Learning in industrial uses Industries are adopting mach"
4188,unknown,"Machine Learning in industrial uses Industries are adopting machine learning across the world. Banks are using machine learning to detect frauds, pass loans, and maintain investment portfolios. Hospitals are using machine learning to detect and diagnose diseases with more accuracy than actual doctors. Companies are running ad campaigns targeted to specific customer segments and managing their supp"
4189,unknown,"their supply chain and inventory control using machine learning. 1.2. MOTIV A TION 11 Enterprise chatbots are now the next big thing, with companies adopting them to interface with customers. Online retailers are building their recommendation systems with more accuracy than before using the latest advancements in machine learning. Self Driving Cars are the latest fascination that has caught the im"
4190,unknown,"Self Driving Cars are the latest fascination that has caught the imaginations of companies like Google, Uber, and T esla who are investing heavily into this future vision using next- generation machine learning technology . These are just a few of the popular use cases and are just the tip of the iceberg. Machine learning can be used in the most creative ways to solve social, economical, and busin"
4191,unknown,"problems. The possibility is just endless. Machine Learning – The Artist This is the coolest part- with so many advancements in Deep Learning a subset field with machine learning things are getting to the next level of reality . Now we are able to produce artwork using machine learning. These are some of the astonishing artworks. Deepfakes Jason Allen, a video game designer in Pueblo, Colorado, sp"
4192,unknown,"Deepfakes Jason Allen, a video game designer in Pueblo, Colorado, spent roughly 80 hours working on his entry to the Colorado State F air’s digital arts competition. Judges awarded his A.I.- generated work (“Théâtre D’opéra Spatial”) first place, which came with a $300 prize. The beautiful model-looking faces below do not exist and have been generated by machine learning’s imagination. 12 CHAPTER "
4193,unknown,"learning’s imagination. 12 CHAPTER 1. INTRODUCTION Neural Style T ransfer Neural style transfer is a cool technique in which a style of an image or artwork is transferred to another image. Music Composer Nowadays, Machine Learning based apps (for example https://www.aiva.ai/) can assist you to create various styles of AI-generated music. Check out this soundtrack music ( https://www.youtube.com/wa"
4194,unknown,"Check out this soundtrack music ( https://www.youtube.com/watch?v=Emidxpkyk6o&a b_channel=Aiva), there is no reason to believe that it was not generated by humans!! W riter This machine learning read a lot of literature from Shakespeare and then produced a piece of text that looked not only meaningful but also Shakespearean. 1.3. WHA T DO WE NEED? 13 Machine Learning – The Gamer In 2011, IBM’s mac"
4195,unknown,"In 2011, IBM’s machine learning system W atson participated in a popular TV game show Jeopardy – which competed and defeated the world’s best Jeopardy human champions. In 2016, Deepmind’s machine learning system AlphaGo played Go with Lee Sudol (the best player and champion of Go) and defeated him. This feat is considered to be one of the greatest achievements in the field of machine learning. Mac"
4196,unknown,"Machine learning-based AI-player has been outcompeting top human players in various real- time strategy games, such as StarCraft and League of Legends. In This Course… In the first term (Michaelmas term) of this course, we will cover the following topics along with some practical exercises. • F undamental concepts of Statistical (Machine) Learning • Linear models (Logistic Regression, Linear Regre"
4197,unknown,"• Beyond Linearity (Polynomial Regression, Step F unctions, Splines) • T ree-based methods (Classification and regression trees, Random forests, Boosting) Y ou may find these topics seem to be nowhere near those fascinating application examples listed above. W ell, they are in fact the cornerstone of the more advanced machine learning algorithms including neural networks. 1.3 What do we need? What"
4198,unknown,"What do we need to achieve “successful” Machine (Statistical) Learning? In my opinion, it requires the following four pillars. 14 CHAPTER 1. INTRODUCTION 1. Suﬀicient Data If you ask any data scientist how much data is needed for machine learning, you’ll most probably get either “It depends” or “The more, the better. ” And the thing is, both answers are correct. It really depends on the type of pr"
4199,unknown,"It really depends on the type of project you’re working on, and it’s always a great idea to have as many relevant and reliable examples in the datasets as you can get to receive accurate results. But the question remains: how much is enough? General speaking, more complex algorithms always require a larger amount of data. The more features (the number of input parameters), model parameters, and va"
4200,unknown,"more features (the number of input parameters), model parameters, and variability of the expected output it should take into account, the more data you need. F or example, you want to train the model to predict housing prices. Y ou are given a table where each row is a house, and the columns are the location, the neighborhood, the number of bedrooms, floors, bathrooms, etc., and the price. In this"
4201,unknown,"bathrooms, etc., and the price. In this case, you train the model to predict prices based on the change of variables in the columns. And to learn how each additional input feature influences the input, you’ll need more data examples. 2. Suﬀicient Computational Resources There are four steps for conducting machine learning, all require suﬀicient computational resources: • Preprocessing input data •"
4202,unknown,"• T raining the machine learning model • Storing the trained machine learning model • Deployment of the model Among all these, training the machine learning model is the most computationally intensive task. F or example, training a neural network often requires intensive computational resources to conduct an enormous amount of matrix multiplications. 3. Performance metrics Performance metrics are "
4203,unknown,"Performance metrics are a part of every machine learning pipeline. They tell you if you’re making progress, and put a number on it. All machine learning models, whether it’s linear regression or neural networks, need a metric to judge performance. Many of the machine learning tasks can be broken down into either Regression or Classifica- tion. There are dozens of metrics for both problems, for exa"
4204,unknown,"tion. There are dozens of metrics for both problems, for example, Root Mean Squared Error for Regression problem and Accuracy for Classification problem. Choosing a “proper” per- formance could be crucial to the whole machine learning process. W e must carefully choose the metrics for evaluating machine learning performance because • How the performance of ML algorithms is measured and compared wi"
4205,unknown,entirely on the metric you choose. • How you weigh the importance of various characteristics in the result will be influenced completely by the metric you choose. 1.4. A NOTE ON THESE NOTES 15 W e will explore some of the popular performance metrics for machine learning in the coming lectures. Note most of the performance metrics (especially in supervised learning) require verifica- tion/labelled 
4206,unknown,"tion/labelled data. F or regression problems, we usually require an observed response variable (verification) in order to calculate performance metrics. And for the classification problems, we usually require labelled data. In machine learning, data labelling is the process of iden- tifying raw data (images, text files, videos, etc.) and adding one or more meaningful and informative labels to prov"
4207,unknown,"informative labels to provide context so that a machine learning model can learn from it. F or example, labels might indicate whether a photo contains a bird or car, which words were uttered in an audio recording, or if an x-ray contains a tumor. 4. Proper Machine Learning Algorithm There are so many algorithms that it can feel overwhelming when algorithm names are thrown around. In this course, w"
4208,unknown,"thrown around. In this course, we will look into some classic machine learning algorithms including for example linear models, tree-based models and neural networks. Knowing where the algorithms fit is much more important than knowing how to apply them. F or example, applying a linear model to the data generated by a nonlinear underlying system is obviously not a good idea. 1.4 A Note on These Not"
4209,unknown,"Since this is a third-year course, you are encouraged to read about it widely , expanding your knowledge beyond the notes contained here. That’s because that is precisely what these are - Notes! They are written to complement the lectures and give you a flavour of the topics covered in this course. They are not meant to be a comprehensive textbook, covering every detail that will be necessary to b"
4210,unknown,"detail that will be necessary to become a well-trained Statistician, Data Scientist or Machine Learner - that requires reading around! …and practice! 1.5 Key References The key references are the following (All of the recommended books are freely and legally available online): • James et al. [2013] - An Introduction to Statistical Learning, by Gareth James, Daniela Witten, T revor Hastie and Rober"
4211,unknown,"Witten, T revor Hastie and Robert Tibshiriani. This book is relatively easy to follow and doesn’t require strong maths background. There are quite a few online resources about this book available here. • F araway[2009] - Linear Models with R, by Julian F araway . This book gives a thorough introduction to linear models and their applications to make predictions and explaining the relationship betw"
4212,unknown,"the relationship between the response and the predictors. Understanding linear models are crucial to a broader competence in the practice of machine learning. https://search- ebscohost-com.ezphost.dur.ac.uk/login.aspx?direct=true&AuthType=cookie,ip,athe ns&db=nlebk&AN=1910500&site=ehost-live 16 CHAPTER 1. INTRODUCTION • Murphy [2012] - Machine Learning : A Probabilistic Perspective, by Kevin P . M"
4213,unknown,"This book gives a comprehensive introduction to machine learning and offers a com- prehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. https://ebookcentral .proquest.com/lib/durham/detail.action?docID=3339490 • Murph"
4214,unknown,"• Murphy [2022] - Probabilistic Machine Learning : An Introduction, by Kevin P . Murphy . This book is similar to Murphy [2012] which covers the most common types of machine learning but from a probabilistic perspective. This book offers a more detailed and up- to-date introduction to machine learning including relevant mathematical background, basic supervised learning as well as more advanced to"
4215,unknown,"est.com/lib/durham/detail.action?docID=6884472 Chapter 2 Definitions and Basic Concepts 2.1 Types of data Data sets can consist of two types of data: • Qualitative (categorical) data consist of attributes, labels, or nonnumerical entries. e.g. name of cities, gender etc. • Quantitative data consist of numerical measurements or counts. e.g. heights, age, tem- perature. Quantitative data can be dist"
4216,unknown,"– Discrete data result when the number of possible values is either a finite number or a “countable” number. e.g. the number of phone calls you received on any given day . – Continuous data result from infinitely many possible values that correspond to some continuous scale that covers a range of values without gaps, interruptions, or jumps. e.g. height, weight, sales and market shares. 2.2 Superv"
4217,unknown,"or jumps. e.g. height, weight, sales and market shares. 2.2 Supervised and Unsupervised Learning Every machine learning task can be broken down into either supervised learning or unsuper- vised learning. Supervised learning involves building a statistical model for predicting or estimating an output based on one or more inputs. Supervised learning aims to learn a mapping 𝑓 from inputs x ∈ 𝒳 to out"
4218,unknown,"Supervised learning aims to learn a mapping 𝑓 from inputs x ∈ 𝒳 to outputs y ∈ 𝒴 . The inputs x are also called the _feature_s, _covariate_s, or _predictor_s ; this is often a fixed- dimensional vector of numbers, such as the height and weight of a person, or the pixels in an image. In this case, 𝒳 = ℝ𝐷 , where 𝐷 is the dimensionality of the vector (i.e., the number of input features). The output "
4219,unknown,"of input features). The output y is also known as the label , target, or response. In supervised learning, we assume that each input example x in the training archive has an associated set of output targets y, and our goal is to learn the input-output mapping 𝑓 . 17 18 CHAPTER 2. DEFINITIONS AND BASIC CONCEPTS The task of Unsupervised learning is to try to “make sense of” data, as opposed to just "
4220,unknown,"learning a mapping. That is, we just get observed “inputs” x ∈ 𝒳 without any corresponding “outputs” y ∈ 𝒴. F rom a probabilistic perspective, we can view the task of unsupervised learning as fitting an unconditional model of the form 𝑝(x), which can generate new data x, whereas supervised learning involves fitting a conditional model, 𝑝(y|x), which specifies (a distribution over) outputs given in"
4221,unknown,"Unsupervised learning avoids the need to collect large labelled datasets for training, which can often be time-consuming and expensive (think of asking doctors to label medical images). Unsupervised learning also avoids the need to learn how to partition the world into often arbitrary categories. Finally , unsupervised learning forces the model to “explain” the high- dimensional inputs, rather tha"
4222,unknown,"dimensional inputs, rather than just the low-dimensional outputs. This allows us to learn richer models of “how the world works” and discover “interesting structure” in the data. Here is a simple example that shows the difference between supervised learning and unsuper- vised learning. Many classical machine (statistical) learning methods such as linear regression and logistic regression as well a"
4223,unknown,"regression as well as more advanced approaches such as random forests and boosting operate in the supervised learning domain. Most part of this course will be devoted to this setting. F or unsupervised learning, we will cover one of the classic linear approaches, principal component analysis, in the first term of the course. 2.3 Classification and Regression Supervised learning can be further divi"
4224,unknown,Regression. • Classification Classification algorithms are used when the output variable is categorical The task of the classification algorithm is to find the mapping function to map the input(x) to the discrete output( y). Example: The best example to understand the Classification problem is Email Spam Detection. The model is trained on the basis of millions of emails on different param- 2.4. P 
4225,unknown,"2.4. P ARAMETRIC VS NON-P ARAMETRIC MODELS 19 eters, and whenever it receives a new email, it identifies whether the email is spam or not. If the email is spam, then it is moved to the Spam folder. • Regression Regression algorithms are used if there is a relationship between the input variable and the output variable. It is used for the prediction of continuous variables, such as W eather forecas"
4226,unknown,"W eather forecasting, Market T rends, etc. Regression is a process of finding the correlations between dependent and independent variables. It helps in predicting the continuous variables such as prediction of Market T rends, prediction of House prices, etc. The task of the Regression algorithm is to find the mapping function to map the input variable(x) to the continuous output variable(y). Examp"
4227,unknown,"Example: Suppose we want to do weather forecasting, so for this, we will use the Regression algorithm. In weather prediction, the model is trained on the past data, and once the training is completed, it can easily predict the weather for future days. 2.4 Parametric vs non-parametric models Machine learning models can be parametric or non-parametric. Parametric models are those that require the sp"
4228,unknown,"that require the specification of some parameters, while non-parametric models do not rely on any specific parameter settings. • Parametric models Assumptions about the form of a function can ease the process of machine learning. Parametric models are characterized by the simplification of the function to a known form. A parametric model is a learner that summarizes data through a collection of pa"
4229,unknown,"parameters, ￿. These parameters are of a fixed size. This means that the model already knows the number of parameters it requires, regardless of its data. As an example, let’s look at a simple linear regression model (we will investigate this model thoroughly in the following chapter) in the functional form: 𝑦 = 𝑏0 + 𝑏1𝑥 + 𝜖 where 𝑏0 and 𝑏1 are model parameters and 𝜖 reflects the model error. F or"
4230,unknown,"where 𝑏0 and 𝑏1 are model parameters and 𝜖 reflects the model error. F or such a model, feeding in more data will impact the value of the parameters in the equation above. It will not increase the complexity of the model. And note this model form assumes a linear relationship between the input 𝑥 and response 𝑦. • Non-parametric models Non-parametric models do not make particular assumptions about "
4231,unknown,"Non-parametric models do not make particular assumptions about the kind of mapping function and do not have a specific form of the mapping function. Therefore they have the freedom to choose any functional form from the training data. One might think that non-parametric means that there are no parameters. However, this is NOT true. Rather, it simply means that the parameters are (not only) adjusta"
4232,unknown,"20 CHAPTER 2. DEFINITIONS AND BASIC CONCEPTS but can also change. This leads to a key distinction between parametric and non- parametric algorithms. W e mentioned that parametric algorithms have a fixed number of parameters, regardless of the amount of training data. However, in the case of non- parametric ones, the number of parameters is dependent on the amount of training data. Usually , the mo"
4233,unknown,"Usually , the more training data, the greater the number of parameters. A consequence of this is that non-parametric models may take much longer to train. The classification and regression tree (which will be introduced later in the course) is an example of a non-parametric model. It makes no distributional assumptions on the data and increasing the amount of training data is very likely to increa"
4234,unknown,"of the tree. 2.5 Uncertainty Arguably , we are living in a deterministic universe without considering quantum theory , which means the underlying system that generates the data is deterministic. One might think that the outcome of rolling dice is purely random. Actually , this is NOT true. If one can observe every related detail including for example the smoothness of the landing surface, the air "
4235,unknown,"resistance and the exact direction and velocity of the dice once it left the hand, one shall be able to predict the outcome precisely . If the underlying system is deterministic, why are we studying machine learning from a probabilistic perspective, for example, we use the form 𝑝(y|x) in supervised learning. The answer is uncertainty. There are two major sources of uncertainty . • Data uncertainty"
4236,unknown,"If there are data involved, there is uncertainty induced by measurement error. Y ou might think that well-made rulers, clocks and thermometers should be trustworthy , and give the right answers. But for every measurement - even the most careful - there is always a margin of “error” . T o account for measurement error, we often use a noise model. F or example 𝑥 = ̃ 𝑥 + 𝜂 2.6. CORRELA TION AND CAUSA"
4237,unknown,"where 𝑥 is the observed value, ̃ 𝑥reflects the “true” value and 𝜂 is the noise model, for example assuming 𝜂 ∼ 𝑁 (0, 1). • Model discrepancy Another major source of uncertainty in modelling is due to model discrepancy . Recall the famous quote from George Box, “All models are wrong but some models are useful” . There is always a difference between the (imperfect) model used to approximate reality "
4238,unknown,"and reality itself; this difference is termedmodel discrepancy. Therefore whatever model we construct, we usually include an error term to account for uncertainty due to model discrepancy . F or example the𝜖 in the linear regression model we saw before. And we often assume 𝜖 is a random draw from some distribution. 𝑦 = 𝑏0 + 𝑏1𝑥 + 𝜖 2.6 Correlation and Causation Machine learning is not only widely "
4239,unknown,"Machine learning is not only widely used for prediction but also widely used for inference. One of the main tasks of inference is to investigate, evaluate and understand the relationship between (for example the predictor and response) variables. Let’s look at the two of the most common concepts, correlation and causation, closely related to this task. While causation and correlation can exist sim"
4240,unknown,"While causation and correlation can exist simultaneously , correlation does not imply causa- tion. Causation means one thing causes another—in other words, action A causes outcome B. On the other hand, correlation is simply a relationship where action A relates to action B—but one event doesn’t necessarily cause the other event to happen. Figure 2.1: https://xkcd.com/925/"
4241,unknown,"22 CHAPTER 2. DEFINITIONS AND BASIC CONCEPTS Pearson correlation coeﬀicient Pearson correlation coeﬀicient(𝑟), often called sample correlation coeﬀicient, is a measure of the strength and the direction of a linear relationship between two variables in the sample, 𝑟 = ∑(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦) √∑(𝑥𝑖 − ̄ 𝑥)2 ∑(𝑦𝑖 − ̄ 𝑦)2 where 𝑟 always lies between -1 and 1. V alues of 𝑟 near -1 or 1 indicate a strong l"
4242,unknown,"where 𝑟 always lies between -1 and 1. V alues of 𝑟 near -1 or 1 indicate a strong linear rela- tionship between the variables whereas values of 𝑟 near 0 indicate a weak linear relationship between variables. If 𝑟 is zero the variables are linearly uncorrelated, that is there is no linear relationship between the two variables."
4243,unknown,2.6. CORRELA TION AND CAUSA TION 23
4244,unknown,24 CHAPTER 2. DEFINITIONS AND BASIC CONCEPTS Chapter 3 Linear Regression 3.1 Simple Linear Regression: Introduction Motivation: Predicting the Price of a Used Car The table below displays data on Age (in years) and Price (in hundreds of dollars) for a sample of cars of a particular make and model. W eiss[2012] Price (𝑦) Age ( 𝑥) 85 5 103 4 70 6 82 5 89 5 98 5 66 6 25 26 CHAPTER 3. LINEAR REGRESSIO
4245,unknown,"25 26 CHAPTER 3. LINEAR REGRESSION Price (𝑦) Age ( 𝑥) 95 6 169 2 70 7 48 7 Enter the data in R. Price<-c(85, 103, 70, 82, 89, 98, 66, 95, 169, 70, 48) Age<- c(5, 4, 6, 5, 5, 5, 6, 6, 2, 7, 7) carSales<-data.frame(Price,Age) str(carSales) ## 'data.frame': 11 obs. of 2 variables: ## $ Price: num 85 103 70 82 89 98 66 95 169 70 ... ## $ Age : num 5 4 6 5 5 5 6 6 2 7 ... cor(Age, Price, method = ""pear"
4246,unknown,"cor(Age, Price, method = ""pearson"") ## [1] -0.9237821 Scatterplot: Age vs. Price library(ggplot2) ggplot(carSales, aes(x=Age, y=Price)) + geom_point() 50 75 100 125 150 175 2 3 4 5 6 7 Age Price ggplot(carSales, aes(x=Age, y=Price)) + geom_point()+ geom_smooth(method=lm, formula= y~x, se=FALSE) 3.1. SIMPLE LINEAR REGRESSION: INTRODUCTION 27 50 75 100 125 150 175 2 3 4 5 6 7 Age Price Obviously , w"
4247,unknown,"Obviously , we see a linear relationship between Age and Price. Simple linear regression Model Simple linear regression (population) 𝑌 = 𝛽 0 + 𝛽1𝑥 + 𝜖 In our example: 𝑃 𝑟𝑖𝑐𝑒 = 𝛽0 + 𝛽1𝐴𝑔𝑒 + 𝜖 Simple linear regression (sample) ̂ 𝑦 = 𝑏0 + 𝑏1𝑥 where the coeﬀicient 𝛽0 (and its estimate 𝑏0 or ̂𝛽0 ) refers to the 𝑦-intercept or simply the intercept or the constant of the regression line, and the coeﬀicie"
4248,unknown,"its estimate 𝑏1 or ̂𝛽1 ) refers to the slope of the regression line. The Least Squares criterion Recall that one of the four pillars we need for successful machine learning is performance metric. In order to estimate the simple linear regression model parameters (here coeﬀicients 𝛽0 and 𝛽1), we need to find the right performance metric. Here comes the least squares. • The least squares criterion i"
4249,unknown,• The least squares criterion is that the line that best fits a set of data points is the one having the smallest possible sum of squared errors. The ‘errors’ are the vertical distances of the data points to the line. • The regression line is the line that fits a set of data points according to the least squares criterion. • The regression equation is the equation of the regression line. • The reg
4250,unknown,"• The regression equation for a set of 𝑛 data points is ̂ 𝑦 = 𝑏0 + 𝑏1 𝑥, where 𝑏1 = 𝑆𝑥𝑦 𝑆𝑥𝑥 = ∑(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦) ∑(𝑥𝑖 − ̄ 𝑥)2 and 𝑏0 = ̄ 𝑦 − 𝑏1 ̄ 𝑥 28 CHAPTER 3. LINEAR REGRESSION • 𝑦 is the dependent variable (or response variable) and 𝑥 is the independent variable (predictor variable or explanatory variable). • 𝑏0 is called the y-intercept and 𝑏1 is called the slope. SSE and the standard erro"
4251,unknown,"This least square regression line minimizes the error sum of squares 𝑆𝑆𝐸 = ∑ 𝑒2 𝑖 = ∑(𝑦𝑖 − ̂ 𝑦𝑖)2 The standard error of the estimate, 𝑠𝑒 = √𝑆𝑆𝐸/(𝑛 − 2), indicates how much, on average, the observed values of the response variable differ from the predicted values of the response variable. Prediction # simple linear regression reg<-lm(Price~Age) print(reg) ## ## Call: 3.2. SIMPLE REGRESSION: COEFFIC"
4252,unknown,## lm(formula = Price ~ Age) ## ## Coefficients: ## (Intercept) Age ## 195.47 -20.26 T o predict the price of a 4-year-old car (𝑥 = 4): ̂ 𝑦 = 195.47 − 20.26(4) = 114.43 Note the prediction ̂ 𝑦has not accounted for any uncertainty yet. 3.2 Simple Regression: Coeﬀicient of Determination Used cars example The table below displays data on Age (in years) and Price (in hundreds of dollars) for a sample 
4253,unknown,"Price (𝑦) Age ( 𝑥) 85 5 103 4 70 6 82 5 89 5 98 5 66 6 95 6 169 2 70 7 48 7 • F or our example,age is the predictor variable and price is the response variable. 30 CHAPTER 3. LINEAR REGRESSION • The regression equation is ̂ 𝑦 = 195.47 − 20.26 𝑥, where the slope 𝑏1 = −20.26 and the intercept 𝑏0 = 195.47 • Prediction: for 𝑥 = 4, that is we would like to predict the price of a 4-year-old car, ̂ 𝑦 = 1"
4254,unknown,"̂ 𝑦 = 195.47 − 20.26(4) = 114.43 or $11443 Extrapolation • Within the range of the observed values of the predictor variable, we can reasonably use the regression equation to make predictions for the response variable. • However, to do so outside the range, which is called Extrapolation, may not be reason- able because the linear relationship between the predictor and response variables may not ho"
4255,unknown,"• T o predict the price of an 11-year old car, ̂ 𝑦 = 195.47 − 20.26(11) = −27.39or $ 2739, this result is unrealistic as no one is going to pay us $2739 to take away their 11-year old car."
4256,unknown,"3.2. SIMPLE REGRESSION: COEFFICIENT OF DETERMINA TION 31 Outliers and influential observations • An outlier is an observation that lies outside the overall pattern of the data. In the context of regression, an outlier is a data point that lies far from the regression line, relative to the other data points. • An influential observation is a data point whose removal causes the regression equation ("
4257,unknown,"(and line) to change considerably . • F rom the scatterplot, it seems that the data point (2,169) might be an influential observation. Removing that data point and recalculating the regression equation yields ̂ 𝑦 = 160.33 − 14.24 𝑥. Coeﬀicient of determination"
4258,unknown,"32 CHAPTER 3. LINEAR REGRESSION • The total variation in the observed values of the response variable, 𝑆𝑆𝑇 = ∑(𝑦 𝑖 − ̄ 𝑦)2, 3.2. SIMPLE REGRESSION: COEFFICIENT OF DETERMINA TION 33 can be partitioned into two components: – The variation in the observed values of the response variable explained by the regression: 𝑆𝑆𝑅 = ∑( ̂ 𝑦𝑖 − ̄ 𝑦)2 – The variation in the observed values of the response variable "
4259,unknown,"regression: 𝑆𝑆𝐸 = ∑(𝑦 𝑖 − ̂ 𝑦𝑖)2 • The coeﬀicient of determination, 𝑅2 (or 𝑅-square), is the proportion of the variation in the observed values of the response variable explained by the regression, which is given by 𝑅2 = 𝑆𝑆𝑅 𝑆𝑆𝑇 = 𝑆𝑆𝑇 − 𝑆𝑆𝐸 𝑆𝑆𝑇 = 1 − 𝑆𝑆𝐸 𝑆𝑆𝑇 where 𝑆𝑆𝑇 = 𝑆𝑆𝑅 + 𝑆𝑆𝐸 . 𝑅2 is a descriptive measure of the utility of the regression equation for making prediction. • The coeﬀicient of dete"
4260,unknown,"• The coeﬀicient of determination 𝑅2 always lies between 0 and 1. A value of 𝑅2 near 0 suggests that the regression equation is not very useful for making predictions, whereas a value of 𝑅2 near 1 suggests that the regression equation is quite “useful” for making predictions. • F or a simple linear regression (one independent variable) ONL Y, 𝑅2 is the square of Pearson correlation coeﬀicient, 𝑟. "
4261,unknown,"• Adjusted 𝑅2 is a modification of𝑅2 which takes into account the number of independent variables, say 𝑘. In a simple linear regression 𝑘 = 1. Adjusted- 𝑅2 increases only when a significant related independent variable is added to the model. Adjusted- 𝑅2 has a crucial role in the process of model building. Adjusted- 𝑅2 is given by Adjusted-𝑅2 = 1 − (1 − 𝑅2) 𝑛 − 1 𝑛 − 𝑘 − 1 Notation used in regress"
4262,unknown,"Quantity Defining formula Computing formula 𝑆𝑥𝑥 ∑(𝑥𝑖 − ̄ 𝑥)2 ∑ 𝑥2 𝑖 − 𝑛 ̄ 𝑥2 𝑆𝑥𝑦 ∑(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦) ∑ 𝑥 𝑖𝑦𝑖 − 𝑛 ̄ 𝑥 ̄ 𝑦 𝑆𝑦𝑦 ∑(𝑦𝑖 − ̄ 𝑦)2 ∑ 𝑦2 𝑖 − 𝑛 ̄ 𝑦2 where ̄ 𝑥 =∑ 𝑥𝑖 𝑛 and ̄ 𝑦 =∑ 𝑦𝑖 𝑛 . And, 𝑆𝑆𝑇 = 𝑆 𝑦𝑦 , 𝑆𝑆𝑅 = 𝑆2 𝑥𝑦 𝑆𝑥𝑥 , 𝑆𝑆𝐸 = 𝑆 𝑦𝑦 − 𝑆2 𝑥𝑦 𝑆𝑥𝑥 and 𝑆𝑆𝑇 = 𝑆𝑆𝑅 + 𝑆𝑆𝐸 . 34 CHAPTER 3. LINEAR REGRESSION Proof 𝑅2 is the square of Pearson correlation coeﬀicient Suppose that we have 𝑛 observations (𝑥"
4263,unknown,"𝑌𝑖 = 𝛽0 + 𝛽1𝑥𝑖 + 𝜖𝑖 where 𝑖 = 1, ..., 𝑛. Let us denote ̂ 𝑦𝑖 = ̂𝛽0 + ̂𝛽1𝑥𝑖 for 𝑖 = 1, ..., 𝑛, where ̂𝛽0 and ̂𝛽1 are the least squares estimators of the parameters 𝛽0 and 𝛽1. The coeﬀicient of the determination 𝑅2 is defined by 𝑅2 = ∑ 𝑛 𝑖=1( ̂ 𝑦𝑖 − ̄ 𝑦)2 ∑ 𝑛 𝑖=1(𝑦𝑖 − ̄ 𝑦)2 Using the facts that ̂𝛽1 = ∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦) ∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)2 and ̂𝛽0 = ̄ 𝑦 − ̂𝛽1 ̄ 𝑥, we obtain 𝑛 ∑ 𝑖=1 ( ̂ 𝑦𝑖 − ̄ "
4264,unknown,"𝑛 ∑ 𝑖=1 ( ̂𝛽0 + ̂𝛽1𝑥𝑖 − ̄ 𝑦)2 = 𝑛 ∑ 𝑖=1 ( ̄ 𝑦 − ̂𝛽1 ̄ 𝑥 +̂𝛽1𝑥𝑖 − ̄ 𝑦)2 = ̂𝛽2 1 𝑛 ∑ 𝑖=1 (𝑥𝑖 − ̄ 𝑥)2 = [∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦)]2 ∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)2 [∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)2]2 = [∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦)]2 ∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)2 (3.1) Hence, 𝑅2 = ∑ 𝑛 𝑖=1( ̂ 𝑦𝑖 − ̄ 𝑦)2 ∑ 𝑛 𝑖=1(𝑦𝑖 − ̄ 𝑦)2 = [∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦)]2 ∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)2 ∑ 𝑛 𝑖=1(𝑦𝑖 − ̄ 𝑦)2 = ⎛⎜⎜ ⎝ ∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦) √∑ 𝑛 𝑖=1("
4265,unknown,"= ⎛⎜⎜ ⎝ ∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦) √∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)2 ∑ 𝑛 𝑖=1(𝑦𝑖 − ̄ 𝑦)2 ⎞⎟⎟ ⎠ 2 = 𝑟2 (3.2) This shows that the coeﬀicient of determination of a simple linear regression is the square of the sample correlation coeﬀicient of (𝑥1, 𝑦1),…,(𝑥𝑛, 𝑦𝑛). 3.3. SIMPLE LINEAR REGRESSION: ASSUMPTIONS 35 3.3 Simple Linear Regression: Assumptions Recall that the simple linear regression model for 𝑌 on 𝑥 is 𝑌 = "
4266,unknown,"𝑌 = 𝛽 0 + 𝛽1𝑥 + 𝜖 where 𝑌 : the dependent or response variable 𝑥 : the independent or predictor variable, assumed known 𝛽0, 𝛽1 : the regression parameters, the intercept and slope of the regression line 𝜖 : the random regression error around the line. and the regression equation for a set of 𝑛 data points is ̂ 𝑦 = 𝑏0 + 𝑏1 𝑥, where 𝑏1 = 𝑆𝑥𝑦 𝑆𝑥𝑥 = ∑(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦) ∑(𝑥𝑖 − ̄ 𝑥)2 and 𝑏0 = ̄ 𝑦 − 𝑏1"
4267,unknown,"∑(𝑥𝑖 − ̄ 𝑥)2 and 𝑏0 = ̄ 𝑦 − 𝑏1 ̄ 𝑥 where 𝑏0 is called the y-intercept and 𝑏1 is called the slope. The residual standard error 𝑠𝑒 can be defined as 𝑠𝑒 = √ 𝑆𝑆𝐸 𝑛 − 2 = √ ∑(𝑦𝑖 − ̂ 𝑦𝑖)2 𝑛 − 2 𝑠𝑒 indicates how much, on average, the observed values of the response variable differ from the predicted values of the response variable. Simple Linear Regression Assumptions (SLR) W e have a collection of 𝑛 pai"
4268,unknown,"W e have a collection of 𝑛 pairs of observations {(𝑥𝑖, 𝑦𝑖)}, and the idea is to use them to estimate the unknown parameters 𝛽0 and 𝛽1. 𝜖𝑖 = 𝑌𝑖 − (𝛽0 + 𝛽1 𝑥𝑖) , 𝑖 = 1, 2, … , 𝑛 W e need to make the following key assumptions on the errors: A. 𝐸(𝜖𝑖) = 0 (errors have mean zero and do not depend on 𝑥) B. 𝑉 𝑎𝑟(𝜖𝑖) = 𝜎2 (errors have a constant variance, homoscedastic, and do not depend on 𝑥) C. 𝜖1, 𝜖2, …"
4269,unknown,"C. 𝜖1, 𝜖2, … 𝜖𝑛 are independent. D. 𝜖𝑖 are all i.i.d. 𝑁 (0, 𝜎 2), meaning that the errors are independent and identically dis- tributed as Normal with mean zero and constant variance 𝜎2. The above assumptions, and conditioning on 𝛽0 and 𝛽1, imply: a. Linearity: 𝐸(𝑌𝑖|𝑋𝑖) = 𝛽0 + 𝛽1 𝑥𝑖 36 CHAPTER 3. LINEAR REGRESSION b. Homogenity or homoscedasticity: 𝑉 𝑎𝑟(𝑌𝑖|𝑋𝑖) = 𝜎2 c. Independence: 𝑌1, 𝑌2, … , 𝑌𝑛 "
4270,unknown,"d. Normality: 𝑌𝑖|𝑋𝑖 ∼ 𝑁 (𝛽0 + 𝛽1𝑥𝑖, 𝜎 2) It is in fact under these assumptions, minimizing the least square criterion is equivalent to maximizing the log-likelihood of the model parameters. Used cars example The table below displays data on Age (in years) and Price (in hundreds of dollars) for a sample of cars of a particular make and model. W eiss[2012] Price (𝑦) Age ( 𝑥) 85 5 103 4 70 6 82 5 89 "
4271,unknown,"103 4 70 6 82 5 89 5 98 5 66 6 95 6 169 2 70 7 48 7 3.3. SIMPLE LINEAR REGRESSION: ASSUMPTIONS 37 W e can see that for each age, the mean price of all cars of that age lies on the regression line 𝐸(𝑌 |𝑥) = 𝛽0 + 𝛽1𝑥. And, the prices of all cars of that age are assumed to be normally distributed with mean equal to 𝛽0 + 𝛽1𝑥 and variance 𝜎2. F or example, the prices of all 4-year-old cars must be norm"
4272,unknown,"4-year-old cars must be normally distributed with mean 𝛽0 + 𝛽1(4) and variance 𝜎2. W e used the least square method to find the best fit for this data set, and residuals can be obtained as 𝑒𝑖 = 𝑦𝑖 − ̂ 𝑦𝑖 = 𝑦𝑖 − (195.47 − 20.26𝑥𝑖). Residual Analysis The easiest way to check the simple linear regression assumptions is by constructing a scatter- plot of the residuals ( 𝑒𝑖 = 𝑦𝑖 − ̂ 𝑦𝑖) against the fit"
4273,unknown,"a good fit, then the residual plot should show an even and random scatter of the residuals. 38 CHAPTER 3. LINEAR REGRESSION Linearity The regression needs to be linear in the parameters. 𝑌 = 𝛽 0 + 𝛽1 𝑥 + 𝜖 𝐸(𝑌𝑖|𝑋𝑖) = 𝛽0 + 𝛽1 𝑥𝑖 ≡ 𝐸(𝜖𝑖|𝑋𝑖) = 𝐸(𝜖𝑖) = 0 The residual plot below shows that a linear regression model is not appropriate for this data. Constant error variance (homoscedasticity) The plot sh"
4274,unknown,"The plot shows the spread of the residuals is increasing as the fitted values (or 𝑥) increase, which indicates that we have Heteroskedasticity (non-constant variance). The standard errors are biased when heteroskedasticity is present, but the regression coeﬀicients will still be unbiased. 3.3. SIMPLE LINEAR REGRESSION: ASSUMPTIONS 39 How to detect? • Residuals plot (fitted vs residuals) • Goldfeld"
4275,unknown,"• Goldfeld–Quandt test • Breusch-Pagan test How to fix? • White’s standard errors • W eighted least squares model • T aking the log Independent errors terms (no autocorrelation) The problem of autocorrelation is most likely to occur in time series data, however, it can also occur in cross-sectional data, e.g. if the model is incorrectly specified. When autocorrelation is present, the regression co"
4276,unknown,"is present, the regression coeﬀicients will still be unbiased, however, the standard errors and test statitics are no longer valid. An example of no autocorrelation An example of positive autocorrelation 40 CHAPTER 3. LINEAR REGRESSION An example of negative autocorrelation How to detect? • Residuals plot • Durbin-W atson test • Breusch-Godfrey test How to fix? • Investigate omitted variables (e.g"
4277,unknown,"• Use advanced models (e.g. AR model) Normality of the errors W e need the errors to be normally distributed. Normality is required for the sampling distri- butions, hypothesis testing and confidence intervals. How to detect? 3.3. SIMPLE LINEAR REGRESSION: ASSUMPTIONS 41 • Histogram of residuals • Q-Q plot of residuals • Kolmogorov–Smirnov test • Shapiro–Wilk test How to fix? • Change the function"
4278,unknown,"• Larger sample if possible Box-Cox T ransformation Normality is an important assumption for many statistical techniques. The Box-Cox trans- formation transforms our data so that it closely resembles a normal distribution. Box-Cox transformation applies to a positive response 𝑦 with the form 𝑦(𝜆) = { 𝑦𝜆 −1 𝜆 𝜆 ≠ 0, log 𝑦 𝜆 = 0 such that 𝑦(𝜆) satisfies the assumptions: a. 𝐸(𝑌 (𝜆) 𝑖 |𝑋𝑖) = 𝛽0 + 𝛽1 𝑥"
4279,unknown,"b. 𝑉 𝑎𝑟(𝑌 (𝜆) 𝑖 |𝑋𝑖) = 𝜎2 c. 𝑌 (𝜆) 1 , 𝑌 (𝜆) 2 , … , 𝑌(𝜆) 𝑛 are all independent given 𝑋𝑖. d. 𝑌 (𝜆) 𝑖 |𝑋𝑖 ∼ 𝑁 (𝛽0 + 𝛽1𝑥𝑖, 𝜎 2) As usually 𝜆 varies over (−2, 2), 𝑦(𝜆) encompasses the reciprocal transformation ( 𝜆 = −1 ), log ( 𝜆 = 0 ), square root ( 𝜆 = 1 2 ), the original scale ( 𝜆 = 1 ) and the square transformation (𝜆 = 2). If the 𝑦𝑖 are not positive we apply the transformation to 𝑦𝑖 + 𝛾, where 𝛾"
4280,unknown,"to make all the 𝑦𝑖 + 𝛾 positive. The Box-Cox method chooses 𝜆 according to the likelihood function. Hint: taking account of the Jacobian of the transformation from 𝑦(𝜆) to 𝑦, namely 𝑦𝜆−1, the density of 𝑦𝑖 is 𝑓 (𝑦𝑖∣𝑥𝑖) = 𝑦𝜆−1 𝑖 (2𝜋𝜎2) 1 2 exp [− 1 2𝜎2 (𝑦(𝜆) 𝑖 − 𝑥𝑖𝛽1 − 𝛽0) 2 ] In general, one can simply consider 𝜆 as a hyperparameter. In machine learning, hyperparam- eters are parameters whose valu"
4281,unknown,"eters are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning. The prefix ‘hyper_’ suggests that they are ‘top-level’ parameters that control the learning process and the model param- eters that result from it. As a machine learning engineer designing a model, you choose and set hyperparameter values that your"
4282,unknown,"42 CHAPTER 3. LINEAR REGRESSION model even begins. F or example, in the simple linear regression, we choose the value of 𝜆 before estimating the model parameter 𝛽0 and 𝛽1. Example: Infant mortality and GDP Let us investigate the relationship between infant mortality and the wealth of a country . W e will use data on 207 countries of the world gathered by the UN in 1998 (the ‘UN’ data set is availa"
4283,unknown,"is available from the R package ‘carData’). The data set contains two variables: the infant mortality rate in deaths per 1000 live births, and the GDP per capita in US dollars. There are some missing data values for some countries, so we will remove the missing data before we fit our model. # install.packages(""carData"") library(carData) data(UN) options(scipen=999) # Remove missing data newUN<-na."
4284,unknown,"# Remove missing data newUN<-na.omit(UN) str(newUN) ## 'data.frame': 193 obs. of 7 variables: ## $ region : Factor w/ 8 levels ""Africa"",""Asia"",..: 2 4 1 1 5 2 3 8 4 2 ... ## $ group : Factor w/ 3 levels ""oecd"",""other"",..: 2 2 3 3 2 2 2 1 1 2 ... ## $ fertility : num 5.97 1.52 2.14 5.13 2.17 ... ## $ ppgdp : num 499 3677 4473 4322 9162 ... ## $ lifeExpF : num 49.5 80.4 75 53.2 79.9 ... ## $ pctUrba"
4285,unknown,"## $ infantMortality: num 124.5 16.6 21.5 96.2 12.3 ... ## - attr(*, ""na.action"")= 'omit' Named int [1:20] 4 6 21 35 38 54 67 75 77 78 ... ## ..- attr(*, ""names"")= chr [1:20] ""American Samoa"" ""Anguilla"" ""Bermuda"" ""Cayman Islands"" ... fit<- lm(infantMortality ~ ppgdp, data=newUN) summary(fit) ## ## Call: ## lm(formula = infantMortality ~ ppgdp, data = newUN) ## ## Residuals: ## Min 1Q Median 3Q Max"
4286,unknown,## -31.48 -18.65 -8.59 10.86 83.59 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 41.3780016 2.2157454 18.675 < 0.0000000000000002 *** ## ppgdp -0.0008656 0.0001041 -8.312 0.0000000000000173 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 3.3. SIMPLE LINEAR REGRESSION: ASSUMPTIONS 43 ## ## Residual standard error: 25.13 on 191 degrees of free
4287,unknown,"## Multiple R-squared: 0.2656, Adjusted R-squared: 0.2618 ## F-statistic: 69.08 on 1 and 191 DF, p-value: 0.0000000000000173 plot(newUN$infantMortality ~ newUN$ppgdp, xlab=""GDP per Capita"", ylab=""Infant mortality (per 1000 births)"", pch=16, col=""cornflowerblue"") abline(fit,col=""red"") 0 20000 40000 60000 80000 100000 0 20 40 60 80 100 GDP per Capita Infant mortality (per 1000 births) W e can see fr"
4288,unknown,"W e can see from the scatterplot that the relationship between the two variables is not linear. There is a concentration of data points at small values of GDP (many poor countries) and a concentration of data points at small values of infant mortality (many countries with very low mortality). This suggests a skewness to both variables which would not conform to the normality assumption. Indeed, th"
4289,unknown,"and only has an 𝑅2 of 0.266. F rom the residual plot below we can see a clear evidence of structure to the residuals sug- gesting the linear relationship is a poor description of the data, and substantial changes in spread suggesting the assumption of homogeneous variance is not appropriate. # diagnostic plots plot(fit,which=1,pch=16,col=""cornflowerblue"") −40 −20 0 20 40 −40 0 20 40 60 80 Fitted v"
4290,unknown,"Residuals lm(infantMortality ~ ppgdp) Residuals vs Fitted AfghanistanChad Guinea−Bissau So we can apply a transformation to one or both variables, e.g. taking the log or adding a quadratic form. Notice that this will not affect (violet) the linearity assumption as the 44 CHAPTER 3. LINEAR REGRESSION regression will still be linear in the parameters. So if we take the logs of both variables gives u"
4291,unknown,"gives us the scatterplot of the transformed data set, below, which appears to show a more promising linear structure. The quality of the regression is now improved, with an 𝑅2 value of 0.766, which is still a little weak due to the rather large spread in the data. fit1<- lm(log(infantMortality) ~ log(ppgdp), data=newUN) summary(fit1) ## ## Call: ## lm(formula = log(infantMortality) ~ log(ppgdp), d"
4292,unknown,## ## Residuals: ## Min 1Q Median 3Q Max ## -1.16789 -0.36738 -0.02351 0.24544 2.43503 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 8.10377 0.21087 38.43 <0.0000000000000002 *** ## log(ppgdp) -0.61680 0.02465 -25.02 <0.0000000000000002 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.5281 on 191 degrees of fr
4293,unknown,"## Multiple R-squared: 0.7662, Adjusted R-squared: 0.765 ## F-statistic: 625.9 on 1 and 191 DF, p-value: < 0.00000000000000022 plot(log(newUN$infantMortality) ~ log(newUN$ppgdp), xlab=""GDP per Capita"", ylab=""Infant mortality (per 1000 births)"", pch=16, col=""cornflowerblue"") abline(fit1,col=""red"") 5 6 7 8 9 10 11 1 2 3 4 GDP per Capita Infant mortality (per 1000 births) So we check the residuals ag"
4294,unknown,"So we check the residuals again, as we can see from the residuals plot below that the log transformation has corrected many of the problems with with residual plot and the residuals now much closer to the expected random scatter. # diagnostic plots plot(fit1,which=1,pch=16,col=""cornflowerblue"") 3.3. SIMPLE LINEAR REGRESSION: ASSUMPTIONS 45 1 2 3 4 5 −1 0 1 2 Fitted values Residuals lm(log(infantMo"
4295,unknown,"Residuals lm(log(infantMortality) ~ log(ppgdp)) Residuals vs Fitted Equatorial Guinea AngolaGabon Now let us check the Normality of the errors by creating a histogram and normal QQ plot for the residuals, before and after the log transformation. The normal quantile (QQ) plot shows the sample quantiles of the residuals against the theoretical quantiles that we would expect if these values were draw"
4296,unknown,"if these values were drawn from a Normal distribution. If the Normal assumption holds, then we would see an approximate straight-line relationship on the Normal quantile plot. par(mfrow=c(2,2)) # before the log transformation. plot(fit, which = 2,pch=16, col=""cornflowerblue"") hist(resid(fit),col=""cornflowerblue"",main="""") # after the log transformation. plot(fit1, which = 2, pch=16, col=""hotpink3"")"
4297,unknown,"hist(resid(fit1),col=""hotpink3"",main="""") −3 −2 −1 0 1 2 3 −1 1 3 Theoretical Quantiles Standardized residuals Normal Q−Q AfghanistanChadGuinea−Bissau resid(fit) Frequency −40 0 20 40 60 80 0 20 50 −3 −2 −1 0 1 2 3 −2 2 Theoretical Quantiles Standardized residuals Normal Q−Q Equatorial Guinea AngolaGabon resid(fit1) Frequency −1 0 1 2 0 30 60 46 CHAPTER 3. LINEAR REGRESSION The normal quantile plot"
4298,unknown,"The normal quantile plot and the histogram of residuals (before the log transformation) show strong departure from the expectation of an approximately straight line, with curvature in the tails which reflects the skewness of the data. Finally , the normal quantile plot and the histogram of residuals suggest that residuals are much closer to Normality after the transformation, with some minor devia"
4299,unknown,3.4 Simple Linear Regression: Inference Simple Linear Regression Assumptions • Linearity of the relationship between the dependent and independent variables • Independence of the errors (no autocorrelation) • Constant variance of the errors (homoscedasticity) • Normality of the error distribution. Simple Linear Regression Model The simple linear regression model for 𝑌 on 𝑥 is 𝑌 = 𝛽 0 + 𝛽1𝑥 + 𝜖 whe
4300,unknown,"𝑌 : the dependent or response variable 𝑥 : the independent or predictor variable, assumed known 𝛽0, 𝛽1 : the regression parameters, the intercept and the slope of the regression line 𝜖 : the random regression error around the line. The simple linear regression equation • The regression equation for a set of 𝑛 data points is ̂ 𝑦 = 𝑏0 + 𝑏1 𝑥, where 𝑏1 = 𝑆𝑥𝑦 𝑆𝑥𝑥 = ∑(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦) ∑(𝑥𝑖 − ̄ 𝑥)2 a"
4301,unknown,"𝑏1 = 𝑆𝑥𝑦 𝑆𝑥𝑥 = ∑(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦) ∑(𝑥𝑖 − ̄ 𝑥)2 and 𝑏0 = ̄ 𝑦 − 𝑏1 ̄ 𝑥 • 𝑦 is the dependent variable (or response variable) and 𝑥 is the independent variable (predictor variable or explanatory variable). • 𝑏0 is called the y-intercept and 𝑏1 is called the slope. 3.4. SIMPLE LINEAR REGRESSION: INFERENCE 47 Residual standard error, 𝑠𝑒 The residual standard error, 𝑠𝑒, is defined by 𝑠𝑒 = √ 𝑆𝑆𝐸 𝑛 − 2 "
4302,unknown,"𝑛 − 2 where 𝑆𝑆𝐸 is the error sum of squares (also known as the residual sum of squares, RSS) which can be defined as 𝑆𝑆𝐸 = ∑ 𝑒2 𝑖 = ∑(𝑦𝑖 − ̂ 𝑦𝑖)2 = 𝑆𝑦𝑦 − 𝑆2 𝑥𝑦 𝑆𝑥𝑥 𝑠𝑒 indicates how much, on average, the observed values of the response variable differ from the predicted values of the response variable. Under the simple linear regression assumptions, 𝑠𝑒 is an unbiased estimate for the error standard"
4303,unknown,"𝑠𝑒 is an unbiased estimate for the error standard deviation 𝜎. Properties of Regression Coeﬀicients Under the simple linear regression assumptions, the least square estimates 𝑏0 and 𝑏1 are unbiased for the 𝛽0 and 𝛽1, respectively , i.e. 𝐸[𝑏0] = 𝛽0 and 𝐸[𝑏1] = 𝛽1. The variances of the least squares estimators in simple linear regression are: 𝑉 𝑎𝑟[𝑏0] = 𝜎2 𝑏0 = 𝜎2 ( 1 𝑛 + ̄ 𝑥2 𝑆𝑥𝑥 ) 𝑉 𝑎𝑟[𝑏1] = 𝜎2 𝑏1"
4304,unknown,"𝑏0 = 𝜎2 ( 1 𝑛 + ̄ 𝑥2 𝑆𝑥𝑥 ) 𝑉 𝑎𝑟[𝑏1] = 𝜎2 𝑏1 = 𝜎2 𝑆𝑥𝑥 𝐶 𝑜𝑣[𝑏0, 𝑏1] = 𝜎𝑏0 ,𝑏1 = −𝜎2 ̄ 𝑥 𝑆𝑥𝑥 W e use𝑠𝑒 to estimate the error standard deviation 𝜎: 𝑠2 𝑏0 = 𝑠2 𝑒 ( 1 𝑛 + ̄ 𝑥2 𝑆𝑥𝑥 ) 𝑠2 𝑏1 = 𝑠2 𝑒 𝑆𝑥𝑥 𝑠𝑏0 ,𝑏1 = −𝑠2 𝑒 ̄ 𝑥 𝑆𝑥𝑥 The proof of some of the properties are provided in the following, the rest can be find in exercise sheets. 48 CHAPTER 3. LINEAR REGRESSION • Proof of 𝐸[𝑏1] = 𝛽1 𝐸(𝑏1) = 𝐸 (∑(𝑥𝑖 − ̄ 𝑥"
4305,unknown,"∑(𝑥𝑖 − ̄ 𝑥)2 ) = 𝐸 (∑(𝑥𝑖 − ̄ 𝑥)𝑦𝑖 − ̄ 𝑦 ∑(𝑥𝑖 − ̄ 𝑥)) ∑(𝑥𝑖 − ̄ 𝑥)2 ) (3.3) Note ∑(𝑥𝑖 − ̄ 𝑥) = 0, hence 𝐸(𝑏1) = 𝐸 (∑(𝑥𝑖 − ̄ 𝑥)𝑦𝑖 ∑(𝑥𝑖 − ̄ 𝑥)2 ) = ∑(𝑥𝑖 − ̄ 𝑥)(𝛽0 + 𝛽1𝑥𝑖) ∑(𝑥𝑖 − ̄ 𝑥)2 = ∑(𝑥𝑖 − ̄ 𝑥)𝛽1𝑥𝑖 ∑(𝑥𝑖 − ̄ 𝑥)2 (3.4) Note ∑(𝑥𝑖 − ̄ 𝑥)2 = ∑(𝑥𝑖 − ̄ 𝑥)𝑥𝑖, hence 𝐸(𝑏1) = 𝛽1 (3.5) • Proof of 𝑉 𝑎𝑟[𝑏1] = 𝜎2 𝑆𝑥𝑥 F rom the proof of 𝐸[𝑏1] = 𝛽1, we have 𝑉 𝑎𝑟(𝑏1) = 𝑉 𝑎𝑟 (∑(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦) ∑(𝑥𝑖 − ̄ 𝑥)2 ) = 𝑉"
4306,unknown,"∑(𝑥𝑖 − ̄ 𝑥)2 ) = 𝑉 𝑎𝑟 (∑(𝑥𝑖 − ̄ 𝑥)𝑦𝑖 ∑(𝑥𝑖 − ̄ 𝑥)2 ) = 𝑉 𝑎𝑟(∑(𝑥𝑖 − ̄ 𝑥)𝑦𝑖) (∑(𝑥𝑖 − ̄ 𝑥)2)2 = ∑ 𝑛 𝑖=1 𝑉 𝑎𝑟[(𝑥𝑖 − ̄ 𝑥)𝑦𝑖] + ∑𝑖≠𝑗 2(𝑥𝑖 − ̄ 𝑥)(𝑥𝑗 − ̄ 𝑥)𝐶 𝑜𝑣(𝑦𝑖, 𝑦𝑗) (∑(𝑥𝑖 − ̄ 𝑥)2)2 (3.6) Since 𝑦𝑖s are independent, hence 𝑉 𝑎𝑟(𝑏1) = ∑ 𝑛 𝑖=1 𝑉 𝑎𝑟[(𝑥𝑖 − ̄ 𝑥)𝑦𝑖] + ∑𝑖≠𝑗 2(𝑥𝑖 − ̄ 𝑥)(𝑥𝑗 − ̄ 𝑥)𝐶 𝑜𝑣(𝑦𝑖, 𝑦𝑗) (∑(𝑥𝑖 − ̄ 𝑥)2)2 = ∑(𝑥𝑖 − ̄ 𝑥)2𝜎2 (∑(𝑥𝑖 − ̄ 𝑥)2)2 = 𝜎2 𝑆𝑥𝑥 (3.7) 3.4. SIMPLE LINEAR REGRESSION: INFERENCE 4"
4307,unknown,"• Proof of 𝑉 𝑎𝑟[𝑏0] = 𝜎2 ( 1 𝑛 + ̄ 𝑥2 𝑆𝑥𝑥 ) 𝑉 𝑎𝑟(𝑏0) = 𝑉 𝑎𝑟 ( ̄ 𝑦 − 𝑏1 ̄ 𝑥) = 𝑉 𝑎𝑟( ̄ 𝑦) + 𝑉 𝑎𝑟(𝑏1 ̄ 𝑥) − 2𝐶 𝑜𝑣( ̄ 𝑦, 𝑏1 ̄ 𝑥) = 𝑉 𝑎𝑟(∑ 𝑦𝑖 𝑛 ) + ̄ 𝑥2𝑉 𝑎𝑟(𝑏1) − 2𝐶 𝑜𝑣( ̄ 𝑦, 𝑏1 ̄ 𝑥) == 𝜎2 𝑛 + ̄ 𝑥2 𝜎2 𝑆𝑥𝑥 − 2𝐶 𝑜𝑣( ̄ 𝑦, 𝑏1 ̄ 𝑥) (3.8) where 𝐶 𝑜𝑣( ̄ 𝑦, 𝑏1 ̄ 𝑥) = 𝐶 𝑜𝑣 (∑ 𝑦𝑖 𝑛 , ∑(𝑥𝑖 − ̄ 𝑥)𝑦𝑖 ∑(𝑥𝑖 − ̄ 𝑥)2 ̄ 𝑥) = ̄ 𝑥 𝑛 ∑(𝑥𝑖 − ̄ 𝑥)2 𝐶 𝑜𝑣[∑ 𝑦𝑖, ∑(𝑥𝑖 − ̄ 𝑥)𝑦𝑖] (3.9) 𝐶 𝑜𝑣[∑ 𝑦𝑖, ∑(𝑥𝑖 − ̄ 𝑥)𝑦𝑖] = 𝐸[(∑ 𝑦𝑖) ∑(𝑥𝑖 − "
4308,unknown,"= 𝐸[∑(𝑥𝑖 − ̄ 𝑥)𝑦2 𝑖 + ∑ 𝑖≠𝑗 (𝑥𝑖 − ̄ 𝑥)𝑦𝑖𝑦𝑗] − 𝑛𝐸[𝑦] ∑(𝑥𝑖 − ̄ 𝑥)𝐸(𝑦𝑖) = ∑(𝑥𝑖 − ̄ 𝑥)(𝑉 𝑎𝑟(𝑦𝑖) + 𝐸(𝑦𝑖)2) + ∑ 𝑖≠𝑗 (𝑥𝑖 − ̄ 𝑥)𝐸(𝑦𝑖)𝐸(𝑦𝑗) − 𝑛𝐸[𝑦]2 ∑(𝑥𝑖 − ̄ 𝑥) = (𝑉 𝑎𝑟(𝑦) + 𝐸(𝑦)2) ∑(𝑥𝑖 − ̄ 𝑥) + 𝐸(𝑦)2 ∑(𝑥𝑖 − ̄ 𝑥) − 𝑛𝐸[𝑦]2 ∑(𝑥𝑖 − ̄ 𝑥) = 0 (3.10) Hence 𝑉 𝑎𝑟(𝑏0) = 𝜎2 ( 1 𝑛 + ̄ 𝑥2 𝑆𝑥𝑥 ) (3.11) Sampling distribution of the least square estimators F or the Normal error simple linear regression model: 𝑏0 ∼ 𝑁 (𝛽0,"
4309,unknown,"𝑏0 ∼ 𝑁 (𝛽0, 𝜎2 𝑏0 ) → 𝑏0 − 𝛽0 𝜎𝑏0 ∼ 𝑁 (0, 1) and 𝑏1 ∼ 𝑁 (𝛽1, 𝜎2 𝑏1 ) → 𝑏1 − 𝛽1 𝜎𝑏1 ∼ 𝑁 (0, 1) W e use𝑠𝑒 to estimate the error standard deviation 𝜎: 𝑏0 − 𝛽0 𝑠𝑏0 ∼ 𝑡𝑛−2 and 𝑏1 − 𝛽1 𝑠𝑏1 ∼ 𝑡𝑛−2 50 CHAPTER 3. LINEAR REGRESSION Degrees of F reedom • In statistics, degrees of freedom are the number of independent pieces of information that go into the estimate of a particular parameter. • Typically , the"
4310,unknown,"• Typically , the degrees of freedom of an estimate of a parameter is equal to the number of independent observations that go into the estimate, minus the number of parameters that are estimated as intermediate steps in the estimation of the parameter itself. • The sample variance has 𝑛 − 1 degrees of freedom since it is computed from n pieces of data, minus the 1 parameter estimated as the interm"
4311,unknown,"of data, minus the 1 parameter estimated as the intermediate step, the sample mean. Similarly , having estimated the sample mean we only have𝑛 − 1 independent pieces of data left, as if we are given the sample mean and any 𝑛 − 1 of the observations then we can determine the value of the remaining observation exactly . 𝑠2 = ∑(𝑥𝑖 − ̄ 𝑥)2 𝑛 − 1 • In linear regression, the degrees of freedom of the re"
4312,unknown,"𝑛 − 1 • In linear regression, the degrees of freedom of the residuals is 𝑑𝑓 = 𝑛 − 𝑘∗, where 𝑘∗ is the number of estimated parameters (including the intercept). So for the simple linear regression, we are estimating 𝛽0 and 𝛽1, thus 𝑑𝑓 = 𝑛 − 2. Inference for the intercept 𝛽0 • Hypotheses: 𝐻0 ∶ 𝛽0 = 0 against 𝐻1 ∶ 𝛽0 ≠ 0 • T est statistic: 𝑡 = 𝑏0 𝑠𝑏0 has a t-distribution with 𝑑𝑓 = 𝑛 − 2 , where 𝑠𝑏0 i"
4313,unknown,"is the standard error of 𝑏0, and given by 𝑠𝑏0 = 𝑠𝑒√ 1 𝑛 + ̄ 𝑥2 𝑆𝑥𝑥 and 𝑠𝑒 = √ 𝑆𝑆𝐸 𝑛 − 2 = √ ∑(𝑦𝑖 − ̂ 𝑦𝑖)2 𝑛 − 2 W e reject𝐻0 at level 𝛼 if |𝑡| > 𝑡𝛼/2 with 𝑑𝑓 = 𝑛 − 2. • 100(1-𝛼)% confidence interval for 𝛽0, 𝑏0 ± 𝑡𝛼/2. 𝑠𝑏0 where 𝑡𝛼/2 is critical value obtained from the t‐distribution table with 𝑑𝑓 = 𝑛 − 2. 3.4. SIMPLE LINEAR REGRESSION: INFERENCE 51 Inference for the slope 𝛽1 • Hypotheses: 𝐻0 ∶ 𝛽1 "
4314,unknown,"• Hypotheses: 𝐻0 ∶ 𝛽1 = 0 against 𝐻1 ∶ 𝛽1 ≠ 0 • T est statistic: 𝑡 = 𝑏1 𝑠𝑏1 has a t-distribution with 𝑑𝑓 = 𝑛 − 2, where 𝑠𝑏1 is the standard error of 𝑏1,and given by 𝑠𝑏1 = 𝑠𝑒 √𝑆𝑥𝑥 and 𝑠𝑒 = √ 𝑆𝑆𝐸 𝑛 − 2 = √ ∑(𝑦𝑖 − ̂ 𝑦𝑖)2 𝑛 − 2 W e reject𝐻0 at level 𝛼 if |𝑡| > 𝑡𝛼/2 with 𝑑𝑓 = 𝑛 − 2. • 100(1-𝛼)% confidence interval for 𝛽1, 𝑏1 ± 𝑡𝛼/2 𝑠𝑏1 where 𝑡𝛼/2 is critical value obtained from the t‐distribution table"
4315,unknown,"Is the regression model “useful”? Goodness of fit test • W e test the null hypothesis 𝐻0 ∶ 𝛽1 = 0 against 𝐻1 ∶ 𝛽1 ≠ 0, the F-statistic 𝐹 = 𝑀 𝑆𝑅 𝑀 𝑆𝐸 = 𝑆𝑆𝑅 𝑆𝑆𝐸/(𝑛 − 2) has F-distribution with degrees of freedom 𝑑𝑓1 = 1 and 𝑑𝑓2 = 𝑛 − 2. • W e reject𝐻0, at level 𝛼, if 𝐹 > 𝐹 𝛼(𝑑𝑓1, 𝑑𝑓2). • F or a simple linear regression ONL Y, F-test is equivalent to t-test for𝛽1. 52 CHAPTER 3. LINEAR REGRESSION Regr"
4316,unknown,"Regression in R (Used cars example) Price<-c(85, 103, 70, 82, 89, 98, 66, 95, 169, 70, 48) Age<- c(5, 4, 6, 5, 5, 5, 6, 6, 2, 7, 7) carSales<-data.frame(Price,Age) str(carSales) ## 'data.frame': 11 obs. of 2 variables: ## $ Price: num 85 103 70 82 89 98 66 95 169 70 ... ## $ Age : num 5 4 6 5 5 5 6 6 2 7 ... # simple linear regression reg<-lm(Price~Age) summary(reg) ## ## Call: ## lm(formula = Pri"
4317,unknown,"## ## Residuals: ## Min 1Q Median 3Q Max ## -12.162 -8.531 -5.162 8.946 21.099 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 195.47 15.24 12.826 0.000000436 *** ## Age -20.26 2.80 -7.237 0.000048819 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 12.58 on 9 degrees of freedom ## Multiple R-squared: 0.8534, Adju"
4318,unknown,"## F-statistic: 52.38 on 1 and 9 DF, p-value: 0.00004882 3.5. SIMPLE LINEAR REGRESSION: CONFIDENCE AND PREDICTION INTER V ALS53 # To obtain the confidence intervals confint(reg, level=0.95) ## 2.5 % 97.5 % ## (Intercept) 160.99243 229.94451 ## Age -26.59419 -13.92833 R output 3.5 Simple Linear Regression: Confidence and Predic- tion intervals Earlier we introduced simple linear regression as a bas"
4319,unknown,"Earlier we introduced simple linear regression as a basic statistical model for the relationship between two random variables. W e used the least square method for estimating the regression parameters. Recall that the simple linear regression model for 𝑌 on 𝑥 is 𝑌 = 𝛽 0 + 𝛽1𝑥 + 𝜖 where 𝑌 : the dependent or response variable 𝑥 : the independent or predictor variable, assumed known 𝛽0, 𝛽1 : the regr"
4320,unknown,"𝛽0, 𝛽1 : the regression parameters, the intercept and the slope of the regression line 𝜖 : the random regression error around the line. and the regression equation for a set of 𝑛 data points is ̂ 𝑦 = 𝑏0 + 𝑏1 𝑥, where 𝑏1 = 𝑆𝑥𝑦 𝑆𝑥𝑥 = ∑(𝑥𝑖 − ̄ 𝑥)(𝑦𝑖 − ̄ 𝑦) ∑(𝑥𝑖 − ̄ 𝑥)2 54 CHAPTER 3. LINEAR REGRESSION and 𝑏0 = ̄ 𝑦 − 𝑏1 ̄ 𝑥 where 𝑏0 is called the y-intercept and 𝑏1 is called the slope. Under the simple"
4321,unknown,"Under the simple linear regression assumptions , the residual standard error 𝑠𝑒 is an unbiased estimate for the error standard deviation 𝜎, where 𝑠𝑒 = √ 𝑆𝑆𝐸 𝑛 − 2 = √ ∑(𝑦𝑖 − ̂ 𝑦𝑖)2 𝑛 − 2 𝑠𝑒 indicates how much, on average, the observed values of the response variable differ from the predicted values of the response variable. Below we will see how we can use these least square estimates for predicti"
4322,unknown,"Below we will see how we can use these least square estimates for prediction. First, we will consider the inference for the conditional mean of the response variable 𝑦 given a particular value of the independent variable 𝑥, let us call this particular value 𝑥∗. Next, we will see how to predict the value of the response variable 𝑌 for a given value of the independent variable 𝑥∗. F or these confide"
4323,unknown,assumptions must hold. Inference for the regression line 𝐸 [𝑌 |𝑥∗] Suppose we are interested in the value of the regression line at a new point 𝑥∗. Let’s denote the unknown true value of the regression line at𝑥 = 𝑥∗ as 𝜇∗. F rom the form of the regression line equation we have 𝜇∗ = 𝜇𝑌 |𝑥∗ = 𝐸 [𝑌 |𝑥∗] = 𝛽0 + 𝛽1𝑥∗ but 𝛽0 and 𝛽1 are unknown. W e can use the least square regression equation to estimat
4324,unknown,"unknown true value of the regression line, so we have ̂ 𝜇∗ = 𝑏0 + 𝑏1𝑥∗ = ̂ 𝑦∗ This is simply a point estimate for the regression line. However, in statistics, a point estimate is often not enough, and we need to express our uncertainty about this point estimate, and one way to do so is via confidence interval. A 100(1 − 𝛼)% confidence interval for the conditional mean 𝜇∗ is ̂ 𝑦∗ ± 𝑡𝛼/2 ⋅ 𝑠𝑒 √ 1 𝑛 "
4325,unknown,"̂ 𝑦∗ ± 𝑡𝛼/2 ⋅ 𝑠𝑒 √ 1 𝑛 + (𝑥∗ − ̄ 𝑥)2 𝑆𝑥𝑥 where 𝑆𝑥𝑥 = ∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)2, and 𝑡𝛼/2 is the 𝛼/2 critical value from the t-distribution with 𝑑𝑓 = 𝑛 − 2. 3.5. SIMPLE LINEAR REGRESSION: CONFIDENCE AND PREDICTION INTER V ALS55 Inference for the response variable 𝑌 for a given 𝑥 = 𝑥∗ Suppose now we are interested in predicting the value of 𝑌 ∗ if we have a new observation at 𝑥∗. At 𝑥 = 𝑥∗, the value of 𝑌 "
4326,unknown,"𝑥∗. At 𝑥 = 𝑥∗, the value of 𝑌 ∗ is unknown and given by 𝑌 ∗ = 𝛽0 + 𝛽1𝑥∗ + 𝜖 where but 𝛽0, 𝛽1 and 𝜖 are unknown. W e will use ̂ 𝑦∗ = 𝑏0 + 𝑏1 𝑥∗ as a basis for our prediction. A 100(1 − 𝛼)% prediction interval for 𝑌 ∗ at 𝑥 = 𝑥∗ is ̂ 𝑦∗ ± 𝑡𝛼/2 ⋅ 𝑠𝑒 √1 + 1 𝑛 + (𝑥∗ − ̄ 𝑥)2 𝑆𝑥𝑥 The extra ’1’ under the square root sign, we have here accounts for the extra variability of a single observation about the mea"
4327,unknown,"Used cars example Estimate the mean price of all 3-year-old cars, 𝐸[𝑌 |𝑥 = 3]: ̂ 𝜇∗ = 195.47 − 20.26(3) = 134.69 = ̂ 𝑦∗"
4328,unknown,"56 CHAPTER 3. LINEAR REGRESSION A 95% confidence interval for the mean price of all 3-year-old cars is ̂ 𝑦∗ ± 𝑡𝛼/2 × 𝑠𝑒√ 1 𝑛 + (𝑥∗ − ̄ 𝑥)2 𝑆𝑥𝑥 [195.47 − 20.26(3)] ± 2.262 × 12.58√ 1 11 + (3 − 5.273)2 (11 − 1) × 2.018 134.69 ± 16.76 that is 117.93 < 𝜇∗ < 151.45 Predict the price of a 3-year-old car, 𝑌 |𝑥 = 3: ̂ 𝑦∗ = 195.47 − 20.26(3) = 134.69 A 95% predictive interval for the price of a 3-year-old "
4329,unknown,"̂ 𝑦∗ ± 𝑡𝛼/2 × 𝑠𝑒√1 + 1 𝑛 + (𝑥∗ − ̄ 𝑥)2 𝑆𝑥𝑥 [195.47 − 20.26(3)] ± 2.262 × 12.58√1 + 1 11 + (3 − 5.273)2 (11 − 1) ∗ ×2.018 134.69 ± 33.025 that is 101.67 < 𝑌 ∗ < 167.72 where 𝑆𝑥𝑥 = ∑ 𝑛 𝑖=1(𝑥𝑖 − ̄ 𝑥)2 = (𝑛 − 1)𝑉 𝑎𝑟(𝑥). Regression in R # Build linear model Price<-c(85, 103, 70, 82, 89, 98, 66, 95, 169, 70, 48) Age<- c(5, 4, 6, 5, 5, 5, 6, 6, 2, 7, 7) carSales<-data.frame(Price=Price,Age=Age) reg <- lm"
4330,unknown,"summary(reg) ## ## Call: ## lm(formula = Price ~ Age, data = carSales) ## ## Residuals: ## Min 1Q Median 3Q Max 3.5. SIMPLE LINEAR REGRESSION: CONFIDENCE AND PREDICTION INTER V ALS57 ## -12.162 -8.531 -5.162 8.946 21.099 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 195.47 15.24 12.826 0.000000436 *** ## Age -20.26 2.80 -7.237 0.000048819 *** ## --- ## Signif. codes: 0"
4331,unknown,"## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 12.58 on 9 degrees of freedom ## Multiple R-squared: 0.8534, Adjusted R-squared: 0.8371 ## F-statistic: 52.38 on 1 and 9 DF, p-value: 0.00004882 mean(Age) ## [1] 5.272727 var(Age) ## [1] 2.018182 qt(0.975,9) ## [1] 2.262157 newage<- data.frame(Age = 3) predict(reg, newdata = newage, interval = ""c"
4332,unknown,"## fit lwr upr ## 1 134.6847 117.9293 151.4401 predict(reg, newdata = newage, interval = ""prediction"") ## fit lwr upr ## 1 134.6847 101.6672 167.7022 W e can plot the confidence and prediction intervals as follows: 58 CHAPTER 3. LINEAR REGRESSION 50 100 150 2 3 4 5 6 7 Age Price 3.6 Multiple Linear Regression: Introduction Multiple linear regression model In simple linear regression, we have one d"
4333,unknown,"In simple linear regression, we have one dependent variable (𝑦) and one independent variable (𝑥). In multiple linear regression, we have one dependent variable (𝑦) and several independent variables (𝑥1, 𝑥2, … , 𝑥𝑘). • The multiple linear regression model, for the population, can be expressed as 𝑌 = 𝛽 0 + 𝛽1𝑥1 + 𝛽2𝑥2 + … + 𝛽𝑘𝑥𝑘 + 𝜖 where 𝜖 is the error term. 3.6. MUL TIPLE LINEAR REGRESSION: INTROD"
4334,unknown,"Or using the matrix notation for a sample of 𝑛 observations: ⎡ ⎢⎢ ⎣ 𝑦1 ⋮ ⋮ 𝑦𝑛 ⎤ ⎥⎥ ⎦ = ⎡ ⎢⎢ ⎣ 1 𝑥 1,1 … 𝑥 1,𝑘 1 𝑥 2,1 … 𝑥 2,𝑘 ⋮ ⋮ ⋮ ⋮ 1 𝑥 𝑛,1 … 𝑥 𝑛,𝑘 ⎤ ⎥⎥ ⎦ ⎡ ⎢⎢ ⎣ 𝛽0 𝛽1 ⋮ 𝛽𝑘 ⎤ ⎥⎥ ⎦ + ⎡ ⎢⎢ ⎣ 𝜖1 ⋮ ⋮ 𝜖𝑛 ⎤ ⎥⎥ ⎦ i.e. Y = X𝛽 + ￿ where [ YT] = (𝑦 1, … , 𝑦𝑛) is the vector of responses; [ X], is the design matrix; [ 𝛽𝑇 ] = (𝛽0, 𝛽1, … , 𝛽𝑘) is the 𝑘 + 1 dimensional parameter vector; [ ￿T] = (𝜖 1, … , 𝜖𝑛) i"
4335,unknown,"of errors. • The corresponding least square estimate, from the sample, of this multiple linear regression model is given by ̂ 𝑦 = ̂𝛽0 + ̂𝛽1𝑥1 + ̂𝛽2𝑥2 + … + ̂𝛽𝑘𝑥𝑘 The LS estimator ̂𝛽 = (XTX) −1 XTY • The coeﬀicient 𝑏0 (or ̂𝛽0) represents the 𝑦-intercept, that is, the value of 𝑦 when 𝑥1 = 𝑥2 = … = 𝑥 𝑘 = 0 . The coeﬀicient 𝑏𝑖 (or ̂𝛽𝑖) (𝑖 = 1, … , 𝑘) is the partial slope of 𝑥𝑖, holding all other 𝑥’s f"
4336,unknown,"holding all other 𝑥’s fixed. So 𝑏𝑖 (or ̂𝛽𝑖) tells us the change in 𝑦 for a unit increase in 𝑥𝑖, holding all other 𝑥’s fixed. Used cars example The table below displays data on Age, Miles and Price for a sample of cars of a particular make and model. Price (𝑦) Age ( 𝑥1) Miles ( 𝑥2) 85 5 57 103 4 40 70 6 77 82 5 60 89 5 49 98 5 47 66 6 58 95 6 39 169 2 8 70 7 69 48 7 89 60 CHAPTER 3. LINEAR REGRESSI"
4337,unknown,60 80 100 140 60 100 140 Price 2 3 4 5 6 7 r = −0.924 Age 20 40 60 80 60 100 140 r = −0.942 2 3 4 5 6 7r = 0.863 20 40 60 80 20 40 60 80 Miles The scatterplot and the correlation matrix show a fairly negative relationship between the price of the car and both independent variables (age and miles). It is desirable to have a relationship between each independent variable and the dependent variable. 
4338,unknown,"the scatterplot also shows a positive relationship between the age and the miles, which is undesirable as it will cause the issue of Multicollinearity . Coeﬀicient of determination, 𝑅2 and adjusted 𝑅2 • Recall that 𝑅2 is a measure of the proportion of the total variation in the observed values of the response variable that is explained by the multiple linear regression in the 𝑘 predictor variables"
4339,unknown,"𝑘 predictor variables 𝑥1, 𝑥2, … , 𝑥𝑘. • 𝑅2 will increase when an additional predictor variable is added to the model. One should not simply select a model with many predictor variables because it has the highest 𝑅2 value, it is often good to have a model with a high 𝑅2 value but only a few x’s included. • Adjusted 𝑅2 is a modification of 𝑅2 that takes into account the number of predictor variables"
4340,unknown,"Adjusted-𝑅2 = 1 − (1 − 𝑅2) 𝑛 − 1 𝑛 − 𝑘 − 1 3.6. MUL TIPLE LINEAR REGRESSION: INTRODUCTION 61 The residual standard error, 𝑠𝑒 • Recall that, Residual = Observed value − Predicted value. 𝑒𝑖 = 𝑦𝑖 − ̂ 𝑦𝑖 • In a multiple linear regression with 𝑘 predictors, the standard error of the estimate, 𝑠𝑒, is defined by 𝑠𝑒 = √ 𝑆𝑆𝐸 𝑛 − (𝑘 + 1) where 𝑆𝑆𝐸 = ∑(𝑦 𝑖 − ̂ 𝑦𝑖)2 • The standard error of the estimate, 𝑠𝑒, i"
4341,unknown,"• The standard error of the estimate, 𝑠𝑒, indicates how much, on average, the observed values of the response variable differs from the predicted values of the response variable. The 𝑠𝑒 is the estimate of the common standard deviation 𝜎. Inferences about a particular predictor variable • T o test whether a particular predictor variable, say𝑥𝑖, is useful for predicting 𝑦 we test the null hypothesis"
4342,unknown,"the null hypothesis 𝐻0 ∶ 𝛽𝑖 = 0 against 𝐻1 ∶ 𝛽𝑖 ≠ 0. • The test statistic 𝑡 = 𝑏𝑖 𝑠𝑏𝑖 has a 𝑡-distribution with degrees of freedom 𝑑𝑓 = 𝑛 − (𝑘 + 1). So we reject 𝐻0, at level 𝛼, if |𝑡| > 𝑡𝛼/2. • Rejection of the null hypothesis indicates that 𝑥𝑖 is useful as a predictor for 𝑦. However, failing to reject the null hypothesis suggests that 𝑥𝑖 may not be useful as a predictor of 𝑦, so we may want to co"
4343,unknown,"of 𝑦, so we may want to consider removing this variable from the regression analysis. • 100(1-𝛼)% confidence interval for 𝛽𝑖 is 𝑏𝑖 ± 𝑡𝛼/2.𝑠𝑏𝑖 where 𝑠𝑏𝑖 is the standard error of 𝑏𝑖."
4344,unknown,"62 CHAPTER 3. LINEAR REGRESSION Is the multiple regression model “useful”? Goodness of fit test T o test how useful is this model, we test the null hypothesis 𝐻0 ∶ 𝛽1 = 𝛽2 = … = 𝛽𝑘 = 0, against 𝐻1 ∶ at least one of the 𝛽𝑖’s is not zero. - The 𝐹 -statistic 𝐹 = 𝑀 𝑆𝑅 𝑀 𝑆𝐸 = 𝑆𝑆𝑅/𝑘 𝑆𝑆𝐸/(𝑛 − 𝑘 − 1) with degrees of freedom 𝑑𝑓1 = 𝑘 and 𝑑𝑓2 = 𝑛 − (𝑘 + 1). W e reject𝐻0, at level 𝛼, if 𝐹 > 𝐹 𝛼(𝑑𝑓1, 𝑑𝑓2). Use"
4345,unknown,"Used cars example continued Multiple regression equation: ̂ 𝑦 = 183.04 − 9.50𝑥1 − 0.82𝑥2 The predicted price for a 4-year-old car that has driven 45 thousand miles is ̂ 𝑦 = 183.04 − 9.50(4) − 0.82(45) = 108.14 (as units of $100 were used, this means $10814) Extrapolation: we need to look at the region (all combined values) not only the range of the observed values of each predictor variable separa"
4346,unknown,"3.6. MUL TIPLE LINEAR REGRESSION: INTRODUCTION 63 Regression in R Price<-c(85, 103, 70, 82, 89, 98, 66, 95, 169, 70, 48) Age<- c(5, 4, 6, 5, 5, 5, 6, 6, 2, 7, 7) Miles<-c(57,40,77,60,49,47,58,39,8,69,89) carSales<-data.frame(Price=Price,Age=Age,Miles=Miles) # Scatterplot matrix # Customize upper panel upper.panel<-function(x, y){ points(x,y, pch=19, col=4) r <- round(cor(x, y), digits=3) txt <- pa"
4347,unknown,"usr <- par(""usr""); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) text(0.5, 0.9, txt) } pairs(carSales, lower.panel = NULL, upper.panel = upper.panel) 60 80 100 140 60 100 140 Price 2 3 4 5 6 7 r = −0.924 Age 20 40 60 80 60 100 140 r = −0.942 2 3 4 5 6 7r = 0.863 20 40 60 80 20 40 60 80 Miles reg <- lm(Price~Age+Miles,data=carSales) summary(reg) ## ## Call: ## lm(formula = Price ~ Age + Miles, data = "
4348,unknown,## 64 CHAPTER 3. LINEAR REGRESSION ## Residuals: ## Min 1Q Median 3Q Max ## -12.364 -5.243 1.028 5.926 11.545 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 183.0352 11.3476 16.130 0.000000219 *** ## Age -9.5043 3.8742 -2.453 0.0397 * ## Miles -0.8215 0.2552 -3.219 0.0123 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard e
4349,unknown,"## Multiple R-squared: 0.9361, Adjusted R-squared: 0.9201 ## F-statistic: 58.61 on 2 and 8 DF, p-value: 0.00001666 confint(reg, level=0.95) ## 2.5 % 97.5 % ## (Intercept) 156.867552 209.2028630 ## Age -18.438166 -0.5703751 ## Miles -1.409991 -0.2329757 Summary"
4350,unknown,"3.6. MUL TIPLE LINEAR REGRESSION: INTRODUCTION 65 Multiple Linear Regression Assumptions • Linearity: F or each set of values, 𝑥1, 𝑥2, … , 𝑥𝑘, of the predictor variables, the condi- tional mean of the response variable 𝑦 is 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + … + 𝛽𝑘𝑥𝑘. • Equal variance (homoscedasticity): The conditional variance of the response vari- able are the same (equal to 𝜎2) for all sets of values, 𝑥1, 𝑥2,"
4351,unknown,"able are the same (equal to 𝜎2) for all sets of values, 𝑥1, 𝑥2, … , 𝑥𝑘, of the predictor variables. • Independent observations : The observations of the response variable are indepen- dent of one another. • Normally: F or each set values,𝑥1, 𝑥2, … , 𝑥𝑘, of the predictor variables, the conditional distribution of the response variable is a normal distribution. • No Multicollinearity : Multicollinea"
4352,unknown,"variables are highly correlated. Multicollinearity • Multicollinearity refers to a situation when two or more predictor variables in our multiple regression model are highly (linearly) correlated. • The least square estimates will remain unbiased, but unstable. • The standard errors (of the affected variables) are likely to be high. • Overall model fit (e.g. R-square, F, prediction) is not affecte"
4353,unknown,Multicollinearity: Detect • Scatterplot Matrix • V ariance Inflation F actors: the V ariance Inflation F actors (VIF) for the𝑖𝑡ℎ predictor is 𝑉 𝐼 𝐹𝑖 = 1 1 − 𝑅2 𝑖 where 𝑅2 𝑖 is the R-square value obtained by regressing the 𝑖𝑡ℎ predictor on the other predictor variables. • 𝑉 𝐼 𝐹 = 1 indicates that there is no correlation between 𝑖𝑡ℎ predictor variable and the other predictor variables. • As a rule o
4354,unknown,"Multicollinearity: How to fix? Ignore: if the model is going to be used for prediction only . Remove: e.g. see if the variables are providing the same information. Combine: combining highly correlated variables. Advanced: e.g. Principal Components Analysis, Partial Least Squares. 66 CHAPTER 3. LINEAR REGRESSION Regression in R (regression assumptions) plot(reg, which=1, pch=19, col=4) 40 60 80 100"
4355,unknown,"−15 −10 −5 0 5 10 Fitted values Residuals lm(Price ~ Age + Miles) Residuals vs Fitted 7 9 10 plot(reg, which=2, pch=19, col=4) 3.6. MUL TIPLE LINEAR REGRESSION: INTRODUCTION 67 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 −1 0 1 2 Theoretical Quantiles Standardized residuals lm(Price ~ Age + Miles) Normal Q−Q 9 7 10 # install.packages(""car"") library(car) vif(reg) ## Age Miles ## 3.907129 3.907129 The value of 𝑉"
4356,unknown,"The value of 𝑉 𝐼 𝐹 = 3.91 indicates a moderate correlation between the age and the miles in the model, but this is not a major concern. 68 CHAPTER 3. LINEAR REGRESSION Chapter 4 Model Assessment and Selection Assessment of model performance is extremely important in practice, since it guides the choice of machine learning algorithm or model, and gives us a measure of the quality of the ultimately "
4357,unknown,"ultimately chosen model. It is important to note that there are in fact two separate goals here: • Model selection: estimating the performance of different models in order to choose the best one. • Model assessment: having chosen a final model, estimating its prediction error (gener- alization error ) over an independent new data sample. 4.1 In-sample vs out-of-sample If we are in a data-rich situ"
4358,unknown,"If we are in a data-rich situation, the best approach for both problems is to randomly divide the dataset into three parts: a training set, a validation/evaluation set, and a test set . • The training set is used to fit the model parameters. • The validation set is used to estimate prediction error for model selection (including choosing the values of hyperparameters). • The test set is used for a"
4359,unknown,"• The test set is used for assessment of the generalization error (also referred to as test error, is the prediction error over an independent test sample.) of the final chosen model. The period that the training set and the validation set are used for the initial parameter estimation and model selection, is called in-sample period. And an out-of-sample period uses test set to evaluate final forec"
4360,unknown,"test set to evaluate final forecasting performance. Ideally , the test set should be kept in a “vault” and be brought out only at the end of the data analysis. Suppose instead that we use the test-set repeatedly , choosing the model with smallest test-set error. Then the test set error of the final chosen model will underestimate the true test error, sometimes substantially . It is diﬀicult to giv"
4361,unknown,"It is diﬀicult to give a general rule on how to choose the number of observations in each of the three parts. A typical split might be 50% for training, and 25% each for validation and 69 70 CHAPTER 4. MODEL ASSESSMENT AND SELECTION testing. 4.2 Cross-V alidation F or the situations where there is insuﬀicient data to split it into three parts. One could conduct k-fold Cross-validation on a single "
4362,unknown,"conduct k-fold Cross-validation on a single training set for both training and validation. k-fold Cross-validation (CV) involves randomly dividing the set of 𝑛 observations into 𝑘 groups (or folds) of approximately equal size. Then, for each group 𝑖 = 1, ..., 𝑘: • F old𝑖 is treated as a validation set, and the model is fit on the remaining 𝑘 − 1 folds. • The performance metric, 𝑃 𝑒𝑟𝑓 𝑜𝑟𝑚𝑎𝑛𝑐𝑒𝑖 (for"
4363,unknown,"on the observations of the held out fold 𝑖. This process results in𝑘 estimates of the test performance,𝑃 𝑒𝑟𝑓 𝑜𝑟𝑚𝑎𝑛𝑐𝑒1, ..., 𝑃 𝑒𝑟𝑓 𝑜𝑟𝑚𝑎𝑛𝑐𝑒𝑘. The 𝑘-fold CV estimate is computed by averaging these values. 𝐶 𝑉(𝑘) = 1 𝑘 𝑘 ∑ 𝑖=1 𝑃 𝑒𝑟𝑓 𝑜𝑟𝑚𝑎𝑛𝑐𝑒𝑖 Figure 4.1 illustrates this nicely . Figure 4.1: An illustration of k-fold CV with 5 folds. 4.3. OVERFITTING VS UNDERFITTING 71 The 𝐶 𝑉(𝑘) as an estimate of the t"
4364,unknown,"The 𝐶 𝑉(𝑘) as an estimate of the test performance can also be used for model selection. Note that to assess the performance of the final chosen model, one still need another independent test sample. 4.3 Overfitting vs Underfitting Overfitting is a common pitfall in machine learning modelling, in which a model tries to fit the training data entirely and ends up “memorizing” the data patterns and th"
4365,unknown,"training data entirely and ends up “memorizing” the data patterns and the noise/random fluctuations. These models fail to generalize and perform well in the case of unseen data scenarios, defeating the model’s purpose. That is why an overfit model results in poor test accuracy . Example of overfitting situation in classification and regression: Detecting overfitting is only possible once we move o"
4366,unknown,"evaluate the model performance using the validation set. 72 CHAPTER 4. MODEL ASSESSMENT AND SELECTION Underfitting is another common pitfall in machine learning modelling, where the model can- not create a mapping between the input and the target variable that reflects the underlying system, for example due to under-observing the features. Underfitting often leads to a higher error in the training"
4367,unknown,"4.4. BIAS V ARIANCE TRADE-OFF 73 Underfitting becomes obvious when the model is too simple and cannot represent a relation- ship between the input and the output. It is detected when the training error is very high and the model is unable to learn from the training data. 4.4 Bias V ariance T rade-off In supervised learning, the model performance can help us to identify or even quantify over- fitti"
4368,unknown,"fitting/underfitting. Often we use the difference between the actual values and predicted values to evaluate the model, such prediction error can in fact be decomposed into three parts: • Bias: The difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the m"
4369,unknown,"and test data. • V ariance: V ariance is the variability of model prediction for a given data point or a value which tells us how uncertainty our model is. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data. • Noise: Irred"
4370,unknown,"rates on test data. • Noise: Irreducible error that we cannot eliminate. Mathematically , assume the relationship between the response 𝑌 and the predictors 𝑋 = (𝑋1, ..., 𝑋𝑝) can be represented as: 𝑌 = 𝑓 (𝑋) + 𝜖 where 𝑓 is some fixed but unknown function of 𝑋 and 𝜖 is a random error term, which is independent of 𝑋 and has mean zero. Consider building a model ̂𝑓 (𝑋) of 𝑓 (𝑋) (for example a linear re"
4371,unknown,74 CHAPTER 4. MODEL ASSESSMENT AND SELECTION squared error (MSE) at a point x is 𝑀 𝑆𝐸 = 𝐸 [(𝑦 − ̂𝑓 (𝑥)) 2 ] = 𝐸 [(𝑓 (𝑥) + 𝜖 − ̂𝑓 (𝑥)) 2 ] = 𝐸 [(𝑓 (𝑥) + 𝜖 − ̂𝑓 (𝑥) + 𝐸[ ̂𝑓 (𝑥)] − 𝐸[ ̂𝑓 (𝑥)]) 2 ] = ... = (𝑓 (𝑥) − 𝐸[ ̂𝑓 (𝑥)]) 2 + 𝐸 [𝜖2] + 𝐸 [(𝐸[ ̂𝑓 (𝑥)] − ̂𝑓 (𝑥)) 2 ] = (𝑓 (𝑥) − 𝐸[ ̂𝑓 (𝑥)]) 2 + 𝑉 𝑎𝑟[𝜖] + 𝑉 𝑎𝑟 [ ̂𝑓 (𝑥)] = 𝐵𝑖𝑎𝑠[ ̂𝑓 (𝑥)]2 + 𝑉 𝑎𝑟[𝜖] + 𝑉 𝑎𝑟 [ ̂𝑓 (𝑥)] = 𝐵𝑖𝑎𝑠2 + 𝑉 𝑎𝑟𝑖𝑎𝑛𝑐𝑒 + 𝐼 𝑟𝑟𝑒𝑑𝑢𝑐𝑖𝑏𝑙𝑒 𝐸𝑟𝑟𝑜
4372,unknown,"= 𝐵𝑖𝑎𝑠[ ̂𝑓 (𝑥)]2 + 𝑉 𝑎𝑟[𝜖] + 𝑉 𝑎𝑟 [ ̂𝑓 (𝑥)] = 𝐵𝑖𝑎𝑠2 + 𝑉 𝑎𝑟𝑖𝑎𝑛𝑐𝑒 + 𝐼 𝑟𝑟𝑒𝑑𝑢𝑐𝑖𝑏𝑙𝑒 𝐸𝑟𝑟𝑜𝑟 (4.1) W e can create a graphical visualization of bias and variance using a bulls-eye diagram. Imag- ine that the center of the target is a model that perfectly predicts the correct values. As we move away from the bulls-eye, our predictions get worse and worse. W e can plot four different cases representing combi"
4373,unknown,"W e can plot four different cases representing combinations of both high and low bias and variance. At its root, dealing with bias and variance is really about dealing with overfitting and under- fitting. High bias and low variance are the most common indicators of underfitting. Similarly low bias and high variance are the most common indicators of overfitting. Bias is reduced and variance is incr"
4374,unknown,"and variance is increased in relation to model complexity . As more and more parameters 4.4. BIAS V ARIANCE TRADE-OFF 75 are added to a model, the complexity of the model rises and variance becomes our primary concern while bias steadily falls. An optimal balance of bias and variance would neither overfit nor underfit the model. Ex- ample of an optimal balanced situation in classification and regr"
4375,unknown,"76 CHAPTER 4. MODEL ASSESSMENT AND SELECTION 4.5 Model selection for Linear regression Consider the standard linear regression model: 𝑦 = 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + … + 𝛽𝑝𝑥𝑝 + 𝜖 Here, 𝑦 is the response variable, the 𝑥’s are the predictor variables and 𝜖 is an error term. Model selection involves identifying a subset of the 𝑝 predictors 𝑥1, ..., 𝑥𝑝 of size 𝑑 that we believe to be related to the response. W"
4376,unknown,"just the reduced set of variables. F or example, if we believe that only𝑥1 and 𝑥2 are related to the response, then we may take 𝑑 = 2 and fit a model of the following form: 𝑦 = 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + 𝜖 where we have deemed variables 𝑥3, ..., 𝑥𝑝 to be irrelevant. But how do we determine which variables are relevant? W e may just‘know’ which variables are most informative for a particular response (for "
4377,unknown,"are most informative for a particular response (for example, from previous data investigations or talking to an expert in the field), but often we need to investigate. How many possible linear models are we trying to choose from? W ell, we will always include the intercept term 𝛽0. Then each variable 𝑋1, ..., 𝑋𝑝 can either be included or not, hence we have 2𝑝 possible models to choose from. Questi"
4378,unknown,"Question: how do we choose which variables should be included in our model? T raining Error The training error of a linear model is evaluated via the Residual Sum of Squares (RSS) , which as we have seen is given by 𝑅𝑆𝑆 = 𝑛 ∑ 𝑖=1 (𝑦𝑖 − ̂ 𝑦𝑖)2, where ̂ 𝑦𝑖 = ̂𝛽0 + ̂𝛽1𝑥𝑖1 + ̂𝛽2𝑥𝑖2 + … + ̂𝛽𝑑𝑥𝑖𝑑. is the predicted response value at input 𝑥𝑖 = (𝑥𝑖1, ..., 𝑥𝑖𝑑). Or simply scaling down the RSS leads to the "
4379,unknown,"leads to the Mean Squared Error (MSE): 𝑀 𝑆𝐸 = 𝑅𝑆𝑆 𝑛 where 𝑛 is the size of the training sample. Unfortunately , neither of these metrics is appro- priate when comparing models of different dimensionality (different number of predictors) because both 𝑅𝑆𝑆 and 𝑀 𝑆𝐸 generally decrease as we include additional predictors to a linear model. In fact, in the extreme case of 𝑛 = 𝑝 both metrics will be 0! T"
4380,unknown,"linear model. In fact, in the extreme case of 𝑛 = 𝑝 both metrics will be 0! This does not mean that we have a “good” model, just that we have overfitted our linear model to perfectly adjust to the training data. Overfitted models will exhibit poor predictive performance (low training error but high prediction error). Our aim is to have simple, interpretable models with relatively small 𝑝 (in relat"
4381,unknown,"4.6. MODEL SELECTION CRITERIA 77 V alidation W e could simply using an independent validation set (either by splitting the data into training and validation set or using cross-validation techniques) to evaluate the model performance and select the “best” model. F or example: Let us assume a hypothetical scenario where the true model is 𝑦 = 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + 𝜖 where 𝛽0 = 1, 𝛽1 = 1.5, 𝛽2 = −0.5 for"
4382,unknown,"where 𝛽0 = 1, 𝛽1 = 1.5, 𝛽2 = −0.5 for some given values of the predictors 𝑥1 and 𝑥2. Let’s set the errors to be normally distributed with zero mean and variance equal to one; i.e., 𝜖 ∼ 𝑁 (0, 1). Also assume that we have three additional predictors 𝑥3, 𝑥4 and 𝑥5 which are irrelevant to 𝑦 (in the sense that 𝛽3 = 𝛽 4 = 𝛽 5 = 0 ), but of course we do not know that beforehand. W e can plot the training"
4383,unknown,"W e can plot the training MSE and prediction MSE against the number of predictors used in the model, such as is illustrated in Figure 4.2. 1 2 3 4 5 0.8 1.0 1.2 Number of Predictors MSE Training MSE Prediction MSE Figure 4.2: T raining MSE and prediction MSE against number of predictors. Notice the following: • T raining error: steady (negligible) decline after adding 𝑥3 – not really obvious how m"
4384,unknown,"many predictors to include. • Prediction error: increase after adding 𝑥3, which indicates overfitting. Clearly here one would select the model with 𝑥1 and 𝑥2! • Prediction errors are larger than training errors – this is generally always the case! (see Q8 in Problem Sheet 1). 4.6 Model Selection Criteria Given that in practice we may not wish to exclude part of the data for calculating a predictio"
4385,unknown,"error using validation, instead of using cross-validation techniques we can also indirectly 78 CHAPTER 4. MODEL ASSESSMENT AND SELECTION estimate prediction error by making anadjustment to the performance measure that accounts for overfitting. Here, we have several model selection criteria that are developed from such kind of adjustment. 𝐶𝑝 F or a given model with𝑑 predictors (out of the available"
4386,unknown,"is 𝐶𝑝 = 1 𝑛 (𝑅𝑆𝑆 + 2𝑑 ̂ 𝜎2), where 𝑅𝑆𝑆 is the residual sum of squares for the model of interest (with 𝑑 predictors), and ̂ 𝜎2 = 𝑅𝑆𝑆 𝑝/(𝑛 − 𝑝 − 1) is an estimate of the error variance for the full model with all 𝑝 possible predictors included. As such, we use 𝑅𝑆𝑆 𝑝 to denote the Residual Sum of Squares for the full model with all 𝑝 possible predictors included. In practice, we choose the model whic"
4387,unknown,"In practice, we choose the model which has the minimum 𝐶𝑝 value: so we essentially penalise models of higher dimensionality (the larger 𝑑 is the greater the penalty). AIC F or linear models (with normal errors, as is often assumed), Mallows’𝐶 𝑝 is equivalent to the Akaike Information Criterion (AIC) (as the two are proportional), this being given by 𝐴𝐼 𝐶 = 1 𝑛 ̂ 𝜎2 (𝑅𝑆𝑆 + 2𝑑 ̂ 𝜎2). BIC Another met"
4388,unknown,"Another metric is the Bayesian information criterion (BIC) 𝐵𝐼 𝐶 = 1 𝑛 ̂ 𝜎2 (𝑅𝑆𝑆 + log(𝑛)𝑑 ̂ 𝜎2), where again the model with the minimum BIC value is selected In comparison to 𝐶𝑝 or AIC, where the penalty is 2𝑑 ̂ 𝜎2, the BIC penalty is log (𝑛)𝑑 ̂ 𝜎2. This means that generally BIC has a heavier penalty (because log (𝑛) > 2 for 𝑛 > 7 ), thus BIC selects models with fewer variables than 𝐶𝑝 or AIC. In "
4389,unknown,"tions. Adjusted R-squared value Another simple method which is not backed up by statistical theory , but that often works well in practice, is to simply adjust the 𝑅2 metric by taking into account the number of predictors as we defined before. The adjusted 𝑅2 value for a model with 𝑑 variables is calculated as follows Adjusted 𝑅2 = 1 − 𝑅𝑆𝑆/(𝑛 − 𝑑 − 1) 𝑇 𝑆𝑆/(𝑛 − 1) . 4.6. MODEL SELECTION CRITERIA 7"
4390,unknown,"In this case we choose the model with the maximum adjusted 𝑅2 value. Example By plotting the various model selection criteria against the number of predictors (Figure 4.3), we can see that, in our example, 𝐶𝑝, AIC and BIC are in agreement (2 predictors). Adjusted 𝑅2 would select 3 predictors. 1 2 3 4 5 5 10 15 20 Number of Predictors Cp 1 2 3 4 5 −122 −116 −110 Number of Predictors BIC 1 2 3 4 5 0"
4391,unknown,"Number of Predictors BIC 1 2 3 4 5 0.69 0.71 0.73 Number of Predictors Adjusted R−squared Figure 4.3: V arious model selection criteria against number of predictors. F urther points • A shortcoming of the previous approaches is that they are not applicable for models that have more variables than the size of the sample ( 𝑑 > 𝑛). • Also, in the setting of penalised regression (which we will see lat"
4392,unknown,"deciding what 𝑑 actually becomes a bit problematic. • CV is an effective computational tool which can be used in all settings. 80 CHAPTER 4. MODEL ASSESSMENT AND SELECTION Chapter 5 Model Search Methods In this section, we consider some methods for selecting subsets of predictors. 5.1 Best Subset Selection T o perform best subset selection , we fit a separate least squares regression for each poss"
4393,unknown,"combination of the 𝑝 predictors. This is often broken up into stages, as follows: 1. Let 𝑀0 denote the nul l modelwhich contains no predictors. This model simply predicts the sample mean of the response for each observation. 2. F or𝑘 = 1, 2, … , 𝑝: • Fit all (𝑝 𝑘) models1 that contain exactly 𝑘 predictors. • Pick the best among these (𝑝 𝑘) models and call it 𝑀𝑘. Here best is defined as having the "
4394,unknown,"𝑘) models and call it 𝑀𝑘. Here best is defined as having the smallest RSS or largest 𝑅2. 3. Select a single best model from among 𝑀0, 𝑀1, … , 𝑀𝑝 using cross-validated prediction error, 𝐶𝑝 (AIC), BIC, or adjusted 𝑅2. It is important to note that use of 𝑅𝑆𝑆 or 𝑅2 in step 2 of the above algorithm is acceptable because the models all have an equal number of predictors. W e can’t use 𝑅𝑆𝑆 in step 3 beca"
4395,unknown,"because, as already discussed, 𝑅𝑆𝑆 decreases monotonically as the number of predictors included in the model increases. Cross-validation could also be used in step 2, but would be computationally more time- consuming as it would ultimately involve fitting all 2𝑝 possible models K times, each time excluding the 𝑖th fold when training and using the 𝑖th fold to calculate a prediction error. By only u"
4396,unknown,"By only using cross-validation at step 3, we only need to fit 𝑝 + 1 models K times. An Illustation: Suppose we have three predictors 𝑋1, 𝑋2 and 𝑋3. In this case best subset selection works as follows: 1 Reminder: (𝑝 𝑘) = 𝑝! 𝑘!(𝑝−𝑘)! is the binomial coeﬀicient. It gives all the possible ways to extract a subset of 𝑘 elements from a fixed set of 𝑝 elements. F or instance, if 𝑝 = 3 and 𝑘 = 2 , the nu"
4397,unknown,"predictors is given by (3 2) = 3! 2!(3−2)! = 1⋅2⋅3 1⋅2⋅1 = 3. 81 82 CHAPTER 5. MODEL SEARCH METHODS 𝑀0 ∶ intercept only (null model) 𝐶1 ∶ ⎧{ ⎨{⎩ 𝑋1 𝑋2 → lowest training 𝑅𝑆𝑆 within 𝐶1 ∶ make this 𝑀1 𝑋3 𝐶2 ∶ ⎧{ ⎨{⎩ 𝑋1, 𝑋2 → lowest training 𝑅𝑆𝑆 within 𝐶2 ∶ make this 𝑀2 𝑋1, 𝑋3 𝑋2, 𝑋3 𝑀3 ∶ 𝑋1, 𝑋2, 𝑋3 (full model) W e then choose one model among 𝑀0, 𝑀1, 𝑀2, 𝑀3 either via validation/cross-validation, or "
4398,unknown,"based on 𝐶𝑝, BIC, adjusted 𝑅2. 5.2 F orwards Stepwise Selection F or computational reasons, best subset selection cannot be applied with very large 𝑝 as it needs to consider all 2𝑝 models. This is true even without using cross-validation which involves fitting the coeﬀicients of at least some of the models K times. Best subset selection may also suffer from statistical problems when 𝑝 is large. Th"
4399,unknown,"may also suffer from statistical problems when 𝑝 is large. The larger the search space, the higher the chance of finding models which look good on (overfit) the training data, even though they might not have any predictive power on future data. F orward stepwise selection is a computationally eﬀicient alternative to best subset selection. This is because it considers a much smaller set of models. "
4400,unknown,"This is because it considers a much smaller set of models. F orward stepwise selection works by starting with the model containing no predictors, and then adding one predictor at a time until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model. More formally , the algorithm proceeds as foll"
4401,unknown,"1. Let 𝑀0 denote the null model which contains no predictors. 2. F or𝑘 = 0, 1, ..., 𝑝 − 1: • Consider all 𝑝 − 𝑘 models that augment the predictors in 𝑀𝑘 with one additional predictor. • Choose the best among these 𝑝 − 𝑘 models and call it 𝑀𝑘+1. Here best is defined as having the smallest RSS or largest 𝑅2. 3. Select a single best model from among 𝑀0, 𝑀1, … , 𝑀𝑝 using cross-validated prediction err"
4402,unknown,"error, 𝐶𝑝 (AIC), BIC, or adjusted 𝑅2. An Illustation: Again, consider three predictors 𝑋1, 𝑋2 and 𝑋3. F orward stepwise selection works as follows: 5.3. BACKW ARDS STEPWISE SELECTION 83 𝑀0 ∶ intercept only (null model) 𝐶1 ∶ ⎧{ ⎨{⎩ 𝑋1 𝑋2 → lowest training 𝑅𝑆𝑆 within 𝐶1 ∶ make this 𝑀1 𝑋3 𝐶2 ∶ {𝑋1, 𝑋2 → lowest training 𝑅𝑆𝑆 within 𝐶2 ∶ make this 𝑀2 𝑋2, 𝑋3 𝑀3 ∶𝑋1, 𝑋2, 𝑋3 (full model) At the end we choo"
4403,unknown,"At the end we choose again one model among 𝑀0, 𝑀1, 𝑀2, 𝑀3 based on validation/cross- validation, or based on 𝐶𝑝, BIC, adjusted 𝑅2. Some general comments: • Best subset selection requires training 2𝑝 models, forward stepwise selection requires only 1 + 𝑝(𝑝 + 1) 2 comparisons. • So for 𝑝 = 20 : best subset selection considers 1,048,576 models, whereas forward stepwise selection considers 211 models!"
4404,unknown,"• There is cost to pay though for the gain in scalability . F orward stepwise selection can be sub-optimal in the sense that the models selected are not the best subset models. • This is because the guided forward stepwise search at each step depends on the previ- ously selected predictor. • As an example, in the illustration above we had 𝑝 = 3. The best possible one-variable model contained 𝑋2. I"
4405,unknown,"model contained 𝑋2. It is quite possible that the best two-variable model contains the pair (𝑋1, 𝑋3), however, we did not consider this combination in the above algorithm. • However, in practice best subset selection and forward stepwise selection perform quite similarly in terms of predictive accuracy. 5.3 Backwards Stepwise Selection Backwards stepwise selection begins with the full model contai"
4406,unknown,"iteratively removes the least useful predictor one-at-a-time. 1. Let 𝑀𝑝 denote the ful l model which contains all predictors. 2. F or𝑘 = 𝑝, 𝑝 − 1, … , 1: • Consider all 𝑘 models that contain all but one of the predictors in 𝑀𝑘, for a total of 𝑘 − 1 predictors. • Choose the best among these 𝑘 models and call it 𝑀𝑘−1. Here best is defined as having the smallest RSS or largest 𝑅2. 3. Select a single "
4407,unknown,"3. Select a single best model from among 𝑀0, 𝑀1, … , 𝑀𝑝 using cross-validated prediction error, 𝐶𝑝 (AIC), BIC, or adjusted 𝑅2. Just like the forward algorithm, backward elimination requires only1 +𝑝(𝑝 + 1) 2 comparisons. 84 CHAPTER 5. MODEL SEARCH METHODS An Illustration: Again, consider three predictors 𝑋1, 𝑋2 and 𝑋3. Backwards stepwise selection works as follows: 𝑀3 ∶𝑋1, 𝑋2, 𝑋3 (full model) 𝐶2 ∶"
4408,unknown,"𝐶2 ∶ ⎧{ ⎨{⎩ 𝑋1, 𝑋2 → lowest training 𝑅𝑆𝑆 within 𝐶2 ∶ make this 𝑀2 𝑋1, 𝑋3 𝑋2, 𝑋3 𝐶1 ∶ {𝑋1 𝑋2 → lowest training 𝑅𝑆𝑆 within 𝐶1 ∶ make this 𝑀1 𝑀0 ∶ intercept only (null model) At the end we choose again one model among 𝑀0, 𝑀1, 𝑀2, 𝑀3 based on cross-validation, or based on 𝐶𝑝, BIC, adjusted 𝑅2. 5.3.1 Concluding Remarks • Best subset, forward stepwise and backward stepwise selection approaches give simi"
4409,unknown,"but not identical results. • Hybrid approaches that combine forward and backward steps also exist (but we will not cover these). • Best subset and forward selection can be applied to some extent when 𝑛 < 𝑝, but only up to the model that contains 𝑛 − 1 predictors. Backward selection cannot be applied when 𝑛 < 𝑝. • The following practical demonstration and the practical classes will involve applying"
4410,unknown,"these methods to real data sets. 5.4 Practical Demonstration In this practical demonstration we will start with an analysis of theHittersdataset included in package ISLR. This dataset consists of 322 records of baseball players. The response variable is the players’ salary and the number of predictors is 19, including variables such as number of hits, years in the league and so forth. 5.4.1 Data h"
4411,unknown,"The first thing to do is to load the data by first installing package ISLR in R by typing install.packages('ISLR') (if you have already installed the package ignore this). Next we load the library and use ?Hitters which will open the help window in RStudio with information about the dataset. library(ISLR) ?Hitters 5.4. PRACTICAL DEMONSTRA TION 85 Incidentally , recall that we can view the help fil"
4412,unknown,"Incidentally , recall that we can view the help file for any function or dataset in R by typing? followed by the function or dataset, as above for dataset Hitters or below for function head. Y ou will need to make use of this to increase your understanding of R - I will not be able to explain every detail of every function throughout these tutorials (although I will explain a lot), and you indeed "
4413,unknown,"lot), and you indeed will wish to vary the commands I call to do your own investigations! T o get a clearer picture of how the data looks we can use commands names(), head() and dim(). The first returns the names of the 20 variables, the second returns the first 6 rows (type ?head to see how to adjust this) and the third gives us the number of rows (sample size) and number of columns (1 response +"
4414,unknown,"names(Hitters) ## [1] ""AtBat"" ""Hits"" ""HmRun"" ""Runs"" ""RBI"" ""Walks"" ## [7] ""Years"" ""CAtBat"" ""CHits"" ""CHmRun"" ""CRuns"" ""CRBI"" ## [13] ""CWalks"" ""League"" ""Division"" ""PutOuts"" ""Assists"" ""Errors"" ## [19] ""Salary"" ""NewLeague"" head(Hitters) ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun ## -Andy Allanson 293 66 1 30 29 14 1 293 66 1 ## -Alan Ashby 315 81 7 24 38 39 14 3449 835 69 ## -Alvin Dav"
4415,unknown,## -Andre Dawson 496 141 20 65 78 37 11 5628 1575 225 ## -Andres Galarraga 321 87 10 39 42 30 2 396 101 12 ## -Alfredo Griffin 594 169 4 74 51 35 11 4408 1133 19 ## CRuns CRBI CWalks League Division PutOuts Assists Errors ## -Andy Allanson 30 29 14 A E 446 33 20 ## -Alan Ashby 321 414 375 N W 632 43 10 ## -Alvin Davis 224 266 263 A W 880 82 14 ## -Andre Dawson 828 838 354 N E 200 11 3 ## -Andres G
4416,unknown,## -Alfredo Griffin 501 336 194 A W 282 421 25 ## Salary NewLeague ## -Andy Allanson NA A ## -Alan Ashby 475.0 N ## -Alvin Davis 480.0 A ## -Andre Dawson 500.0 N ## -Andres Galarraga 91.5 N ## -Alfredo Griffin 750.0 A dim(Hitters) ## [1] 322 20 Prior to proceeding to any analysis we must check whether the data contains any missing values. T o do this we can use a combination of the commands sum() 
4417,unknown,values. T o do this we can use a combination of the commands sum() and is.na(). W e find that we have 59 missing Salary entries. T o remove these 59 rows we make use of the command na.omit() and then double-check. 86 CHAPTER 5. MODEL SEARCH METHODS sum( is.na( Hitters ) ) # checks for missing data in the entire dataframe. ## [1] 59 sum( is.na( Hitters$Salary ) ) # checks for missing data in the re
4418,unknown,## [1] 59 sum( is.na( Hitters$Salary ) ) # checks for missing data in the response only. ## [1] 59 Hitters = na.omit( Hitters ) # removes entries to the dataframe with any data # missing. dim( Hitters ) ## [1] 263 20 sum( is.na( Hitters ) ) # check that there is now no ## [1] 0 # missing data in the dataframe. 5.4.2 Best Subset Selection Now we are ready to start with best subset selection. There 
4419,unknown,"Now we are ready to start with best subset selection. There are different packages in R which perform best subset as well as stepwise selections. W e will use the function regsubsets() from library leaps (again install.packages('leaps') is needed if not already installed). regsubsets() essentially performs step 2 of the algorithm presented in Section 5.1, although yields results that make step 3 r"
4420,unknown,"yields results that make step 3 relatively straightforward. Note that the . part of Salary~. indicates that we wish to consider all possible predictors in the subset selection method. W e will store the results from best subset in an object called best (any name would do) and sum- marize the results via summary(), which outputs the best models of different dimensionality . library(leaps) best = re"
4421,unknown,"library(leaps) best = regsubsets(Salary~., Hitters) summary(best) ## Subset selection object ## Call: regsubsets.formula(Salary ~ ., Hitters) ## 19 Variables (and intercept) ## Forced in Forced out ## AtBat FALSE FALSE ## Hits FALSE FALSE ## HmRun FALSE FALSE ## Runs FALSE FALSE ## RBI FALSE FALSE ## Walks FALSE FALSE ## Years FALSE FALSE ## CAtBat FALSE FALSE ## CHits FALSE FALSE ## CHmRun FALSE "
4422,unknown,## CHmRun FALSE FALSE ## CRuns FALSE FALSE 5.4. PRACTICAL DEMONSTRA TION 87 ## CRBI FALSE FALSE ## CWalks FALSE FALSE ## LeagueN FALSE FALSE ## DivisionW FALSE FALSE ## PutOuts FALSE FALSE ## Assists FALSE FALSE ## Errors FALSE FALSE ## NewLeagueN FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: exhaustive ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI
4423,unknown,"## 1 ( 1 ) "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""*"" ## 2 ( 1 ) "" "" ""*"" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""*"" ## 3 ( 1 ) "" "" ""*"" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""*"" ## 4 ( 1 ) "" "" ""*"" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""*"" ## 5 ( 1 ) ""*"" ""*"" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""*"" ## 6 ( 1 ) ""*"" ""*"" "" "" "" "" "" "" ""*"" "" "" "" "" "" "" "" "" "" "" ""*"" ## 7 ( 1 ) "" "" ""*"" "" "" "" "" "" "" ""*"" "" "" ""*"" ""*"""
4424,unknown,"## 8 ( 1 ) ""*"" ""*"" "" "" "" "" "" "" ""*"" "" "" "" "" "" "" ""*"" ""*"" "" "" ## CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN ## 1 ( 1 ) "" "" "" "" "" "" "" "" "" "" "" "" "" "" ## 2 ( 1 ) "" "" "" "" "" "" "" "" "" "" "" "" "" "" ## 3 ( 1 ) "" "" "" "" "" "" ""*"" "" "" "" "" "" "" ## 4 ( 1 ) "" "" "" "" ""*"" ""*"" "" "" "" "" "" "" ## 5 ( 1 ) "" "" "" "" ""*"" ""*"" "" "" "" "" "" "" ## 6 ( 1 ) "" "" "" "" ""*"" ""*"" "" "" "" "" "" "" ## 7 ( 1 ) "" "" "" "" ""*"" ""*"" "" "" "" "" "" "" ## 8 ("
4425,unknown,"## 7 ( 1 ) "" "" "" "" ""*"" ""*"" "" "" "" "" "" "" ## 8 ( 1 ) ""*"" "" "" ""*"" ""*"" "" "" "" "" "" "" The asterisks *indicate variables which are included in model𝑀𝑘, 𝑘 = 1, ..., 19as discussed in the subset selection algorithm of Section 5.1. F or instance, we see that for the model with one predictor that variable is CRBI, while for the model with four predictors the selected variables are Hits, CRBI, DivisionW and Put"
4426,unknown,"are Hits, CRBI, DivisionW and PutOuts. As we see the summary() command displayed output up to the best model with 8 predictors; this is the default option in regsubsets(). T o change this we can use the extra argument nvmax. Below we re-run the command for all best models (19 in total). Now we also store the output of summary() in an object called results and further investigate what this contains"
4427,unknown,"results and further investigate what this contains via names(). As we can see there is a lot of useful information! Of particular use to us, we can see that it returns 𝑅2, RSS, adjusted- 𝑅2, 𝐶𝑝 and BIC. W e can easily extract these quantities; for instance below we see the 𝑅2 values for the 19 models. best = regsubsets(Salary~., data = Hitters, nvmax = 19) results = summary(best) names(results) ##"
4428,unknown,"## [1] ""which"" ""rsq"" ""rss"" ""adjr2"" ""cp"" ""bic"" ""outmat"" ""obj"" 88 CHAPTER 5. MODEL SEARCH METHODS results$rsq ## [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227 ## [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164 ## [15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159 Lets store these quantities as separate objects. W e can also stack them togethe"
4429,unknown,"matrix as done below and see the values. RSS = results$rss r2 = results$rsq Cp = results$cp BIC = results$bic Adj_r2 = results$adjr2 cbind(RSS, r2, Cp, BIC, Adj_r2) ## RSS r2 Cp BIC Adj_r2 ## [1,] 36179679 0.3214501 104.281319 -90.84637 0.3188503 ## [2,] 30646560 0.4252237 50.723090 -128.92622 0.4208024 ## [3,] 29249297 0.4514294 38.693127 -135.62693 0.4450753 ## [4,] 27970852 0.4754067 27.856220 "
4430,unknown,"## [5,] 27149899 0.4908036 21.613011 -144.07143 0.4808971 ## [6,] 26194904 0.5087146 14.023870 -147.91690 0.4972001 ## [7,] 25906548 0.5141227 13.128474 -145.25594 0.5007849 ## [8,] 25136930 0.5285569 7.400719 -147.61525 0.5137083 ## [9,] 24814051 0.5346124 6.158685 -145.44316 0.5180572 ## [10,] 24500402 0.5404950 5.009317 -143.21651 0.5222606 ## [11,] 24387345 0.5426153 5.874113 -138.86077 0.5225"
4431,unknown,"## [12,] 24333232 0.5436302 7.330766 -133.87283 0.5217245 ## [13,] 24289148 0.5444570 8.888112 -128.77759 0.5206736 ## [14,] 24248660 0.5452164 10.481576 -123.64420 0.5195431 ## [15,] 24235177 0.5454692 12.346193 -118.21832 0.5178661 ## [16,] 24219377 0.5457656 14.187546 -112.81768 0.5162219 ## [17,] 24209447 0.5459518 16.087831 -107.35339 0.5144464 ## [18,] 24201837 0.5460945 18.011425 -101.86391"
4432,unknown,"## [19,] 24200700 0.5461159 20.000000 -96.30412 0.5106270 W e know that RSS should steadily decrease and that 𝑅2 increase as we add predictors. Lets check that by plotting the values of RSS and 𝑅2. W e would like to have the two plots next to each other, so will make use of the command par(mfrow()), with the c(1, 2) indicating we require the plotting window to contain 1 row and 2 columns, before c"
4433,unknown,"plot(). par(mfrow = c(1, 2)) plot(RSS, xlab = ""Number of Predictors"", ylab = ""RSS"", type = ""l"", lwd = 2) plot(r2, xlab = ""Number of Predictors"", ylab = ""R-square"", type = ""l"", lwd = 2) 5.4. PRACTICAL DEMONSTRA TION 89 2 4 6 8 10 −5 −4 −3 −2 −1 0 Log Lambda Coefficients 8 8 8 8 8 Results are as expected. Above the argument type = 'l' is used for the lines to appear (otherwise by default R would sim"
4434,unknown,"(otherwise by default R would simply plot the points) and the argument lwd controls the thickness of the lines. Plots in R are very customizable (type?parto see all available options). Now let us find how many predictors are included in the optimal models under 𝐶 𝑝, BIC and adjusted-𝑅2 and produce some plots illustrating this information. which.min(Cp) ## [1] 10 which.min(BIC) ## [1] 6 which.max(A"
4435,unknown,"## [1] 11 par(mfrow = c(1, 3)) plot(Cp, xlab = ""Number of Predictors"", ylab = ""Cp"", type = 'l', lwd = 2) points(10, Cp[10], col = ""red"", cex = 2, pch = 8, lwd = 2) plot(BIC, xlab = ""Number of Predictors"", ylab = ""BIC"", type = 'l', lwd = 2) points(6, BIC[6], col = ""red"", cex = 2, pch = 8, lwd = 2) plot(Adj_r2, xlab = ""Number of Predictors"", ylab = ""Adjusted RSq"", type = ""l"", lwd = 2) points(11, Adj"
4436,unknown,"90 CHAPTER 5. MODEL SEARCH METHODS 5 10 15 20 40 60 80 100 Number of Predictors Cp 5 10 15 −150 −130 −110 −90 Number of Predictors BIC 5 10 15 0.35 0.40 0.45 0.50 Number of Predictors Adjusted RSq 𝐶 𝑝 and adjusted- 𝑅2 select 10 and 11 predictors respectively , while the optimal BIC model has only 6 predictors. In the code above we use the command points() to highlight the optimal model; the first "
4437,unknown,"optimal model; the first two arguments of this command correspond to x-axis and y-axis co- ordinates. So, for example, we know that the optimal 𝐶𝑝 model is the one with 10 predictors; therefore the first argument is set to 10 and the second argument is set to Cp[10] (i.e., 𝐶𝑝 at its minimum). The regsubsets() in-built function for plotting the results is very convenient when the output from summar"
4438,unknown,"output from summary() is diﬀicult to read due to there being many possible predictors. Below we see the visualisation for the BIC models. Remark: Because previously we used par(mfrow(1,3)) we have to either close the plotting panel by typing dev.off() (this returns the window plot to the default state), or respecifying the window splitting using par( mfrow = c(1,1) ). plot(best, scale = ""bic"") sea"
4439,unknown,"par( mfrow = c(1,1) ). plot(best, scale = ""bic"") seatpos.pr Variances 0 1 2 3 4 5 The top row corresponds to the best model, while the bottom row to the worst model accord- ing to the chosen criterion. White squares correspond to variables that are excluded under each model. Finally , we extract the model coeﬀicients for any model we choose via the command coef. F or instance, the best models unde"
4440,unknown,"coef(best,10) # Cp ## (Intercept) AtBat Hits Walks CAtBat CRuns ## 162.5354420 -2.1686501 6.9180175 5.7732246 -0.1300798 1.4082490 5.4. PRACTICAL DEMONSTRA TION 91 ## CRBI CWalks DivisionW PutOuts Assists ## 0.7743122 -0.8308264 -112.3800575 0.2973726 0.2831680 coef(best,6) # BIC ## (Intercept) AtBat Hits Walks CRBI DivisionW ## 91.5117981 -1.8685892 7.6043976 3.6976468 0.6430169 -122.9515338 ## P"
4441,unknown,"## PutOuts ## 0.2643076 coef(best,11) # adj-Rsq ## (Intercept) AtBat Hits Walks CAtBat CRuns ## 135.7512195 -2.1277482 6.9236994 5.6202755 -0.1389914 1.4553310 ## CRBI CWalks LeagueN DivisionW PutOuts Assists ## 0.7852528 -0.8228559 43.1116152 -111.1460252 0.2894087 0.2688277 5.4.3 F orward Stepwise Selection W e can use the same function to also perform forward or backward selection by only chang"
4442,unknown,"one of its arguments. F orward selection can be implemented in the following way . fwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = ""forward"") summary(fwd) ## Subset selection object ## Call: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = ""forward"") ## 19 Variables (and intercept) ## Forced in Forced out ## AtBat FALSE FALSE ## Hits FALSE FALSE ## HmRun FALSE FA"
4443,unknown,## Runs FALSE FALSE ## RBI FALSE FALSE ## Walks FALSE FALSE ## Years FALSE FALSE ## CAtBat FALSE FALSE ## CHits FALSE FALSE ## CHmRun FALSE FALSE ## CRuns FALSE FALSE ## CRBI FALSE FALSE ## CWalks FALSE FALSE ## LeagueN FALSE FALSE ## DivisionW FALSE FALSE ## PutOuts FALSE FALSE ## Assists FALSE FALSE ## Errors FALSE FALSE ## NewLeagueN FALSE FALSE 92 CHAPTER 5. MODEL SEARCH METHODS ## 1 subsets o
4444,unknown,"## 1 subsets of each size up to 19 ## Selection Algorithm: forward ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI ## 1 ( 1 ) "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""*"" ## 2 ( 1 ) "" "" ""*"" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""*"" ## 3 ( 1 ) "" "" ""*"" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""*"" ## 4 ( 1 ) "" "" ""*"" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""*"" ## 5 ( 1 ) ""*"" ""*"" "" "" "" "
4445,unknown,"## 6 ( 1 ) ""*"" ""*"" "" "" "" "" "" "" ""*"" "" "" "" "" "" "" "" "" "" "" ""*"" ## 7 ( 1 ) ""*"" ""*"" "" "" "" "" "" "" ""*"" "" "" "" "" "" "" "" "" "" "" ""*"" ## 8 ( 1 ) ""*"" ""*"" "" "" "" "" "" "" ""*"" "" "" "" "" "" "" "" "" ""*"" ""*"" ## 9 ( 1 ) ""*"" ""*"" "" "" "" "" "" "" ""*"" "" "" ""*"" "" "" "" "" ""*"" ""*"" ## 10 ( 1 ) ""*"" ""*"" "" "" "" "" "" "" ""*"" "" "" ""*"" "" "" "" "" ""*"" ""*"" ## 11 ( 1 ) ""*"" ""*"" "" "" "" "" "" "" ""*"" "" "" ""*"" "" "" "" "" ""*"" ""*"" ## 12 ( 1 ) ""*"" ""*"" "" "" ""*"" "" "" ""*"" "" "" ""*"" "
4446,unknown,"## 13 ( 1 ) ""*"" ""*"" "" "" ""*"" "" "" ""*"" "" "" ""*"" "" "" "" "" ""*"" ""*"" ## 14 ( 1 ) ""*"" ""*"" ""*"" ""*"" "" "" ""*"" "" "" ""*"" "" "" "" "" ""*"" ""*"" ## 15 ( 1 ) ""*"" ""*"" ""*"" ""*"" "" "" ""*"" "" "" ""*"" ""*"" "" "" ""*"" ""*"" ## 16 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" "" "" ""*"" ""*"" "" "" ""*"" ""*"" ## 17 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" "" "" ""*"" ""*"" "" "" ""*"" ""*"" ## 18 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" "" "" ""*"" ""*"" ## 19 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" "
4447,unknown,"## CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN ## 1 ( 1 ) "" "" "" "" "" "" "" "" "" "" "" "" "" "" ## 2 ( 1 ) "" "" "" "" "" "" "" "" "" "" "" "" "" "" ## 3 ( 1 ) "" "" "" "" "" "" ""*"" "" "" "" "" "" "" ## 4 ( 1 ) "" "" "" "" ""*"" ""*"" "" "" "" "" "" "" ## 5 ( 1 ) "" "" "" "" ""*"" ""*"" "" "" "" "" "" "" ## 6 ( 1 ) "" "" "" "" ""*"" ""*"" "" "" "" "" "" "" ## 7 ( 1 ) ""*"" "" "" ""*"" ""*"" "" "" "" "" "" "" ## 8 ( 1 ) ""*"" "" "" ""*"" ""*"" "" "" "" "" "" "" ## 9 ( 1 ) ""*"" "" "" ""*"" ""*"""
4448,unknown,"## 10 ( 1 ) ""*"" "" "" ""*"" ""*"" ""*"" "" "" "" "" ## 11 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" "" "" "" "" ## 12 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" "" "" "" "" ## 13 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" "" "" ## 14 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" "" "" ## 15 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" "" "" ## 16 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" "" "" ## 17 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ## 18 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ## 19 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" "
4449,unknown,"The analysis will then be essentially identical to the previous analysis for best subset selection. One interesting thing to check is whether the results from best subset and forward stepwise selection are in agreement (we know that this will not necessarily be the case). F or example, let’s have a look at the models with 6 and 7 predictors. 5.4. PRACTICAL DEMONSTRA TION 93 coef(best, 6) ## (Inter"
4450,unknown,"5.4. PRACTICAL DEMONSTRA TION 93 coef(best, 6) ## (Intercept) AtBat Hits Walks CRBI DivisionW ## 91.5117981 -1.8685892 7.6043976 3.6976468 0.6430169 -122.9515338 ## PutOuts ## 0.2643076 coef(fwd, 6) ## (Intercept) AtBat Hits Walks CRBI DivisionW ## 91.5117981 -1.8685892 7.6043976 3.6976468 0.6430169 -122.9515338 ## PutOuts ## 0.2643076 coef(best, 7) ## (Intercept) Hits Walks CAtBat CHits CHmRun ##"
4451,unknown,"## 79.4509472 1.2833513 3.2274264 -0.3752350 1.4957073 1.4420538 ## DivisionW PutOuts ## -129.9866432 0.2366813 coef(fwd, 7) ## (Intercept) AtBat Hits Walks CRBI CWalks ## 109.7873062 -1.9588851 7.4498772 4.9131401 0.8537622 -0.3053070 ## DivisionW PutOuts ## -127.1223928 0.2533404 The results for the model with 6 predictors are in agreement (which is good to know because this would be one of the "
4452,unknown,"this would be one of the three models one would likely select), but as we see the results for the models with 7 predictors are different. 5.4.4 Best Subset Selection: V alidation In this section, we consider applying best subset selection not based on model selection criteria but using the validation approach. As we know, for validation (and cross-validation) we need to be able to predict from our"
4453,unknown,"models. Unfortunately , regsubsets() doesn’t have its own built-in function for that (most similar types of R functions have). Y ou are welcome to try writing your own code for this step, but it is beyond the scope of the course. F or the purposes of performing subset selection by validation in this course, we will just make use of the functionpredict.regsubsetsbelow which will do the predictions "
4454,unknown,"which will do the predictions for us. The important aspects of the function are the arguments. object should be the result of a call to the function regsubsets() (for example the object best in Section 5.4.2 or fwd in Section 5.4.3), newdata should be a dataframe of data at which we wish to predict at, and id indicates that we wish to use the resulting model from regsubsets with id number of predi"
4455,unknown,"regsubsets with id number of predictors (so in this case could range from 1,2,…19). The output is the response prediction for each combination of inputs. Note that if you wish to use this function in your own code you will need to define it by copy and pasting this code segment. 94 CHAPTER 5. MODEL SEARCH METHODS predict.regsubsets = function(object, newdata, id, ...){ form = as.formula(object$cal"
4456,unknown,"mat = model.matrix(form, newdata) coefi = coef(object, id = id) xvars = names(coefi) mat[, xvars]%*%coefi } The validation approach is based on splitting the data once into a training sample and a validation or testing sample. W e also need to decide how to split the data. A common approach is to use 2/3 of the sample for training and 1/3 of the sample for testing (although it does not have to be "
4457,unknown,"Note the first line: set.seed. If we removed this command, then every time we ran the following code we could get different results (for example, variables selected) due to the effect of the random sample (do feel free to try this, or alternatively changing the seed number). set.seed ensures a particular random state for any particular integer value, thus making the code reproducible (i.e. the sam"
4458,unknown,"set.seed(10) # for the results to be reproducible dim(Hitters) ## [1] 263 20 training.obs = sample(1:263, 175) Hitters.train = Hitters[training.obs, ] Hitters.test = Hitters[-training.obs, ] dim(Hitters.train) ## [1] 175 20 dim(Hitters.test) ## [1] 88 20 V alidation is then executed as follows. W e first applyregsubsets() to the training set. By doing so note how we are only using the training dat"
4459,unknown,"in Section 5.1. best.val = regsubsets(Salary~., data = Hitters.train, nvmax = 19) Then we create an empty vector called val.error where we will store the 19 (because we have 19 models) validation MSEs. W e then use a for loop inside which we (i) generate predictions using predict.regsubsets() and (ii) calculate the validation MSEs. At the end we just locate which model produces the lowest MSE. The"
4460,unknown,"val.error<-c() for(i in 1:19){ pred = predict.regsubsets(best.val, Hitters.test, i) val.error[i] = mean((Hitters.test$Salary - pred)^2) } 5.4. PRACTICAL DEMONSTRA TION 95 val.error ## [1] 199324.8 184215.9 196949.2 192709.1 180806.0 179388.2 178929.9 176754.4 ## [9] 177029.2 163013.2 165134.5 164986.7 166072.1 164983.9 164890.7 165144.4 ## [17] 164983.7 164962.6 164959.9 which.min(val.error) ## [1"
4461,unknown,"In this instance we see that the model with 10 predictors is selected as the best. This was also the model selected by 𝐶𝑝 in Section 5.4.2. 5.4.5 What about inference? Note that the aim of subset selection is to find which predictors should be included within our model, not to identify the specific model (including coeﬀicient values). Therefore, it is important to note that there is a final step t"
4462,unknown,"important to note that there is a final step to fitting a model by validation, and that is to train the same 10-predictor model to the entire data. This will yield different coeﬀicient values to the current coeﬀicients. The current coeﬀicients should not be used because these were “learned” from the reduced training sample (that is, we didn’t use all of the available data). F or the purpose of fit"
4463,unknown,"lm() in R. coef(best.val, 10) # Check which variables to use in the lm. ## (Intercept) AtBat Hits Runs Walks CAtBat ## -21.6323918 -0.7961450 4.5874937 -3.1211583 7.5830718 -0.1058408 ## CRuns CRBI CWalks DivisionW PutOuts ## 1.9895304 0.3010303 -1.1736748 -121.3385526 0.1989660 ls10 = lm(Salary ~ AtBat + Hits + Runs + Walks + CAtBat + CRuns + CRBI + CWalks + Division + PutOuts, data = Hitters) As"
4464,unknown,"As we have seen the output from regsubsets() relates to the model selection procedure; i.e. we get the values of 𝐶 𝑝, BIC, adjusted-𝑅2. W e can also extract model specific regression coeﬀicients via the command coef(). However, the lm object above is required for quantities such as standard errors, confidence and prediction intervals etc., for example, recalling that summary() provides a summary ,"
4465,unknown,"summary() provides a summary , and confint() provides confidence intervals. In addition, we can now use this model for prediction purposes using the built-in predict function for linear models (which is distinct to the predict.regsubsets function we defined above). summary(ls10) ## ## Call: ## lm(formula = Salary ~ AtBat + Hits + Runs + Walks + CAtBat + ## CRuns + CRBI + CWalks + Division + PutOut"
4466,unknown,## 96 CHAPTER 5. MODEL SEARCH METHODS ## Residuals: ## Min 1Q Median 3Q Max ## -927.60 -177.70 -30.03 142.91 1986.30 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 144.30288 66.52508 2.169 0.031007 * ## AtBat -1.84992 0.52681 -3.512 0.000528 *** ## Hits 7.64658 1.82552 4.189 0.0000388 *** ## Runs -2.92542 2.33580 -1.252 0.211576 ## Walks 6.28202 1.68895 3.719 0.000246 *
4467,unknown,## CAtBat -0.12749 0.05751 -2.217 0.027517 * ## CRuns 1.45072 0.41373 3.506 0.000537 *** ## CRBI 0.66823 0.20074 3.329 0.001002 ** ## CWalks -0.80479 0.26367 -3.052 0.002515 ** ## DivisionW -117.14522 39.33350 -2.978 0.003182 ** ## PutOuts 0.27111 0.07406 3.661 0.000307 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 312.8 on 252 degrees o
4468,unknown,"## Multiple R-squared: 0.5375, Adjusted R-squared: 0.5191 ## F-statistic: 29.29 on 10 and 252 DF, p-value: < 0.00000000000000022 confint(ls10, 1:11) ## 2.5 % 97.5 % ## (Intercept) 13.2868988 275.31885341 ## AtBat -2.8874302 -0.81240354 ## Hits 4.0513571 11.24181209 ## Runs -7.5256017 1.67476709 ## Walks 2.9557696 9.60827296 ## CAtBat -0.2407504 -0.01423684 ## CRuns 0.6359135 2.26553592 ## CRBI 0.2"
4469,unknown,"## CWalks -1.3240670 -0.28550387 ## DivisionW -194.6094923 -39.68094337 ## PutOuts 0.1252518 0.41696960 test.data <- data.frame( ""AtBat"" = 322, ""Hits"" = 90, ""HmRun"" = 14, ""Runs"" = 40, ""RBI"" = 22, ""Walks"" = 40, ""Years"" = 4, ""CAtBat"" = 3000, ""CHits"" = 830, ""CHMRun"" = 100, ""CRuns"" = 250, ""CRBI"" = 600, ""CWalks"" = 300, ""League"" = ""N"", ""Division"" = ""W"", ""PutOuts"" = 600, ""Assists"" = 60, ""Errors"" = 7, ""ne"
4470,unknown,"""newLeague"" = ""N"" ) predict( ls10, newdata = test.data ) 5.4. PRACTICAL DEMONSTRA TION 97 ## 1 ## 556.3127 5.4.6 V ariance of selection based on validation When using validation and cross-validation for best subset selection (or any stepwise method) it is important to understand that results depend on the random splitting between training and testing samples. This is generally a drawback which is "
4471,unknown,"and testing samples. This is generally a drawback which is not discussed a lot, especially for the simple validation approach. The below code implements the validation approach presented previously but now starting from 50 different random seeds, storing the number of predictors in the selected model each time. min.valid = c() for(n in 1:50){ set.seed(n) training.obs = sample(1:263, 175) Hitters.t"
4472,unknown,"Hitters.train = Hitters[training.obs, ] Hitters.test = Hitters[-training.obs, ] best.val = regsubsets(Salary~., data = Hitters.train, nvmax = 19) val.error<-c() for(i in 1:19){ pred = predict.regsubsets(best.val, Hitters.test, i) val.error[i] = mean((Hitters.test$Salary - pred)^2) } val.error min.valid[n] = which.min(val.error) } The histogram below shows that the final selection of the best model"
4473,unknown,"} The histogram below shows that the final selection of the best model can vary a lot, even in terms of the number of predictors! Note that in addition, the histogram doesn’t tell the whole story because each time, say , a 10-predictor model is chosen, it may be choosing different predictors. In this case we can see that the validation method is most often choosing a model with 8, 9 or 10 predicto"
4474,unknown,"hist(min.valid, col = 3, breaks = seq( from = 0.5, to = 19.5, length = 20 ), xlab = 'Number of Predictors', main = 'Best Subset Selection with validation') abline(v = mean(min.valid), col = 2, lwd = 4) legend('topright', legend=c('Average selection'),bty = 'n', lty = 1, lwd = 4, col = 2) 98 CHAPTER 5. MODEL SEARCH METHODS Best Subset Selection with validation Number of Predictors Frequency 0 5 10 "
4475,unknown,"Frequency 0 5 10 15 20 0 2 4 6 8 10 12 Average selection 5.4.7 Cross-validation Now lets turn our attention to the K -fold cross-validation (CV) approach. This procedure is generally preferable to simple validation because it utilizes the entire sample for training as well as for testing. In contrast to the application of validation in Section 5.4.4, where we used the training data for both steps "
4476,unknown,"for both steps 2 and 3 of the algorithm presented in Section5.1, we here use the full dataset in the initial regsubsets() command (step 2), and cross-validation for step 3 only . Therefore step 2 proceeds as before: best = regsubsets(Salary~., data = Hitters, nvmax = 19) Since step 3 involves fitting models with specified predictors, we need to once again use the lm() function in R. Since this wou"
4477,unknown,"lm() function in R. Since this would involve manually specifying 19 models using lm(), we will instead demonstrate how to obtain a K -fold cross-validation error for just 3 of the models suggested by regsubsets() only . In particular, we are going to answer the following question…which of the different models chosen under 𝐶𝑝, BIC and adjusted 𝑅2 respectively in Section 5.4.2 should we choose? Reca"
4478,unknown,"𝐶𝑝 yielded the model with 10 variables, BIC the one with 6, and adjusted 𝑅2 the one with 11. Therefore, we will calculate the K -fold cross-validation error of the models with 6, 10 and 11 variables suggested by regsubsets() only . Let’s remind ourselves of the variables in those three models: coef(best,10) # Cp ## (Intercept) AtBat Hits Walks CAtBat CRuns ## 162.5354420 -2.1686501 6.9180175 5.773"
4479,unknown,"## CRBI CWalks DivisionW PutOuts Assists ## 0.7743122 -0.8308264 -112.3800575 0.2973726 0.2831680 5.4. PRACTICAL DEMONSTRA TION 99 coef(best,6) # BIC ## (Intercept) AtBat Hits Walks CRBI DivisionW ## 91.5117981 -1.8685892 7.6043976 3.6976468 0.6430169 -122.9515338 ## PutOuts ## 0.2643076 coef(best,11) # adj-Rsq ## (Intercept) AtBat Hits Walks CAtBat CRuns ## 135.7512195 -2.1277482 6.9236994 5.6202"
4480,unknown,"## CRBI CWalks LeagueN DivisionW PutOuts Assists ## 0.7852528 -0.8228559 43.1116152 -111.1460252 0.2894087 0.2688277 Linear models based on the whole training set would be constructed as follows: ls10 = lm(Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + Division + PutOuts + Assists, data = Hitters) ls6 = lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, data = Hitters)"
4481,unknown,"data = Hitters) ls11 = lm(Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + + League + Division + PutOuts + Assists, data = Hitters) Now we turn to calculating K -fold cross-validation errors for these models. A first question is how many folds to use. Common choices in practice are 5 and 10, leading to 5-fold and 10-fold CV. In this example we will use 10 folds. Of course, often th"
4482,unknown,"10-fold CV. In this example we will use 10 folds. Of course, often the sample size n will not be perfectly divisable by K (as in our case where n=263 and K =10). This does not matter, since it is not a problem if some folds have fewer observations. Lets see how we can create folds in R. First we need to form an indicator vector with length equal to 263 containing the numbers 1,2,3,4,5,6,7,8,9,10 r"
4483,unknown,"equal to 263 containing the numbers 1,2,3,4,5,6,7,8,9,10 roughly the same number of times. This can be done by combining the commandscut()and seq()in R as below. The command table() then tabulates how many of each integer are present in the resulting vector. k = 10 folds = cut(1:263, breaks=10, labels=FALSE) folds ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [26] 1 1 2 2 2 2 2 2 2 2"
4484,unknown,## [51] 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [76] 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 ## [101] 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 ## [126] 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 ## [151] 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ## [176] 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 ## [201] 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9
4485,unknown,"## [226] 9 9 9 9 9 9 9 9 9 9 9 10 10 10 10 10 10 10 10 10 10 10 10 10 10 ## [251] 10 10 10 10 10 10 10 10 10 10 10 10 10 100 CHAPTER 5. MODEL SEARCH METHODS table(folds) ## folds ## 1 2 3 4 5 6 7 8 9 10 ## 27 26 26 26 27 26 26 26 26 27 It is then very important to randomly re-shuffle this vector, because we do not want the fold creation procedure to be deterministic. W e can do this very easily in"
4486,unknown,command sample(). set.seed(2) folds = sample(folds) folds ## [1] 8 10 8 7 3 5 3 6 9 2 3 4 5 9 7 2 10 5 7 6 2 10 8 6 4 ## [26] 1 5 8 7 5 10 4 5 10 8 8 1 5 9 2 2 10 8 10 1 1 7 9 2 6 ## [51] 5 8 3 4 8 2 1 6 8 10 5 7 2 3 7 7 1 1 7 3 9 8 4 3 6 ## [76] 7 5 5 10 2 5 6 2 1 4 5 9 1 3 3 8 3 10 8 6 1 2 6 1 4 ## [101] 9 6 4 10 9 1 9 7 9 8 4 8 6 1 10 2 10 10 7 9 3 9 4 4 5 ## [126] 6 10 3 9 2 8 8 6 2 2 6 1 9 1 
4487,unknown,"## [151] 1 10 10 1 3 3 3 7 3 4 2 9 4 6 8 9 7 2 1 7 4 3 5 7 8 ## [176] 4 9 7 9 5 2 1 1 6 4 4 5 10 6 5 1 10 9 7 3 1 5 2 7 7 ## [201] 4 6 7 10 3 6 5 10 4 9 9 5 7 7 2 6 9 5 5 3 2 3 5 5 9 ## [226] 3 6 1 1 9 8 6 8 9 10 4 4 10 10 5 2 3 3 6 3 4 10 9 6 2 ## [251] 1 8 6 7 8 4 2 8 1 10 2 7 4 Finally , we need to create amatrix this time with K =10 rows and 3 columns. The validation MSEs of the folds for mode"
4488,unknown,"MSEs of the folds for model ls10 will be stored in the first column of this matrix, the validation MSEs of the folds for model ls6 will be stored in the second column of this matrix, and the validation MSEs of the folds for model ls11 will be stored in the final column. cv.errors = matrix(NA, nrow = k, ncol = 3, dimnames = list(NULL, c(""ls10"", ""ls6"", ""ls11""))) cv.errors ## ls10 ls6 ls11 ## [1,] NA"
4489,unknown,"## [2,] NA NA NA ## [3,] NA NA NA ## [4,] NA NA NA ## [5,] NA NA NA ## [6,] NA NA NA ## [7,] NA NA NA ## [8,] NA NA NA ## [9,] NA NA NA ## [10,] NA NA NA Above NA means “not available” . The NA entries will be replaced with the MSEs as the loop below progresses. The part dimnames = list(NULL, c(""ls10"", ""ls6"", ""ls11"")) just as- signs no names to the rows and names the columns ""ls10"", ""ls6"" and ""ls1"
4490,unknown,"5.4. PRACTICAL DEMONSTRA TION 101 Now for calculating the cross-validation errors we will need a for loop to loop over the folds. The important part is to understand what data = Hitters[folds!=i, ] is doing. In R != means “not equal” . So for example when i=1 we discard the rows of Hitter for which the corresponding fold vector equals 1. W e then predict only for the rows for which the fold vector"
4491,unknown,"vector equals i (in Hitters[folds==i, ] and Hitters$Salary[folds==i]). W e then store each validation MSE in the appropriate row and column of cv.errors. for(i in 1:k){ ls10 = lm(Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + Division + PutOuts + Assists, data = Hitters[folds!=i, ] ) ls6 = lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, data = Hitters[folds!=i, ]) l"
4492,unknown,"ls11 = lm(Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + + League + Division + PutOuts + Assists, data = Hitters[folds!=i, ]) pred10 <- predict( ls10, newdata = Hitters[folds==i, ] ) pred6 <- predict( ls6, newdata = Hitters[folds==i, ] ) pred11 <- predict( ls11, newdata = Hitters[folds==i, ] ) cv.errors[i,] = c( mean( (Hitters$Salary[folds==i]-pred10)^2), mean( (Hitters$Salary[fo"
4493,unknown,"mean( (Hitters$Salary[folds==i]-pred11)^2) ) } cv.errors ## ls10 ls6 ls11 ## [1,] 132495.25 127446.51 130581.86 ## [2,] 111992.83 110608.25 109426.81 ## [3,] 87822.97 98247.02 87132.03 ## [4,] 113161.24 98812.43 112384.13 ## [5,] 225490.54 231835.72 227007.73 ## [6,] 83622.26 46935.72 85460.28 ## [7,] 90430.09 95275.05 91963.45 ## [8,] 100105.45 106254.47 98519.80 ## [9,] 97701.55 97551.93 104063."
4494,unknown,"## [9,] 97701.55 97551.93 104063.92 ## [10,] 48005.26 63602.55 46157.69 Finally , we can take the average of the MSEs in each column of the matrix to obtain the K -fold cross-validation error for each of the proposed models. cv.mean.errors <- colMeans(cv.errors) cv.mean.errors ## ls10 ls6 ls11 ## 109082.7 107657.0 109269.8 W e may choosels6 as it has slightly smaller average cross-validation error"
4495,unknown,"W e may choosels6 as it has slightly smaller average cross-validation error, although in this case there is not that much in it. Since we also tend to favour models with fewer predictors 102 CHAPTER 5. MODEL SEARCH METHODS as they tend to be more interpretable, we would probably choose ls6. Chapter 6 Shrinkage Methods 6.1 Motivation Issue due to multicollinearity Recall that multicollinearity refe"
4496,unknown,"Recall that multicollinearity refers to a situation when two or more predictor variables in the linear regression model are highly (linearly) correlated. The least square estimates will become “unstable” . Here we explore the issue via the following simulation study . This is a simulation study because we simulate data based on a known model, add some random noise, and then investigate how differe"
4497,unknown,"random noise, and then investigate how different methods use this data to estimate the regression parameters. W e can then evaluate the performance of the method based on how close the estimated parameter values are to the true values. W e assume a correct model of 𝑦 = 𝑥1 − 𝑥2 + 𝜖 so that we have true generating 𝛽0 = 0, 𝛽1 = 1 and 𝛽2 = −1, and noise term 𝜖 ∼ 𝒩(0, 1). In other words, for any specif"
4498,unknown,"random noise. Rather than sampling the training data independently , however, we assume a high level of collinearity between 𝑥1 and 𝑥2 by simulating 𝑥1 ∼ 𝒩(0, 1) and 𝑥2 ∼ 𝒩(0.9𝑥 1, 0.22). In other words, 𝑥2 is a scaling of 𝑥1 with just a small amount of random noise added - there is definitely a strong correlation between the values of those predictors! F eel free to look through the following cod"
4499,unknown,F eel free to look through the following code to understand how the simulation study is working and the plots below are generated! Note that (𝑋𝑇 𝑋)−1𝑋𝑇 𝑌 is the regression coeﬀicient estimates for LS. # Number of iterations for the simulation study. iter <- 50 # Number of training points. n <- 10 103 104 CHAPTER 6. SHRINKAGE METHODS # Empty matrix of NAs for the regression coefficients for LS b_LS
4500,unknown,"b_LS <- matrix( NA, nrow = 2, ncol = iter ) # For each iteration... for( i in 1:iter ){ # As we can see x2 is highly correlated with x1 - COLLINEARITY! x1 <- rnorm( n ) x2 <- 0.9 * x1 + rnorm( n, 0, 0.2 ) # Design matrix. x <- cbind( x1, x2 ) # y = x1 - x2 + e, where error e is N(0,1) y <- x1 - x2 + rnorm( n ) # LS regression components - the below calculation obtains them given the # training dat"
4501,unknown,"# training data and response generated in this iteration, and stores # those estimates in the relevant column of the matrix b_LS. b_LS[,i]<- solve( t(x) %*% x ) %*% t(x) %*% y } # Now we plot the results. # Plot the LS estimates for beta_1. plot( b_LS[1,], col = 'blue', pch = 16, ylim = c( min(b_LS), max(b_LS) ), ylab = 'Estimated coefficients', xlab = 'Least squares', bty ='l' ) # Add the LS esti"
4502,unknown,"# Add the LS estimates for beta_2 in a different colour. points( b_LS[2,], col = 'red', pch = 17 ) # Add horizontal lines representing the true values of beta_1 and beta_2. abline( a = 1, b = 0, col = 'blue' ) # Plot the true value of beta_1 = 1. abline( a = -1, b = 0, col='red' ) # Plot the true value of beta_2 = -1. # Add a legend to this plot. legend( 'bottomleft', cex = 1.3, text.col=c('blue',"
4503,unknown,"legend = c( expression( beta[1] ),expression( beta[2] ) ), bty = 'n' ) # Add a title over the top of both plots. mtext( expression( 'High correlations with'~n == 10~','~ p == 2 ), side = 3, line = -3, outer = TRUE ) 6.1. MOTIV A TION 105 0 10 20 30 40 50 −6 −4 −2 0 2 4 6 Least squares Estimated coefficientsβ1 β2 High correlations with n = 10 , p = 2 The figure plots the LS coeﬀicients for 50 diffe"
4504,unknown,"The figure plots the LS coeﬀicients for 50 different sample sets. W e can see that although the LS coeﬀicients are unbiased estimates they are very unstable (high variance) in the presence of multicollinearity . This will affect the prediction error. Issue due to 𝑝 is close to 𝑛 Another issue is when 𝑝 is close to 𝑛. W e investigate this case by running the simulation above again, but this time sa"
4505,unknown,"above again, but this time sampling both 𝑥1 and 𝑥2 from a standard normal distribution 𝒩(0, 1) (making the two predictors independent) and lowering the number of training points to 3. I have removed the detailed comments this time - try to see (and understand!) how it is different to the simulation above. iter <- 50 n <- 3 b_LS <- matrix( NA, nrow = 2, ncol = iter ) for( i in 1:iter ){ x1 <- rnorm"
4506,unknown,"x1 <- rnorm( n ) x2 <- rnorm( n ) x <- cbind( x1, x2 ) y <- x1 - x2 + rnorm( n ) b_LS[,i]<- solve( t(x) %*% x ) %*% t(x) %*% y } plot( b_LS[1,], col = 'blue', pch = 16, ylim = c( min(b_LS), max(b_LS) ), ylab = 'Estimated coefficients', xlab = 'Least squares', bty ='l' ) 106 CHAPTER 6. SHRINKAGE METHODS points( b_LS[2,], col = 'red', pch = 17 ) abline( a = 1, b = 0, col = 'blue' ) abline( a = -1, b"
4507,unknown,"abline( a = -1, b = 0, col='red' ) # Add a legend to this plot. legend( 'bottomleft', cex = 1.3, text.col=c('blue','red'), legend = c( expression( beta[1] ),expression( beta[2] ) ), bty = 'n' ) mtext( expression('No correlations with'~n == 3~','~ p == 2), side = 3, line = -3, outer = TRUE ) 0 10 20 30 40 50 −10 −5 0 5 Least squares Estimated coefficientsβ1 β2 No correlations with n = 3 , p = 2 LS "
4508,unknown,"LS estimates suffer again from high variance. Of course, having only 𝑛 = 3 training runs isn’t ideal in any situation, but it illustrates how LS estimates are affected when 𝑝 is close to 𝑛. Y ou may want to run the simulation above again but increase the number of training points and the number of predictors. Many modern machine learning applications suffer from both of these problems we have just"
4509,unknown,"explored - that is, 𝑝 can be large (and of similar magnitude to 𝑛) and it is very likely that there will be strong correlations among the many predictors! T o address these issues, one may adopt shrinkage methods. In previous lectures, we have seen discrete model search methods, where one option is to pick the best model based on information criteria (e.g. 𝐶𝑝, BIC, etc.). In this case, we generall"
4510,unknown,"model fit⏟ RSS + penalty on model dimensionality 6.2. RIDGE REGRESSION 107 across models of differing dimensionality (number of predictors). Shrinkage methods offer a continuous analogue (which is much faster). In this case we train only the full model, but the estimated regression coeﬀicients minimise a general solution of the form model fit⏟ RSS + penalty on size of coeﬀicients . This approach i"
4511,unknown,"This approach is called penalised or regularised regression. 6.2 Ridge Regression Recall that Least Squares (LS) produces estimates of 𝛽0, 𝛽1, 𝛽2, … , 𝛽𝑝 by minimising 𝑅𝑆𝑆 = 𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝛽0 − 𝑝 ∑ 𝑗=1 𝛽𝑗𝑥𝑖𝑗) 2 . On the other hand, ridge regression minimises the cost function 𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝛽0 − 𝑝 ∑ 𝑗=1 𝛽𝑗𝑥𝑖𝑗) 2 ⏟ ⏟ ⏟ ⏟ ⏟ ⏟ ⏟ ⏟ ⏟ ⏟ ⏟ ⏟ ⏟ model fit + 𝜆 𝑝 ∑ 𝑗=1 𝛽2 𝑗 ⏟ penalty = 𝑅𝑆𝑆 + 𝜆 𝑝 ∑ 𝑗=1 𝛽2 𝑗 , "
4512,unknown,"𝛽2 𝑗 , where 𝜆 ≥ 0 is a tuning parameter. The role of this parameter is crucial; it controls the trade-off between model fit and the size of the coeﬀicients. In fact, the estimates based on the ridge regression cost function is the solution to the least squares problem subject to the constraint ∑ 𝑝 𝑗=1 𝛽2 𝑗 ≤ 𝑐 (Hint: Lagrange Multiplier). The following figure is the geometric interpretation of th"
4513,unknown,"108 CHAPTER 6. SHRINKAGE METHODS The green contours represent the RSS in terms of different values of parameters of a linear regression model. Each contour is a connection of spots where the RSS is the same, centered with the (Ordinary) Least Squares estimate where the RSS is the lowest. Note that the LS estimate is the point where it best fits the training set (low-bias). The blue circle represen"
4514,unknown,"The blue circle represents the constraint ∑ 𝑝 𝑗=1 𝛽2 𝑗 ≤ 𝑐 . Unlike the LS estimates, the ridge estimates change as the size of the blue circle changes. It is simply where the circle meets the most outer contour. How ridge regression works is how we tune the size of the circle, which is via tuning 𝜆. F rom the ridge regression cost function, observe that: • when 𝜆 = 0 the penalty term has no effec"
4515,unknown,"• when 𝜆 = 0 the penalty term has no effect and the ridge solution is the same as the LS solution. • as 𝜆 gets larger naturally the penalty term gets larger; so in order to minimise theentire function (model fit + penalty) the regression coeﬀicients will necessarily get smal ler! • Unlike LS, ridge regression does not produce one set of coeﬀicients, it produces different sets of coeﬀicients for di"
4516,unknown,sets of coeﬀicients for different values of 𝜆! • As 𝜆 is increasing the coeﬀicients are shrunk towards 0 (for really large𝜆 the coeﬀicients are almost 0). T uning𝜆 is in fact balancing the bias and variance. Another way to view the ridge regression is via vector representation. That is the ridge estimates ̂𝛽𝑅 = (XTX + ￿I) −1 XTY shifts the XTX in the LS estimates ̂𝛽 = (XTX) −1 XTY by 𝜆𝐼 . Note in 
4517,unknown,"−1 XTY by 𝜆𝐼 . Note in the cases of “perfect” multicollinearity and 𝑛 < 𝑝 , XTX is singular (non- invertible) and shifting XTX by 𝜆𝐼 leads to an invertible matrix (XTX + ￿I). Demonstration in multicollinearity issue The following code conducts the same simulation study as in the motivation and adds ridge estimates for comparison. Note that (𝑋𝑇 𝑋 + 𝜆𝐼 )−1𝑋𝑇 𝑌 are the regression coeﬀicient esti- mat"
4518,unknown,"6.2. RIDGE REGRESSION 109 # Number of iterations for the simulation study. iter <- 50 # Number of training points. n <- 10 # lambda coefficient - feel free to explore this parameter! lambda <- 0.3 # Empty matrix of NAs for the regression coefficients, one for LS # and one for ridge. b_LS <- matrix( NA, nrow = 2, ncol = iter ) b_ridge <- matrix( NA, nrow = 2, ncol = iter ) # For each iteration... f"
4519,unknown,"# As we can see x2 is highly correlated with x1 - COLLINEARITY! x1 <- rnorm( n ) x2 <- 0.9 * x1 + rnorm( n, 0, 0.2 ) # Design matrix. x <- cbind( x1, x2 ) # y = x1 - x2 + e, where error e is N(0,1) y <- x1 - x2 + rnorm( n ) # LS regression components - the below calculation obtains them given the # training data and response generated in this iteration, and stores # those estimates in the relevant"
4520,unknown,"b_LS[,i]<- solve( t(x) %*% x ) %*% t(x) %*% y # Ridge regression components - notice that the only difference relative # to the LS estimates above is the addition of a lambda x identity matrix # inside the inverse operator. b_ridge[,i]<- solve( t(x) %*% x + lambda * diag(2) ) %*% t(x) %*% y } # Now we plot the results. First, split the plotting window # such that it has 1 row and 2 columns. par(mf"
4521,unknown,"# such that it has 1 row and 2 columns. par(mfrow=c(1,2)) # Plot the LS estimates for beta_1. plot( b_LS[1,], col = 'blue', pch = 16, ylim = c( min(b_LS), max(b_LS) ), ylab = 'Estimated coefficients', xlab = 'Least squares', bty ='l' ) 110 CHAPTER 6. SHRINKAGE METHODS # Add the LS estimates for beta_2 in a different colour. points( b_LS[2,], col = 'red', pch = 17 ) # Add horizontal lines represent"
4522,unknown,"# Add horizontal lines representing the true values of beta_1 and beta_2. abline( a = 1, b = 0, col = 'blue' ) # Plot the true value of beta_1 = 1. abline( a = -1, b = 0, col='red' ) # Plot the true value of beta_2 = -1. # Do the same for the ridge estimates. plot( b_ridge[1,], col = 'blue', pch = 16, ylim = c(min(b_LS), max(b_LS)), ylab = 'Estimated coefficients', xlab = 'Ridge', bty ='l' ) point"
4523,unknown,"points( b_ridge[2,], col = 'red', pch = 17 ) abline( a = 1, b = 0, col = 'blue' ) abline( a = -1, b = 0, col = 'red' ) # Add a legend to this plot. legend( 'bottomleft', cex = 1.3, text.col=c('blue','red'), legend = c( expression( beta[1] ),expression( beta[2] ) ), bty = 'n' ) # Add a title over the top of both plots. mtext( expression( 'High correlations with'~n == 10~','~ p == 2 ), side = 3, lin"
4524,unknown,"0 10 20 30 40 50 −4 −2 0 2 4 Least squares Estimated coefficients 0 10 20 30 40 50 −4 −2 0 2 4 Ridge Estimated coefficientsβ1 β2 High correlations with n = 10 , p = 2 W e can see that ridge regression introduces some bias but decreases the variance, which could result in better predictions. Demonstration in 𝑝 is close to 𝑛 issue Similarly , the following code conducts the same simulation study as "
4525,unknown,"is close to 𝑛 issue and adds ridge estimates for comparison. 6.2. RIDGE REGRESSION 111 iter <- 50 n <- 3 lambda <- 0.3 b_LS <- matrix( NA, nrow = 2, ncol = iter ) b_ridge <- matrix( NA, nrow = 2, ncol = iter ) for( i in 1:iter ){ x1 <- rnorm( n ) x2 <- rnorm( n ) x <- cbind( x1, x2 ) y <- x1 - x2 + rnorm( n ) b_LS[,i]<- solve( t(x) %*% x ) %*% t(x) %*% y b_ridge[,i]<- solve( t(x) %*% x + lambda * "
4526,unknown,"b_ridge[,i]<- solve( t(x) %*% x + lambda * diag(2) ) %*% t(x) %*% y } par(mfrow=c(1,2)) plot( b_LS[1,], col = 'blue', pch = 16, ylim = c( min(b_LS), max(b_LS) ), ylab = 'Estimated coefficients', xlab = 'Least squares', bty ='l' ) points( b_LS[2,], col = 'red', pch = 17 ) abline( a = 1, b = 0, col = 'blue' ) abline( a = -1, b = 0, col='red' ) plot( b_ridge[1,], col = 'blue', pch = 16, ylim = c(min("
4527,unknown,"ylab = 'Estimated coefficients', xlab = 'Ridge', bty ='l' ) points( b_ridge[2,], col = 'red', pch = 17 ) abline( a = 1, b = 0, col = 'blue' ) abline( a = -1, b = 0, col = 'red' ) # Add a legend to this plot. legend( 'bottomleft', cex = 1.3, text.col=c('blue','red'), legend = c( expression( beta[1] ),expression( beta[2] ) ), bty = 'n' ) mtext( expression('No correlations with'~n == 3~','~ p == 2), "
4528,unknown,"side = 3, line = -3, outer = TRUE ) 112 CHAPTER 6. SHRINKAGE METHODS 0 10 20 30 40 50 −5 0 5 10 20 Least squares Estimated coefficients 0 10 20 30 40 50 −5 0 5 10 20 Ridge Estimated coefficientsβ1 β2 No correlations with n = 3 , p = 2 Ridge estimates are again more stable and can be an improvement over LS. Scalability and F eature Selection • Ridge regression is applicable for any number of predic"
4529,unknown,"• Ridge regression is applicable for any number of predictors even when𝑛 < 𝑝, remember that LS solutions do not exist in this case. It is also much faster than the model-search based methods since we train only one model. • Strictly speaking, ridge regression does not perform feature selection (since the coeﬀi- cients are almost never exactly 0), but it can give us a good idea of which predictors "
4530,unknown,"are not influential and can be combined with post-hoc analysis based on ranking the absolute values of the coeﬀicients. Scaling • The LS estimates are scale invariant: multiplying a feature 𝑋𝑗 by some constant 𝑐 will scale down ̂𝛽𝑗 by 1/𝑐, so 𝑋𝑗 ̂𝛽𝑗 will remain unaffected. • In ridge regression (and any shrinkage method) the scaling of the features matters! If a relevant feature is in a smal ler s"
4531,unknown,"a relevant feature is in a smal ler scale (that is, the numbers are smaller, e.g. if you use metres as opposed to millimetres) in relation to other features it will have a larger coeﬀicient which constitutes a greater proportion of the penalty term and will thus shrink proportional ly faster to zero. • So, we want the features to be on the same scale. • In practice, we standardise: ̃ 𝑥𝑖𝑗 = 𝑥𝑖𝑗 √∑ "
4532,unknown,"√∑ 1 𝑛 (𝑥𝑖𝑗 − ̄ 𝑥𝑗)2 , so that all predictors have variance equal to one. 6.2. RIDGE REGRESSION 113 Practical Demonstration W e will continue analysing theHitters dataset included in package ISLR, this time applying the method of ridge regression. W e perform the usual steps of removing missing data, as seen in Section 5.4. library(ISLR) Hitters = na.omit(Hitters) dim(Hitters) ## [1] 263 20 sum(is"
4533,unknown,"library(ISLR) Hitters = na.omit(Hitters) dim(Hitters) ## [1] 263 20 sum(is.na(Hitters)) ## [1] 0 T o conduct ridge regression, the first thing to do is to load up the packageglmnet (remember to run the command install.packages('glmnet') the first time). library(glmnet) ## Loading required package: Matrix ## Loaded glmnet 4.1-4 Initially , we will make use of function glmnet() which implements ridg"
4534,unknown,"Initially , we will make use of function glmnet() which implements ridge regression without cross-validation, but it does give a range of solutions over a grid of 𝜆 values. The grid is produced automatically - we do not need to specify anything. Of course, if we want to we can define the grid manually . Let’s have a look first at the help document of glmnet(). ?glmnet A first important thing to ob"
4535,unknown,?glmnet A first important thing to observe is that glmnet() requires a different syntax than regsubsets() for declaring the response variable and the predictors. Now the response should be a vector and the predictor variables must be stacked column-wise as a matrix. This is easy to do for the response. F or the predictors we have to make use of themodel.matrix() command. Let us define as y the res
4536,unknown,"command. Let us define as y the response and as x the matrix of predictors. y = Hitters$Salary x = model.matrix(Salary~., Hitters)[,-1] # Here we exclude the first column # because it corresponds to the # intercept. head(x) ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun ## -Alan Ashby 315 81 7 24 38 39 14 3449 835 69 ## -Alvin Davis 479 130 18 66 72 76 3 1624 457 63 ## -Andre Dawson "
4537,unknown,## -Andres Galarraga 321 87 10 39 42 30 2 396 101 12 ## -Alfredo Griffin 594 169 4 74 51 35 11 4408 1133 19 ## -Al Newman 185 37 1 23 8 21 2 214 42 1 ## CRuns CRBI CWalks LeagueN DivisionW PutOuts Assists Errors 114 CHAPTER 6. SHRINKAGE METHODS ## -Alan Ashby 321 414 375 1 1 632 43 10 ## -Alvin Davis 224 266 263 0 1 880 82 14 ## -Andre Dawson 828 838 354 1 0 200 11 3 ## -Andres Galarraga 48 46 33 
4538,unknown,"## -Alfredo Griffin 501 336 194 0 1 282 421 25 ## -Al Newman 30 9 24 1 0 76 127 7 ## NewLeagueN ## -Alan Ashby 1 ## -Alvin Davis 0 ## -Andre Dawson 1 ## -Andres Galarraga 1 ## -Alfredo Griffin 0 ## -Al Newman 0 See how the command model.matrix() automatically transformed the categorical variables in Hitters with names League, Division and NewLeague to dummy variables with names LeagueN, DivisionW "
4539,unknown,"LeagueN, DivisionW and NewLeagueN. Other important things that we see in the help docu- ment are that glmnet() uses a grid of 100 values for 𝜆 (nlambda = 100) and that parameter alpha is used to select between ridge implementation and lasso implementation. The default option is alpha = 1 which runs lasso (see Chapter 6.3) - for ridge we have to set alpha = 0. Finally we see the default option stan"
4540,unknown,"standardize the predictor variables manually . Now we are ready to run our first ridge regression! ridge = glmnet(x, y, alpha = 0) Using the command names() we can find what is returned by the function. The most important components are ""beta"" and ""lambda"". W e can easily extract each using the $ symbol. names(ridge) ## [1] ""a0"" ""beta"" ""df"" ""dim"" ""lambda"" ""dev.ratio"" ## [7] ""nulldev"" ""npasses"" ""je"
4541,unknown,"## [7] ""nulldev"" ""npasses"" ""jerr"" ""offset"" ""call"" ""nobs"" ridge$lambda ## [1] 255282.09651 232603.53864 211939.68135 193111.54419 175956.04684 ## [6] 160324.59659 146081.80131 133104.29670 121279.67783 110505.52551 ## [11] 100688.51919 91743.62866 83593.37754 76167.17227 69400.69061 ## [16] 63235.32454 57617.67259 52499.07736 47835.20402 43585.65633 ## [21] 39713.62675 36185.57761 32970.95062 30041"
4542,unknown,## [26] 24941.31502 22725.59734 20706.71790 18867.19015 17191.08098 ## [31] 15663.87273 14272.33745 13004.42232 11849.14529 10796.49988 ## [36] 9837.36858 8963.44387 8167.15622 7441.60857 6780.51658 ## [41] 6178.15417 5629.30398 5129.21213 4673.54706 4258.36202 ## [46] 3880.06088 3535.36696 3221.29471 2935.12376 2674.37546 ## [51] 2436.79131 2220.31349 2023.06696 1843.34326 1679.58573 ## [56] 1530
4543,unknown,## [56] 1530.37596 1394.42158 1270.54501 1157.67329 1054.82879 6.2. RIDGE REGRESSION 115 ## [61] 961.12071 875.73739 797.93930 727.05257 662.46322 ## [66] 603.61181 549.98860 501.12913 456.61020 416.04621 ## [71] 379.08581 345.40887 314.72370 286.76451 261.28915 ## [76] 238.07694 216.92684 197.65566 180.09647 164.09720 ## [81] 149.51926 136.23638 124.13351 113.10583 103.05782 ## [86] 93.90245 85.5
4544,unknown,"## [91] 58.97348 53.73443 48.96082 44.61127 40.64813 ## [96] 37.03706 33.74679 30.74882 28.01718 25.52821 dim(ridge$beta) ## [1] 19 100 ridge$beta[,1:3] ## 19 x 3 sparse Matrix of class ""dgCMatrix"" ## s0 s1 ## AtBat 0.00000000000000000000000000000000000122117189 0.0023084432 ## Hits 0.00000000000000000000000000000000000442973647 0.0083864077 ## HmRun 0.00000000000000000000000000000000001784943997 "
4545,unknown,## Runs 0.00000000000000000000000000000000000749101852 0.0141728323 ## RBI 0.00000000000000000000000000000000000791287003 0.0149583386 ## Walks 0.00000000000000000000000000000000000931296087 0.0176281785 ## Years 0.00000000000000000000000000000000003808597789 0.0718368521 ## CAtBat 0.00000000000000000000000000000000000010484944 0.0001980310 ## CHits 0.00000000000000000000000000000000000038587587 0
4546,unknown,## CHmRun 0.00000000000000000000000000000000000291003649 0.0054982153 ## CRuns 0.00000000000000000000000000000000000077415313 0.0014629668 ## CRBI 0.00000000000000000000000000000000000079894300 0.0015098418 ## CWalks 0.00000000000000000000000000000000000084527517 0.0015957724 ## LeagueN -0.00000000000000000000000000000000001301216845 -0.0230232091 ## DivisionW -0.0000000000000000000000000000000001
4547,unknown,## PutOuts 0.00000000000000000000000000000000000048911970 0.0009299203 ## Assists 0.00000000000000000000000000000000000007989093 0.0001517222 ## Errors -0.00000000000000000000000000000000000037250274 -0.0007293365 ## NewLeagueN -0.00000000000000000000000000000000000258502617 -0.0036169801 ## s2 ## AtBat 0.0025301407 ## Hits 0.0091931957 ## HmRun 0.0369201974 ## Runs 0.0155353349 ## RBI 0.016395016
4548,unknown,## Walks 0.0193237654 ## Years 0.0787191800 ## CAtBat 0.0002170325 ## CHits 0.0007992257 ## CHmRun 0.0060260057 ## CRuns 0.0016034328 116 CHAPTER 6. SHRINKAGE METHODS ## CRBI 0.0016548119 ## CWalks 0.0017488195 ## LeagueN -0.0250669217 ## DivisionW -0.3663713386 ## PutOuts 0.0010198028 ## Assists 0.0001663687 ## Errors -0.0008021006 ## NewLeagueN -0.0038293583 As we see above glmnet()defines the g
4549,unknown,"As we see above glmnet()defines the grid starting with large values of𝜆. This is because this helps speed up the optimisation procedure. Also, glmnet() performs some internal checks on the predictor variables in order to define the grid values. Therefore, it is generally better to let glmnet() define the grid automatically , rather than manually defining a grid. W e further see that as expected ri"
4550,unknown,"that as expected ridge$beta had 19 rows (number of predictors) and 100 columns (length of the grid). Above the first 3 columns are displayed. Notice that ridge$beta does not return the estimates of the intercepts. W e can use the command coef() for this, which returns all betas. coef(ridge)[,1:3] ## 20 x 3 sparse Matrix of class ""dgCMatrix"" ## s0 s1 ## (Intercept) 535.92588212927762469917070120573"
4551,unknown,## AtBat 0.00000000000000000000000000000000000122117189 0.0023084432 ## Hits 0.00000000000000000000000000000000000442973647 0.0083864077 ## HmRun 0.00000000000000000000000000000000001784943997 0.0336900933 ## Runs 0.00000000000000000000000000000000000749101852 0.0141728323 ## RBI 0.00000000000000000000000000000000000791287003 0.0149583386 ## Walks 0.00000000000000000000000000000000000931296087 0.0
4552,unknown,## Years 0.00000000000000000000000000000000003808597789 0.0718368521 ## CAtBat 0.00000000000000000000000000000000000010484944 0.0001980310 ## CHits 0.00000000000000000000000000000000000038587587 0.0007292122 ## CHmRun 0.00000000000000000000000000000000000291003649 0.0054982153 ## CRuns 0.00000000000000000000000000000000000077415313 0.0014629668 ## CRBI 0.0000000000000000000000000000000000007989430
4553,unknown,## CWalks 0.00000000000000000000000000000000000084527517 0.0015957724 ## LeagueN -0.00000000000000000000000000000000001301216845 -0.0230232091 ## DivisionW -0.00000000000000000000000000000000017514595071 -0.3339842326 ## PutOuts 0.00000000000000000000000000000000000048911970 0.0009299203 ## Assists 0.00000000000000000000000000000000000007989093 0.0001517222 ## Errors -0.000000000000000000000000000
4554,unknown,## Errors -0.00000000000000000000000000000000000037250274 -0.0007293365 ## NewLeagueN -0.00000000000000000000000000000000000258502617 -0.0036169801 ## s2 ## (Intercept) 527.1582433004 ## AtBat 0.0025301407 ## Hits 0.0091931957 ## HmRun 0.0369201974 ## Runs 0.0155353349 6.3. LASSO REGRESSION 117 ## RBI 0.0163950163 ## Walks 0.0193237654 ## Years 0.0787191800 ## CAtBat 0.0002170325 ## CHits 0.000799
4555,unknown,## CHmRun 0.0060260057 ## CRuns 0.0016034328 ## CRBI 0.0016548119 ## CWalks 0.0017488195 ## LeagueN -0.0250669217 ## DivisionW -0.3663713386 ## PutOuts 0.0010198028 ## Assists 0.0001663687 ## Errors -0.0008021006 ## NewLeagueN -0.0038293583 W e can produce a plot similar to the ones seen previously via the following simple code. On the left we have the regularisation paths based on log 𝜆. Specifyi
4556,unknown,"the left we have the regularisation paths based on log 𝜆. Specifying xvar is important, as the default is to plot the ℓ1-norm on the 𝑥-axis (see Chapter 6.3). Don’t worry about the row of 19’s along the top of the plot for now, except to note that this is the number of predictor variables. The reason for their presence will become clear in Chapter 6.3. plot(ridge, xvar = 'lambda') 4 6 8 10 12 −100"
4557,unknown,"4 6 8 10 12 −100 −50 0 50 Log Lambda Coefficients 19 19 19 19 19 6.3 Lasso Regression Disadvantage of Ridge Regression • Unlike model search methods which select models that include subsets of predictors, ridge regression will include al l 𝑝 predictors. • Even for those predictors that are irrelevant to the response, their coeﬀicients will only get close to zero but never exactly zero! 118 CHAPTER"
4558,unknown,"get close to zero but never exactly zero! 118 CHAPTER 6. SHRINKAGE METHODS • Even if we could perform a post-hoc analysis to reduce the number of predictors, ideally we would like a method that sets these coeﬀicients equal to zero automatical ly. The Lasso Similar to ridge regression it penalises the size of the coeﬀicients, but instead of the squares it penalises the absolute values . The lasso r"
4559,unknown,"𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝛽0 − 𝑝 ∑ 𝑗=1 𝛽𝑗𝑥𝑖𝑗) 2 + 𝜆 𝑝 ∑ 𝑗=1 |𝛽𝑗| = 𝑅𝑆𝑆 + 𝜆 𝑝 ∑ 𝑗=1 |𝛽𝑗| The only difference is that the 𝛽2 𝑗 term in the ridge regression penalty has been replaced by |𝛽𝑗| in the lasso penalty . Lasso offers the same benefits as ridge: • it introduces some bias but decreases the variance, so it improves predictive perfor- mance. • it is extremely scalable and can cope with 𝑛 < 𝑝 problems. As w"
4560,unknown,"As with ridge regression, the lasso shrinks the coeﬀicient estimates towards zero. However, in the case of the lasso, the ℓ1 penalty has the effect of forcing some of the coeﬀicient estimates to be exactly equal to zero when the tuning parameter 𝜆 is suﬀiciently large. Hence, much like best subset selection, the lasso performs variable selection. As a result, models generated from the lasso are ge"
4561,unknown,"from the lasso are generally much easier to interpret than those produced by ridge regression. W e say that the lasso yields sparse models, that is, models that involve only a subset of the variables. As with ridge regression, the choice of 𝜆 is critical, but we defer discussion of this until Chapter 6.4. Ridge vs. Lasso Here we make some comparisons between ridge regression and lasso regression. "
4562,unknown,"Ridge and Lasso with ℓ-norms W e have talked a little bit about theℓ-norms, but here we clarify the difference between ridge and lasso using them. Lets just denote 𝛽 = (𝛽 1, … , 𝛽𝑝). Then a more compact representation of ridge and lasso minimisation is the following: • Ridge: ∑ 𝑛 𝑖=1 (𝑦𝑖 − 𝛽0 − ∑ 𝑝 𝑗=1 𝛽𝑗𝑥𝑖𝑗) 2 + 𝜆‖𝛽‖2 2, where ‖𝛽‖2 = √𝛽2 1 + … + 𝛽2𝑝 is the ℓ2-norm of 𝛽. • Lasso: ∑ 𝑛 𝑖=1 (𝑦𝑖 − 𝛽0 "
4563,unknown,"𝑛 𝑖=1 (𝑦𝑖 − 𝛽0 − ∑ 𝑝 𝑗=1 𝛽𝑗𝑥𝑖𝑗) 2 + 𝜆‖𝛽‖1, where ‖𝛽‖1 = |𝛽 1| + … + |𝛽𝑝| is the ℓ1-norm of 𝛽. That is why we say that ridge uses ℓ2-penalisation, while lasso uses ℓ1-penalisation. 6.3. LASSO REGRESSION 119 Ridge and Lasso as Constrained Optimisation Problems • Ridge: minimise ∑ 𝑛 𝑖=1 (𝑦𝑖 − 𝛽0 − ∑ 𝑝 𝑗=1 𝛽𝑗𝑥𝑖𝑗) 2 subject to ∑ 𝑝 𝑗=1 𝛽2 𝑗 ≤ 𝑐. • Lasso: minimise ∑ 𝑛 𝑖=1 (𝑦𝑖 − 𝛽0 − ∑ 𝑝 𝑗=1 𝛽𝑗𝑥𝑖𝑗) 2 subj"
4564,unknown,"𝑝 𝑗=1 𝛽2 𝑗 ≤ 𝑐. • Lasso: minimise ∑ 𝑛 𝑖=1 (𝑦𝑖 − 𝛽0 − ∑ 𝑝 𝑗=1 𝛽𝑗𝑥𝑖𝑗) 2 subject to ∑ 𝑝 𝑗=1 |𝛽𝑗| ≤ 𝑐. • The words subject to above essentially imposes the corresponding constraints. • Threshold 𝑐 may be considered as a budget: large 𝑐 allows coeﬀicients to be large. • There is a one-to-one correspondence between penalty parameter 𝜆 and threshold 𝑐, meaning for any given regression problem, given a sp"
4565,unknown,"corresponding 𝑐, and vice-versa. • When 𝜆 is large, then 𝑐 is smal l, and vice-versa! • The ridge constraint is smooth, while the lasso constraint is edgy. This is nicely illus- trated in Figure 6.1. 𝛽1 and 𝛽2 are not allowed to go outside of the red areas. Figure 6.1: Left: the constraint |𝛽1| + |𝛽2| ≤ 𝑐, Right: the constraint 𝛽2 1 + 𝛽2 2 ≤ 𝑐. It is this difference between the smooth and edgy con"
4566,unknown,"It is this difference between the smooth and edgy constraints that result in the lasso having coeﬀicient estimates that are exactly zero and ridge not. W e illustrate this even further in Figure 6.2. The least squares solution is marked as ̂𝛽, while the blue diamond and circle represent the lasso and ridge regression constraints (as in Figure 6.1). If 𝑐 is suﬀiciently large (making 𝑐 larger result"
4567,unknown,"large (making 𝑐 larger results in the diamond and circle respectively getting larger), then the constraint regions will contain ̂𝛽, and so the ridge and lasso estimates will be the same as the least squares estimates. The contours that are centered around ̂𝛽 represent regions of constant RSS. As the ellipses expand away from the least squares coeﬀicient estimates, the RSS increases. The lasso and "
4568,unknown,"ridge regression coeﬀicient estimates are given by the first point at which an ellipse contacts the constraint region. Since ridge regression has a circular constraint with no sharp points, this intersection will generally NOT occur on an axis, and so the ridge regression coeﬀicient estimates will be exclusively non-zero. However, the lasso constraint has corners at each of the axes, and 120 CHAPT"
4569,unknown,"120 CHAPTER 6. SHRINKAGE METHODS Figure 6.2: Contours of the error and constraint functions for the lasso (left) and ridge regression (right). The solid blue areas are the constraint regions|𝛽1|+|𝛽 2| ≤ 𝑐 and 𝛽2 1 +𝛽 2 2 ≤ 𝑐 respectively . The red ellipses are the contours of the Residual Sum of Squares, with the values along each contour being the same, and outer contours having a higher value (l"
4570,unknown,"squares ̂𝛽 has minimal RSS). so the ellipse will often intersect the constraint region at an axis (in fact in the higher dimensional case, the chance of the hyper-ellipse hitting the corners of the hyper-dimond gets much higher). When this occurs, one of the coeﬀicient estimates will equal zero. In Figure 6.2, the intersection occurs at 𝛽1 = 0, and so the resulting model will only include 𝛽2. Diff"
4571,unknown,"Different 𝜆’s • Both ridge and lasso are convex optimisation problems Boyd and V andenberghe[2009]. • In optimisation, convexity is a desired property . • In fact, the ridge solution exists in closed form (meaning we have a formula for it). • Lasso does not have a closed form solution, but nowadays there exist very eﬀicient optimisation algorithms for the lasso solution. • Package glmnet in R impl"
4572,unknown,"ridge and lasso. • Due to clever optimisation tricks, finding the solutions for multiple 𝜆’s takes the same time as finding the solution for one single 𝜆 value. This is very useful: it implies that we can obtain the solutions for a grid of values of 𝜆 very fast and then pick a specific 𝜆 that is optimal in regard to some objective function (see Chapter 6.4). Predictive Performance So, lasso has a "
4573,unknown,"6.3. LASSO REGRESSION 121 This must mean that it is also better for prediction, right? W ell, not necessarily . Things are a bit more complicated than that ...it all comes down to the nature of the actual underlying mechanism. Let’s explain this rather cryptic answer ... Generally… • When the actual data-generating mechanism is sparse lasso has the advantage . • When the actual data-generating mec"
4574,unknown,• When the actual data-generating mechanism is dense ridge has the advantage . Sparse mechanisms: F ew predictors are relevant to the response → good setting for lasso regression. Dense mechanisms : A lot of predictors are relevant to the response → good setting for ridge regression. Figure 6.3 roughly illustrates how prediction mean square error may depend on the density of the mechanism (number 
4575,unknown,of the mechanism (number of predictors). Figure 6.3: Rough illustration of the effect of mechanism density on prediction Mean Squared Error. This figure really is a rough extreme illustration! The actual performance will also depend upon: • Signal strength (the magnitude of the effects of the relevant variables). • The correlation structure among predictors. • Sample size 𝑛 vs. number of predictor
4576,unknown,"• Sample size 𝑛 vs. number of predictors 𝑝. So…which is better? • In general, none of the two shrinkage methods will dominate in terms of predictive performance under all settings. 122 CHAPTER 6. SHRINKAGE METHODS • Lasso performs better when few predictors have a substantial effect on the response variable. • Ridge performs better when a lot of predictors have a substantial effect on the response"
4577,unknown,"• Keep in mind: a few and a lot are always relative to the total number of available predictors. • Overall: in most applications, lasso is more robust. Question: What should I do if I have to choose between ridge and lasso? Answer: Keep a part of the data for testing purposes only and compare the predictive performance of the two methods. Practical Demonstration W e will continue analysing theHitt"
4578,unknown,lasso regression. W e perform the usual steps of removing missing data as we have seen in previous practical demonstrations. library(ISLR) Hitters = na.omit(Hitters) dim(Hitters) ## [1] 263 20 sum(is.na(Hitters)) ## [1] 0 W e will now use glmnet() to implement lasso regression. library(glmnet) Having learned how to implement ridge regression in previous section the analysis is very easy . T o run 
4579,unknown,"easy . T o run lasso all we have to do is remove the option alpha = 0. W e do not need to specify explicitly alpha = 1 because this is the default option in glmnet(). In this part again we call the response y and extract the matrix of predictors, which we call x using the command model.matrix(). y = Hitters$Salary x = model.matrix(Salary~., Hitters)[,-1] # Here we exclude the first column # becaus"
4580,unknown,# because it corresponds to the # intercept. head(x) ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun ## -Alan Ashby 315 81 7 24 38 39 14 3449 835 69 ## -Alvin Davis 479 130 18 66 72 76 3 1624 457 63 ## -Andre Dawson 496 141 20 65 78 37 11 5628 1575 225 ## -Andres Galarraga 321 87 10 39 42 30 2 396 101 12 ## -Alfredo Griffin 594 169 4 74 51 35 11 4408 1133 19 6.3. LASSO REGRESSION 123 
4581,unknown,6.3. LASSO REGRESSION 123 ## -Al Newman 185 37 1 23 8 21 2 214 42 1 ## CRuns CRBI CWalks LeagueN DivisionW PutOuts Assists Errors ## -Alan Ashby 321 414 375 1 1 632 43 10 ## -Alvin Davis 224 266 263 0 1 880 82 14 ## -Andre Dawson 828 838 354 1 0 200 11 3 ## -Andres Galarraga 48 46 33 1 0 805 40 4 ## -Alfredo Griffin 501 336 194 0 1 282 421 25 ## -Al Newman 30 9 24 1 0 76 127 7 ## NewLeagueN ## -Al
4582,unknown,"## -Alan Ashby 1 ## -Alvin Davis 0 ## -Andre Dawson 1 ## -Andres Galarraga 1 ## -Alfredo Griffin 0 ## -Al Newman 0 W e then run lasso regression. lasso = glmnet(x, y) lambda and beta are still the most important components of the output. names(lasso) ## [1] ""a0"" ""beta"" ""df"" ""dim"" ""lambda"" ""dev.ratio"" ## [7] ""nulldev"" ""npasses"" ""jerr"" ""offset"" ""call"" ""nobs"" lasso$lambda ## [1] 255.2820965 232.60353"
4583,unknown,lasso$lambda ## [1] 255.2820965 232.6035386 211.9396813 193.1115442 175.9560468 160.3245966 ## [7] 146.0818013 133.1042967 121.2796778 110.5055255 100.6885192 91.7436287 ## [13] 83.5933775 76.1671723 69.4006906 63.2353245 57.6176726 52.4990774 ## [19] 47.8352040 43.5856563 39.7136268 36.1855776 32.9709506 30.0419022 ## [25] 27.3730624 24.9413150 22.7255973 20.7067179 18.8671902 17.1910810 ## [31] 
4584,unknown,## [31] 15.6638727 14.2723374 13.0044223 11.8491453 10.7964999 9.8373686 ## [37] 8.9634439 8.1671562 7.4416086 6.7805166 6.1781542 5.6293040 ## [43] 5.1292121 4.6735471 4.2583620 3.8800609 3.5353670 3.2212947 ## [49] 2.9351238 2.6743755 2.4367913 2.2203135 2.0230670 1.8433433 ## [55] 1.6795857 1.5303760 1.3944216 1.2705450 1.1576733 1.0548288 ## [61] 0.9611207 0.8757374 0.7979393 0.7270526 0.66246
4585,unknown,"## [67] 0.5499886 0.5011291 0.4566102 0.4160462 0.3790858 0.3454089 ## [73] 0.3147237 0.2867645 0.2612891 0.2380769 0.2169268 0.1976557 ## [79] 0.1800965 0.1640972 dim(lasso$beta) ## [1] 19 80 lasso$beta[,1:3] # this returns only the predictor coefficients ## 19 x 3 sparse Matrix of class ""dgCMatrix"" 124 CHAPTER 6. SHRINKAGE METHODS ## s0 s1 s2 ## AtBat . . . ## Hits . . . ## HmRun . . . ## Runs ."
4586,unknown,"## RBI . . . ## Walks . . . ## Years . . . ## CAtBat . . . ## CHits . . . ## CHmRun . . . ## CRuns . . 0.01515244 ## CRBI . 0.07026614 0.11961370 ## CWalks . . . ## LeagueN . . . ## DivisionW . . . ## PutOuts . . . ## Assists . . . ## Errors . . . ## NewLeagueN . . . coef(lasso)[,1:3] # this returns intercept + predictor coefficients ## 20 x 3 sparse Matrix of class ""dgCMatrix"" ## s0 s1 s2 ## (Int"
4587,unknown,## (Intercept) 535.9259 512.70866859 490.92996088 ## AtBat . . . ## Hits . . . ## HmRun . . . ## Runs . . . ## RBI . . . ## Walks . . . ## Years . . . ## CAtBat . . . ## CHits . . . ## CHmRun . . . ## CRuns . . 0.01515244 ## CRBI . 0.07026614 0.11961370 ## CWalks . . . ## LeagueN . . . ## DivisionW . . . ## PutOuts . . . ## Assists . . . ## Errors . . . ## NewLeagueN . . . W e see that lasso$beta 
4588,unknown,"W e see that lasso$beta has 19 rows (number of predictors) and 80 columns. (Note the number of 𝜆 is less than the default setting, 100 (The reason lies in the stopping criteria of 6.4. CHOOSING 𝜆 125 the algorithm. According to the default internal settings, the computations stop if either the fractional change in deviance (relates to the log-likelihoods) down the path is less than 105 or the frac"
4589,unknown,"or the fraction of explained deviance (similar to the concept of 𝑅2) reaches 0.999). Using coef instead includes the intercept term. The key difference this time (in comparison to the application to ridge regression is that some of the elements of the output are replaced with a “ . ”, indicating that the corresponding predictor is not included in the lasso regression model with that value of 𝜆. W "
4590,unknown,"W e now produce a plot of regularisation (Regularisation refers to the technique that discour- ages learning a more complex or flexible model, so as to avoid the risk of overfitting.) paths based on log 𝜆. The numbers along the top now correspond to the number of predictors in the model for the corresponding (log) 𝜆 values. Recall the row of 19’s across the top of the corresponding plot for ridge "
4591,unknown,"predictors. plot(lasso, xvar = 'lambda') −2 0 2 4 −100 −50 0 50 Log Lambda Coefficients 19 17 12 6 6.4 Choosing 𝜆 Importance of 𝜆 • As we have seen, the penalty parameter 𝜆 is of crucial importance in penalised regres- sion. • F or𝜆 = 0 we essentially just get the LS estimates of the full model. • F or very large𝜆: all ridge estimates become extremely small, while all lasso estimates are exactly z"
4592,unknown,"are exactly zero! W e require a principled way to fine-tune 𝜆 in order to get optimal results. Selection Criteria? • In Chapter 4.6, we used the selection criteria 𝐶𝑝, BIC and adjusted- 𝑅2 as criteria for choosing particular models during model search methods. 126 CHAPTER 6. SHRINKAGE METHODS • Perhaps we can do that here, for instance, by defining a grid of values for 𝜆 and calculating the corres"
4593,unknown,"calculating the corresponding 𝐶𝑝 under each model, before then finding the model with the corresponding 𝜆 that yields the lowest 𝐶𝑝 value? • Not really - the problem is that all these techniques depend on model dimensionality (number of predictors). • With model-search methods we choose a model with 𝑑 predictors, so it is clear that the model’s dimensionality is 𝑑. • Shrinkage methods, however, pe"
4594,unknown,"• Shrinkage methods, however, penalise dimensionality in a different way - the very notion of model dimensionality becomes obscure somehow - does a constrained predictor count towards dimensionality in the same way as an unconstrained one? Cross-V alidation The only viable strategy is 𝐾-fold cross-validation. The steps are the following: 1. Choose the number of folds 𝐾. 2. Split the data according"
4595,unknown,"2. Split the data accordingly into training and testing sets. 3. Define a grid of values for 𝜆. 4. F or each𝜆 calculate the validation Mean Squared Error (MSE) within each fold. 5. F or each𝜆 calculate the overall cross-validation MSE. 6. Locate under which 𝜆 cross-validation MSE is minimised. This value of 𝜆 is known as the minimum_CV 𝜆. Seems diﬀicult, but fortunately glmnet in R will do all of "
4596,unknown,"Seems diﬀicult, but fortunately glmnet in R will do all of these things for us automatically! W e will explore using cross-validation for choosing 𝜆 further via practical demonstration. Practical Demonstration W e will continue analysing the Hitters dataset included in package ISLR. Perform the usual steps of removing missing data. library(ISLR) Hitters = na.omit(Hitters) dim(Hitters) ## [1] 263 2"
4597,unknown,"Hitters = na.omit(Hitters) dim(Hitters) ## [1] 263 20 sum(is.na(Hitters)) ## [1] 0 Ridge regression In this part again we call the response y and extract the matrix of predictors, which we call x using the command model.matrix(). y = Hitters$Salary x = model.matrix(Salary~., Hitters)[,-1] # Here we exclude the first column # because it corresponds to the 6.4. CHOOSING 𝜆 127 # intercept term. head("
4598,unknown,# intercept term. head(x) ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun ## -Alan Ashby 315 81 7 24 38 39 14 3449 835 69 ## -Alvin Davis 479 130 18 66 72 76 3 1624 457 63 ## -Andre Dawson 496 141 20 65 78 37 11 5628 1575 225 ## -Andres Galarraga 321 87 10 39 42 30 2 396 101 12 ## -Alfredo Griffin 594 169 4 74 51 35 11 4408 1133 19 ## -Al Newman 185 37 1 23 8 21 2 214 42 1 ## CRuns CR
4599,unknown,## -Alan Ashby 321 414 375 1 1 632 43 10 ## -Alvin Davis 224 266 263 0 1 880 82 14 ## -Andre Dawson 828 838 354 1 0 200 11 3 ## -Andres Galarraga 48 46 33 1 0 805 40 4 ## -Alfredo Griffin 501 336 194 0 1 282 421 25 ## -Al Newman 30 9 24 1 0 76 127 7 ## NewLeagueN ## -Alan Ashby 1 ## -Alvin Davis 0 ## -Andre Dawson 1 ## -Andres Galarraga 1 ## -Alfredo Griffin 0 ## -Al Newman 0 F or implementing cro
4600,unknown,"F or implementing cross-validation (CV) in order to select 𝜆 we will have to use the function cv.glmnet() from library glmnet. After loading library glmnet, we have a look at the help document by typing ?cv.glmnet. W e see the argument nfolds = 10, so the default option is 10-fold CV (this is something we can change if we want to). T o perform cross-validation for selection of 𝜆 we simply run the "
4601,unknown,"library(glmnet) set.seed(1) ridge.cv = cv.glmnet(x, y, alpha=0) W e extract the names of the output components as follows: names( ridge.cv ) ## [1] ""lambda"" ""cvm"" ""cvsd"" ""cvup"" ""cvlo"" ## [6] ""nzero"" ""call"" ""name"" ""glmnet.fit"" ""lambda.min"" ## [11] ""lambda.1se"" ""index"" lambda.min provides the optimal value of 𝜆 as suggested by the cross-validation MSEs. ridge.cv$lambda.min ## [1] 25.52821 128 CHAPTE"
4602,unknown,"ridge.cv$lambda.min ## [1] 25.52821 128 CHAPTER 6. SHRINKAGE METHODS lambda.1se provides a different value of 𝜆, known as the 1-standard error (1-se) 𝜆. This is the maximum value that 𝜆 can take while still falling within the one standard error interval of the minimum_CV 𝜆. ridge.cv$lambda.1se ## [1] 1843.343 W e see that in this case the 1-se 𝜆 is much larger than the min-CV 𝜆. This means that we"
4603,unknown,"expect the coeﬀicients under 1-se 𝜆 to be much smaller. W e can see that this is indeed the case by looking at the corresponding coeﬀicients from the two models, rounding them to 3 decimal places. round(cbind( coef(ridge.cv, s = 'lambda.min'), coef(ridge.cv, s = 'lambda.1se')), digits = 3 ) ## 20 x 2 sparse Matrix of class ""dgCMatrix"" ## s1 s1 ## (Intercept) 81.127 159.797 ## AtBat -0.682 0.102 ##"
4604,unknown,## HmRun -1.366 1.289 ## Runs 1.015 0.703 ## RBI 0.713 0.687 ## Walks 3.379 0.926 ## Years -9.067 2.715 ## CAtBat -0.001 0.009 ## CHits 0.136 0.034 ## CHmRun 0.698 0.254 ## CRuns 0.296 0.069 ## CRBI 0.257 0.071 ## CWalks -0.279 0.066 ## LeagueN 53.213 5.396 ## DivisionW -122.834 -29.097 ## PutOuts 0.264 0.068 ## Assists 0.170 0.009 ## Errors -3.686 -0.236 ## NewLeagueN -18.105 4.458 Let’s plot the
4605,unknown,Let’s plot the results based on the output of ridge.cv using plot. What is plotted is the estimated CV MSE for each value of (log) 𝜆 on the 𝑥-axis. The dotted line on the far left indicates the value of 𝜆 which minimises CV error. The dotted line roughly in the middle of the 𝑥-axis indicates the 1-standard-error 𝜆 - recall that this is the maximum value that 𝜆 can take while still falling within t
4606,unknown,"𝜆 can take while still falling within the on standard error interval of the minimum-CV 𝜆. The second line of code has manually added a dot-dash horizontal line at the upper end of the 1-standard deviation interval of the MSE at the minimum-CV 𝜆 to illustrate this point further. 6.4. CHOOSING 𝜆 129 plot(ridge.cv) abline( h = ridge.cv$cvup[ridge.cv$index[1]], lty = 4 ) 4 6 8 10 12 100000 140000 1800"
4607,unknown,"100000 140000 180000 220000 Log(λ) Mean−Squared Error 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 W e can also plot the ridge regularisation paths and add the minimum-CV and 1-se 𝜆 values to the plot. ridge = glmnet(x, y, alpha = 0) plot(ridge, xvar = 'lambda') abline(v = log(ridge.cv$lambda.min), lty = 3) # careful to use the log here # and below abline(v = log(ridge.cv$lambda.1se), lty = 3) 4 6"
4608,unknown,4 6 8 10 12 −100 −50 0 50 Log Lambda Coefficients 19 19 19 19 19 Important note: Any method/technique that relies on validation/cross-validation is subject to variability . If we re-run this code under a different random seed results will change. 130 CHAPTER 6. SHRINKAGE METHODS Sparsifying the ridge coeﬀicients? One thing mentioned in Chapter 6.2 is that one can potentially consider some kind of 
4609,unknown,"hoc analysis in order to set some of the ridge coeﬀicients equal to zero. A very simplistic approach is to just examine the ranking of the absolute coeﬀicients and decide to use a “cutting-threshold”. W e can use a combination of the commands sort() and abs() for this. beta.1se = coef( ridge.cv, s = 'lambda.1se' )[-1] rank.coef = sort( abs( beta.1se ), decreasing = TRUE ) rank.coef ## [1] 29.09666"
4610,unknown,"rank.coef ## [1] 29.096663826 5.396487460 4.457548079 2.714623469 1.289060569 ## [6] 0.925962429 0.702915318 0.686866069 0.446840519 0.253594871 ## [11] 0.235989099 0.102483884 0.071334608 0.068874010 0.067805863 ## [16] 0.066114944 0.034359576 0.009201998 0.008746278 The last two coeﬀicients are quite small, so we could potentially set these two equal to zero (setting a threshold of say , 0.01). "
4611,unknown,"(setting a threshold of say , 0.01). Equivalently we may set a larger threshold of say , 1, in which case we would remove far more variables from the model. W e could check whether this sparsification would offer any benefits in terms of predictive performance by , for example, using K -fold cross-validation to evaluate the standard ridge regression fit (using glmnet with alpha = 0) with the 𝑖th f"
4612,unknown,"alpha = 0) with the 𝑖th fold excluded, and predicting the output for the 𝑖th fold using predict (see ?predict.glmnet for details). Lasso regression As we did with ridge, we can use the function cv.glmnet() to find the min-CV 𝜆 and the 1 standard error (1-se) 𝜆 under lasso. The code is the following. set.seed(1) lasso.cv = cv.glmnet(x, y) lasso.cv$lambda.min ## [1] 1.843343 lasso.cv$lambda.1se ## ["
4613,unknown,"lasso.cv = cv.glmnet(x, y) lasso.cv$lambda.min ## [1] 1.843343 lasso.cv$lambda.1se ## [1] 69.40069 W e see that in this case the 1-se 𝜆 is much larger than the min-CV 𝜆. This means that we expect the coeﬀicients under 1-se 𝜆 to be much smaller (maybe exactly zero). Let’s have a look at the corresponding coeﬀicients from the two models, rounding them to 3 decimal places. round(cbind( coef(lasso.cv,"
4614,unknown,"places. round(cbind( coef(lasso.cv, s = 'lambda.min'), coef(lasso.cv, s = 'lambda.1se')), 3) ## 20 x 2 sparse Matrix of class ""dgCMatrix"" ## s1 s1 ## (Intercept) 142.878 127.957 6.4. CHOOSING 𝜆 131 ## AtBat -1.793 . ## Hits 6.188 1.423 ## HmRun 0.233 . ## Runs . . ## RBI . . ## Walks 5.148 1.582 ## Years -10.393 . ## CAtBat -0.004 . ## CHits . . ## CHmRun 0.585 . ## CRuns 0.764 0.160 ## CRBI 0.388"
4615,unknown,"## CWalks -0.630 . ## LeagueN 34.227 . ## DivisionW -118.981 -8.062 ## PutOuts 0.279 0.084 ## Assists 0.224 . ## Errors -2.436 . ## NewLeagueN . . Finally , lets plot the results of the object lasso.cv which we have created, and compare them to that of the lasso object generated in previous section. par(mfrow=c(1,2)) plot(lasso.cv) lasso = glmnet(x, y) plot(lasso, xvar = 'lambda') abline(v = log(l"
4616,unknown,"abline(v = log(lasso.cv$lambda.min), lty = 3) # careful to use the log here # and below abline(v = log(lasso.cv$lambda.1se), lty = 3) −2 0 2 4 100000 160000 220000 Log(λ) Mean−Squared Error 19 18 14 11 6 6 4 −2 0 2 4 −100 −50 0 50 Log Lambda Coefficients 19 17 12 6 132 CHAPTER 6. SHRINKAGE METHODS Notice now how the numbers across the top of the left-hand plot decrease as 𝜆 increases. This is indi"
4617,unknown,"This is indicating how many predictors are included for the corresponding 𝜆 value. Comparing Predictive Performance for Different 𝜆 W e can investigate which of the two different 𝜆-values (minimum-CV and 1-se) may be preferable in terms of predictive performance using the following code. Here, we split the data into a training and test set. W e then use cross-validation to find minimum-CV 𝜆 and 1-"
4618,unknown,"1-se 𝜆, and use the resulting model to predict the response at the test data, comparing it to the actual response values by MSE. W e then repeat this procedure many times using different splits in the data. repetitions = 50 mse.1 = c() mse.2 = c() set.seed(1) for(i in 1:repetitions){ # Step (i) data splitting training.obs = sample(1:263, 175) y.train = Hitters$Salary[training.obs] x.train = model."
4619,unknown,"y.test = Hitters$Salary[-training.obs] x.test = model.matrix(Salary~., Hitters[-training.obs, ])[,-1] # Step (ii) training phase lasso.train = cv.glmnet(x.train, y.train) # Step (iii) generating predictions predict.1 = predict(lasso.train, x.test, s = 'lambda.min') predict.2 = predict(lasso.train, x.test, s = 'lambda.1se') # Step (iv) evaluating predictive performance mse.1[i] = mean((y.test-predi"
4620,unknown,"mse.1[i] = mean((y.test-predict.1)^2) mse.2[i] = mean((y.test-predict.2)^2) } par(mfrow=c(1,1)) boxplot(mse.1, mse.2, names = c('min-CV lasso','1-se lasso'), ylab = 'Test MSE', col = 7) 6.4. CHOOSING 𝜆 133 min−CV lasso 1−se lasso 80000 120000 160000 200000 Test MSE The approach based on min-CV is better, which makes sense as it minimised the cross- validation error (in cv.glmnet) in the first plac"
4621,unknown,"validation error (in cv.glmnet) in the first place. The 1-se 𝜆 can be of benefit in some cases for selecting models with fewer predictors that may be more interpretable. However, in applications where the 1-se 𝜆 is much larger than the min-CV 𝜆 (like here) we in general expect results as above. 134 CHAPTER 6. SHRINKAGE METHODS Chapter 7 Principal Component Analysis 7.1 Reminder So far, we have foc"
4622,unknown,"So far, we have focused on linear models of the form 𝑦 = 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + … + 𝛽𝑝𝑥𝑝 + 𝜖 and argued in favour of selecting sparser models which are simpler to interpret and also have better predictive performance in comparison to the model that includes all 𝑝 predictors. Specifically , we saw • Model-search methods: discrete procedures which require full or partial enumeration of all possible mode"
4623,unknown,"all possible models and select one best model. • Coeﬀicient shrinkage methods: continuous procedures which shrink the coeﬀicients of the full model to zero (penalised regression). Note that both are supervised learning approaches and effectively reduce the dimensionality of the problem. There is another alternative unsupervised learning approach for dimension reduction, where only the predictors a"
4624,unknown,"7.2 Linear approximation Consider that there is a 𝑞-dimensional random vector 𝑋 ∼ (𝑚, Σ), where 𝑚 = 𝐸(𝑋) = ⎡⎢ ⎣ 𝑚1 ⋮ 𝑚𝑞 ⎤⎥ ⎦ and 135 136 CHAPTER 7. PRINCIP AL COMPONENT ANAL YSIS Σ = 𝑉 𝑎𝑟(𝑋) = ⎡⎢ ⎣ Σ11 … Σ 1𝑞 ⋮ ⋱ ⋮ Σ𝑞1 … Σ 𝑞𝑞 ⎤⎥ ⎦ If we were to approximate 𝑋 through a single straight line, what is the “best’ ’ line? One approach to solve this problem dates back to Pearson [1901]: Minimize the expe"
4625,unknown,"squared distances between 𝑋 and their projections 𝑋′ onto the line. W e make in the following use of geometrical considerations which are provided in Figure 7.1. Figure 7.1: Geometrical considerations for PCA By Pythagoras, 𝐸(𝑋𝑋 ′2 ) = 𝐸(𝑚𝑋 2 ) − 𝐸(𝑚𝑋′2 ) where 𝑋𝑋 ′ denotes the length of the line segment connecting 𝑋 and 𝑋′, i.e. 𝑋𝑋 ′ = ||𝑋 − 𝑋′||. As 𝐸(𝑚𝑋 2 ) does not depend on the fitted line, m"
4626,unknown,"2 ) does not depend on the fitted line, minimizing 𝐸(𝑋𝑋 ′2 ) is the same as maximizing 𝐸(𝑚𝑋′2 ). Investigating this quantity further, let 𝛾 be a unit vector pointing in the direction of the vector 𝑋′ − 𝑚 and let 𝑡 ∈ ℝ be a scalar such that the projected point is 7.2. LINEAR APPROXIMA TION 137 given by 𝑋′ = 𝑚 + 𝑡𝛾 ∈ ℝ𝑞 . The Euclidean distance between 𝑚 and 𝑋′, 𝑚𝑋′, equals |𝑡|. W e have by definiti"
4627,unknown,"𝑐𝑜𝑠(𝛾, 𝑋 − 𝑚) = 𝑡 ||𝑋 − 𝑚|| and, using the properties of the dot product gives 𝛾𝑇 (𝑋 − 𝑚) = ||𝛾||||𝑋 − 𝑚||𝑐𝑜𝑠(𝛾, 𝑋 − 𝑚) = 𝑡. T o maximizing𝐸(𝑚𝑋′2 ), we have 𝐸(𝑚𝑋′2 ) = 𝐸( 𝑚𝑋′ 𝑚𝑋′) = 𝐸(𝛾 𝑇 (𝑋 − 𝑚)(𝑋 − 𝑚)𝑇 𝛾) = = V ar(𝛾𝑇 (𝑋 − 𝑚)) = V ar(𝛾𝑇 𝑋) = 𝛾 𝑇 V ar(𝑋)𝛾 = 𝛾 𝑇 Σ𝛾. Hence, we need to maximize 𝛾𝑇 Σ𝛾 under the constraint ||𝛾||2 = 𝛾 𝑇 𝛾 = 1 . This is a con- strained maximization problem which can be a"
4628,unknown,"tiplier. Define 𝑃 (𝛾) = 𝛾𝑇 Σ𝛾 − 𝜆(𝛾𝑇 𝛾 − 1). Then from Chapter 12 Mardia et al. [1979] 𝜕𝑃 𝜕𝛾 = 2Σ𝛾 − 2𝜆𝛾, and setting this equal to zero yields Σ𝛾 = 𝜆𝛾 (7.1) So, 𝛾 must be an eigenvector of Σ. But which one? (T o fix terms, order the 𝑞 eigenvalues of Σ ∈ ℝ 𝑞×𝑞 such that 𝜆1 ≥ … ≥ 𝜆 𝑞 , and let the orthogonal vectors 𝛾1, … , 𝛾𝑞 denote the corresponding eigenvectors.) Multiplying (( 7.1)) with 𝛾𝑇 , o"
4629,unknown,"corresponding eigenvectors.) Multiplying (( 7.1)) with 𝛾𝑇 , one has 𝛾𝑇 Σ𝛾 = 𝜆𝛾 𝑇 𝛾 V ar(𝛾𝑇 𝑋) = 𝜆 The left hand side is now exactly what we want to maximize. Maximizing V ar (𝛾𝑇 𝑋) means then that we need to choose the eigenvector 𝛾1 corresponding to the largest eigenvalue 𝜆1. Some notes: • The new random variable 𝛾𝑇 1 𝑋 is that linear combination of 𝑋 = (𝑋 1, … , 𝑋𝑞 )𝑇 with maximal variance, and "
4630,unknown,"𝑔1(𝑡) = 𝑚 + 𝑡𝛾1, (𝑡 ∈ ℝ) is the corresponding first principal component line. 138 CHAPTER 7. PRINCIP AL COMPONENT ANAL YSIS • Similarly , we define higher-order PC’s: The 𝑗−th eigenvector 𝛾𝑗 maximizes V ar(𝛾𝑇 𝑋) over all 𝛾 which are orthogonal to 𝛾1, … , 𝛾𝑗−1. The 𝑗−th PC is given by 𝛾𝑇 𝑗 𝑋, and 𝑔𝑗(𝑡) = 𝑚 + 𝑡𝛾𝑗 is the corresponding 𝑗−th PC line (See Krzanowski [2000]). • F ordata 𝑍, we need to rep"
4631,unknown,"yielding ̂𝜆1, … , ̂𝜆𝑞 , ̂ 𝛾1, … , ̂ 𝛾𝑞 . • F rom now on for the rest of this section, we will omit all “hats” on Σ, 𝑚, 𝜆, etc, meaning that the formulas hold for either the random vector or the corresponding estimate, depending on context. Note that principal components minimize the (sum of squared) orthogonal distances between data 𝑋1, … , 𝑋𝑛 and the line, unlike vertical distances as in the regr"
4632,unknown,"7.3. DECOMPOSING V ARIANCE 139 7.3 Decomposing variance Recall that, for the 𝑗−th eigenvector 𝛾𝑗 of Σ, one has Σ𝛾𝑗 = 𝜆𝑗𝛾𝑗 𝑗 = 1, … , 𝑞 F or each𝑗, either side of this equation is a 𝑞 × 1vector. W e can bind these𝑞 vectors together to form 𝑞 × 𝑞 matrices: (Σ𝛾1, … , Σ𝛾𝑞 ) = (𝜆 1𝛾1, … , 𝜆1𝛾𝑞 ) Σ (𝛾1, … , 𝛾𝑞 ) = (𝛾 1, … , 𝛾𝑞 ) ⎛⎜ ⎝ 𝜆1 ⋱ 𝜆𝑞 ⎞⎟ ⎠ ΣΓ = ΓΛ Σ = ΓΛΓ −1 = ΓΛΓ𝑇 (7.2) This decomposition is cal"
4633,unknown,"in order to find the 𝜆′ 𝑗𝑠 and 𝛾′ 𝑗𝑠. Now, we have 𝜆𝑗 = V ar(𝛾𝑇 𝑗 𝑋) so the 𝜆𝑗 provide some decomposition of variance. So, each 𝜆𝑗 takes the share of something - but the share of what? Let’s therefore compute their sum: 𝜆1 + … + 𝜆𝑞 = T r(Λ)=T r(Γ𝑇 ΣΓ)=T r(ΓΓ𝑇 Σ) = T r(Σ) ≡ TV(𝑋) The trace of the variance matrix is called the total variance. Therefore 𝜆𝑗 𝜆1 + … + 𝜆𝑞 = V ar(𝛾𝑇 𝑗 𝑋) TV(𝑋) is the prop"
4634,unknown,"𝑗 𝑋) TV(𝑋) is the proportion of total variance explained by the 𝑗−th principal component . A simple graphical tool illustrating this decomposition is the scree plot, which plots 𝜆𝑗 vs. 𝑗. Practical Example F or the 4-dimensional space spanned by the four variables “T AX”, “DLIC”, “INC”, “ROAD” of the fuel consumption data set, we obtain a scree plot in R as follows: fuelcons <- read.table(""https:/"
4635,unknown,"fuelcons <- read.table(""https://www.maths.dur.ac.uk/users/hailiang.du/data/FUEL.dat"", header=TRUE) fuel <- fuelcons[,c(""TAX"", ""DLIC"", ""INC"", ""ROAD"")] fuel.pr <- prcomp(fuel) plot(fuel.pr) 140 CHAPTER 7. PRINCIP AL COMPONENT ANAL YSIS fuel.pr Variances 0 5 10 15 20 25 30 Can we create it by hand? Let’s look at the output of the prcomp function: fuel.pr ## Standard deviations (1, .., p=4): ## [1] 5."
4636,unknown,"## [1] 5.5610991 3.5210898 0.7482262 0.5562322 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## TAX -0.04687723 0.15579496 -0.96634218 0.19928183 ## DLIC 0.99686606 -0.05351563 -0.05128624 0.02763787 ## INC 0.01607022 -0.01033709 -0.20428171 -0.97872564 ## ROAD -0.06166301 -0.98628452 -0.14772101 0.04023709 Here, the item Standard deviations contains the (ordered) values 𝑆𝐷[𝛾 𝑇 𝑗 𝑋] = √𝑉 𝑎𝑟"
4637,unknown,"Here, the item Standard deviations contains the (ordered) values 𝑆𝐷[𝛾 𝑇 𝑗 𝑋] = √𝑉 𝑎𝑟[𝛾𝑇 𝑗 𝑋] = √𝜆𝑗, and the item Rotation is the same as Γ. Both items can be extracted directly from the prcomp object (here: fuel.pr) through the components \$sdev and \$rot, respectively . Hence, we get the 𝜆𝑗 via fuel.pr$sdev^2 ## [1] 30.9258227 12.3980734 0.5598424 0.3093942 which is immediately confirmed to be th"
4638,unknown,eigen(var(fuel))$values ## [1] 30.9258227 12.3980734 0.5598424 0.3093942 and the proportions of variance explained are given by 7.4. SCALING 141 fuel.pr$sdev^2/ sum(fuel.pr$sdev^2) ## [1] 0.699787972 0.280542985 0.012668086 0.007000957 One may also want to look at the built-in R summary output. summary(fuel.pr) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 5.5611 3.5211 0.7
4639,unknown,"## Proportion of Variance 0.6998 0.2805 0.01267 0.0070 ## Cumulative Proportion 0.6998 0.9803 0.99300 1.0000 W e would conclude from the scree plot and the summary output that 𝑑 = 2 principal com- ponents capture the essential information provided by the data cloud. 7.4 Scaling If the variables operate on different scales/units, the decomposition of variance may not be meaningful as it depends on "
4640,unknown,"meaningful as it depends on the units chosen. It is a problem for the fuel data as all units are different, and our decomposition of variance may just reflect which variables are recorded on smaller or larger units! Assume, for instance, that INC had been measured in $ (instead of 1000$), or the proportions DLIC had been measured on the interval [0,1] (rather than in percent). This would have had "
4641,unknown,"percent). This would have had a massive impact on Σ in the corresponding directions, and, hence, on the principal components found for this data set (T ry this!). T o avoid problems of this type, standardize all variables by dividing through their standard deviation, i.e. set ̃𝑋𝑗 = 𝑋 𝑗/𝑆𝐷(𝑋𝑗), and then apply PCA to ̃𝑋 = ( ̃𝑋1, … , ̃𝑋𝑞 )𝑇 . Also, observe that for eigenvalues 𝜆1, … , 𝜆𝑞 of the varia"
4642,unknown,"𝜆1 + … + 𝜆𝑞 = … = T r(𝑉 𝑎𝑟[ ̃𝑋]) = 1 + … + 1 = 𝑞 i.e. the eigenvalues of the correlation matrix always add up to the dimension of the random vector. Practical Example - Scaling for fuel data fuel.pr1 <- prcomp(fuel, scale=TRUE) fuel.pr1 ## Standard deviations (1, .., p=4): ## [1] 1.2567608 1.0695586 0.9578833 0.5992131 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## TAX 0.7101892 -0.081337"
4643,unknown,## DLIC -0.3185570 -0.68187354 -0.5219691 0.4013952 ## INC -0.1241055 -0.61747664 0.7603559 -0.1586799 ## ROAD -0.6154271 0.38360828 0.3364450 0.6007486 142 CHAPTER 7. PRINCIP AL COMPONENT ANAL YSIS plot(fuel.pr1) fuel.pr1 Variances 0.0 0.5 1.0 1.5 sum(fuel.pr1$sdev^2) ## [1] 4 Note carefully that the values given atStandard deviations: are 𝑆𝐷(𝛾 𝑇 𝑗 ̃𝑋); NOT 𝑆𝐷( ̃𝑋𝑗). W e also verify that ∑ 𝜆𝑗 = 4
4644,unknown,"𝑗 ̃𝑋); NOT 𝑆𝐷( ̃𝑋𝑗). W e also verify that ∑ 𝜆𝑗 = 4. 7.5 Data compression (Dimension reduction) Let 𝑥1, … , 𝑥𝑛 be sampled from the 𝑞− dim. r.v. 𝑋 ∼ (𝑚, Σ) , yielding a data matrix (or ‘data frame’) ⎛⎜ ⎝ 𝑥𝑇 1 ⋮ 𝑥𝑇 𝑛 ⎞⎟ ⎠ = ⎛⎜ ⎝ 𝑥11 ⋯ 𝑥 1𝑞 ⋮ ⋱ ⋮ 𝑥𝑛1 ⋯ 𝑥 𝑛𝑞 ⎞⎟ ⎠ ∈ ℝ𝑛×𝑞 . Assume a PCA has been carried out as usual, yielding 𝑚, Σ, 𝛾1, … 𝛾𝑞 , 𝜆1, … 𝜆𝑞 . Suppose we wish to compress the data towards a smal"
4645,unknown,"wish to compress the data towards a smaller dimension 𝑑 ≤ 𝑞 (for instance, in order to have fewer predictors or save storage space and computing time, etc.). How to choose 𝑑? There are different conventions (all more or less heuristic) on how to extract such information from a scree plot (of 𝜆𝑗 versus 𝑗), but the general idea is the following: The scree plot usually can be split into a left–hand p"
4646,unknown,"can be split into a left–hand part (which contains the components which capture significant variation in the data) and a right–hand part (which just contains noise or “scree”). So, one cuts off where this breakpoint, also called “knee”, is deemed to be situated. How does the actual compression step work? Data compression means projection. W e project all data points 𝑥𝑖, 𝑖 = 1, … , 𝑛 onto the 𝑑−dim"
4647,unknown,"components: 𝑓 ∶ ℝ 𝑞 ⟶ ℝ𝑑, 𝑥𝑖 ↦ (𝛾1, ⋯ , 𝛾𝑑)𝑇 (𝑥𝑖 − 𝑚), 𝑖 = 1, … , 𝑛 7.5. DA T A COMPRESSION (DIMENSION REDUCTION) 143 (the 𝑓 (𝑥𝑖) ≡ 𝑡𝑖 are called principal component scores). If desired, scores can be mapped back to the data space: 𝑔 ∶ ℝ𝑑 ⟶ ℝ𝑞 , 𝑡𝑖 ↦ 𝑚 + (𝛾1, ⋯ , 𝛾𝑑)𝑡𝑖, 𝑖 = 1, … , 𝑛 which leads to ‘reconstructed’ values 𝑟𝑖 ≡ (𝑔 ∘ 𝑓 )(𝑥𝑖). Obviously , the original data will not be exactly reconstru"
4648,unknown,"exactly reconstructed (as we have dismissed some information), unless 𝑑 = 𝑞 . In the latter case, using the orthogonality of Γ, (𝑔 ∘ 𝑓 )(𝑥𝑖) = 𝑔(𝑓 (𝑥 𝑖)) = 𝑚 + (𝛾1, ⋯ , 𝛾𝑑)(𝛾1, ⋯ , 𝛾𝑑)𝑇 (𝑥𝑖 − 𝑚) = 𝑚 + ΓΓ 𝑇 (𝑥𝑖 − 𝑚) = 𝑚 + 𝑥𝑖 − 𝑚 = 𝑥𝑖 The following R code implements the above procedure. W e consider firstly just a single data point, and proceed with compression/reconstruction of the full fuel data s"
4649,unknown,"𝑑 = 2, without scaling. # Work firstly with a single observation. Say, case 1: x1 <- fuel[1,] mode(x1) # wrong data type: lists can't be used for computation ## [1] ""list"" x1<- as.numeric(x1) mode(x1) # that does it. ## [1] ""numeric"" m <- colMeans(fuel) t1 <- t(fuel.pr$rot[,c(1,2)]) %*% (x1-m) # Score for case 1 r1 <- m+ fuel.pr$rot[,c(1,2)]%*% t1 # Reconstruction for case 1 x1 ## [1] 9.000 52.500"
4650,unknown,"t1 ## [,1] ## PC1 -4.370997 ## PC2 3.997192 r1 ## [,1] ## TAX 8.495976 ## DLIC 52.462122 ## INC 4.130271 ## ROAD 1.892577 144 CHAPTER 7. PRINCIP AL COMPONENT ANAL YSIS plot(rbind(fuel,t(r1)), col=c(2, rep(1,47),3))# red=original, green=approx TAX 45 55 65 5 6 7 8 9 10 0 5 10 45 55 65 DLIC INC 3.0 3.5 4.0 4.5 5.0 0 5 10 15 5 7 93.0 4.0 5.0 ROAD # For the full data matrix, we make use of the scores "
4651,unknown,"# fuel.pr$x # is the same as: # t(fuel.pr$rot[,]) %*% (t(fuel)-m) T <- t(fuel.pr$x[,c(1,2)]) # All scores R <- t(m + fuel.pr$rot[,c(1,2)]%*% T) # All reconstructed values plot(rbind(fuel, R), col=c(rep(1,48),rep(3,48))) 7.5. DA T A COMPRESSION (DIMENSION REDUCTION) 145 TAX 45 55 65 5 6 7 8 9 10 0 5 10 45 55 65 DLIC INC 3.0 3.5 4.0 4.5 5.0 0 5 10 15 5 7 93.0 4.0 5.0 ROAD # black=original, green=app"
4652,unknown,"ROAD # black=original, green=approx boxplot(fuel) TAX DLIC INC ROAD 0 10 20 30 40 50 60 70 Obviously , PCA favors ROAD and DLIC which have the largest variance. Hence, when “compressing” the data we have essentially eliminated the “less variable” variables, T AX and INC. Note that the results would have changed dramatically if, for instance, INC would have been measured in $ (instead of 1000$). T "
4653,unknown,"T o eliminate the effect of the units, we repeat the same analysis using scaled data. s <- apply(fuel, 2, sd) # calculates the column standard deviations fuel.s <- sweep(fuel, 2, s, ""/"") # divides all columns by their standard deviations 146 CHAPTER 7. PRINCIP AL COMPONENT ANAL YSIS boxplot(fuel.s) TAX DLIC INC ROAD 0 2 4 6 8 10 12 fuel.pr1 <- prcomp(fuel.s) # (equivalent to fuel.pr1 in the exampl"
4654,unknown,"plot(fuel.pr1) # suggests d=3 fuel.pr1 Variances 0.0 0.5 1.0 1.5 ms <- colMeans(fuel.s) # calculates means of scaled data set Ts <- t(fuel.pr1$x[,c(1,2,3)]) # 3d-scores for scaled data Rs <- t(ms + fuel.pr1$rot[,c(1,2,3)]%*% Ts) # Reconstructed scaled data plot(rbind(fuel.s, Rs), col=c(rep(1,48),rep(3,48)))# black=original, green=approx 7.6. PRINCIP AL COMPONENT REGRESSION 147 TAX 8 10 12 5 6 7 8 "
4655,unknown,"TAX 8 10 12 5 6 7 8 9 10 0 2 4 8 9 10 11 12 13 DLIC INC 6 7 8 9 0 1 2 3 4 5 5 7 96 7 8 9 ROAD 7.6 Principal Component Regression Principal Component Regression Consider a linear regression problem with the model form 𝑦 = 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + … + 𝛽𝑝𝑥𝑝 + 𝜖 Assume that 𝑝 is quite large, which may be impractical in practice. In such a case, may consider reducing the number of “predictors”, prior to regr"
4656,unknown,"consider reducing the number of “predictors”, prior to regression, via PCA. That is instead of utilizing the 𝑝 original predictors, this approach utilises 𝑑 transformed variables (principal components) which are linear combinations of the original predictors, where 𝑑 < 𝑝, then fit a regression model using the principal components. This technique, which combines the worlds of unsupervised and super"
4657,unknown,"Mathematically: 1. Define 𝑑 < 𝑝 linear transformations 𝑍𝑘, 𝑘 = 1, ..., 𝑑of 𝑋1, 𝑋2, … , 𝑋𝑝 as 𝑍𝑘 = 𝑝 ∑ 𝑗=1 𝜙𝑗𝑘𝑋𝑗, when applying PCA, 𝜙1𝑘, ..., 𝜙𝑝𝑘 is simply the 𝛾𝑘 obtained from eigen decomposition of the sample variance of data 𝑋. 2. Fit a linear regression model of the form 𝑦𝑖 = 𝜃0 + 𝑑 ∑ 𝑘=1 𝜃𝑘𝑧𝑖𝑘, 𝑖 = 1, … , 𝑛. 148 CHAPTER 7. PRINCIP AL COMPONENT ANAL YSIS W e havedimension reduction : Instead o"
4658,unknown,"Interestingly , with some re-ordering we get: 𝑑 ∑ 𝑘=1 𝜃𝑘𝑧𝑖𝑘 = 𝑑 ∑ 𝑘=1 𝜃𝑘 ( 𝑝 ∑ 𝑗=1 𝜙𝑗𝑘𝑥𝑖𝑗) = 𝑝 ∑ 𝑗=1 ( 𝑑 ∑ 𝑘=1 𝜃𝑘𝜙𝑗𝑘) 𝑥𝑖𝑗 = 𝑝 ∑ 𝑗=1 𝛽𝑗𝑥𝑖𝑗 The key idea is that often a small number of principal components suﬀice to explain most of the variability of the data, as well as the relationship with the response. In other words, we assume that the directions in which 𝑋1, ..., 𝑋𝑝 show the most variation are"
4659,unknown,"assume that the directions in which 𝑋1, ..., 𝑋𝑝 show the most variation are the directions that are associated with Y . This assumption is not guaranteed, but it is often reasonable enough to give good results. Another nice property of the new transformed variables (principal components) is that they are uncorrelated this would give a solution to the problem of multicol linearity. How Many Compone"
4660,unknown,"How Many Components? • Once again we have a tuning problem. • In ridge and lasso the tuning parameter ( 𝜆) was continuous - in PCR the tuning pa- rameter is the number of components ( 𝑑), which is discrete. • Some methods calculate the total variance of X explained as we add further components. When the incremental increase is negligible, we stop. One such popular method is the scree plot (we have"
4661,unknown,"scree plot (we have seen it in the previous session. • Alternatively , we can use (yes, you guessed it…) cross-validation! What about Interpretability? • One drawback of PCR is that it lacks interpretability , because the estimates ̂𝜃𝑘 for 𝑘 = 1, … , 𝑑are the coeﬀicients of the principal components, which have no meaningful interpretation. • W ell, that is partially true, but there is one thing we"
4662,unknown,"• W ell, that is partially true, but there is one thing we can do: once we fit LS on the transformed variables we can use the transformation to obtain the corresponding estimates of the original predictors ̂𝛽𝑗 = 𝑑 ∑ 𝑘=1 ̂𝜃𝑘𝜙𝑗𝑘, for 𝑗 = 1, … , 𝑝 • However, for 𝑑 < 𝑝 these will not correspond to the LS estimates of the full model. In fact, the PCR estimates get shrunk in a discrete step-wise fashion"
4663,unknown,"is why PCR is still a shrinkage method). PCR - a Summary Advantages: 7.6. PRINCIP AL COMPONENT REGRESSION 149 • Similar to ridge and lasso, it can result in improved predictive performance in compar- ison to Least Squares by resolving problems caused by multicollinearity and/or large 𝑝. • It is a simple two-step procedure which first reduces dimensionality from 𝑝 + 1to 𝑑 + 1 and then utilizes LS. "
4664,unknown,"Disadvantages: • It does not perform feature selection. • The issue of interpretability . • It relies on the assumption that the directions in which the predictors show the most variation are the directions which are predictive of the response. This is not always the case. Sometimes, it is the last principal components which are associated with the response! Another dimension reduction method, Par"
4665,unknown,"response! Another dimension reduction method, Partial Least Squares, is more effective in such settings, but we do not have time in this course to cover this approach. Final Comments: • In general, PCR will tend to do well in cases when the first few principal components are suﬀicient to capture most of the variation in the predictors as well as the relationship with the response. • W e note that "
4666,unknown,"• W e note that even though PCR provides a simple way to perform regression using 𝑑 < 𝑝 predictors, it is not a feature selection method. This is because each of the 𝑑 principal components used in the regression is a linear combination of all 𝑝 of the original features. • In general, dimension-reduction based regression methods are not very popular nowa- days, mainly due to the fact that they do n"
4667,unknown,"penalised regression methods. • However, PCA is a very important tool in unsupervised learning where it is used exten- sively in a variety of applications! Practical Demonstration F or PCR, we will make use of the package pls, so we have to install this before first use by executing install.packages('pls'). The main command is called pcr(). The argument scale=TRUE standardises the columns of the p"
4668,unknown,"scale=TRUE standardises the columns of the predictor matrix, while argument validation = 'CV' picks the optimal number of principal components via cross-validation. The com- mand summary() provides information on CV error and variance explained (RMSEP - Root Mean Squared Error (of Prediction)). Note that adjCV is an adjusted estimate of CV error accounting for bias. Under TRAINING, we can see the "
4669,unknown,"data explained by the 𝑞 largest PCs, 𝑞 = 1, ..., 𝑝. library(pls) # for performing PCR ## ## Attaching package: 'pls' ## The following object is masked from 'package:stats': ## ## loadings 150 CHAPTER 7. PRINCIP AL COMPONENT ANAL YSIS library(ISLR) # for Hitters dataset set.seed(1) pcr.fit = pcr( Salary ~ ., data = Hitters, scale = TRUE, validation = ""CV"" ) summary(pcr.fit) ## Data: X dimension: 26"
4670,unknown,## Y dimension: 263 1 ## Fit method: svdpc ## Number of components considered: 19 ## ## VALIDATION: RMSEP ## Cross-validated using 10 random segments. ## (Intercept) 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps ## CV 452 352.5 351.6 352.3 350.7 346.1 345.5 ## adjCV 452 352.1 351.2 351.8 350.1 345.5 344.6 ## 7 comps 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps ## CV 345.4 348.5 350.4 353.
4671,unknown,## CV 345.4 348.5 350.4 353.2 354.5 357.5 360.3 ## adjCV 344.5 347.5 349.3 351.8 353.0 355.8 358.5 ## 14 comps 15 comps 16 comps 17 comps 18 comps 19 comps ## CV 352.4 354.3 345.6 346.7 346.6 349.4 ## adjCV 350.2 352.3 343.6 344.5 344.3 346.9 ## ## TRAINING: % variance explained ## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps 8 comps ## X 38.31 60.16 70.84 79.03 84.29 88.63 92.26 94.96 
4672,unknown,## Salary 40.63 41.58 42.17 43.22 44.90 46.48 46.69 46.75 ## 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps 15 comps ## X 96.28 97.26 97.98 98.65 99.15 99.47 99.75 ## Salary 46.86 47.76 47.82 47.85 48.10 50.40 50.55 ## 16 comps 17 comps 18 comps 19 comps ## X 99.89 99.97 99.99 100.00 ## Salary 53.01 53.85 54.61 54.61 W e can use the command validationplot() to plot the validation error. The 
4673,unknown,"val.type = 'MSEP' specifies to select Mean Squared Error (of Prediction), other options are Root Mean Squared Error (of Prediction), RMSEP and 𝑅2; type ?validationplot for help. validationplot( pcr.fit, val.type = 'MSEP' ) 7.6. PRINCIP AL COMPONENT REGRESSION 151 0 5 10 15 120000 160000 200000 Salary number of components MSEP F rom the plot above it is not obvious which PCR model minimises the CV "
4674,unknown,"MSEP F rom the plot above it is not obvious which PCR model minimises the CV error. W e would like to extract this information automatically , but it turns out that is not so easy withpcr()! One way to do this is via the code below (try to understand what it does!). min.pcr = which.min( MSEP( pcr.fit )$val[1,1, ] ) - 1 min.pcr ## 7 comps ## 7 So, it is the model with 7 principal components that mi"
4675,unknown,"## 7 So, it is the model with 7 principal components that minimises CV. Now that we know which model it is we can find the corresponding estimates in terms of the original ̂𝛽’s using the command coef() we have used before. Also we can generate predictions via predict() (below the first 6 predictions are displayed). coef(pcr.fit, ncomp = min.pcr) ## , , 7 comps ## ## Salary ## AtBat 27.005477 ## Hi"
4676,unknown,## Hits 28.531195 ## HmRun 4.031036 ## Runs 29.464202 ## RBI 18.974255 ## Walks 47.658639 ## Years 24.125975 ## CAtBat 30.831690 ## CHits 32.111585 ## CHmRun 21.811584 ## CRuns 34.054133 152 CHAPTER 7. PRINCIP AL COMPONENT ANAL YSIS ## CRBI 28.901388 ## CWalks 37.990794 ## LeagueN 9.021954 ## DivisionW -66.069150 ## PutOuts 74.483241 ## Assists -3.654576 ## Errors -6.004836 ## NewLeagueN 11.401041
4677,unknown,"head( predict( pcr.fit, ncomp = min.pcr ) ) ## , , 7 comps ## ## Salary ## -Alan Ashby 568.7940 ## -Alvin Davis 670.3840 ## -Andre Dawson 921.6077 ## -Andres Galarraga 495.8124 ## -Alfredo Griffin 560.3198 ## -Al Newman 135.5378 Extension: Regularisation Paths for PCR What about regularisation paths with pcr()? This is again tricky because there is no built- in command. The below piece of code can"
4678,unknown,"in command. The below piece of code can be used to produce regularisation paths. The regression coeﬀicients from all models using 1 up to 19 principal components are stored in pcr.fit$coefficients, but this is an object of class list which is not very convenient to work with. Therefore, we create a matrix called coef.mat which has 19 rows and 19 columns. W e then store the coeﬀicients inpcr.fit$co"
4679,unknown,"W e then store the coeﬀicients inpcr.fit$coefficientsto the columns of coef.mat, starting from the model that has 1 principal component, using a for loop. W e then use plot() to plot the path of the first row of coef.mat, followed by a for a for loop calling the command lines() to superimpose the paths of the rest of the rows of coef.mat. coef.mat = matrix(NA, 19, 19) for(i in 1:19){ coef.mat[,i] "
4680,unknown,"coef.mat[,i] = pcr.fit$coefficients[,,i] } plot(coef.mat[1,], type = 'l', ylab = 'Coefficients', xlab = 'Number of components', ylim = c(min(coef.mat), max(coef.mat))) for(i in 2:19){ lines(coef.mat[i,], col = i) } abline(v = min.pcr, lty = 3) 7.6. PRINCIP AL COMPONENT REGRESSION 153 5 10 15 −400 −200 0 200 400 Number of components Coefficients W e can see from the plot the somewhat strange way in"
4681,unknown,"Scree plots Finally , consider here that we used the in-built CV within the function pcr to select the number of coeﬀicients. W e can, however, use the visual aid of a scree plot to help us decide on the appropriate number of principal components. This amounts to plotting as a bar chart the cumulative proportion of variance attributed to the first 𝑞 principal components, such as was shown in the X"
4682,unknown,"shown in the Xrow of the % variance explainedmatrix when we called summary(pcr.fit). This information is a little diﬀicult to extract from the summary object, however, so we construct the scree plot manually as shown below (try to understand what is happening): PVE <- rep(NA,19) for(i in 1:19){ PVE[i]<- sum(pcr.fit$Xvar[1:i])/pcr.fit$Xtotvar } barplot( PVE, names.arg = 1:19, main = ""scree plot"", x"
4683,unknown,"xlab = ""number of PCs"", ylab = ""proportion of variance explained"" ) 154 CHAPTER 7. PRINCIP AL COMPONENT ANAL YSIS 1 2 3 4 5 6 7 8 9 11 13 15 17 19 scree plot number of PCs proportion of variance explained 0.0 0.2 0.4 0.6 0.8 1.0 W e may , for example, decide that we need the number of principal components required to explain 95% of the variance in the predictors. In this case, we could look at the"
4684,unknown,"explain 95% of the variance in the predictors. In this case, we could look at the scree plot and the output from summary(pcr.fit) to elicit that we need to select the first 9 principal components in order to explain 95% of the variance in X. Chapter 8 Polynomial Regression 8.1 The Assumption of Linearity • So far, we have worked under the assumption that the relationships between predictors and th"
4685,unknown,"and the response are (first-order) linear. • In reality , they are almost never exactly linear. • However, as long as the relationships are approximately linear, linear models work fine. • But what can we do when the relationships are clearly non-linear?? Example −2 −1 0 1 2 0 5 10 Predictor Response Figure 8.1: First-order polynomial regression line through non-linear data. Figure 8.1 shows a fit"
4686,unknown,Figure 8.1 shows a fitted regression line from the model 𝑦 = 𝛽0 + 𝛽1𝑥 + 𝜖: clearly not a good fit! Why not add 𝑥2 in the equation in order to bend the straight line? See Figure 8.2. 155 156 CHAPTER 8. POL YNOMIAL REGRESSION −2 −1 0 1 2 0 5 10 Predictor Response Figure 8.2: Second-order polynomial regression line through non-linear data. The fitted regression line from the model 𝑦 = 𝛽 0 + 𝛽1𝑥 + 𝛽2𝑥
4687,unknown,"especially on the left tail. Why not make the line even more flexible by adding an 𝑥3-term? see Figure 8.3. The fitted regression line from model 𝑦 = 𝛽0 + 𝛽1𝑥 + 𝛽2𝑥2 + 𝛽3𝑥3 + 𝜖 looks just right. 8.2 Polynomial Regression Models • W e have just implementedpolynomial regression - as easy as that! • In general, polynomial models are of the form 𝑦 = 𝑓 (𝑥) = 𝛽0 + 𝛽1𝑥 + 𝛽2𝑥2 + 𝛽3𝑥3 + … + 𝛽𝑑𝑥𝑑 + 𝜖, where"
4688,unknown,"𝑦 = 𝑓 (𝑥) = 𝛽0 + 𝛽1𝑥 + 𝛽2𝑥2 + 𝛽3𝑥3 + … + 𝛽𝑑𝑥𝑑 + 𝜖, where 𝑑 is called the degree of the polynomial. • Notice how convenient this is: the non-linear relationship between 𝑦 and 𝑥 is captured by the polynomial terms but the models remain linear in the parameters/coeﬀicients (the models are additive). • This means we can use standard Least Squares for estimation! • F or example, previously we fitted a "
4689,unknown,"as 𝑦 = 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + 𝛽3𝑥3 + 𝜖 with the predictors simply being 𝑋1 = 𝑋, 𝑋2 = 𝑋2 and 𝑋3 = 𝑋3. What degree 𝑑 should I choose? • What about 𝑑 here - do we have a tuning-problem again? • In this case not really . In practice, values of 𝑑 greater than 3 or 4 are rarely used. 8.2. POL YNOMIAL REGRESSION MODELS 157 −2 −1 0 1 2 0 5 10 Predictor Response Figure 8.3: Third-order polynomial regression li"
4690,unknown,"Predictor Response Figure 8.3: Third-order polynomial regression line through non-linear data. • This is because if 𝑑 is large the fitted curve becomes overly flexible/complex and can take very strange shapes. • W e want the curve to pass through the data in a smooth way . Example: Overfitting W e fit a polynomial of degree 3 to some training data. On the left of Figure 8.4, we compare the trainin"
4691,unknown,"the training data (red points) with their fitted values (that is, the model predictions at the training data inputs) joined up as a line. On the right, we compare some test data with their model predicted values (again, joined up as a line). W e can see the learned curve fits relatively nicely on both the training data and the test data (it is very similar in both plots, noting that the fit is onl"
4692,unknown,"noting that the fit is only plotted across the range of test points in the right-hand plot). W e then fit a polynomial of degree 14 to the same training data. On the left of Figure 8.5, we can see that the fitted curve tries to pass through every training data point. The result of this overfitting on prediction is clearly seen on the right. Example: W age data W e consider the Wage dataset from li"
4693,unknown,"W e consider the Wage dataset from library ISLR, which contains income and demographic information for males who reside in the central Atlantic region of the United States. In particular, each panel of Figure 8.6 is a plot of wage against age. W e then see the results of plotting polynomial models ̂𝑓 (𝑥) (solid lines) of age for wage of various degrees - 2, 3 and 4 respectively . W e can see some "
4694,unknown,"respectively . W e can see some slight differences between the models for this data. The purpose of fitting any of these polynomial models is because we are interested in mod- elling wage as a function ̂𝑓 of the predictor age as best we can, this leading to an understand- ing of the relationship between age and wage. F or example. in the left-hand panel we have a degree-2 polynomial fit ̂𝑓 . This "
4695,unknown,158 CHAPTER 8. POL YNOMIAL REGRESSION −2 −1 0 1 2 −4 0 2 4 6 8 10 Training data Predictor Response −2 −1 0 1 2 −4 0 2 4 6 8 10 New data Predictor Response Figure 8.4: Degree-3 polynomial regression line through training and test data. −2 −1 0 1 2 −4 0 2 4 6 8 10 Training data Predictor Response −2 −1 0 1 2 −4 0 2 4 6 8 10 New data Predictor Response Figure 8.5: Degree-14 polynomial regression line
4696,unknown,"8.3. PRACTICAL DEMONSTRA TION 159 age, 𝑥0 say , as ̂𝑓 (𝑥0) = ̂𝛽0 + ̂𝛽1𝑥0 + ̂𝛽2𝑥2 0 where ̂𝛽0, ̂𝛽1, ̂𝛽2 are the least squares fitted coeﬀicients. Least squares also returns variance estimates for each of the fitted coeﬀicients ̂𝛽0, ̂𝛽1, ̂𝛽2, as well as the covariances between pairs of coeﬀicient estimates. W e can use these to estimate the point-wise standard error of the (mean) prediction ̂𝑓 (𝑥0)."
4697,unknown,"The coloured points on either side of the solid-line fitted curve are therefore 2× standard error curves, that is they correspond to ̂𝑓 (𝑥) ± 2 ×SE[ ̂𝑓 (𝑥)], where SE [ ̂𝑓 (𝑥)] corresponds to the standard error on the mean prediction at 𝑥. W e plot twice the standard error because, for normally distributed error terms, this quantity corresponds to an approximate95% confidence interval (in this cas"
4698,unknown,"20 40 60 80 50 150 250 Degree−2 Polynomial Wage$age Wage$wage 20 40 60 80 50 150 250 Degree−3 Polynomial Wage$age Wage$wage 20 40 60 80 50 150 250 Degree−4 Polynomial Wage$age Wage$wage Figure 8.6: Plots of ‘wage‘ against ‘age‘ and the fits from three polynomial models. Disadvantages • The fitted curve from polynomial regression is obtained by global training. That is, we use the entire range of v"
4699,unknown,"use the entire range of values of the predictor to fit the curve. • This can be problematic: if we get new samples from a specific subregion of the predictor this might change the shape of the curve in other subregions! • Ideally , we would like the curve to adapt local ly within subregions which are relevant to the analysis. • A first simple approach for resolving this is via step functions (see "
4700,unknown,8.3 Practical Demonstration In this section we will start with the analysis of the Boston dataset included in the R library MASS. This dataset consists of 506 samples. The response variable is median value of owner- occupied homes in Boston ( medv). The dataset has 13 associated predictor variables. 160 CHAPTER 8. POL YNOMIAL REGRESSION Initial steps W e load libraryMASS(installing the package is 
4701,unknown,"W e load libraryMASS(installing the package is not needed forMASS), check the help document for Boston, check for missing entries and use commands names() and head() to get a better picture of how this dataset looks like. library(MASS) help(Boston) sum(is.na(Boston)) ## [1] 0 names(Boston) ## [1] ""crim"" ""zn"" ""indus"" ""chas"" ""nox"" ""rm"" ""age"" ## [8] ""dis"" ""rad"" ""tax"" ""ptratio"" ""black"" ""lstat"" ""medv"" "
4702,unknown,head(Boston) ## crim zn indus chas nox rm age dis rad tax ptratio black lstat ## 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 4.98 ## 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 9.14 ## 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 ## 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 ## 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622
4703,unknown,"## 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 5.21 ## medv ## 1 24.0 ## 2 21.6 ## 3 34.7 ## 4 33.4 ## 5 36.2 ## 6 28.7 W e will analyse medv with respect to the predictor lstat (percentage of lower status popu- lation). head(cbind(Boston$medv, Boston$lstat)) ## [,1] [,2] ## [1,] 24.0 4.98 ## [2,] 21.6 9.14 ## [3,] 34.7 4.03 ## [4,] 33.4 2.94 ## [5,] 36.2 5.33 ## [6,] 28.7 5.21 F "
4704,unknown,"F or convenience we will just name the responsey and the predictor x. W e will also pre-define the labels for the x and y-axes that we will use repeatedly in figures throughout this practical. y = Boston$medv x = Boston$lstat 8.3. PRACTICAL DEMONSTRA TION 161 y.lab = 'Median Property Value' x.lab = 'Lower Status (%)' First, lets plot the two variables. plot( x, y, cex.lab = 1.1, col=""darkgrey"", xl"
4705,unknown,"main = """", bty = 'l' ) 10 20 30 10 20 30 40 50 Lower Status (%) Median Property Value The plot suggests a non-linear relationship between medv and lstat. Polynomial regression Let us start by fitting to the data a degree-2 polynomial using the command lm() and summarising the results using summary(). poly2 = lm( y ~ x + I(x^2) ) summary( poly2 ) ## ## Call: ## lm(formula = y ~ x + I(x^2)) ## ## Re"
4706,unknown,## Min 1Q Median 3Q Max ## -15.2834 -3.8313 -0.5295 2.3095 25.4148 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 42.862007 0.872084 49.15 <0.0000000000000002 *** ## x -2.332821 0.123803 -18.84 <0.0000000000000002 *** ## I(x^2) 0.043547 0.003745 11.63 <0.0000000000000002 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 162 CHAPTER 8. POL YNOMI
4707,unknown,"## ## Residual standard error: 5.524 on 503 degrees of freedom ## Multiple R-squared: 0.6407, Adjusted R-squared: 0.6393 ## F-statistic: 448.5 on 2 and 503 DF, p-value: < 0.00000000000000022 Important note: Above we see that we need to enclose x^2 within the envelope function I(). This is because x^2 is a function of x and when we use a function (any function) of a predictor in lm() we need to do "
4708,unknown,"the usual output from summary(). The above syntax is not very convenient because we need to add manually further polynomial terms. F or instance, for a 4-degree polynomial we would need to use lm(y ~ x + I(x^2)+ I(x^3)+ I(x^4)). F ortunately , we have the functionpoly() which makes things easier for us. poly2 = lm(y ~ poly(x, 2, raw = TRUE)) summary(poly2) ## ## Call: ## lm(formula = y ~ poly(x, 2"
4709,unknown,"## ## Residuals: ## Min 1Q Median 3Q Max ## -15.2834 -3.8313 -0.5295 2.3095 25.4148 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 42.862007 0.872084 49.15 <0.0000000000000002 *** ## poly(x, 2, raw = TRUE)1 -2.332821 0.123803 -18.84 <0.0000000000000002 *** ## poly(x, 2, raw = TRUE)2 0.043547 0.003745 11.63 <0.0000000000000002 *** ## --- ## Signif. codes: 0 '***' 0.001 '"
4710,unknown,"## ## Residual standard error: 5.524 on 503 degrees of freedom ## Multiple R-squared: 0.6407, Adjusted R-squared: 0.6393 ## F-statistic: 448.5 on 2 and 503 DF, p-value: < 0.00000000000000022 The argument raw = TRUE above is used in order to get the same coeﬀicients as previously . If we do not use this argument the coeﬀicients will differ because they will be calculated on a different (orthogonal)"
4711,unknown,"a different (orthogonal) basis. However, this may not be that important because with poly- nomial regression it is often the case that we are not interested in the regression coeﬀicients as the model becomes more complex. In terms of fitting the curve poly(x, 2, raw = TRUE)) and poly(x, 2)) will give the same result! Now, lets see how we can produce a plot similar to those shown in the W age data "
4712,unknown,"A first important step is that we need to create an object, which we name sort.x, which has the sorted values of predictor x in a ascending order. Without sort.x we will not be able to produce the plots! Then, we need to use predict() with sort.x as input in order 8.3. PRACTICAL DEMONSTRA TION 163 to proceed to the next steps. sort.x = sort(x) sort.x[1:10] # the first 10 sorted values of x ## [1] "
4713,unknown,"## [1] 1.73 1.92 1.98 2.47 2.87 2.88 2.94 2.96 2.97 2.98 pred2 = predict(poly2, newdata = list(x = sort.x), se = TRUE) names(pred2) ## [1] ""fit"" ""se.fit"" ""df"" ""residual.scale"" The object pred2 contains fit, which are the fitted values , and se.fit, which are the standard errors of the mean prediction, that we need in order to construct the approximate 95% confidence intervals (of the mean predicti"
4714,unknown,95% confidence intervals (of the mean prediction). With this information we can construct the confidence intervals using cbind(). Lets see how the first 10 fitted values and confidence intervals look like. pred2$fit[1:10] # the first 10 fitted values of the curve ## 1 2 3 4 5 6 7 8 ## 38.95656 38.54352 38.41374 37.36561 36.52550 36.50468 36.37992 36.33840 ## 9 10 ## 36.31765 36.29691 se.bands2 = c
4715,unknown,"se.bands2 = cbind( pred2$fit - 2 * pred2$se.fit, pred2$fit + 2 * pred2$se.fit ) se.bands2[1:10,] # the first 10 confidence intervals of the curve ## [,1] [,2] ## 1 37.58243 40.33069 ## 2 37.20668 39.88036 ## 3 37.08853 39.73895 ## 4 36.13278 38.59845 ## 5 35.36453 37.68647 ## 6 35.34546 37.66390 ## 7 35.23118 37.52865 ## 8 35.19314 37.48365 ## 9 35.17413 37.46117 ## 10 35.15513 37.43870 Now that w"
4716,unknown,"## 10 35.15513 37.43870 Now that we have all that is needed we can finally produce the plot! Final detail: Below we use lines() for pred2$fit because this is a vector, but for se.bands2, which is a matrix, we have to use matlines(). plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Degree-2 polynomial"", bty = 'l') lines(sort.x, pred2$fit, lwd = 2, col = ""red"") matlines("
4717,unknown,"164 CHAPTER 8. POL YNOMIAL REGRESSION 10 20 30 10 20 30 40 50 Degree−2 polynomial Lower Status (%) Median Property Value Having seen this procedure in detail once, plotting the fitted curves for polynomials of dif- ferent degrees is straightforward. The code below produces a plot of degree-2 up to degree-5 polynomial fits. poly3 = lm(y ~ poly(x, 3)) poly4 = lm(y ~ poly(x, 4)) poly5 = lm(y ~ poly(x"
4718,unknown,"poly5 = lm(y ~ poly(x, 5)) pred3 = predict(poly3, newdata = list(x = sort.x), se = TRUE) pred4 = predict(poly4, newdata = list(x = sort.x), se = TRUE) pred5 = predict(poly5, newdata = list(x = sort.x), se = TRUE) se.bands3 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit) se.bands4 = cbind(pred4$fit + 2*pred4$se.fit, pred4$fit-2*pred4$se.fit) se.bands5 = cbind(pred5$fit + 2*pred5$se.fi"
4719,unknown,"par(mfrow = c(2,2)) # Degree-2 plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Degree-2 polynomial"", bty = 'l') lines(sort.x, pred2$fit, lwd = 2, col = ""red"") matlines(sort.x, se.bands2, lwd = 2, col = ""red"", lty = 3) # Degree-3 plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Degree-3 polynomial"", bty = 'l') lines(sort.x, pred3$fit, lwd ="
4720,unknown,"matlines(sort.x, se.bands3, lwd = 2, col = ""darkviolet"", lty = 3) 8.3. PRACTICAL DEMONSTRA TION 165 # Degree-4 plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Degree-4 polynomial"", bty = 'l') lines(sort.x, pred4$fit, lwd = 2, col = ""blue"") matlines(sort.x, se.bands4, lwd = 2, col = ""blue"", lty = 3) # Degree-5 plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, yla"
4721,unknown,"main = ""Degree-5 polynomial"", bty = 'l') lines(sort.x, pred5$fit, lwd = 2, col = ""black"") matlines(sort.x, se.bands5, lwd = 2, col = ""black"", lty = 3) 10 20 30 10 20 30 40 50 Degree−2 polynomial Lower Status (%) Median Property Value 10 20 30 10 20 30 40 50 Degree−3 polynomial Lower Status (%) Median Property Value 10 20 30 10 20 30 40 50 Degree−4 polynomial Lower Status (%) Median Property Value "
4722,unknown,"10 20 30 10 20 30 40 50 Degree−5 polynomial Lower Status (%) Median Property Value F or this data it is not clear which curve fits better. All four curves look reasonable given the data that we have available. Without further indications, one may choose the degree-2 polynomial since it is simpler and seems to do about as well as the others. However, if we 166 CHAPTER 8. POL YNOMIAL REGRESSION want"
4723,unknown,"want to base our decision on a more formal procedure, rather than on a subjective decision, one option is to use the classical statistical methodology of analysis-of-variance (ANOV A). Specifically , we will perform sequential comparisons based on the F-test, comparing first the linear model vs. the quadratic model (degree-2 polynomial), then the quadratic model vs. the cubic model (degree-3 polyn"
4724,unknown,"vs. the cubic model (degree-3 polynomial) and so on. W e therefore have to fit the simple linear model, and we also choose to fit the degree-6 polynomial to investigate the effects of an additional predictor as well. W e can perform this analysis in RStudio using the command anova() as displayed below. poly1 = lm(y ~ x) poly6 = lm(y ~ poly(x, 6)) anova(poly1, poly2, poly3, poly4, poly5, poly6) ## "
4725,unknown,"## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ poly(x, 2, raw = TRUE) ## Model 3: y ~ poly(x, 3) ## Model 4: y ~ poly(x, 4) ## Model 5: y ~ poly(x, 5) ## Model 6: y ~ poly(x, 6) ## Res.Df RSS Df Sum of Sq F Pr(>F) ## 1 504 19472 ## 2 503 15347 1 4125.1 151.8623 < 0.00000000000000022 *** ## 3 502 14616 1 731.8 26.9390 0.0000003061 *** ## 4 501 13968 1 647.8 23.8477 0.0000014062 "
4726,unknown,## 5 500 13597 1 370.7 13.6453 0.0002452 *** ## 6 499 13555 1 42.4 1.5596 0.2123125 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 The hypothesis that is checked at each step is that the decrease in RSS is not significant . The hypothesis is rejected if the p-value (column Pr(>F)) is smaller than a given significance level (say 0.05). If the hypothesis is rejected then we 
4727,unknown,"level (say 0.05). If the hypothesis is rejected then we move on to the next comparison. Based on this reasoning and the results above we would select the 5-degree polynomial. Note that alternatively , we can always use (yes, you guessed it…) cross-validation to decide the degree of the polynomial model! W e could also try polynomial regression with multiple predictors. F or example, include averag"
4728,unknown,"average number of rooms per dwelling rm as the second predictor. Note that in this case we will also including the interaction terms 𝑥1𝑥2. x1=Boston$lstat x2=Boston$rm polym1 <- lm(y ~ poly(x1, 2) + poly(x2 , 2) + x1:x2) summary(polym1) 8.3. PRACTICAL DEMONSTRA TION 167 ## ## Call: ## lm(formula = y ~ poly(x1, 2) + poly(x2, 2) + x1:x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.9520 -2.5690 -"
4729,unknown,"## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 40.20765 4.91368 8.183 0.000000000000002315 *** ## poly(x1, 2)1 106.09456 61.94682 1.713 0.08739 . ## poly(x1, 2)2 13.47448 7.30579 1.844 0.06572 . ## poly(x2, 2)1 108.16782 12.75828 8.478 0.000000000000000259 *** ## poly(x2, 2)2 36.83909 6.27520 5.871 0.000000007919304425 *** ## x1:x2 -0.23121 0.06422 -3.600 0.00035 *** ##"
4730,unknown,"## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 4.554 on 500 degrees of freedom ## Multiple R-squared: 0.7573, Adjusted R-squared: 0.7548 ## F-statistic: 312 on 5 and 500 DF, p-value: < 0.00000000000000022 # we could use polym() makes things easier for us polym2=lm(y ~ polym(x1, x2, degree=2) ) summary(polym2) ## ## Call: ## lm(formula = y ~ p"
4731,unknown,"## ## Call: ## lm(formula = y ~ polym(x1, x2, degree = 2)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.9520 -2.5690 -0.4172 2.0432 27.6293 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 21.8222 0.2828 77.178 < 0.0000000000000002 ## polym(x1, x2, degree = 2)1.0 -127.0837 6.6032 -19.246 < 0.0000000000000002 ## polym(x1, x2, degree = 2)2.0 13.4745 7.3058 1.844 0.06572 "
4732,unknown,"## polym(x1, x2, degree = 2)0.1 61.9766 6.0811 10.192 < 0.0000000000000002 ## polym(x1, x2, degree = 2)1.1 -585.8312 162.7253 -3.600 0.00035 ## polym(x1, x2, degree = 2)0.2 36.8391 6.2752 5.871 0.00000000792 ## ## (Intercept) *** ## polym(x1, x2, degree = 2)1.0 *** ## polym(x1, x2, degree = 2)2.0 . 168 CHAPTER 8. POL YNOMIAL REGRESSION ## polym(x1, x2, degree = 2)0.1 *** ## polym(x1, x2, degree = "
4733,unknown,"## polym(x1, x2, degree = 2)1.1 *** ## polym(x1, x2, degree = 2)0.2 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 4.554 on 500 degrees of freedom ## Multiple R-squared: 0.7573, Adjusted R-squared: 0.7548 ## F-statistic: 312 on 5 and 500 DF, p-value: < 0.00000000000000022 Chapter 9 Step F unctions 9.1 Step F unctions Step functions use cu"
4734,unknown,"Step functions use cut-points 𝑐1, 𝑐2, … , 𝑐𝐾 in the range of X in order to construct 𝐾 + 1 dummy variables in the following way: 𝐶0(𝑋) = 𝐼 (𝑋 ≤ 𝑐1) 𝐶1(𝑋) = 𝐼 (𝑐1 < 𝑋 ≤ 𝑐2) ⋮ 𝐶𝐾−1 (𝑋) = 𝐼 (𝑐𝐾−1 < 𝑋 ≤ 𝑐𝐾 ) 𝐶𝐾 (𝑋) = 𝐼 (𝑐𝐾 < 𝑋) Above 𝐼 (⋅) is the indicator function. F or example 𝐼 (𝑐1 < 𝑋 ≤ 𝑐 2) = 1 if 𝑐1 < 𝑋 ≤ 𝑐 2 and equals 0 otherwise. Note: trivial ly we have that: 𝐶0(𝑋) + 𝐶1(𝑋) + … + 𝐶𝐾 (𝑋) = 1 s"
4735,unknown,"one of the 𝐾 + 1 intervals. After defining appropriate intervals and constructing the corresponding dummy variables we fit LS to the following model: 𝑦 = 𝑓 (𝑥) = 𝛽0 + 𝛽1𝐶1(𝑥) + 𝛽2𝐶2(𝑥) + … + 𝛽𝐾 𝐶𝐾 (𝑥) + 𝜖. Inclusion of 𝐶0(𝑥) is not needed because this effect is represented by 𝛽0. This means that if 𝑋 ≤ 𝑐1 our prediction is 𝛽0, while if 𝑐𝑗 < 𝑋 ≤ 𝑐𝑗+1 we predict 𝛽0 + 𝛽𝑗 for 𝑗 = 1, … , 𝐾. In other wo"
4736,unknown,"𝑗 = 1, … , 𝐾. In other words, 𝛽0 can be interpreted as the mean value of 𝑌 for 𝑋 ≤ 𝑐 1, and 𝛽𝑗 represents the average increase in the response for 𝑋 in 𝑐𝑗 < 𝑋 ≤ 𝑐𝑗+1 relative to 𝑋 ≤ 𝑐1. Example: W age Data W e apply step-function regression to model the Wage data introduced in Section 8.2, with different numbers of step functions (Figure 9.1. W e can see that this modelling approach is local: new "
4737,unknown,is local: new samples within a sampling interval affect only the predicted effect within that interval. W e see that this approach creates a bumpy fit to the data. This is desirable sometimes in applications in biomedicine or social sciences where the goal is to report overall summaries within specific age groups. In other applications smoothness is preferable. 169 170 CHAPTER 9. STEP FUNCTIONS 20
4738,unknown,"20 40 60 80 50 150 250 3 cutpoints Wage$age Wage$wage 20 40 60 80 50 150 250 4 cutpoints Wage$age Wage$wage 20 40 60 80 50 150 250 5 cutpoints Wage$age Wage$wage Figure 9.1: Step function regression with different numbers of cutpoints applied to the W age data. 9.2 Practical Demonstration W e once again make use of the Boston dataset in library MASS. library(""MASS"") y = Boston$medv x = Boston$lsta"
4739,unknown,"y = Boston$medv x = Boston$lstat y.lab = 'Median Property Value' x.lab = 'Lower Status (%)' F or step function regression we can make use of the command cut(), which automatically assigns samples to intervals given a specific number of intervals. W e can check how this works by executing the following line of code. table(cut(x, 2)) ## ## (1.69,19.9] (19.9,38] ## 430 76 What we see is that cut(x, 2"
4740,unknown,"What we see is that cut(x, 2) automatically created a factor with two levels, corresponding to the intervals (1.69, 19.9] and (19.9, 38], and assigned each entry in 𝑥 to one of these factors depending on which interval it was in. The command table() tells us that 430 samples of x fall within the first interval and that 76 samples fall within the second interval. Note that cut(x, 2) generated 2 int"
4741,unknown,"cut(x, 2) generated 2 intervals, but this means there is only 1 cutpoint (at 19.9). The number of cutpoints is naturally one less than the number of intervals, but it is important to be aware that cut requires specification of the number of required intervals . So, we can use cut() within lm() to easily fit regression models with step functions. Below we consider 4 models with 1, 2, 3 and 4 cutpoi"
4742,unknown,"step2 = lm(y ~ cut(x, 2)) step3 = lm(y ~ cut(x, 3)) step4 = lm(y ~ cut(x, 4)) 9.2. PRACTICAL DEMONSTRA TION 171 step5 = lm(y ~ cut(x, 5)) The analysis then is essentially the same as previously . The following code produces the 2 by 2 plot of the fitted lines of the four models, along with approximate 95% confidence intervals for the mean predictions. pred2 = predict(step2, newdata = list(x = sort"
4743,unknown,"pred3 = predict(step3, newdata = list(x = sort(x)), se = TRUE) pred4 = predict(step4, newdata = list(x = sort(x)), se = TRUE) pred5 = predict(step5, newdata = list(x = sort(x)), se = TRUE) se.bands2 = cbind(pred2$fit + 2*pred2$se.fit, pred2$fit-2*pred2$se.fit) se.bands3 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit) se.bands4 = cbind(pred4$fit + 2*pred4$se.fit, pred4$fit-2*pred4$se."
4744,unknown,"se.bands5 = cbind(pred5$fit + 2*pred5$se.fit, pred5$fit-2*pred5$se.fit) par(mfrow = c(2,2)) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""1 cutpoint"", bty = 'l') lines(sort(x), pred2$fit, lwd = 2, col = ""red"") matlines(sort(x), se.bands2, lwd = 1.4, col = ""red"", lty = 3) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""2 cutpoints"", bty "
4745,unknown,"main = ""2 cutpoints"", bty = 'l') lines(sort(x), pred3$fit, lwd = 2, col = ""darkviolet"") matlines(sort(x), se.bands3, lwd = 1.4, col = ""darkviolet"", lty = 3) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""3 cutpoints"", bty = 'l') lines(sort(x), pred4$fit, lwd = 2, col = ""blue"") matlines(sort(x), se.bands4, lwd = 1.4, col = ""blue"", lty = 3) plot(x, y, cex.lab = 1.1, co"
4746,unknown,"main = ""4 cutpoints"", bty = 'l') lines(sort(x), pred5$fit, lwd = 2, col = ""black"") matlines(sort(x), se.bands5, lwd = 1.4, col = ""black"", lty = 3) 172 CHAPTER 9. STEP FUNCTIONS 10 20 30 10 20 30 40 50 1 cutpoint Lower Status (%) Median Property Value 10 20 30 10 20 30 40 50 2 cutpoints Lower Status (%) Median Property Value 10 20 30 10 20 30 40 50 3 cutpoints Lower Status (%) Median Property Value"
4747,unknown,"Median Property Value 10 20 30 10 20 30 40 50 4 cutpoints Lower Status (%) Median Property Value Note that we do not necessarily need to rely on the automatic selections of cutpoints used by cut(). W e can define the intervals if we want to. F or instance, if we want cutpoints at 10, 20 and 30 we can do the following breaks4 = c(min(x), 10, 20, 30, max(x)) table(cut(x, breaks = breaks4)) ## ## (1."
4748,unknown,"table(cut(x, breaks = breaks4)) ## ## (1.73,10] (10,20] (20,30] (30,38] ## 218 213 62 12 Note that when defining the sequence of breaks, we included min(x) and max(x) at the start and end in order to ensure the intervals covered the entire range of x. Then our model would be: step.new4 = lm(y ~ cut(x, breaks = breaks4)) summary(step.new4) 9.2. PRACTICAL DEMONSTRA TION 173 ## ## Call: ## lm(formula"
4749,unknown,"## ## Call: ## lm(formula = y ~ cut(x, breaks = breaks4)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.4803 -4.6239 -0.4239 2.8968 20.6197 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 29.3803 0.4415 66.540 <0.0000000000000002 ## cut(x, breaks = breaks4)(10,20] -10.4563 0.6281 -16.648 <0.0000000000000002 ## cut(x, breaks = breaks4)(20,30] -16.6770 0.9383 -17.773 <0."
4750,unknown,"## cut(x, breaks = breaks4)(30,38] -18.6886 1.9331 -9.668 <0.0000000000000002 ## ## (Intercept) *** ## cut(x, breaks = breaks4)(10,20] *** ## cut(x, breaks = breaks4)(20,30] *** ## cut(x, breaks = breaks4)(30,38] *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 6.519 on 501 degrees of freedom ## (1 observation deleted due to missingness) ##"
4751,unknown,"## Multiple R-squared: 0.4925, Adjusted R-squared: 0.4895 ## F-statistic: 162.1 on 3 and 501 DF, p-value: < 0.00000000000000022 Although somewhat trivial, we can make predictions at new data points using the constructed linear model as usual. newx <- c(10.56, 5.89) preds = predict(step.new4, newdata = list(x = newx), se = TRUE) preds ## $fit ## 1 2 ## 18.92394 29.38028 ## ## $se.fit ## 1 2 ## 0.44"
4752,unknown,"## $se.fit ## 1 2 ## 0.4466955 0.4415432 ## ## $df ## [1] 501 ## ## $residual.scale ## [1] 6.519307 174 CHAPTER 9. STEP FUNCTIONS Chapter 10 Splines 10.1 Regression Splines Why Splines? • W e have seen that polynomial regression leads to flexible and smooth curves, but is trained globally which is problematic. • Step functions are trained locally but produce “bumpy” fits, which are desirable only "
4753,unknown,in specific applications. • Regression splines bridge these differences by providing adaptive local smoothness. Piecewise Polynomials • Regression based on splines is a general approach which encompasses different models. • The basis of regression splines is piecewise polynomial regression. • Piecewise polynomials are not fitted over the entire range of𝑋 but over different regions of 𝑋. • Polynomi
4754,unknown,"mial regression. Standard polynomial regression: 𝑦𝑖 = 𝛽0 + 𝛽1𝑥𝑖 + 𝛽2𝑥2 𝑖 + 𝛽3𝑥3 𝑖 + … + 𝛽𝑑𝑥𝑑 𝑖 + 𝜖𝑖. Piecewise polynomial regression: 𝑦𝑖 = {𝛽01 + 𝛽11𝑥𝑖 + 𝛽21𝑥2 𝑖 + 𝛽31𝑥3 𝑖 + … + 𝛽𝑑1𝑥𝑑 𝑖 + 𝜖𝑖, if 𝑥𝑖 < 𝑐 𝛽02 + 𝛽12𝑥𝑖 + 𝛽22𝑥2 𝑖 + 𝛽32𝑥3 𝑖 + … + 𝛽𝑑2𝑥𝑑 𝑖 + 𝜖𝑖, if 𝑥𝑖 ≥ 𝑐, where 𝑖 = 1, … , 𝑛in both cases. • The above piecewise polynomial requires two LS fits: one for the samples where 𝑥 < 𝑐, and one for th"
4755,unknown,"and one for the samples where 𝑥 ≥ 𝑐. • Within this framework the cutpoint 𝑐 is called a knot. • When there is no knot we have standard polynomial regression. • When we include only the intercept terms ( 𝛽01, 𝛽02) we have step-function regression. 175 176 CHAPTER 10. SPLINES Knots • Before we had 1 knot leading to 2 different models. • Of course, we can introduce more than one knot. In general, if "
4756,unknown,"are fitting 𝐾 + 1 polynomial models. • Using more knots leads to more flexible polynomials. • Of course, too many knots → overfitting! • More on the selection of number of knots later… Polynomial Degree Linear piecewise: 𝑦 = {𝛽01 + 𝛽11𝑥, if 𝑥𝑖 < 𝑐, 𝛽02 + 𝛽12𝑥, if 𝑥𝑖 ≥ 𝑐. Cubic piecewise: 𝑦 = {𝛽01 + 𝛽11𝑥 + 𝛽21𝑥2 + 𝛽31𝑥3, if 𝑥𝑖 < 𝑐, 𝛽02 + 𝛽12𝑥 + 𝛽22𝑥2 + 𝛽32𝑥3, if 𝑥𝑖 ≥ 𝑐. • Here, for simplicity we co"
4757,unknown,"• In the first case 𝑑 = 1, while in the second 𝑑 = 3. • The quadratic case with 𝑑 = 2 is usually not considered: it is typically outperformed by the cubic case under complex data structures and does not perform remarkably better than the linear case on simpler data structures. • In general, cubic piecewise is the most popular choice. Example: W age Data W e consider theWage data once again, howeve"
4758,unknown,"W e consider theWage data once again, however, for visual clarity only a subset of the data is considered. In Figure 10.1 James et al. [2013], we have fit a piecewise cubic polynomial with a single knot placed at age = 50. W e can see that there are two different curves on the left and right of the knot (which is what we wanted), however, the discontinuity at age = 50 is obviously not what we want"
4759,unknown,"obviously not what we wanted! How do we fix this? The answer is to use constraints again! The two polynomials in the above plot are of the following form. wage = {𝑓1(age) = 𝛽01 + 𝛽11age + 𝛽21age2 + 𝛽31age3, if age < 50, 𝑓2(age) = 𝛽02 + 𝛽12age + 𝛽22age2 + 𝛽32age3, if age ≥ 50, W e can make these two cubic polynomial lines meet at age = 50 by imposing the constraint 𝛽01 + 𝛽1150 + 𝛽21502 + 𝛽31503 = 𝛽"
4760,unknown,"𝛽01 + 𝛽1150 + 𝛽21502 + 𝛽31503 = 𝛽02 + 𝛽1250 + 𝛽22502 + 𝛽32503 Where we can see this as a constraint on a single parameter, since, by rearranging, we get 𝛽01 = 𝛽02 + (𝛽12 − 𝛽11)50 + (𝛽22 − 𝛽21)502 + (𝛽32 − 𝛽31)503 10.1. REGRESSION SPLINES 177 Figure 10.1: Unconstrained piecewise cubic polynomials are fit to a subset of the W age data. In other words, if we have estimates for ̂𝛽11, ̂𝛽21, ̂𝛽31, ̂𝛽02,"
4761,unknown,"defined by the constraint. This constraint leads to the continuous piecewise cubic polynomial seen in Figure 10.2. Figure 10.2: Constrained piecewise cubic polynomials are fit to a subset of the W age data. This looks better, although the small dip at age = 50 doesn’t make it perfect. W e therefore extend further the strategy based on constraints. • So far we have set 𝑓1(age = 50) = 𝑓2(age = 50), "
4762,unknown,"• W e can do the same for the first derivative : 𝑓 ′ 1(age = 50) = 𝑓 ′ 2(age = 50). This will 178 CHAPTER 10. SPLINES constrain an additional parameter (coeﬀicient), but it will make the function smoother at age = 50. • Doing the same for the second derivative: 𝑓 ″ 1 (age = 50) = 𝑓 ″ 2 (age = 50), will constrain one more parameter, and the function will become very smooth at age = 50. Can we add a"
4763,unknown,"Can we add a further constraint on the third derivative? No, our function is cubic so the 3rd derivative is a constant (it is not a function of age). A cubic piecewise polynomial with these three constraints is called a cubic spline. W e fit a cubic spline in Figure 10.3, Figure 10.3: A cubic spline is fit to a subset of the W age data. Cubic splines are so popular because they look perfectly smoo"
4764,unknown,"that is the claim… Constraints and Degrees of F reedom • In the previous example, we started with a cubic piecewise polynomial with 8 uncon- strained parameters, so we started with 8 degrees of freedom (df). • W e initially imposed one constraint, which restricted one parameter, so we lost a degree of freedom → 8 - 1 = 7 df. • With the further two constraints: 8 - 3 = 5 df. In general , a cubic sp"
4765,unknown,"In general , a cubic spline with 𝐾 knots has 4 + 𝐾 degrees of freedom. This is useful to know because as we will see in RStudio we can specify either the number of knots 𝐾 or just the degrees of freedom. General Definition A degree-𝑑 regression spline is a piecewise degree-𝑑 polynomial with continuity in derivatives up to degree 𝑑 − 1 at each knot . 10.1. REGRESSION SPLINES 179 The degrees of free"
4766,unknown,"• Linear spline: just continuous (2+ 𝐾 df) • Quadratic spline: continuous and continuous 1st derivative (3+ 𝐾 df) • Cubic spline: continuous and continuous 1st and 2nd derivatives (4+ 𝐾 df) • And so on… In general, regression splines give us the maximum amount of continuity we can have given the degree 𝑑. Spline Basis Representation A degree- 𝑑 spline with knots at 𝜉𝑘 for 𝑘 = 1, … , 𝐾 can be repre"
4767,unknown,"basis functions, denoted by 𝑏𝑖 for 𝑖 = 1, … , 𝐾 + 𝑑, so that: 𝑦 = 𝛽0 + 𝛽1𝑏1(𝑥) + … + 𝛽𝐾+𝑑 𝑏𝐾+𝑑 (𝑥) + 𝜖. where: 𝑏1(𝑥) = 𝑥1 ⋮ 𝑏𝑑(𝑥) = 𝑥𝑑 𝑏(𝑘+𝑑)(𝑥) = (𝑥 − 𝜉𝑘)𝑑 +, 𝑘 = 1, … , 𝐾 , where (𝑥 − 𝜉𝑘)𝑑 + = {(𝑥 − 𝜉𝑘)𝑑 if 𝑥 > 𝜉𝑘 0 otherwise. Example: linear spline, one knot In the illustration shown in Figure10.4, we just have𝑦 = {𝛽1𝑥, if 𝑥 < knot 𝛽1𝑥 + 𝛽2(𝑥 − knot), if 𝑥 ≥ knot. See the recommended literature"
4768,unknown,"Example: W age Data In Figure 10.5, we fit a linear, quadratic and cubic spline with three knots (at the dotted lines) to a subset of the Wage data. ## Warning in if (se.fit) list(fit = predictor, se.fit = se, df = df, ## residual.scale = sqrt(res.var)) else predictor: the condition has length > 1 and ## only the first element will be used ## Warning in if (se.fit) list(fit = predictor, se.fit = s"
4769,unknown,"## residual.scale = sqrt(res.var)) else predictor: the condition has length > 1 and ## only the first element will be used 180 CHAPTER 10. SPLINES Figure 10.4: An illustration of a linear spline with a single knot. ## Warning in if (se.fit) list(fit = predictor, se.fit = se, df = df, ## residual.scale = sqrt(res.var)) else predictor: the condition has length > 1 and ## only the first element will "
4770,unknown,"20 40 60 80 50 100 200 300 Linear spline age 20 40 60 80 50 100 200 300 Quadratic spline wage 20 40 60 80 50 100 200 300 Cubic spline Figure 10.5: A linear, quadratic and cubic spline with three knots fit to a subset of the W age data. 10.2 Practical Demonstration W e will continue analysing the Boston dataset included in library MASS. Initial steps W e load libraryMASS again. As previously , we w"
4771,unknown,"W e load libraryMASS again. As previously , we will analysemedv with respect to the predictor lstat. Once again, for convenience, we will name the response y and the predictor x, and pre-define plot axis labels. 10.2. PRACTICAL DEMONSTRA TION 181 library(MASS) y = Boston$medv x = Boston$lstat y.lab = 'Median Property Value' x.lab = 'Lower Status (%)' plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab "
4772,unknown,"main = """", bty = 'l') 10 20 30 10 20 30 40 50 Lower Status (%) Median Property Value Regression Splines F or this analysis we will require package splines. library(splines) Initially let’s fit regression splines by specifying knots. F rom the previous plot it is not clear where exactly we should place knots, so we will make use of the command summary in order to find the 25th, 50th and 75th percen"
4773,unknown,"place the knots. summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.73 6.95 11.36 12.65 16.95 37.97 cuts = summary(x)[c(2, 3, 5)] cuts ## 1st Qu. Median 3rd Qu. ## 6.950 11.360 16.955 Also, before we fit the splines it is again important to sort the variable x. sort.x = sort(x) F or a start lets fit a linear spline using our selected placement of knots. F or this we can use command lm() and "
4774,unknown,"command lm() and inside it we use the command bs() in which we specify degree = 1 for a linear spline and knots = cuts for the placement of the knots at the three percentiles. W e 182 CHAPTER 10. SPLINES also calculate the corresponding fitted values and confidence intervals exactly in the same way we did in previous practical demonstrations. spline1 = lm(y ~ bs(x, degree = 1, knots = cuts)) pred1"
4775,unknown,"pred1 = predict(spline1, newdata = list(x = sort.x), se = TRUE) se.bands1 = cbind(pred1$fit + 2 * pred1$se.fit, pred1$fit - 2 * pred1$se.fit) Having done that we can now produce the corresponding plot. plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Linear Spline"", bty = 'l') lines(sort.x, pred1$fit, lwd = 2, col = ""red"") matlines(sort.x, se.bands1, lwd = 2, col = ""re"
4776,unknown,"10 20 30 10 20 30 40 50 Linear Spline Lower Status (%) Median Property Value Using ?bs we see that instead of using the argument knots we can use the argument df, which are the degrees of freedom. W e know that splines have (𝑑 + 1) + 𝐾degrees of freedom, where 𝑑 is the degree of the polynomial. So in this case we have 1+1+3 = 5 degrees of freedom. Selecting df = 5 in bs() will automatically use 3 "
4777,unknown,"freedom. Selecting df = 5 in bs() will automatically use 3 knots placed at the 25th, 50th and 75th percentiles. Below we check whether the plot based on df=5 is indeed the same as the previous plot and as we can see it is. spline1df = lm(y ~ bs(x, degree = 1, df = 5)) pred1df = predict(spline1df, newdata = list(x = sort.x), se = TRUE) se.bands1df = cbind( pred1df$fit + 2 * pred1df$se.fit, pred1df$"
4778,unknown,"pred1df$fit - 2 * pred1df$se.fit ) par(mfrow = c(1, 2)) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Linear Spline (with knots)"", bty = 'l') lines(sort.x, pred1$fit, lwd = 2, col = ""red"") matlines(sort.x, se.bands1, lwd = 2, col = ""red"", lty = 3) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Linear Spline (with df)"", bty = 'l') 10.2."
4779,unknown,"10.2. PRACTICAL DEMONSTRA TION 183 lines(sort.x, pred1df$fit, lwd = 2, col = ""darkred"") matlines(sort.x, se.bands1df, lwd = 2, col = ""red"", lty = 3) 10 20 30 10 20 30 40 50 Linear Spline (with knots) Lower Status (%) Median Property Value 10 20 30 10 20 30 40 50 Linear Spline (with df) Lower Status (%) Median Property Value Having seen how this works we can also fit a degree-2 (quadratic) and degr"
4780,unknown,"Having seen how this works we can also fit a degree-2 (quadratic) and degree-3 (cubic) spline to the data, all we have to do is change degree = 1 to degree = 2 and degree = 3 respectively . Also we increase the respective degrees of freedom from df = 5 to df = 6 and df = 7 in order to keep the same number (and position) of knots in the quadratic and cubic spline models. spline2 = lm(y ~ bs(x, degr"
4781,unknown,"pred2 = predict(spline2, newdata = list(x = sort.x), se = TRUE) se.bands2 = cbind(pred2$fit + 2 * pred2$se.fit, pred2$fit - 2 * pred2$se.fit) spline3 = lm(y ~ bs(x, degree = 3, df = 7)) pred3 = predict(spline3, newdata = list(x = sort.x), se = TRUE) se.bands3 = cbind(pred3$fit + 2 * pred3$se.fit, pred3$fit - 2 * pred3$se.fit) par(mfrow = c(1,3)) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.l"
4782,unknown,"main = ""Linear Spline"", bty = 'l') lines(sort.x, pred1$fit, lwd = 2, col = ""darkred"") matlines(sort.x, se.bands1, lwd = 2, col = ""darkred"", lty = 3) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Quadratic Spline"", bty = 'l') lines(sort.x, pred2$fit, lwd = 2, col = ""darkgreen"") matlines(sort.x, se.bands2, lwd = 2, col = ""darkgreen"", lty = 3) plot(x, y, cex.lab = 1.1,"
4783,unknown,"main = ""Cubic Spline"", bty = 'l') lines(sort.x, pred3$fit, lwd = 2, col = ""darkblue"") 184 CHAPTER 10. SPLINES matlines(sort.x, se.bands3, lwd = 2, col = ""darkblue"", lty = 3) 10 20 30 10 20 30 40 50 Linear Spline Lower Status (%) Median Property Value 10 20 30 10 20 30 40 50 Quadratic Spline Lower Status (%) Median Property Value 10 20 30 10 20 30 40 50 Cubic Spline Lower Status (%) Median Property"
4784,unknown,"10 20 30 10 20 30 40 50 Cubic Spline Lower Status (%) Median Property Value 10.3 Natural Splines Splines, especially cubic splines, seem to be the best tool we have so far, right? W ell, there is always room for improvement: • Regression splines usually have high variance at the outer range of the predictor (the tails). • Sometimes the confidence intervals at the tails are wiggly (especially for s"
4785,unknown,• Sometimes the confidence intervals at the tails are wiggly (especially for small sample sizes). Natural splines are extensions of regression splines which remedy these problems to some extent. Additional Constraints Natural splines impose two additional constraints at each boundary region: • The spline function is constrained to be close to linear when 𝑋 < smallest knot ( 𝑐1) by 𝑓 ″ 0 (𝑐1) = 0. 
4786,unknown,"• The spline function is constrained to be close to linear when 𝑋 > largest knot ( 𝑐𝑛) by 𝑓 ″ 𝑛 (𝑐𝑛) = 0. These additional constraints of natural splines general ly result in more stable estimates at the boundaries. Example: W age Data W e can see from Figure 10.6 that use of a natural spline has constrained the model at the lower and upper ends, and reduced the variance. 10.4. PRACTICAL DEMONSTRA"
4787,unknown,10.4. PRACTICAL DEMONSTRA TION 185 30 40 50 60 50 100 150 200 250 Cubic spline Natural spline Figure 10.6: Fitting cubic and natural splines to a subset of the W age data. How Many Knots and Where? • Provided there is evidence from the data we can do it empirically: – Place knots where it is clearly obvious that we have a distributional shift in direc- tion. – Place more knots on regions where we 
4788,unknown,"– Place fewer knots in places which look more stable. • Alternatively , we can place knots in a uniform fashion (e.g. at the 25th, 50th and 75th percentiles, as we saw how to do in Section 10.2). 10.4 Practical Demonstration F or natural splines, we can use the command ns() in RStudio. As with the command bs() previously , we again have the option to either specify the knots manually (via the argu"
4789,unknown,"knots) or to simply pre-define the degrees of freedom (via the argument df). Below we use the latter option to fit four natural splines with 1, 2, 3 and 4 degrees of freedom. As we see using 1 degree of freedom actually results in just a linear model. spline.ns1 = lm(y ~ ns(x, df = 1)) pred.ns1 = predict(spline.ns1, newdata = list(x = sort.x), se = TRUE) se.bands.ns1 = cbind(pred.ns1$fit + 2 * pre"
4790,unknown,"pred.ns1$fit - 2 * pred.ns1$se.fit) spline.ns2 = lm(y ~ ns(x, df = 2)) pred.ns2 = predict(spline.ns2, newdata = list(x = sort.x), se = TRUE) se.bands.ns2 = cbind(pred.ns2$fit + 2 * pred.ns2$se.fit, pred.ns2$fit - 2 * pred.ns2$se.fit) spline.ns3 = lm(y ~ ns(x, df = 3)) pred.ns3 = predict(spline.ns3, newdata = list(x = sort.x), se = TRUE) se.bands.ns3 = cbind(pred.ns3$fit + 2 * pred.ns3$se.fit, pred"
4791,unknown,"pred.ns3$fit - 2 * pred.ns3$se.fit) 186 CHAPTER 10. SPLINES spline.ns4 = lm(y ~ ns(x, df = 4)) pred.ns4 = predict(spline.ns4, newdata = list(x = sort.x), se = TRUE) se.bands.ns4 = cbind(pred.ns4$fit + 2 * pred.ns4$se.fit, pred.ns4$fit - 2 * pred.ns4$se.fit) par(mfrow = c(2, 2)) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Natural Spline (1 df)"", bty = 'l') lines(so"
4792,unknown,"main = ""Natural Spline (1 df)"", bty = 'l') lines(sort.x, pred.ns1$fit, lwd = 2, col = ""darkred"") matlines(sort.x, se.bands.ns1, lwd = 2, col = ""darkred"", lty = 3) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Natural Spline (2 df)"", bty = 'l') lines(sort.x, pred.ns2$fit, lwd = 2, col = ""darkgreen"") matlines(sort.x, se.bands.ns2, lwd = 2, col = ""darkgreen"", lty = 3) "
4793,unknown,"plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Natural Spline (3 df)"", bty = 'l') lines(sort.x, pred.ns3$fit, lwd = 2, col = ""darkblue"") matlines(sort.x, se.bands.ns3, lwd = 2, col = ""darkblue"", lty = 3) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Natural Spline (4 df)"", bty = 'l') lines(sort.x, pred.ns4$fit, lwd = 2, col = ""brown"") "
4794,unknown,"matlines(sort.x, se.bands.ns4, lwd = 2, col = ""brown"", lty = 3) 10.4. PRACTICAL DEMONSTRA TION 187 10 20 30 10 20 30 40 50 Natural Spline (1 df) Lower Status (%) Median Property Value 10 20 30 10 20 30 40 50 Natural Spline (2 df) Lower Status (%) Median Property Value 10 20 30 10 20 30 40 50 Natural Spline (3 df) Lower Status (%) Median Property Value 10 20 30 10 20 30 40 50 Natural Spline (4 df) "
4795,unknown,"10 20 30 10 20 30 40 50 Natural Spline (4 df) Lower Status (%) Median Property Value Below we plot the cubic spline next to the natural cubic spline for comparison. As we can see, the natural cubic spline is generally smoother and closer to linear on the right boundary of the predictor space, where it has, additionally , narrower confidence intervals in comparison to the cubic spline. par(mfrow = "
4796,unknown,"par(mfrow = c(1,2)) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Cubic Spline"", bty = 'l') lines(sort.x, pred3$fit, lwd = 2, col = ""blue"") matlines(sort.x, se.bands3, lwd = 2, col = ""blue"", lty = 3) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Natural Spline (3 df)"", bty = 'l') lines(sort.x, pred.ns3$fit, lwd = 2, col = ""darkblue"") "
4797,unknown,"matlines(sort.x, se.bands.ns3, lwd = 2, col = ""darkblue"", lty = 3) 188 CHAPTER 10. SPLINES 10 20 30 10 20 30 40 50 Cubic Spline Lower Status (%) Median Property Value 10 20 30 10 20 30 40 50 Natural Spline (3 df) Lower Status (%) Median Property Value 10.5 Smoothing Splines Smoothing splines are quite different from the non-linear modelling methods we have seen so far. Unlike regression splines an"
4798,unknown,"far. Unlike regression splines and natural splines, there are no knots! Smoothing splines turns the discrete problem of selecting a number of knots into a continuous penalisation problem. The maths here is rather complicated, so we mainly introduce this topic by intuition rather than go into a load of technical details. Minimisation W e seek a function 𝑔 among all possible functions (linear or non"
4799,unknown,"𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝑔(𝑥𝑖))2 + 𝜆 ∫ (𝑔″(𝑡)) 2 d𝑡, where 𝜆 ≥ 0 is a tuning parameter. The function 𝑔 that minimises the above quantity is called a smoothing spline. Model Fit + Penalty T erm 𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝑔(𝑥𝑖))2 ⏟ ⏟ ⏟ ⏟ ⏟ ⏟ ⏟ Model fit + 𝜆 ∫ (𝑔″(𝑡)) 2 d𝑡 ⏟ ⏟ ⏟ ⏟ ⏟ ⏟ ⏟ Penalty term • When 𝜆 = 0 we are just left with the model fit term. • When 𝜆 → ∞… well more on that later. • Lets examine the two terms sep"
4800,unknown,"10.5. SMOOTHING SPLINES 189 The Model Fit T erm 𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝑔(𝑥𝑖))2 This is a RSS but not our “usual” RSS: • In all previous approaches, function 𝑔 was linear in the parameters (coeﬀicients of the predictors) and we minimised with respect to these parameters. • Here, we have one predictor but the minimisation is with respect to 𝑔! • So, if we let 𝑔 be unconstrained and just minimise the above RS"
4801,unknown,"• So, if we let 𝑔 be unconstrained and just minimise the above RSS, then we can end up with any weird-looking non linear function that passes through exactly every data point, leading to an RSS equal to 0 ( overfitting!). Thus, the penalty on 𝑔… The Penalty T erm 𝜆 ∫ (𝑔″(𝑡)) 2 d𝑡 • 𝑔″(𝑡) is the 2nd derivative of 𝑔 at a point 𝑡 within the space of predictor 𝑋. • The 2nd derivative of a function cat"
4802,unknown,"• The 2nd derivative of a function catches wiggles or non-linearities: if 𝑔″(𝑡∗)2 at a point 𝑡∗ is large, this means that 𝑔 at this point 𝑡∗ suddenly jumps either up or down. • The integration above over every 𝑡 in the space of 𝑋 is essentially a sum. The larger it is the more non-linear the function 𝑔 is. Putting it all T ogether Minimizing 𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝑔(𝑥𝑖))2 + 𝜆 ∫ (𝑔″(𝑡)) 2 d𝑡, with respect t"
4803,unknown,"Minimizing 𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝑔(𝑥𝑖))2 + 𝜆 ∫ (𝑔″(𝑡)) 2 d𝑡, with respect to 𝑔 turns out to be interesting. • When 𝜆 = 0 we get an extremely wiggly non-linear function 𝑔 (completely useless). • As 𝜆 increases, the function 𝑔 will become smoother. • In the theoretical case of 𝜆 → ∞, 𝑔″ will be zero everywhere. When does that happen? When 𝑔(𝑥) = 𝛽0 + 𝛽1𝑥. So, in this case we return back to the linear model!"
4804,unknown,"That is why the penalty term is also called a roughness penalty within the framework of smoothing splines. Solution Surprisingly , the solution for any finite and non-zero 𝜆 is that the function 𝑔 is a natural cubic spline but with knots placed on each individual sample point 𝑥1, … , 𝑥𝑛. But doesn’t that lead to overfitting? No, because now we have once again the penalty parameter 𝜆 which can shri"
4805,unknown,"190 CHAPTER 10. SPLINES T uning𝜆 • As usual 𝜆 can be tuned via cross-validation. • Also, 𝜆 is associated with the effective degrees of freedom of a smoothing spline. These are similar to the degrees of freedom in standard spline models and can be used as an alternative to cross-validation as a way to fix 𝜆. 10.6 Practical Demonstration Boston Data F or fitting smoothing splines we use the commands"
4806,unknown,"Boston Data F or fitting smoothing splines we use the commandsmooth.splines()instead of lm(). Under smoothing splines there are no knots to specify; the only parameter is𝜆. This can be specified via cross-validation by specifyingcv = TRUEinside smooth.splines(). Alternatively , we can specify the effective degrees of freedom which correspond to some value of 𝜆. Below we use cross-validation as wel"
4807,unknown,"cross-validation as well as a smoothing spline with 3 effective degrees of freedom (via the argument df = 3). In this case we see that tuning 𝜆 through cross-validation results in a curve which is slightly wiggly on the right boundary of the predictor space. smooth1 = smooth.spline(x, y, df = 3) smooth2 = smooth.spline(x, y, cv = TRUE) par(mfrow = c(1,2)) plot(x, y, cex.lab = 1.1, col=""darkgrey"", "
4808,unknown,"main = ""Smoothing Spline (3 df)"", bty = 'l') lines(smooth1, lwd = 2, col = ""brown"") plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Smoothing Spline (CV)"", bty = 'l') lines(smooth2, lwd = 2, col = ""darkorange"") 10 20 30 10 20 30 40 50 Smoothing Spline (3 df) Lower Status (%) Median Property Value 10 20 30 10 20 30 40 50 Smoothing Spline (CV) Lower Status (%) Median Pr"
4809,unknown,"Lower Status (%) Median Property Value 10.6. PRACTICAL DEMONSTRA TION 191 Wage Data Lets see how cubic splines, natural cubic splines and smoothing splines compare on the Wage data. W e will take sample size into consideration and see how the fitted curves look like for: • A small sample of 50. • A medium sample of 200. • A large sample of 1000. F or the cubic spline and natural cubic spline we ha"
4810,unknown,"F or the cubic spline and natural cubic spline we have to define the number and position of the knots. F or simplicity we will adopt the previous strategy: 3 knots placed at the ages of 30, 40 and 60. F or the smoothing spline, everything is automatic - 𝜆 will be determined via CV by setting cv = TRUE. Note that the code below runs this comparison for a sample size of 𝑛 = 200. T o plot results for"
4811,unknown,"for 𝑛 = 50 and 𝑛 = 1000 (or any other value of 𝑛!), run the code yourself in R and change the line n <- 200 so that n is your chosen size. library(""ISLR"") library(""splines"") # Required for regression and natural splines. agelims=range(Wage$age) age.grid=seq(from=agelims[1],to=agelims[2]) set.seed(1) # Number of data points - change this to investigate # small, medium and large samples. n <- 200 # "
4812,unknown,"n <- 200 # Take the a sample of n points from the data. ind = sample(1:3000, n) Wage1 = Wage[ind,] # Label subset of data as Wage1. # Cubic Spline fitbs = lm(wage~bs(age, degree = 3, knots = c(30,40,60)), data = Wage1) predbs = predict(fitbs, newdata = list(age = age.grid), se = T) # Natural Spline fitns = lm(wage~ns(age, knots = c(30,40,60)), data = Wage1) predns = predict(fitns, newdata = list(a"
4813,unknown,"# Smoothing Spline fitss = smooth.spline(Wage1$age, Wage1$wage, cv = TRUE) # Generate the Plots. par(mfrow=c(1,3)) 192 CHAPTER 10. SPLINES # Cubic Spline plot(Wage1$age, Wage1$wage, col = ""darkgray"", pch = 19, main = 'Cubic spline', bty = 'l', xlab = 'age', ylab = 'wage', cex.lab = 1.4) lines(age.grid, predbs$fit, lwd = 2, col = 'darkgreen') abline(v = c(30,40,60), lty = 'dashed') # Natural Spline"
4814,unknown,"plot(Wage1$age, Wage1$wage, col = ""darkgray"", pch = 19, main = 'Natural cubic spline', bty = 'l', xlab = 'age', ylab = 'wage', cex.lab = 1.4) lines(age.grid, predns$fit, lwd = 2, col = 'darkviolet') abline(v = c(30,40,60), lty = 'dashed') # Smoothing Spline plot(Wage1$age, Wage1$wage, col = ""darkgray"", pch = 19, main = 'Smoothing spline', bty = 'l', xlab = 'age', ylab = 'wage', cex.lab = 1.4) line"
4815,unknown,"lines(fitss, lwd = 2, col = 'brown') 20 30 40 50 60 70 50 100 200 300 Cubic spline age wage 20 30 40 50 60 70 50 100 200 300 Natural cubic spline age wage 20 30 40 50 60 70 50 100 200 300 Smoothing spline age wage • If you run the above code for various sample sizes, you will see that, overall, the smoothing spline method is more robust: the shape of the curve remains more or less stable across di"
4816,unknown,"stable across different sample sizes. Importantly , this means that the fitted curve from the smoothing spline under 𝑛 = 50 would fit well to the data with 𝑛 = 1000. • That is not the case for the other two methods, especially for the cubic spline. These methods are more sensitive to the particular sample being used to train them. Drawback Y ou will notice that we didn’t plot confidence intervals,"
4817,unknown,"Y ou will notice that we didn’t plot confidence intervals, because there is no built-in function in R package smooth.splines (that we used) that produces confidence intervals for smoothing splines. This is not a flaw of the package; this is a generic issue with smoothing splines as it is not as straightforward as it is with other spline methods to calculate confidence intervals. 10.6. PRACTICAL DE"
4818,unknown,"One can get confidence-like intervals by using: • Bayesian credible intervals • Bootstrap These methods require additional work and are beyond the scope of this course. 194 CHAPTER 10. SPLINES Chapter 11 Generalised Additive Models 11.1 Review • W e started with polynomial regression, which is a flexible smoother but is global and therefore overall sensitive to changes in the data. • Then we consi"
4819,unknown,"• Then we considered regression based on step functions, which has the advantage of being local, but is not smooth. • W e have introduced piecewise polynomials. These achieve local smoothness, but can result in strange-looking discontinuous curves. • W e started to add constraints, which ensure continuity and smoothness, leading to more modern methods like cubic splines and natural splines. • Fina"
4820,unknown,"• Finally , we discussed smoothing splines, which are continuous non-linear smoothers that bypass the problem of knot selection altogether. All the non-linear models we have seen so far • Global Polynomials • Step F unctions • Regression Splines • Natural Splines • Smoothing Splines take as input one predictor and utilise suitable transformations of the predictor (namely powers) to produce flexibl"
4821,unknown,"powers) to produce flexible curves that fit data that exhibit non-linearities. This final chapter covers the case of multiple predictors! 11.2 GAMs A Generalised Additive Model (GAM) is an extension of the multiple linear model, which recall is 𝑦 = 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + … + 𝛽𝑝𝑥𝑝 + 𝜖. In order to allow for non-linear effects a GAM replaces each linear component 𝛽𝑗𝑥𝑗 with a non-linear function 𝑓𝑗(𝑥𝑗). "
4822,unknown,"non-linear function 𝑓𝑗(𝑥𝑗). 195 196 CHAPTER 11. GENERALISED ADDITIVE MODELS So, in this case we have the following general formula 𝑦 = 𝛽0 + 𝑓1(𝑥1) + 𝑓2(𝑥2) + … + 𝑓𝑝(𝑥𝑝) + 𝜖. This is called an additive model because we estimate each 𝑓𝑗(𝑥𝑗) for 𝑗 = 1, … , 𝑝 and then add together all of these individual contributions. Backfitting algorithm A simple iterative procedure, backfitting algorithm, can be u"
4823,unknown,"• W e set ̂𝛽0 = ̄ 𝑦, and it never changes. • F or each predictor𝑗, fit the function ̂𝑓𝑗 to {𝑦𝑖 − ̂𝛽0 − ∑𝑘≠𝑗 ̂𝑓𝑘(𝑥𝑖𝑘)}𝑛 𝑖=1 using the current estimates of the other functions ̂𝑓𝑘. • Continue the process until the estimates ̂𝑓𝑗 stabilize. Flexibility Note that because we can have a different function 𝑓𝑗 for each 𝑋𝑗, GAMs are extremely flexible. So, for example a GAM may include: • Any kind of non-li"
4824,unknown,• Any kind of non-linear polynomial method from the ones we have seen for continuous predictors. • Step functions which are more appropriate for categorical predictors. • Linear models if that seems more appropriate for some predictors. Example: W age Data GAMs are very useful as they estimate the contribution of the effects of each predictor. Pros and Cons Pros: • V ery flexible in choosing non-l
4825,unknown,• V ery flexible in choosing non-linear models and generalisable to different types of re- sponses. • Because of the additivity we can still interpret the contribution of each predictor while considering the other predictors fixed. • GAMs can outperform linear models in terms of prediction. Cons: • Additivity is convenient but it is also one of the main limitations of GAMs. • GAMs might miss non-l
4826,unknown,"• GAMs might miss non-linear interactions among predictors. Of course, we can add manually interaction terms but ideally we would prefer a procedure which does that automatically . 11.2. GAMS 197 Figure 11.1: GAM using cubic splines for year and age, and step functions for education (which is categorical). 198 CHAPTER 11. GENERALISED ADDITIVE MODELS 11.3 Practical Demonstration In this final part "
4827,unknown,"In this final part we will fit a generalised additive model (GAM) utilising more than one predictor from the Boston dataset. library(MASS) y = Boston$medv x = Boston$lstat y.lab = 'Median Property Value' x.lab = 'Lower Status (%)' W e first use the command names() in order to check once again the available predictor variables. names(Boston) ## [1] ""crim"" ""zn"" ""indus"" ""chas"" ""nox"" ""rm"" ""age"" ## [8]"
4828,unknown,"## [8] ""dis"" ""rad"" ""tax"" ""ptratio"" ""black"" ""lstat"" ""medv"" Let’s say that we want to use predictorslstat, indusand chasfor the analysis (use ?Boston again to check what these refer to). F or GAMs we will make use of the library gam in RStudio, so the first thing that we have to do is to install this package by executing install.packages(""gam"") once. Then we load the library . library(gam) ## Loadin"
4829,unknown,"the library . library(gam) ## Loading required package: foreach ## Loaded gam 1.20 The main function is gam(). Inside this function we can use any combination of non-linear and linear modelling of the various predictors. F or, example below we use a cubic spline with 3 degrees of freedom for lstat, a smoothing spline with 3 degrees of freedom for indus and a simple linear model for variable chas. "
4830,unknown,"a simple linear model for variable chas. W e then plot the contributions of each predictor using the command plot(). As we can see, GAMs are very useful as they estimate the contribution of the effects of each predictor. gam = gam( medv ~ bs(lstat, degree = 3, df = 5) + s(indus, df = 5) + chas, data = Boston ) par( mfrow = c(1,3) ) plot( gam, se = TRUE, col = ""blue"" ) 11.3. PRACTICAL DEMONSTRA TIO"
4831,unknown,"10 20 30 −10 0 10 20 30 lstat bs(lstat, degree = 3, df = 5) 0 5 10 15 20 25 −4 −2 0 2 4 indus s(indus, df = 5) 0.0 0.4 0.8 0 1 2 3 4 5 chas partial for chas Note that simply using chas inside gam() is just fitting a linear model for this variable. However, one thing that we observe is that variable is binary as it only takes the values of 0 and 1. This we can see from the x-axis of the chas plot o"
4832,unknown,"0 and 1. This we can see from the x-axis of the chas plot on the right above. So, it would be preferable to use a step function for this variable. In order to do this we have to change the variable chas to a factor. W e first create a second object called Boston1 (in order not to change the initial dataset Boston) and then we use the command factor() to change variable chas. Then we fit again the "
4833,unknown,"variable chas. Then we fit again the same model. As we can see below now gam() fitted a step function for variable chas which is more appropriate. Boston1 = Boston Boston1$chas = factor(Boston1$chas) gam1 = gam( medv ~ bs(lstat, degree = 3, df = 5) + s(indus, df = 5) + chas, data = Boston1 ) par(mfrow = c(1,3)) plot(gam1, se = TRUE, col = ""blue"") 10 20 30 −10 0 10 20 30 lstat bs(lstat, degree = 3,"
4834,unknown,"0 5 10 15 20 25 −4 −2 0 2 4 indus s(indus, df = 5) 0 1 2 3 4 5 partial for chas chas 0 1 W e can make predictions fromgam objects, just like lm objects, using the predict() method for the class gam. Here we make predictions on some new data. Note that when assigning the value 0 to chas, we enclose it in """" since we informed R to treat chas as a categorical factor with two levels - ""0"" and ""1"". pre"
4835,unknown,"preds <- predict( gam1, newdata = data.frame( chas = ""0"", indus = 3, lstat = 5 ) ) 200 CHAPTER 11. GENERALISED ADDITIVE MODELS preds ## 1 ## 32.10065 Chapter 12 Logistic Regression 12.1 Motivation In previous weeks, we have been focusing on regression problems, where the response variables are quantitative. Here we will see how we can work with qualitative response variables which leads to classif"
4836,unknown,"Qualitative variables take values in an unordered set. F or example: • patient status ∈ {dead, alive} • weather ∈ { rainy , cloudy , sunny} T ake the case where we are trying to predict a patient’s outcome. W e might have access to a number of variables (quantitative or qualitative) that indicate the patient’s health status, e.g. height, weight, age, gender, heart rate, consciousness. • Quantitati"
4837,unknown,"• Qualitative variables: gender, consciousness The task here is to build a function that takes features 𝑋 (e.g. height, age) as input and predicts the value for the response 𝑌 (i.e. dead or alive). While predicting the response itself is valuable, we often want to estimate the probability that 𝑋 maps to one of the potential categories. F or example, if we are looking at insurance fraud, estimating"
4838,unknown,"F or example, if we are looking at insurance fraud, estimating the probability that a claim is fraudulent is often more valuable to a bank than the actual classification of the event in fraudulent or not. The same applies if we are looking at health outcomes, say for example, whether someone is likely to have cancer or not in the future. While life threatening, esti- mating probabilities and conse"
4839,unknown,"mating probabilities and consequently the risk associated to a patient (ideally over time) is more valuable than asserting their future state. Example: The Default data set Here we will look at the Default dataset that contains information on credit card balance and income for 10,000 people, and also whether they have defaulted and whether they are a student or not. 201 202 CHAPTER 12. LOGISTIC RE"
4840,unknown,"201 202 CHAPTER 12. LOGISTIC REGRESSION library(""ISLR2"") ## ## Attaching package: 'ISLR2' ## The following object is masked _by_ '.GlobalEnv': ## ## Hitters ## The following object is masked from 'package:MASS': ## ## Boston ## The following objects are masked from 'package:ISLR': ## ## Auto, Credit data(Default) head(Default) ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No"
4841,unknown,"## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 It’s good practice to check for missing data, here we check for missing values in the default variable: print(paste(""Missing data:"", sum(is.na(Default$default)),sep="" "",collapse="""")) ## [1] ""Missing data: 0"" table(Default$default,dnn=""Default"") ## "
4842,unknown,"## [1] ""Missing data: 0"" table(Default$default,dnn=""Default"") ## Default ## No Yes ## 9667 333 W e now produce apairs plot to look at the relationship between all variables in our dataset: pairs(Default) 12.1. MOTIV A TION 203 default 1.0 1.4 1.8 1.0 1.4 1.8 0 40000 1.0 1.4 1.8 student balance 0 1000 2000 0 20000 60000 1.0 1.4 1.80 1000 2500 income The pairs plot isn’t particularly interesting but"
4843,unknown,"The pairs plot isn’t particularly interesting but we note that balance and income are the only two quantitative predictors. Let’s produce a scatter of balance vs income and colour the points by the default variable. library(scales) #loads the alpha function to add transparency to colours plot(Default$balance, Default$income, col=alpha(c(""red"",""blue"")[Default$default],0.6), pch=3, xlab=""Balance"", y"
4844,unknown,"0 500 1000 1500 2000 2500 0 20000 40000 60000 Balance Income There seems to be a pattern emerging where users with a high balance are more likely to default. Let’s inspect this using a boxplot: 204 CHAPTER 12. LOGISTIC REGRESSION boxplot(balance~default, data=Default) No Y es 0 500 1000 2000 default balance boxplot(income~default,data=Default) No Y es 0 20000 40000 60000 default income It seems th"
4845,unknown,"It seems that balanceis likely to be a good predictor for whether someone is likely todefault or not, but not income. Let’s take default as our response variable Y and we will rewrite it as: 𝑌 = { 0, 𝑖𝑓 𝑁 𝑜, 1, 𝑖𝑓 𝑌 𝑒𝑠. 12.1. MOTIV A TION 205 Note that by treating the response 𝑌 as a quantitative variable, we can apply the linear regression model ( 𝐸(𝑌 |𝑋 = 𝑥) = 𝛽0 + 𝛽1𝑥): lm.fit = lm(as.numeric(d"
4846,unknown,"lm.fit = lm(as.numeric(default == ""Yes"") ~ balance, data=Default) summary(lm.fit) ## ## Call: ## lm(formula = as.numeric(default == ""Yes"") ~ balance, data = Default) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.23533 -0.06939 -0.02628 0.02004 0.99046 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -0.075191959 0.003354360 -22.42 <0.0000000000000002 *** ## balance 0.000"
4847,unknown,"## balance 0.000129872 0.000003475 37.37 <0.0000000000000002 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.1681 on 9998 degrees of freedom ## Multiple R-squared: 0.1226, Adjusted R-squared: 0.1225 ## F-statistic: 1397 on 1 and 9998 DF, p-value: < 0.00000000000000022 Let’s explore it further by producing a scatter plot of balance vs def"
4848,unknown,"regression line to the plot. plot(Default$balance, as.numeric(Default$default==""Yes""),col=""red"",xlab=""balance"",ylab=""default"") abline(lm.fit, col=""blue"", lwd = 2) 0 500 1000 1500 2000 2500 0.0 0.2 0.4 0.6 0.8 1.0 balance default 206 CHAPTER 12. LOGISTIC REGRESSION par(mfrow=c(1,2)) plot(y=resid(lm.fit), x=fitted(lm.fit)) qqnorm(resid(lm.fit)) −0.05 0.05 0.15 0.25 −0.2 0.2 0.4 0.6 0.8 1.0 fitted(lm"
4849,unknown,"resid(lm.fit) −4 −2 0 2 4 −0.2 0.2 0.4 0.6 0.8 1.0 Normal Q−Q Plot Theoretical Quantiles Sample Quantiles par(mfrow=c(1,1)) This doesn’t seem like a particularly good model… 12.2 Simple Logistic Regression Note that in the Default data example, 𝐸(𝑌 |𝑋 = 𝑥) = 𝑃 (𝑌 = 1|𝑋 = 𝑥) , which means the linear regression model is trying to predict the probability of defaulting. Unfortunately , for balances cl"
4850,unknown,"balances close to zero we predict a negative probability of defaulting; if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of defaulting, regardless of credit card balance, must fall between 0 and 1. T o avoid the inadequacies of the linear model fit on a binary response, we must model the proba"
4851,unknown,"probability of our response using a function that gives outputs between 0 and 1 for all values of 𝑋. In logistic regression, we use the logistic function to transform/map the outputs to (0, 1). Say 𝑝(𝑋) = 𝑃 (𝑌 = 1|𝑋) , we write the logistic function 𝑝(𝑋) = exp(𝛽0 + 𝛽1𝑋) 1 + exp(𝛽0 + 𝛽1𝑋) 12.2. SIMPLE LOGISTIC REGRESSION 207 This transformation would guarantee that 𝑝(𝑋) ∈ (0, 1) for any 𝛽0, 𝛽1, and"
4852,unknown,ln 𝑝(𝑋) 1 − 𝑝(𝑋) = 𝛽0 + 𝛽1𝑋 and we can modify this to take any number of response variables. This transformation is called the logit transformation. The standard approach for producing estimates 𝛽0 and 𝛽1 of the regression coeﬀicients in the logistic regression model is to use maximum likelihood estimation. One of the advantages of this method is that maximum likelihood estimators have a number of
4853,unknown,"of this method is that maximum likelihood estimators have a number of nice theoretical properties which can be exploited to derive confidence intervals for the regression coeﬀicients, perform hypothesis tests, and so on. In R, all this goes on under the hood when we use the glm function to fit the logistic regression model. When using the glm function, we need to set the argument family=""binomial"""
4854,unknown,"set the argument family=""binomial"" to indicate that we want to fit a logistic regression model, and not some other kind of generalised linear model. Now let’s try to fit a linear logistic regression model to the Default dataset using the glm function in R with family set to binomial. glm.fit = glm(as.numeric(Default$default==""Yes"") ~ balance, data = Default, family = ""binomial"") summary(glm.fit) #"
4855,unknown,"summary(glm.fit) ## ## Call: ## glm(formula = as.numeric(Default$default == ""Yes"") ~ balance, ## family = ""binomial"", data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2697 -0.1465 -0.0589 -0.0221 3.7589 ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -10.6513306 0.3611574 -29.49 <0.0000000000000002 *** ## balance 0.0054989 0.0002204 24.95 <0.000000"
4856,unknown,## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1596.5 on 9998 degrees of freedom ## AIC: 1600.5 ## ## Number of Fisher Scoring iterations: 8 First note that now the fitted values in our regression are between 0 and 1. 208 CHAPTER 12. L
4857,unknown,"summary(glm.fit$fitted.values) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000237 0.0003346 0.0021888 0.0333000 0.0142324 0.9810081 And a plot of our model output looks far more sensible. In the plot below, we have the raw data in red, the fitted logistic curve in blue, and the fitted values in black. plot(Default$balance, as.numeric(Default$default==""Yes""),col=""red"",xlab=""balance"",ylab=""defaul"
4858,unknown,"points(glm.fit$data$balance,glm.fit$fitted.values, col = ""black"", pch = 4) curve(predict(glm.fit,data.frame(balance = x),type=""resp""),col=""blue"",lwd=2,add=TRUE) 0 500 1000 1500 2000 2500 0.0 0.2 0.4 0.6 0.8 1.0 balance default F rom the logistic regression summary , we have estimates for𝛽0 and 𝛽1 and we can estimate the probability of default for an individual given their balance. Say someone has "
4859,unknown,"of £500, then ̂ 𝑝(𝑋) = exp( ̂𝛽0 + ̂𝛽1𝑋) 1 + exp( ̂𝛽0 + ̂𝛽1𝑋) and ̂ 𝑝equals exp(sum(glm.fit$coefficients*c(1,500)))/(1+exp(sum(glm.fit$coefficients*c(1,500)))) ## [1] 0.0003699132 Repeat the above experiment with different values for the balance variable. What do you see? 12.3. LOGISTIC REGRESSION WITH SEVERAL V ARIABLES 209 12.3 Logistic regression with several variables W e can extend the logit t"
4860,unknown,Simply write: ln 𝑝(𝑋) 1 − 𝑝(𝑋) = 𝛽0 + 𝛽1𝑋1 + 𝛽2𝑋2 + … + 𝛽𝑝𝑋𝑝 and 𝑝(𝑋) = exp(𝛽0 + 𝛽1𝑋1 + 𝛽2𝑋2 + … + 𝛽𝑝𝑋𝑝) 1 + exp(𝛽0 + 𝛽1𝑋1 + 𝛽2𝑋2 + … + 𝛽𝑝𝑋𝑝) And this can even be extended to the Generalised Additive Models to allow for non-linear relationships to be considered in the model by: ln 𝑝(𝑋) 1 − 𝑝(𝑋) = 𝛽0 + 𝑓1(𝑋1) + 𝑓2(𝑋2) + … + 𝑓𝑝(𝑋𝑝) 12.4 Interpreting coeﬀicients The interpretation of the coeﬀicients 
4861,unknown,"The interpretation of the coeﬀicients in the logistic regression isn’t as simple as in the linear regression scenario since we are predicting the probability of an outcome rather than the outcome itself. If 𝛽1 = 0, then there is no relationship between 𝑌 and 𝑋. If 𝛽1 > 0, 𝑋 gets larger and so does the probability that 𝑌 = 1 . If 𝛽1 < 0, the opposite happens, that is, as 𝑋 gets larger, the probabil"
4862,unknown,"the probability that 𝑌 = 1 gets smaller. 12.5 Significance W e also want to investigate whether the coeﬀicient values are significant. That is, we want to know whether the coeﬀicients are statistically different from zero. In linear regression, we use a 𝑡-test to check significance for coeﬀicients. In logistic regression, we use a 𝑧-test (normal test) instead. Let’s look at the summary of the regr"
4863,unknown,"summary(glm.fit) #default vs balance ## ## Call: ## glm(formula = as.numeric(Default$default == ""Yes"") ~ balance, ## family = ""binomial"", data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2697 -0.1465 -0.0589 -0.0221 3.7589 ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) 210 CHAPTER 12. LOGISTIC REGRESSION ## (Intercept) -10.6513306 0.3611574 -29.49 <0.00000000000"
4864,unknown,"## balance 0.0054989 0.0002204 24.95 <0.0000000000000002 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1596.5 on 9998 degrees of freedom ## AIC: 1600.5 ## ## Number of Fisher Scoring iterations: 8 In this case, we see that balance"
4865,unknown,"In this case, we see that balance is likely to be a good predictor for default and the 𝑧-test indicates that we should include this variable in our model. And as we had seen before, the higher the balance, the more likely someone is to default. Chapter 13 T ree-based Models T ree-based models are a class of nonparametric algorithms that work by partitioning the feature space into a number of small"
4866,unknown,"using a set of splitting rules. F or example, the following figures show a tree-based classification model built on two predic- tors. Definitions • Nodes: The subgroups that are formed recursively using binary partitions formed by asking simple yes-or-no questions about each feature (e.g., 𝑥1 < 0.7?). • W e refer to the first subgroup at the top of the tree as theroot node (this node contains all "
4867,unknown,all of the training data). • The final subgroups at the bottom of the tree are called the terminal nodes or leaves. • The rest of the subgroups are referred to as internal nodes. • The connections between nodes are called branches. • Depth of a tree is the length of the longest path from a root to a leaf. • Size of a tree is the number of terminal nodes in the tree. In the tree model example above
4868,unknown,"In the tree model example above, there are 8 internal nodes and 9 terminal nodes; the depth of the tree is 8 and the size of the tree is 9. 211 212 CHAPTER 13. TREE-BASED MODELS Figure 13.1: An example of classification tree based one two predictors. As we’ll see, tree-based models offer many benefits; however, classic tree-based models, clas- sification and regression trees, typically lack in pre"
4869,unknown,"sification and regression trees, typically lack in predictive performance compared to more complex algorithms like neural networks. F ortunately , we can blend powerful ensemble algo- rithms into the tree-based model, for example, random forests and boosting trees, to improve the predictive performance. In this chapter, we will explore various tree-based models. 13.1 Classification and Regression "
4870,unknown,"There are many methodologies for constructing tree-based models, but the most well-known and classic is the classification and regression tree (CAR T) algorithm. A classification and regression tree partitions the training data into homogeneous subgroups (i.e., groups with similar response values) and then fits a simple constant in each subgroup. Mathematically , A classification and regression tr"
4871,unknown,"A classification and regression tree ends in a leaf node which corresponds to some hyper- rectangle in feature (predictor) space, 𝑅𝑗, 𝑗 = {1, ..., 𝐽 } , i.e. the feature space defined by 𝑋1, 𝑋2, ..., 𝑋𝑝 is divided into 𝐽 distinct and non-overlapping regions. • Regression tree, assigns a value to each 𝑅𝑗, that is the model predicts the output based on the average response values for all observation"
4872,unknown,"on the average response values for all observations that fall in that subgroup. • Classification tree, assigns a class label to each𝑅𝑗, that is the model predicts the output based on the class that has majority representation or provides predicted probabilities using the proportion of each class within the subgroup. 13.1. CLASSIFICA TION AND REGRESSION TREES 213 Interpretation Classification trees"
4873,unknown,"Classification trees mimic some decision making processes. F or example, the following deci- sion tree is used by A&E doctors to decide what bed type to assign: Note that trees can be constructed even in the presence of qualitative predictor variables, for example, eye colors (blue, green, brown). As tree methods can produce simple rules that are easy to interpret and visualize with tree diagrams,"
4874,unknown,"are easy to interpret and visualize with tree diagrams, tree methods often refer to decision trees, which perhaps makes them popular for the feeling of interpretability and of being like a data-learned expert system. Building trees In theory , the regions 𝑅𝑗 could have any shape. But we choose hyper (high-dimensional)- rectangles (boxes) for simplicity . The left panel is an example of an invalid "
4875,unknown,"the right is an example of a valid one. Performance Metric F or regression trees, the goal is to find 𝑅𝑗, 𝑗 = {1, ..., 𝐽 } that minimizing the RSS 𝐽 ∑ 𝑗=1 ∑ 𝑖∶𝑥𝑖 ∈𝑅𝑗 (𝑦𝑖 − ̂ 𝑦𝑅𝑗 )2, where ̂ 𝑦𝑅𝑗 is the mean response for the training observations within the jth rectangle. F or classification trees, we may adopt classification error rate 𝐸 = 1 − max𝑘( ̂ 𝑝𝑗𝑘) as perfor- mance criteria, where ̂ 𝑝𝑗𝑘 rep"
4876,unknown,"mance criteria, where ̂ 𝑝𝑗𝑘 represents the proportion of training observations in the jth region that are from the kth class. However, it turns out that the classification error rate is not suﬀiciently sensitive for tree-growing, especially when the data is noisy . And in practice, two other metrics are preferable: 214 CHAPTER 13. TREE-BASED MODELS • Gini index: 𝐺 = ∑ 𝐾 𝑘=1 ̂ 𝑝𝑗𝑘(1 − ̂ 𝑝𝑗𝑘) • Entr"
4877,unknown,• Gini index: 𝐺 = ∑ 𝐾 𝑘=1 ̂ 𝑝𝑗𝑘(1 − ̂ 𝑝𝑗𝑘) • Entropy: 𝐷 = − ∑ 𝐾 𝑘=1 ̂ 𝑝𝑗𝑘 log ̂ 𝑝𝑗𝑘 Note that Both functions will take on a value near zero if the ̂ 𝑝𝑗𝑘’s are all near zero or near one. Greedy fitting algorithm It is computationally infeasible to consider every possible partition of the feature space into 𝐽 boxes. Think about how many ways one could divide this 2-D space into 5 boxes. Not to menti
4878,unknown,"to mention high dimensional feature space. Therefore, in practice, we adopt the so-called Greedy(top-down) fitting algorithm: 1. Initialise hyper-rectangle, 𝑅 = ℝ𝑑. 2. Find the best (based on some objective function) split on variable𝑥𝑖 at location 𝑠, gives 𝑅1(𝑖, 𝑠) = {𝑥 ∈ 𝑅 ∶ 𝑥𝑖 < 𝑠} and 𝑅2(𝑖, 𝑠) = {𝑥 ∈ 𝑅 ∶ 𝑥𝑖 ≥ 𝑠}. 3. Repeat step 2. twice, once with 𝑅 = 𝑅1(𝑖, 𝑠) and once with 𝑅 = 𝑅2(𝑖, 𝑠) There "
4879,unknown,"There will be some stopping rule, for example, requiring a minimum number (for example, 5) of training observations in each hyper-rectangle (box). Note that such a greedy fitting algorithm does NOT guarantee an optimal tree since it is constructed stepwisely . Pruning trees The fitting procedure described above may produce good predictions on the training set, but is likely to overfit the data, le"
4880,unknown,"is likely to overfit the data, leading to poor test set performance. T o balance this trade- off between variance and bias and avoid overfitting, we often prune trees by adopting the so-called weakest link pruning algorithm in practice. The weakest link pruning (or cost complexity pruning) introduces a nonnegative tuning pa- rameter 𝛼, for regression trees minimising |𝑇 | ∑ 𝑗=1 ∑ 𝑖∶𝑥𝑖 ∈𝑅𝑗 (𝑦𝑖 − ̂ "
4881,unknown,"rameter 𝛼, for regression trees minimising |𝑇 | ∑ 𝑗=1 ∑ 𝑖∶𝑥𝑖 ∈𝑅𝑗 (𝑦𝑖 − ̂ 𝑦𝑅𝑗 )2 + 𝛼|𝑇 |, where |𝑇 | is the number of terminal nodes (size) of the tree 𝑇 (for classification trees simply replace the RSS with classification objective function, e.g. Gini index). Minimisation is achieved by attempting to remove terminal nodes from the base of the tree upward. Similar to Lasso regression, 𝛼 is a penalt"
4882,unknown,"to Lasso regression, 𝛼 is a penalty on the size (complexity) of the tree. When 𝛼 = 0, we will keep the whole tree and when 𝛼 = ∞, we will prune everything back to null tree (the forecast is simply the average of the response). And as always, we may Select 𝛼 using cross-validation. Pros and Cons The advantages of CAR T are: • Simple and easy to interpret 13.1. CLASSIFICA TION AND REGRESSION TREES 2"
4883,unknown,"• Akin to common decision making processes • Good visualisations • Easy to handle qualitative predictors (avoid dummy variables) The disadvantages of CAR T are: • T rees tend to have high variance, small changes in the sample lead to dramatic cascading changes in the fit! • poor prediction accuracy . Practical Demonstration This practical demonstration will be based on synthetic (surrogate) datase"
4884,unknown,"This practical demonstration will be based on synthetic (surrogate) datasets. As the name suggests, quite obviously , a synthetic dataset is a repository of data that is generated program- matically (artificially), it is not collected by any real-life survey or experiment. In practice, it is almost impossible to know the underlying system behind the real data. F or synthetic data, however, we know"
4885,unknown,"data, however, we know exactly what is the underlying system behind the data. It provides a flexible and rich “test” environment to help us to explore a methodology , demonstrate its effectiveness and uncover its pros and cons by conducting experiments upon. F or example, if we want to test a linear regression fitting algorithm, we may create a synthetic dataset using a linear regression model and"
4886,unknown,"a linear regression model and pretend not to know the parameters of the model. W e have introduced a greedy (top-down) fitting algorithm for building classification and regression trees. Let’s create a synthetic dataset to explore the algorithm. set.seed(212) #set.seed() function is used to set the random generator state. #So that the random data generated later can be reproduced using the same ""s"
4887,unknown,"#So that the random data generated later can be reproduced using the same ""seed"". data_surrogate <- data.frame(x = c(runif(200, 0, 0.4), runif(400, 0.4, 0.65), runif(300, 0.65, 1)), y = c(rnorm(200, 1.5), rnorm(400, 0), rnorm(300, 2))) In this synthetic dataset we created, 𝑥 and 𝑦 are paired, for the first 200 observations, 𝑥 is drawn uniformly between 0 and 0.4 and 𝑦 is drawn from a normal distri"
4888,unknown,"drawn uniformly between 0 and 0.4 and 𝑦 is drawn from a normal distribution with mean 1.5 and standard deviation 1; for the next 400 observations, 𝑥 is drawn uniformly between 0.4 and 0.65 and 𝑦 is drawn from a normal distribution with mean 0 and standard deviation 1; and for the last 300 observations, 𝑥 is drawn uniformly between 0.65 and 1 and y is drawn from a normal distribution with mean 2 an"
4889,unknown,"Here is a visualization of the data set. plot(x=data_surrogate$x[1:200], y=data_surrogate$y[1:200], col='blue', xlab=""X"", ylab=""Y"", pch=19, xlim=c(0,1),ylim=c(-2,4)) points(x=data_surrogate$x[201:600], y=data_surrogate$y[201:600], col='red', pch=19) points(x=data_surrogate$x[601:900], y=data_surrogate$y[601:900], col='green', pch=19) 216 CHAPTER 13. TREE-BASED MODELS 0.0 0.2 0.4 0.6 0.8 1.0 −2 −1 "
4890,unknown,"216 CHAPTER 13. TREE-BASED MODELS 0.0 0.2 0.4 0.6 0.8 1.0 −2 −1 0 1 2 3 4 X Y Given this data set and knowing how it was generated, the best tree-based model we would expect when modelling variable 𝑦 using the variable 𝑥 is a regression tree with three terminal nodes, when x is between 0 and 0.4, y equals 1.5; when x is between 0.4 and 0.65, y equals 0; x is between 0.65 and 1, y equals 2. Now let"
4891,unknown,"Now let’s build a regression tree model for this dataset using R. library(""tree"") Y ou may need to install the “tree” package first, using install.packages(“tree”). tree_fit=tree(y~x,data_surrogate) The “tree” function is to fit a classification or regression tree using the greedy fitting algo- rithm. summary(tree_fit) ## ## Regression tree: ## tree(formula = y ~ x, data = data_surrogate) ## Numbe"
4892,unknown,"## Residual mean deviance: 1.001 = 897.9 / 897 ## Distribution of residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -3.137000 -0.662000 -0.001074 0.000000 0.638100 2.984000 #The ""summary"" function prints a summary of the fitted tree object. tree_fit ## node), split, n, deviance, yval ## * denotes terminal node 13.1. CLASSIFICA TION AND REGRESSION TREES 217 ## ## 1) root 900 1565.0 1.01200 ## "
4893,unknown,"## 2) x < 0.650076 600 883.8 0.55940 ## 4) x < 0.398517 200 194.1 1.55700 * ## 5) x > 0.398517 400 391.4 0.06086 * ## 3) x > 0.650076 300 312.4 1.91700 * #print each node as well as the corresponding statistics The deviance is simply the residual sum of squares (RSS) for the tree/subtree. plot(tree_fit) text(tree_fit,pretty=0) |x < 0.650076 x < 0.398517 1.55700 0.06086 1.91700 #If pretty = 0 then "
4894,unknown,"1.91700 #If pretty = 0 then the level names of a factor split attributes are used unchanged. This is a nice way to plot the tree model. Is it consistent with our previous expectations? Clearly , one of the advantages of the regression and classification tree model is its interpre- tation. Now let’s examine the model performance by creating an independent data set (the same way that data_surrogate "
4895,unknown,"that data_surrogate was created) as a test set, naming it data_surrogate_test and using tree_pred=predict(tree_fit,data_surrogate_test) to generate the model prediction of y variable (in the test set) using the tree model fitted early . set.seed(347) data_surrogate_test <- data.frame(x = c(runif(200, 0, 0.4), runif(400, 0.4, 0.65), runif(300, 0.65, 1)), y = c(rnorm(200, 1.5), rnorm(400, 0), rnorm("
4896,unknown,"tree_pred=predict(tree_fit,data_surrogate_test) Draw a scatter plot (same as the one drawn for data set “data_surrogate”) for the test set and add the prediction of y , “tree_pred”, to the plot. And calculate the residual sum of squares. 218 CHAPTER 13. TREE-BASED MODELS plot(x=data_surrogate_test$x[1:200], y=data_surrogate_test$y[1:200], col='blue', xlab=""X"", ylab=""Y"", pch=19, xlim=c(0,1), ylim=c"
4897,unknown,"col='blue', xlab=""X"", ylab=""Y"", pch=19, xlim=c(0,1), ylim=c(-2,4)) points(x=data_surrogate_test$x[201:600], y=data_surrogate_test$y[201:600], col='red', pch=19) points(x=data_surrogate_test$x[601:900], y=data_surrogate_test$y[601:900], col='green', pch=19) points(x=data_surrogate_test$x,y=tree_pred,col='black',pch=8) 0.0 0.2 0.4 0.6 0.8 1.0 −2 −1 0 1 2 3 4 X Y pred_mse=mean((data_surrogate_test$y-"
4898,unknown,"−2 −1 0 1 2 3 4 X Y pred_mse=mean((data_surrogate_test$y-tree_pred)^2) pred_mse ## [1] 1.005645 So far, everything seems all right! The regression tree works almost perfectly . Note that we have used a rather large data set as a train set to fit the tree, what if the historical data is rather small? Create an independent training data set (the same way that data_surrogate was created) but with the"
4899,unknown,"but with the number of observations 10 times smaller. And draw a scatter plot of the data. Then build a regression tree, name “tree_fit_s”, using the new small training set and use the new tree model to predict the y variable in the same test set data_surrogate_test and record the residual sum of squares and compare it with the one you calculated before using the tree model fitted with the large t"
4900,unknown,"======================================================== set.seed(739) data_surrogate_s <- data.frame(x = c(runif(20, 0, 0.4), runif(40, 0.4, 0.65), runif(30, 0.65, 1)), y = c(rnorm(20, 1.5), rnorm(40, 0), rnorm(30, 2))) plot(x=data_surrogate_s$x[1:20], y=data_surrogate_s$y[1:20], col='blue', xlab=""X"", ylab=""Y"", pch=19, xlim=c(0,1),ylim=c(-2,4)) 13.1. CLASSIFICA TION AND REGRESSION TREES 219 point"
4901,unknown,"points(x=data_surrogate_s$x[21:60], y=data_surrogate_s$y[21:60], col='red', pch=19) points(x=data_surrogate_s$x[61:90], y=data_surrogate_s$y[61:90], col='green', pch=19) 0.0 0.2 0.4 0.6 0.8 1.0 −2 −1 0 1 2 3 4 X Y tree_fit_s=tree(y~x,data_surrogate_s) summary(tree_fit_s) ## ## Regression tree: ## tree(formula = y ~ x, data = data_surrogate_s) ## Number of terminal nodes: 7 ## Residual mean devianc"
4902,unknown,"## Distribution of residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.5420 -0.7364 0.0396 0.0000 0.5973 2.2330 tree_fit_s ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 90 131.600 1.0660 ## 2) x < 0.653469 60 90.670 0.7376 ## 4) x < 0.427732 26 29.650 1.4510 ## 8) x < 0.160006 8 14.560 2.0450 * ## 9) x > 0.160006 18 11.020 1.1880 * ## 5) x > 0.427732 34 37.640 0"
4903,unknown,"## 10) x < 0.515981 16 15.670 -0.1998 ## 20) x < 0.458374 6 1.973 0.3927 * ## 21) x > 0.458374 10 10.330 -0.5554 * ## 11) x > 0.515981 18 17.340 0.5398 220 CHAPTER 13. TREE-BASED MODELS ## 22) x < 0.59835 9 2.474 0.9361 * ## 23) x > 0.59835 9 12.040 0.1435 * ## 3) x > 0.653469 30 21.480 1.7240 * plot(tree_fit_s) text(tree_fit_s,pretty=0) |x < 0.653469 x < 0.427732 x < 0.160006 x < 0.515981 x < 0.4"
4904,unknown,"x < 0.458374 x < 0.59835 2.0450 1.1880 0.3927 −0.5554 0.9361 0.1435 1.7240 tree_pred_s=predict(tree_fit_s,data_surrogate_test) plot(x=data_surrogate_test$x[1:200], y=data_surrogate_test$y[1:200], col='blue', xlab=""X"", ylab=""Y"", pch=19, xlim=c(0,1), ylim=c(-2,4)) points(x=data_surrogate_test$x[201:600], y=data_surrogate_test$y[201:600], col='red', pch=19) points(x=data_surrogate_test$x[601:900], y="
4905,unknown,"points(x=data_surrogate_test$x[601:900], y=data_surrogate_test$y[601:900], col='green', pch=19) points(x=data_surrogate_test$x,y=tree_pred_s,col='black',pch=8) 0.0 0.2 0.4 0.6 0.8 1.0 −2 −1 0 1 2 3 4 X Y 13.1. CLASSIFICA TION AND REGRESSION TREES 221 pred_mse=mean((data_surrogate_test$y-tree_pred_s)^2) pred_mse ## [1] 1.296465 The newly fitted tree, “tree_fit_s”, seems to overfit the data, which l"
4906,unknown,"The newly fitted tree, “tree_fit_s”, seems to overfit the data, which led to poor forecast performance in the test set. Now we use the cv.tree() function to see whether pruning the tree using the weakest link algorithm will improve performance. tree_cv_prune=cv.tree(tree_fit_s,FUN=prune.tree) tree_cv_prune ## $size ## [1] 7 6 5 4 3 1 ## ## $dev ## [1] 127.1730 134.5368 127.4856 127.7807 127.2861 1"
4907,unknown,"## ## $k ## [1] -Inf 2.826888 3.370479 4.065974 4.633948 21.419490 ## ## $method ## [1] ""deviance"" ## ## attr(,""class"") ## [1] ""prune"" ""tree.sequence"" plot(tree_cv_prune) size deviance 130 140 150 1 2 3 4 5 6 7 21.0 4.6 4.1 3.4 2.8 −Inf 222 CHAPTER 13. TREE-BASED MODELS Note the output k in “tree_cv_prune” is the cost-complexity parameter in the weakest link pruning introduced in the lecture ( 𝛼)."
4908,unknown,"pruning introduced in the lecture ( 𝛼). tree_fit_prune=prune.tree(tree_fit_s,best=3) # you may also prune the tree by spefifying the cost-complexity parameter k # for example tree_fit_prune=prune.tree(tree_fit_s,k=5) tree_fit_prune ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 90 131.60 1.0660 ## 2) x < 0.653469 60 90.67 0.7376 ## 4) x < 0.427732 26 29.65 1.4510 * ## "
4909,unknown,"## 5) x > 0.427732 34 37.64 0.1917 * ## 3) x > 0.653469 30 21.48 1.7240 * plot(tree_fit_prune) text(tree_fit_prune,pretty=0) |x < 0.653469 x < 0.427732 1.4510 0.1917 1.7240 Use the pruned tree model to predict the y variable in the test set and record the mean squared error and compare the mean squared error with that resulted from an unpruned tree. tree_pred_prune=predict(tree_fit_prune,data_surr"
4910,unknown,"plot(x=data_surrogate_test$x[1:200], y=data_surrogate_test$y[1:200], col='blue', xlab=""X"", ylab=""Y"", pch=19, xlim=c(0,1), ylim=c(-2,4)) points(x=data_surrogate_test$x[201:600], y=data_surrogate_test$y[201:600], col='red', pch=19) points(x=data_surrogate_test$x[601:900], y=data_surrogate_test$y[601:900], col='green', pch=19) points(x=data_surrogate_test$x,y=tree_pred_prune,col='black',pch=8) 13.1. "
4911,unknown,"13.1. CLASSIFICA TION AND REGRESSION TREES 223 0.0 0.2 0.4 0.6 0.8 1.0 −2 −1 0 1 2 3 4 X Y pred_mse_prune=mean((data_surrogate_test$y-tree_pred_prune)^2) pred_mse_prune ## [1] 1.119899 pred_mse-pred_mse_prune ## [1] 0.1765653 How would we know that the improved forecast performance is due to pruning rather than the “unlucky” small training set? T o demonstrate the robustness of the conclusion, one"
4912,unknown,"conduct the same experiments many times using an independent training set. First, create a function named “prune_improvement(k)” that creates an independent small training set and compares pruned and unpruned trees with cost-complexity parameter as input parameter and returns the difference in prediction mean squared error. Then run the function 1024 times and calculate the average improvement in "
4913,unknown,"======================================================== prune_improvement <- function(k) { data_surrogate_s <- data.frame(x = c(runif(20, 0, 0.4), runif(40, 0.4, 0.65), runif(30, 0.65, 1)),y = c(rnorm(20, 1.5), rnorm(40, 0), rnorm(30, 2))) tree_fit_s=tree(y~x,data_surrogate_s) tree_pred_s=predict(tree_fit_s,data_surrogate_test) pred_mse_s=mean((data_surrogate_test$y-tree_pred_s)^2) tree_fit_prune"
4914,unknown,"tree_pred_prune=predict(tree_fit_prune,data_surrogate_test) pred_mse_prune=mean((data_surrogate_test$y-tree_pred_prune)^2) dif_t=pred_mse_s-pred_mse_prune return(dif_t) } mean(sapply(1:1024, FUN=function(i){prune_improvement(5)})) 224 CHAPTER 13. TREE-BASED MODELS ## [1] 0.09926117 13.2 Bagging W e have seen CAR T has attractive features, but biggest problem is high variance. Boot- strap aggregati"
4915,unknown,"strap aggregating (an ensemble algorithms), also called bagging, is designed to improve the stability and accuracy of regression and classification algorithms by model averaging. Al- though bagging helps to reduce variance and avoid overfitting, model interpretability will be sacrificed. Bootstrap A bootstrap sample of size 𝑚 is (𝑦⋆ 𝑖 , 𝑥⋆ 𝑖 )𝑚 𝑖=1, where each (𝑦⋆ 𝑖 , 𝑥⋆ 𝑖 ) is a uniformly random "
4916,unknown,"from the training data (𝑦1, 𝑥1), ..., (𝑦𝑛, 𝑥𝑛). A bootstrap sample is a random sample of the data taken with replacement , which means samples in the original data set may appear more than once in the bootstrap sample. Note that when sampling from the training data (𝑦1, 𝑥1), ..., (𝑦𝑛, 𝑥𝑛) in pairs, relationship between 𝑦 and 𝑥 is reserved. There is a nice mathematical property about bootstrap samp"
4917,unknown,"There is a nice mathematical property about bootstrap samples. Let (𝑦⋆ 𝑖 , 𝑥⋆ 𝑖 )𝑚 𝑖=1 are an approximation to drawing IID samples from ℙ(𝑋, 𝑌 ). If we set 𝑚 = 𝑛 , then notionally we sampled whole new training set. However, we will only have on average ≈ 63.2% of the original training data in the bootstrap resample. ℙ(𝑠𝑎𝑚𝑝𝑙𝑒 𝑖 𝑖𝑠 𝑖𝑛 𝑡ℎ𝑒 𝑏𝑜𝑜𝑠𝑡𝑟𝑎𝑝 𝑟𝑒𝑠𝑎𝑚𝑝𝑙𝑒) = 1 − ℙ(𝑠𝑎𝑚𝑝𝑙𝑒 𝑖 𝑖𝑠 𝑛𝑜𝑡 𝑖𝑛 𝑡ℎ𝑒 𝑏𝑜𝑜𝑠𝑡𝑟𝑎𝑝 𝑟𝑒"
4918,unknown,"= 1 − ℙ[(𝑥⋆ 1, 𝑦⋆ 1) ≠ (𝑥𝑖, 𝑦𝑖) ∩ ⋯ ∩ (𝑥⋆ 𝑛, 𝑦⋆ 𝑛) ≠ (𝑥𝑖, 𝑦𝑖)] = 1 − 𝑛 ∏ 𝑗=1 ℙ[(𝑥⋆ 𝑗 , 𝑦⋆ 𝑗 ) ≠ (𝑥𝑖, 𝑦𝑖)] = 1 − (1 − 1 𝑛 ) 𝑛 ≈ 1 − 𝑒−1 ≈ 0.632 (13.1) Note that we don’t really imagine we have a new IID training sample! But bootstrap methods allow us to produce model-free estimates of sample distributions. If we have a lot of data then the estimate will be quite good. Intuitively we can roughly app"
4919,unknown,"then the estimate will be quite good. Intuitively we can roughly approximate the sampling distribution of trees for training sets of size 𝑛. W e then can achieve variance reduction by averaging many sampled trees. Bagging for CAR T W e can apply bagging to CAR T in the following. F or𝑏 = 1, ..., 𝐵, draw 𝑛 bootstrap samples (𝑦⋆ 𝑖 , 𝑥⋆ 𝑖 )𝑛 𝑖=1 and each time fit a tree and get ̂𝑓 ∗𝑏(𝑥), the predicti"
4920,unknown,"13.2. BAGGING 225 • F or regression trees, average all the predictions to obtain: ̂𝑓 𝑏𝑎𝑔(𝑥) = 1 𝐵 𝐵 ∑ 𝑏=1 ̂𝑓 ∗𝑏(𝑥) • F or classification trees, we choose the class label for which the most bootstrapped trees “voted” . Or to construct a probabilistic prediction for 𝑗𝑡ℎ class by directly averaging tree output probabilities ̂𝑓 𝑏𝑎𝑔 𝑗 (𝑥) = 1 𝐵 𝐵 ∑ 𝑏=1 ̂𝑓 ∗𝑏 𝑗 (𝑥) Bagging for CAR T address the overfitt"
4921,unknown,"• Grow large trees with minimal (or no) pruning. Relies on bagging procedure directly to avoid overfit. • Prune each tree as was described in the last lecture. Another nice property of bagging for CAR T is that it automatically provides out of sample evaluation. In fact, we don’t need to do cross-validation for bagging, because we know on average 36.8% of original data will NOT be in bootstrap sam"
4922,unknown,"average 36.8% of original data will NOT be in bootstrap sample so can be used as a test set. Such kind of Out Of Bag (OOB) error estimation is approximately equal to a 3-fold cross validation. Pros and Cons The advantages of bagging are: • If you have a lot of data, bagging is a good choice because the empirical distribution will be close to the true population distribution • Bagging is not restri"
4923,unknown,"• Bagging is not restricted to trees, it can be used with any type of method. • It is most useful when it is desirable to reduce the variance of a predictor. Note under the (inaccurate) independence assumption, variance will be reduced by a factor of ∼ 1/𝐵 for 𝐵 bootstrap resamples (Central limit theorem). • Out of sample evaluation without using cross-validation, since any given observation will "
4924,unknown,"will not be used in around 36.8% of the models. The disadvantages of bagging are: • W e lose interpretability as the final estimate is not a tree. • Computational cost is multiplied by a factor of at least 𝐵. • Bagging trees are correlated, the more correlated random variables are, the less the variance reduction of their average. 226 CHAPTER 13. TREE-BASED MODELS 13.3 Random F orests Bagging impr"
4925,unknown,"Bagging improved on a single CAR T model by reducing the variance through resampling. But, in fact the bagged models are still quite correlated. And the more correlated random variables are, the less the variance reduction of their average. Can we make them more independent in order to produce a better variance reduction? Random forecasts achieve this by not only resampling observations, but by re"
4926,unknown,"𝒳′ ⊂ 𝒳. Methodology The Random F orests algorithm can be implemented in the following: 1) T ake a bootstrap resample (𝑦⋆ 𝑖 , 𝑥⋆ 𝑖 )𝑛 𝑖=1 of the training data as for bagging. 2) Building a tree, each time a split in a tree is considered, random select 𝑚 predictors out of the full set of 𝑝 predictors as split candidates, and find the “optimal” split within those 𝑚 predictors. (typically choose 𝑚 ≈ √"
4927,unknown,"3) Repeat 1) and 2), average the prediction of all trees. Interpretation Classification and regression trees are easy to interpret and follow common human decision making mechanisms. Linear models provides statistical significance tests (e.g. t-test, z-test of parameter significance). Random F orecasts may seem great, but we’ve sacrificed inter- pretability with bagging and random subspace. There "
4928,unknown,"help us to quantify the importance of each variable. 1) F or each feature variable𝑥𝑗, 𝑗 = 1, …, 𝑝, loop over each tree in the forest, • find all nodes that make a split on 𝑥𝑗 • compute the improvement in loss criterion the split causes (e.g. accuracy/Gini/etc) • sum improvements across nodes in the tree Finally , sum improvement across all tress. 2) W e already discussed that bagging enables out o"
4929,unknown,"2) W e already discussed that bagging enables out of bag error estimate. W e can then compute out of bag variable importance for each feature𝑥𝑗, 𝑗 = 1, …, 𝑝 in the following: • F or each tree, take the out of bag samples data matrix, 𝑥𝑜𝑜𝑏, and compute the predictive accuracy for that tree; • T ake𝑥𝑜𝑜𝑏 and randomly permute all the entries𝑗𝑡ℎ column (break the ties between 𝑥𝑗 and the rest of variabl"
4930,unknown,• Pass the modified 𝑥𝑜𝑜𝑏∗ through the tree and compute the change in predictive accuracy for that tree. Finally average the decrease in accuracy over all trees. Pros and Cons The advantages of random forests are: 13.4. BOOSTING 227 • Inherits the advantages of bagging and trees • V ery easy to parallelise • W orks well with high dimensional data • T uning is rarely needed (easy to get good quality
4931,unknown,The disadvantages of random forests are: • Often quite suboptimal for regression. • Same extrapolation issue as trees. • Harder to implement and memory hungry model. 13.4 Boosting Boosting is an extremely popular machine learning algorithm that has proven successful across many domains and is one of the leading methods for winning Kaggle competitions. Whereas random forests build an ensemble of de
4932,unknown,"Whereas random forests build an ensemble of deep independent trees, Boosting builds an ensemble of shallow trees in sequence with each tree learning and improving on the previous one. Although shallow trees by themselves are rather weak predictive models, they can be “boosted” to produce a powerful “committee” that, when appropriately tuned, is often hard to beat with other algorithms. Methodology"
4933,unknown,"Methodology Boosting is similar to bagging in the sense that we combine results from multiple classifiers, but it is fundamentally different in approach: 1) Set ̂𝑓 (𝑥) = 0 and 𝜖𝑖 = 𝑦𝑖 for 𝑖 = 1, …, 𝑛. 2) F or𝑏 = 1, …, 𝐵, iterate: i) Fit a model (eg tree) ̂𝑓 𝑏(𝑥) to the response 𝜖1, …, 𝜖𝑛 ii) Update the predictive model to ̂𝑓 (𝑥) ← ̂𝑓 (𝑥) + 𝜆 ̂𝑓 𝑏(𝑥) iii) Update the residuals 𝜖𝑖 ← 𝜖𝑖 − 𝜆 ̂𝑓 𝑏(𝑥) ∀𝑖"
4934,unknown,"3) Output as final boosted model ̂𝑓 (𝑥) = 𝐵 ∑ 𝑏=1 𝜆 ̂𝑓 𝑏(𝑥) Intuitively Boosting is a • Slow learning approach: Aim to learn usually simple model (which leads to low variance, but high bias) in each round. Then bring bias down by repeating over many rounds. • Sequential learning: Each round of boosting aims to correct the error of the previous rounds. • Generic methodology: Any model at all can be"
4935,unknown,"228 CHAPTER 13. TREE-BASED MODELS T uning parameters for boosting There are three main hyper-parameters that need to be tuned for building boosting tree model. 1) The number of trees 𝐵. Unlike bagging and random forests, boosting can overfit if 𝐵 is too large, although this overfitting tends to occur slowly if at all. W e use cross- validation to select 𝐵. 2) The shrinkage parameter 𝜆, a small pos"
4936,unknown,"2) The shrinkage parameter 𝜆, a small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. V ery small 𝜆 can require using a very large value of 𝐵 in order to achieve good performance. 3) The number of splits 𝑑 in each tree, which controls the complexity of the boosted ensemble. Often 𝑑 = 1 works well, i"
4937,unknown,"ensemble. Often 𝑑 = 1 works well, in which case each tree consists of a single split and resulting in an additive model. More generally 𝑑 is the interaction depth, and controls the interaction order of the boosted model, since 𝑑 splits can involve at most 𝑑 variables. Boosting discussion Boosting is an incredibly powerful technique and regularly is instru- mental in winning machine learning compet"
4938,unknown,"mental in winning machine learning competitions. Any model at all can be boosted. However, tuning is needed (not trivial) and some diﬀiculties in interpretation and extrapolation. 13.5 Practical Demonstration W e now explore all the tree-based model by analyzing the Boston housing data. The goal is to build a model to predict variable medv using the rest of variables. library(ISLR) library(tree) l"
4939,unknown,"library(ISLR) library(tree) library(MASS) W e first divide the dataset into two part, training set (half the dataset) and test set (the other half the dataset). set.seed (1) train = sample (1: nrow(Boston ), nrow(Boston )/2) data_train=Boston[train,] data_test=Boston[-train,] Fit a regression tree to the training set, plot the tree. tree.boston=tree(medv~.,data_train) summary(tree.boston) ## ## Re"
4940,unknown,"summary(tree.boston) ## ## Regression tree: ## tree(formula = medv ~ ., data = data_train) ## Variables actually used in tree construction: ## [1] ""rm"" ""lstat"" ""crim"" ""age"" 13.5. PRACTICAL DEMONSTRA TION 229 ## Number of terminal nodes: 7 ## Residual mean deviance: 10.38 = 2555 / 246 ## Distribution of residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -10.1800 -1.7770 -0.1775 0.0000 1.9230 16"
4941,unknown,"plot(tree.boston) text(tree.boston,pretty =0) |rm < 6.9595 lstat < 14.405 rm < 6.543 crim < 11.4863age < 93.95 rm < 7.553 21.38 27.73 18.09 14.43 10.32 33.42 45.38 Predict medv using the test set and record the mean squared error. yhat=predict(tree.boston,data_test) mean((yhat -data_test$medv)^2) ## [1] 35.28688 Prune the tree using cv.tree() and plot the pruned tree cv.boston =cv.tree(tree.boston"
4942,unknown,"plot(cv.boston) 230 CHAPTER 13. TREE-BASED MODELS size deviance 5000 10000 15000 20000 1 2 3 4 5 6 7 11000 3400 1100 800 640 200 −Inf prune.boston =prune.tree(tree.boston ,best =6) plot(prune.boston) text(prune.boston,pretty =0) |rm < 6.9595 lstat < 14.405 rm < 6.543 crim < 11.4863 rm < 7.553 21.38 27.73 16.23 10.32 33.42 45.38 Predict medvusing the test set and pruned tree, recorder the mean squa"
4943,unknown,"it with unpruned tree. yhat=predict(prune.boston,data_test) mean((yhat -data_test$medv)^2) ## [1] 35.16439 Here we apply bagging and random forests to the Boston data, using the randomF orest package in R. Note that bagging is simply a special case of a random forest with m = 13.5. PRACTICAL DEMONSTRA TION 231 p. Therefore, the randomF orest() function can be used to perform both random forests an"
4944,unknown,"bagging. W e perform bagging as follows: library (randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: 'randomForest' ## The following object is masked from 'package:ggplot2': ## ## margin #you may need to install the randomForest library first set.seed (472) bag.boston =randomForest(medv~.,data=data_train,mtry=13, ntree=10, importan"
4945,unknown,"## Warning in randomForest.default(m, y, ...): invalid mtry: reset to within valid ## range bag.boston ## ## Call: ## randomForest(formula = medv ~ ., data = data_train, mtry = 13, ntree = 10, importance = TRUE) ## Type of random forest: regression ## Number of trees: 10 ## No. of variables tried at each split: 12 ## ## Mean of squared residuals: 13.45607 ## % Var explained: 82.49 Note there are 1"
4946,unknown,"Note there are 13 predictors in the Boston data, the argument mtry=13 indicates that all 13 predictors are considered for each split of the tree, in other words, bagging is conducted. ntree=10 indicates 10 bootstrap trees are generated in this bagged model. The MSR and % variance explained are based on OOB (out-of-bag) estimates W e can also evaluate the performance of bagged model using the test "
4947,unknown,"pred.bag = predict (bag.boston,newdata =data_test) plot(pred.bag , data_test$medv) abline (0,1) 232 CHAPTER 13. TREE-BASED MODELS 10 20 30 40 50 10 20 30 40 50 pred.bag data_test$medv mean(( pred.bag -data_test$medv)^2) ## [1] 22.44938 The test set MSE associated with the bagged regression tree is much smaller than that obtained using an optimally-pruned single CAR T. W e now Increase the number o"
4948,unknown,"W e now Increase the number of bootstrap trees generated in the bagged model to 100 and 1000, record the test set MSE respectively and compare them to the bagged model with 10 bootstrap trees. bag.boston =randomForest(medv~.,data=data_train,mtry=13, ntree=100, importance =TRUE) ## Warning in randomForest.default(m, y, ...): invalid mtry: reset to within valid ## range pred.bag = predict (bag.bosto"
4949,unknown,"mean(( pred.bag -data_test$medv)^2) ## [1] 23.13702 bag.boston =randomForest(medv~.,data=data_train,mtry=13, ntree=1000, importance =TRUE) ## Warning in randomForest.default(m, y, ...): invalid mtry: reset to within valid ## range pred.bag = predict (bag.boston,newdata =data_test) mean(( pred.bag -data_test$medv)^2) ## [1] 23.54552 W e now Grow a random forest proceeds in exactly the same way , ex"
4950,unknown,"W e now Grow a random forest proceeds in exactly the same way , except that we use a smaller value of the mtry argument. By default, randomForest() uses 𝑝/3 variables when building 13.5. PRACTICAL DEMONSTRA TION 233 a random forest of regression trees, we may also try √𝑝 variables when building a random forest of classification trees. rf.boston =randomForest(medv~.,data=data_train,mtry=4, ntree=10"
4951,unknown,"pred.rf = predict (rf.boston,newdata =data_test) mean(( pred.rf -data_test$medv)^2) ## [1] 20.41462 T o view the importance of each variable, apply the importance() function. (type ?im- portance in the console for detailed description of importance() function) And use the varImpPlot() to plot the importance measures and interpret the results. importance(rf.boston) ## %IncMSE IncNodePurity ## crim "
4952,unknown,## crim 8.8561695 1236.90238 ## zn 1.5940368 150.70830 ## indus 4.2096189 850.89234 ## chas 0.3066732 24.94713 ## nox 5.8090684 881.53719 ## rm 16.7918585 8110.68177 ## age 5.9184141 771.57077 ## dis 3.4916527 824.61307 ## rad 2.5007048 203.89994 ## tax 4.7607576 664.49261 ## ptratio 3.3599677 1250.70664 ## lstat 10.0709051 4496.94612 varImpPlot(rf.boston) 234 CHAPTER 13. TREE-BASED MODELS chas zn
4953,unknown,"zn rad ptratio dis indus tax nox age crim lstat rm 0 5 10 15 %IncMSE chas zn rad tax age dis indus nox crim ptratio lstat rm 0 2000 6000 IncNodePurity rf.boston W e can write a for loop to record the test set prediction MSE for all 13 possible values of mtry. test.err=double(13) for(mtry_t in 1:13){ fit=randomForest(medv~.,data=data_train,mtry=mtry_t,ntree=100) pred=predict(fit,data_test) test.err"
4954,unknown,"pred=predict(fit,data_test) test.err[mtry_t]=mean(( pred -data_test$medv)^2) } ## Warning in randomForest.default(m, y, ...): invalid mtry: reset to within valid ## range plot(test.err) 13.5. PRACTICAL DEMONSTRA TION 235 2 4 6 8 10 12 20 22 24 26 Index test.err W e now use the gbm package, and within it the gbm() function, to fit boosted regression trees to the Boston data set. library (gbm) ## Lo"
4955,unknown,"set.seed (517) boost.boston =gbm(medv~.,data=data_train, distribution=""gaussian"", n.trees =1000, interaction.depth =2) summary(boost.boston) chas indus age dis lstat Relative influence 0 10 20 30 40 ## var rel.inf 236 CHAPTER 13. TREE-BASED MODELS ## rm rm 41.9696443 ## lstat lstat 31.3357911 ## crim crim 5.7915015 ## dis dis 5.3682171 ## nox nox 4.0082986 ## age age 3.6575000 ## ptratio ptratio 3"
4956,unknown,"## tax tax 1.9878973 ## indus indus 1.3455310 ## rad rad 1.0149034 ## zn zn 0.1938735 ## chas chas 0.1795809 Y ou may look up the function gbm via ?gbm. The option distribution=“gaussian” since this is a regression problem; if it were a binary classification problem, we would use dis- tribution=“bernoulli” . The argument n.trees=1000 indicates that we want 1000 trees, and the option interaction.de"
4957,unknown,"the option interaction.depth=2 specifies the number of splits performed on each tree. The summary() function produces a relative influence plot and also outputs the relative influence statistics. W e now use the boosted model to predict medv on the test set, and record MSE. Note the boosted model outperfom random forests model built early . pred.boost=predict(boost.boston,newdata =data_test, n.tre"
4958,unknown,"mean((pred.boost-data_test$medv)^2) ## [1] 15.91325 The default shrinkage parameter lambda is 0.001. W e can specify its value by adding argu- ment shrinkage in the gbm() function, for example shrinkage=0.05. Part I Problem Sheets 237 Problem Sheets Problem Sheet 1 Q1 Which of the following problem correspond to supervised learning? More than one correct answer can be chosen. A. Given a dataset of"
4959,unknown,"A. Given a dataset of 60,000 small square 28×28 pixel grayscale images of handwritten single digits between 0 and 9. Classify an image of a handwritten single digit into one of 10 classes representing integer values from 0 to 9. B. Find clusters of genes that interact with each other C. Find an investment portfolio that is likely to make a profit in the coming month D. Predict whether a website us"
4960,unknown,D. Predict whether a website user will click on an ad Q2 Which of the following statement about parametric and non-parametric models is not true? Select all that apply: A. A parametric approach reduces the problem of estimating a functional form 𝑓 down to the problem of estimating a set of parameters because it assumes a form for 𝑓 . B. A non-parametric approach does not assume a functional form𝑓 
4961,unknown,a large number of observations to estimate 𝑓 . C. The advantages of a parametric approach to regression or classification are the simpli- fying of modelling 𝑓 to a few parameters and therefore the estimations of the model parameters are more accurate compared to a non-parametric approach. D. The disadvantages of a parametric approach to regression or classification are a po- tential to inaccuratel
4962,unknown,tential to inaccurately estimate 𝑓 if the form of 𝑓 assumed is wrong or to overfit the observations if more complex models are used. Q3 Why is linear regression important to understand? Select all that apply: A. Linear regression is very extensible and can be used to capture nonlinear effects 239 240 B. The linear model is often correct C. The linear model is hardly ever a good representation of t
4963,unknown,"is an important piece in many more complex methods D. Understanding simpler methods sheds light on more complex ones Q4 Suppose that we build a simple linear regression 𝑌 = 𝛽 0 + 𝛽1𝑋 + 𝜖 based on 𝑛 observations (𝑥1, 𝑦1),…,(𝑥𝑛, 𝑦𝑛). Which of the following indicates a fairly strong relationship between 𝑋 and 𝑌 ? More than one correct answer can be chosen. A. The 𝑝-value for the null hypothesis 𝛽1 = "
4964,unknown,"B. The estimated parameters ̂𝛽1 >> 1 and ̂𝛽0 ≈ 0 C. 𝑅2 = 0.9 D. The 𝑡-statistic for the null hypothesis 𝛽1 = 0 is 40 Q5 In the case of simple linear regression, show that the least squares line always passes through the point ( ̄ 𝑥, ̄ 𝑦). Q6 Consider a simple linear regression problem, prove that under the standard four assumptions (listed in the lecture notes), maximum likelihood estimation (MLE)"
4965,unknown,"squares estimation (LSE) of the model parameters. Q7 A company manufactures an electronic device to be used in a very wide temperature range. The company knows that increased temperature shortens the lifetime of the device, and a study is therefore performed in which the lifetime is determined as a function of temperature. The following data is found: T emperature in Celcius Lifetime in hours 10 4"
4966,unknown,"10 420 20 365 30 285 40 220 50 176 60 117 70 69 80 34 90 5 241 Construct a linear regression model for this problem. Estimate the model parameters. Cal- culate the 95% confidence interval for the slope in the linear regression model. Q8 Consider a simple linear regression model, fit by least squares to a set of training data (𝑥1, 𝑦1), ..., (𝑥𝑁 , 𝑦𝑁 ) drawn at random from a population. Let ̂𝛽0 and "
4967,unknown,"estimates. Suppose we have some test data (𝑥′ 1, 𝑦′ 1), ..., (𝑥′ 𝑀 , 𝑦′ 𝑀 ) drawn at random from the same population as the training data. Prove that 𝐸[ 1 𝑁 𝑁 ∑ 𝑖=1 (𝑦𝑖 − ̂𝛽0 − ̂𝛽1𝑥𝑖)2] ≤ 𝐸[ 1 𝑀 𝑀 ∑ 𝑖=1 (𝑦′ 𝑖 − ̂𝛽0 − ̂𝛽1𝑥′ 𝑖)2] Problem Sheet 1 Solution Q1 Which of the following problem correspond to supervised learning? More than one correct answer can be chosen. A. Given a dataset of 60,000 small"
4968,unknown,"A. Given a dataset of 60,000 small square 28×28 pixel grayscale images of handwritten single digits between 0 and 9. Classify an image of a handwritten single digit into one of 10 classes representing integer values from 0 to 9. B. Find clusters of genes that interact with each other C. Find an investment portfolio that is likely to make a profit in the coming month D. Predict whether a website us"
4969,unknown,"D. Predict whether a website user will click on an ad Solution: A, C, D Q2 Which of the following statement about parametric and non-parametric models is not true? Select all that apply: A. A parametric approach reduces the problem of estimating a functional form 𝑓 down to the problem of estimating a set of parameters because it assumes a form for 𝑓 . B. A non-parametric approach does not assume a"
4970,unknown,a large number of observations to estimate 𝑓 . C. The advantages of a parametric approach to regression or classification are the simpli- fying of modelling 𝑓 to a few parameters and therefore the estimations of the model parameters are more accurate compared to a non-parametric approach. D. The disadvantages of a parametric approach to regression or classification are a po- tential to inaccuratel
4971,unknown,"tential to inaccurately estimate 𝑓 if the form of 𝑓 assumed is wrong or to overfit the observations if more complex models are used. Solution: B, C 242 Q3 Why is linear regression important to understand? Select all that apply: A. Linear regression is very extensible and can be used to capture nonlinear effects B. The linear model is often correct C. The linear model is hardly ever a good represen"
4972,unknown,"is an important piece in many more complex methods D. Understanding simpler methods sheds light on more complex ones Solution: A, C, D Q4 Suppose that we build a simple linear regression 𝑌 = 𝛽 0 + 𝛽1𝑋 + 𝜖 based on 𝑛 observations (𝑥1, 𝑦1),…,(𝑥𝑛, 𝑦𝑛). Which of the following indicates a fairly strong relationship between 𝑋 and 𝑌 ? More than one correct answer can be chosen. A. The 𝑝-value for the nul"
4973,unknown,"B. The estimated parameters ̂𝛽1 >> 1 and ̂𝛽0 ≈ 0 C. 𝑅2 = 0.9 D. The 𝑡-statistic for the null hypothesis 𝛽1 = 0 is 40 Solution: C Q5 In the case of simple linear regression, show that the least squares line always passes through the point ( ̄ 𝑥, ̄ 𝑦). Solution: Simply using the formular of the linear square estimate of 𝛽0, ̂𝛽0 = ̄ 𝑦 − ̂𝛽1 ̄ 𝑥. Q6 Consider a simple linear regression problem, proof t"
4974,unknown,"Q6 Consider a simple linear regression problem, proof that under the standard four assumptions (listed in the lecture notes), maximum likelihood estimation (MLE) is equivalent to the least squares estimation (LSE) of the model parameters. Solution: Given the IID and normality distribution, the log-likelihood function can be written as follows: 𝑙(𝛽) = ∑ log 𝑝(𝑦𝑖|𝑥𝑖, 𝛽) 𝑙( ̂𝛽) = ∑ log 𝑝(𝑦𝑖|𝑥𝑖, ̂𝛽) ="
4975,unknown,"= ∑ log [( 1 2𝜋𝜎2 ) 1 2 exp (− 1 2𝜎2 (𝑦𝑖 − ( ̂𝛽0 + ̂𝛽1𝑥𝑖)) 2 )] ∝ − 1 2𝜎2 𝑆𝑆𝐸( ̂𝛽) (13.2) 243 Q7 A company manufactures an electronic device to be used in a very wide temperature range. The company knows that increased temperature shortens the lifetime of the device, and a study is therefore performed in which the lifetime is determined as a function of temperature. The following data is found: T "
4976,unknown,"10 420 20 365 30 285 40 220 50 176 60 117 70 69 80 34 90 5 Construct a linear regression model for this problem. Estimate the model parameters. Cal- culate the 95% confidence interval for the slope in the linear regression model. Solution: ̂𝛽0 = 453.556 and ̂𝛽1 = −5.313 Confidence interval: (−5.915, −4.705) Q8 Consider a simple linear regression model, fit by least squares to a set of training dat"
4977,unknown,"(𝑥1, 𝑦1), ..., (𝑥𝑁 , 𝑦𝑁 ) drawn at random from a population. Let ̂𝛽0 and ̂𝛽1 be the least squares estimates. Suppose we have some test data (𝑥′ 1, 𝑦′ 1), ..., (𝑥′ 𝑀 , 𝑦′ 𝑀 ) drawn at random from the same population as the training data. Prove that 𝐸[ 1 𝑁 𝑁 ∑ 𝑖=1 (𝑦𝑖 − ̂𝛽0 − ̂𝛽1𝑥𝑖)2] ≤ 𝐸[ 1 𝑀 𝑀 ∑ 𝑖=1 (𝑦′ 𝑖 − ̂𝛽0 − ̂𝛽1𝑥′ 𝑖)2] Solution: Note 𝐸[ 1 𝑀 𝑀 ∑ 𝑖=1 (𝑦′ 𝑖 − ̂𝛽0 − ̂𝛽1𝑥′ 𝑖)2] = 𝐸[(𝑦 ′ 𝑖 − ̂𝛽0 − "
4978,unknown,"𝑖)2] = 𝐸[(𝑦 ′ 𝑖 − ̂𝛽0 − ̂𝛽1𝑥′ 𝑖)2] since for IID random variables 𝑧𝑖, 𝐸[ 1 𝑛 𝑛 ∑ 𝑖=1 𝑧𝑖] = 𝐸(𝑧) 244 Therefore, it doesn’t matter whether we have 𝑀 test points or 𝑁 test points, i.e. 𝐸[ 1 𝑀 𝑀 ∑ 𝑖=1 (𝑦′ 𝑖 − ̂𝛽0 − ̂𝛽1𝑥′ 𝑖)2] = 𝐸[ 1 𝑁 𝑁 ∑ 𝑖=1 (𝑦′ 𝑖 − ̂𝛽0 − ̂𝛽1𝑥′ 𝑖)2] Define the random variables: 𝐴 = 1 𝑁 ∑ 𝑁 𝑖=1(𝑦𝑖 − ̂𝛽0 − ̂𝛽1𝑥𝑖)2 and 𝐵 = 1 𝑁 ∑ 𝑁 𝑖=1(𝑦′ 𝑖 − ̃𝛽0 − ̃𝛽1𝑥′ 𝑖)2 where ̃𝛽0 and ̃𝛽1 are the LS "
4979,unknown,"estimates of the linear regression parameters based on the TEST set. Note that {𝑦𝑖, 𝑥𝑖} and {𝑦′ 𝑖 , 𝑥′ 𝑖} are samples from the same distribution, hence 𝐴 and 𝐵 have the same distribution and 𝐸(𝐴) = 𝐸(𝐵) . As ̃𝛽0 and ̃𝛽1 are the LS estimates, 𝐵 = 1 𝑁 𝑁 ∑ 𝑖=1 (𝑦′ 𝑖 − ̃𝛽0 − ̃𝛽1𝑥′ 𝑖)2 ≤ 1 𝑁 𝑁 ∑ 𝑖=1 (𝑦′ 𝑖 − ̂𝛽0 − ̂𝛽1𝑥′ 𝑖)2 Therefore 𝐸 [ 1 𝑁 𝑁 ∑ 𝑖=1 (𝑦𝑖 − ̂𝛽0 − ̂𝛽1𝑥𝑖)2] = 𝐸 [ 1 𝑁 𝑁 ∑ 𝑖=1 (𝑦′ 𝑖 − ̃𝛽0 − ̃"
4980,unknown,"𝑁 𝑁 ∑ 𝑖=1 (𝑦′ 𝑖 − ̃𝛽0 − ̃𝛽1𝑥′ 𝑖)2] ≤ 𝐸 [ 1 𝑁 𝑁 ∑ 𝑖=1 (𝑦′ 𝑖 − ̂𝛽0 − ̂𝛽1𝑥′ 𝑖)2] Problem Sheet 2 Q1 Make a sketch of typical squared bias, variance, irreducible error, training error and test error curves (i.e. five curves) on a single graph, where the x-axis represents the complexity of the model and the y-axis represents the values for each curve. Explain why each of the five curves has the shape s"
4981,unknown,Q2 Which of the following statement about modelling is NOT true? Select all that apply: A. A fitted model with more predictors will necessarily have a lower training error than a model with fewer predictors. B. The advantage of building a very complex model for regression or classification is ob- taining a better fit to the data which will lead to smaller test error. C. The disadvantages for a ver
4982,unknown,C. The disadvantages for a very complex approach for regression or classification are i) requires estimating a greater number of parameters; ii) follows the noise too closely which might lead to overfitting. D. A more complex model would usually be preferred to a less complex approach when we are interested in prediction and not the interpretability of the results. 245 Q3 Y ou are fitting a linear
4983,unknown,Q3 Y ou are fitting a linear regression model to a data set. The model has up to 10 predictors and 100 observations. Which of the following is NOT true in terms of using the model selection criteria to select a number of predictors to include? A. Mallows’ 𝐶𝑝 will likely select a model with more predictors than AIC B. Mallows’ 𝐶𝑝 will select the same model as AIC C. Mallows’ 𝐶𝑝 will likely select a
4984,unknown,"C. Mallows’ 𝐶𝑝 will likely select a model with fewer predictors than BIC D. Mallows’ 𝐶𝑝 will likely select a model with more predictors than BIC Q4 F ollowing Q3, which of the model selection approach will lead to the smallest test MSE? A. F orward Stepwise Selection B. Backward Stepwise Selection C. Best Subset Selection D. Not enough information Q5 F ollowing Q3, if you decide to use Best Subset"
4985,unknown,"Q5 F ollowing Q3, if you decide to use Best Subset Selection to decide which predictors to include, how many different models will you end up considering? How many if you use F orward Stepwise Selection? Q6 Which of the following statement about multicollinearity is TRUE? Select all that apply: A. When predictors are correlated, it indicates that changes in one predictor are associated with shifts"
4986,unknown,"with shifts in another predictor. The stronger the correlation, the more diﬀicult it is to change one predictor without changing another. B. When predictors are highly correlated, the coeﬀicient estimates become very sensitive to small changes in the model. C. Multicollinearity reduces the precision of the estimated coeﬀicients, which weakens the statistical power of the regression model. One migh"
4987,unknown,"statistical power of the regression model. One might not be able to trust the p-values to identify independent variables that are statistically significant. D. Multicollinearity does not influence the predictions, precision of the predictions. If the primary goal is to make predictions without understanding the role of each predictor, there is no need to reduce multicollinearity . 246 Q7 Consider "
4988,unknown,"246 Q7 Consider a simple linear regression problem, prove that under the standard four assumptions (listed in the lecture notes): • The least square estimates 𝑏0 and 𝑏1 are unbiased estimates. • The variances of the least squares estimators in simple linear regression are: 𝑉 𝑎𝑟[𝑏0] = 𝜎2 𝑏0 = 𝜎2 ( 1 𝑛 + ̄ 𝑥2 𝑆𝑥𝑥 ) 𝑉 𝑎𝑟[𝑏1] = 𝜎2 𝑏1 = 𝜎2 𝑆𝑥𝑥 • The covariance of the least squares estimators in simple "
4989,unknown,"𝐶 𝑜𝑣[𝑏0, 𝑏1] = 𝜎𝑏0 ,𝑏1 = −𝜎2 ̄ 𝑥 𝑆𝑥𝑥 Q8 Suppose that you work part-time at a bowling alley that is open daily from noon to midnight. Although business is usually slow from noon to 6 P .M., the owner has noticed that it is better on hotter days during the summer, perhaps because the premises are comfortably air- conditioned. The owner shows you some data that she gathered last summer. This data set"
4990,unknown,includes the maximum temperature (in F ahrenheit) and the number of lines bowled between noon and 6 P .M. for each of 20 days. (The maximum temperatures ranged from77∘𝐹 to 95∘𝐹 during this period.) The owner would like to know if she can estimate tomorrow’s business from noon to 6 P .M. by looking at tomorrow’s weather forecast. She asks you to analyze the data. Let 𝑥 be the maximum temperature fo
4991,unknown,"the data. Let 𝑥 be the maximum temperature for a day and 𝑦 the number of lines bowled between noon and 6 P .M. on that day . The computer output based on the data for 20 days provided the following results: ̂ 𝑦 = −432 + 7.7𝑥, 𝑠𝑒 = 28.17, 𝑆𝑥𝑥 = 607, 𝑎𝑛𝑑 ̄ 𝑥 = 87.5 Assume that the weather forecasts are reasonably accurate. a. Does the maximum temperature seem to be a useful predictor of bowling acti"
4992,unknown,"between noon and 6 P .M.? Use an appropriate statistical procedure based on the information given. Use 𝛼 = 0.05. b. The owner wants to know how many lines of bowling she can expect, on average, for days with a maximum temperature of 90∘𝐹 . Answer using a 95% confidence level. c. The owner has seen tomorrow’s weather forecast, which predicts a high of90∘𝐹 . About how many lines of bowling can she e"
4993,unknown,"how many lines of bowling can she expect? Answer using a 95% confidence level. d. The owner asks you how many lines of bowling she could expect if the high temperature were 100∘𝐹 . Give a point estimate, together with an appropriate warning to the owner. 247 Problem Sheet 2 Solution Q1 Make a sketch of typical squared bias, variance, irreducible error, training error and test error curves (i.e. fi"
4994,unknown,"error curves (i.e. five curves) on a single graph, where the x-axis represents the complexity of the model and the y-axis represents the values for each curve. Explain why each of the five curves has the shape sketched. Q2 Which of the following statement about modelling is NOT true? Select all that apply: A. A fitted model with more predictors will necessarily have a lower training error than a m"
4995,unknown,model with fewer predictors. B. The advantage of building a very complex model for regression or classification is ob- taining a better fit to the data which will lead to smaller test error. C. The disadvantages for a very complex approach for regression or classification are i) requires estimating a greater number of parameters; ii) follows the noise too closely which might lead to overfitting. D
4996,unknown,"D. A more complex model would usually be preferred to a less complex approach when we are interested in prediction and not the interpretability of the results. Solution: A, B F or A, note the two models may be structurally different and do not share predictors at all. F or B, not necessarily lead to small TEST error. Q3 Y ou are fitting a linear regression model to a data set. The model has up to "
4997,unknown,Q3 Y ou are fitting a linear regression model to a data set. The model has up to 10 predictors and 100 observations. Which of the following is NOT true in terms of using the model selection criteria to select a number of predictors to include? 248 A. Mallows’ 𝐶𝑝 will likely select a model with more predictors than AIC B. Mallows’ 𝐶𝑝 will select the same model as AIC C. Mallows’ 𝐶𝑝 will likely sele
4998,unknown,"C. Mallows’ 𝐶𝑝 will likely select a model with fewer predictors than BIC D. Mallows’ 𝐶𝑝 will likely select a model with more predictors than BIC Solution: A, C Q4 F ollowing Q3, which of the model selection approach will lead to the smallest test MSE? A. F orward Stepwise Selection B. Backward Stepwise Selection C. Best Subset Selection D. Not enough information Solution: D Q5 F ollowing Q3, if yo"
4999,unknown,"Q5 F ollowing Q3, if you decide to use Best Subset Selection to decide which predictors to include, how many different models will you end up considering? How many if you use F orward Stepwise Selection? Solution: i) 210 = 1024 ii) 1 + 10×(10+1) 2 = 56 Q6 Which of the following statement about multicollinearity is TRUE? Select all that apply: A. When predictors are correlated, it indicates that ch"
5000,unknown,"A. When predictors are correlated, it indicates that changes in one predictor are associated with shifts in another predictor. The stronger the correlation, the more diﬀicult it is to change one predictor without changing another. B. When predictors are highly correlated, the coeﬀicient estimates become very sensitive to small changes in the model. C. Multicollinearity reduces the precision of the"
5001,unknown,"C. Multicollinearity reduces the precision of the estimated coeﬀicients, which weakens the statistical power of the regression model. One might not be able to trust the p-values to identify independent variables that are statistically significant. D. Multicollinearity does not influence the predictions, precision of the predictions. If the primary goal is to make predictions without understanding "
5002,unknown,"is no need to reduce multicollinearity . Solution: A, B, C, D 249 Q7 Consider a simple linear regression problem, prove that under the standard four assumptions (listed in the lecture notes): • The least square estimates 𝑏0 and 𝑏1 are unbiased estimates. • The variances of the least squares estimators in simple linear regression are: 𝑉 𝑎𝑟[𝑏0] = 𝜎2 𝑏0 = 𝜎2 ( 1 𝑛 + ̄ 𝑥2 𝑆𝑥𝑥 ) 𝑉 𝑎𝑟[𝑏1] = 𝜎2 𝑏1 = 𝜎2 𝑆"
5003,unknown,"𝑏1 = 𝜎2 𝑆𝑥𝑥 • The covariance of the least squares estimators in simple linear regression is: 𝐶 𝑜𝑣[𝑏0, 𝑏1] = 𝜎𝑏0 ,𝑏1 = −𝜎2 ̄ 𝑥 𝑆𝑥𝑥 Solution: Note all the expectation and variance is conditioned on 𝑋 is known. • Proof of 𝐸[𝑏1] = 𝛽1 see lecture notes • Proof of 𝐸[𝑏0] = 𝛽0 𝐸(𝑏0) = 𝐸 ( ̄ 𝑦 − 𝑏1 ̄ 𝑥) = 1 𝑛 ∑ 𝐸(𝑦𝑖) − 𝐸(𝑏1 ̄ 𝑥) = 1 𝑛 ∑(𝛽0 + 𝛽1𝑥𝑖) − 𝐸(𝑏1) 1 𝑛 ∑ 𝑥𝑖 = 1 𝑛 ∑(𝛽0 + 𝛽1𝑥𝑖) − 𝛽1 1 𝑛 ∑ 𝑥𝑖 = 𝛽0 (13."
5004,unknown,"1 𝑛 ∑ 𝑥𝑖 = 𝛽0 (13.3) • Proof of 𝑉 𝑎𝑟[𝑏0] see lecture notes • Proof of 𝑉 𝑎𝑟[𝑏1] see lecture notes • Proof of 𝐶 𝑜𝑣[𝑏0, 𝑏1] 𝐶 𝑜𝑣[𝑏0, 𝑏1] = 𝐶 𝑜𝑣[ ̄ 𝑦 − ̄ 𝑥𝑏1, 𝑏1] = 𝐶 𝑜𝑣[ ̄ 𝑦, 𝑏1] − ̄ 𝑥𝐶 𝑜𝑣[𝑏1, 𝑏1] = 0 − ̄ 𝑥𝑉 𝑎𝑟(𝑏1) = − ̄ 𝑥𝜎2 𝑆𝑥𝑥 (13.4) See the lecture notes of proof 𝑉 𝑎𝑟[𝑏1] for the proof of 𝐶 𝑜𝑣[ ̄ 𝑦, 𝑏1] = 0. 250 Q8 Suppose that you work part-time at a bowling alley that is open daily from noon to "
5005,unknown,"Although business is usually slow from noon to 6 P .M., the owner has noticed that it is better on hotter days during the summer, perhaps because the premises are comfortably air- conditioned. The owner shows you some data that she gathered last summer. This data set includes the maximum temperature (in F ahrenheit) and the number of lines bowled between noon and 6 P .M. for each of 20 days. (The "
5006,unknown,noon and 6 P .M. for each of 20 days. (The maximum temperatures ranged from77∘𝐹 to 95∘𝐹 during this period.) The owner would like to know if she can estimate tomorrow’s business from noon to 6 P .M. by looking at tomorrow’s weather forecast. She asks you to analyze the data. Let 𝑥 be the maximum temperature for a day and 𝑦 the number of lines bowled between noon and 6 P .M. on that day . The compu
5007,unknown,"provided the following results: ̂ 𝑦 = −432 + 7.7𝑥, 𝑠𝑒 = 28.17, 𝑆𝑥𝑥 = 607, 𝑎𝑛𝑑 ̄ 𝑥 = 87.5 Assume that the weather forecasts are reasonably accurate. a. Does the maximum temperature seem to be a useful predictor of bowling activity between noon and 6 P .M.? Use an appropriate statistical procedure based on the information given. Use 𝛼 = 0.05. b. The owner wants to know how many lines of bowling she "
5008,unknown,"b. The owner wants to know how many lines of bowling she can expect, on average, for days with a maximum temperature of 90∘𝐹 . Answer using a 95% confidence level. c. The owner has seen tomorrow’s weather forecast, which predicts a high of90∘𝐹 . About how many lines of bowling can she expect? Answer using a 95% confidence level. d. The owner asks you how many lines of bowling she could expect if t"
5009,unknown,"d. The owner asks you how many lines of bowling she could expect if the high temperature were 100∘𝐹 . Give a point estimate, together with an appropriate warning to the owner. Solution: • a. ̂ 𝑦 = −432 + 7.7𝑥, 𝑠𝑒 = 28.17, 𝑆𝑥𝑥 = 607, 𝑎𝑛𝑑 ̄ 𝑥 = 87.5 𝑏1 = 7.7, 𝑠𝑏1 = 𝑠𝑒/√𝑆𝑥𝑥 = 28.17/ √ 607 = 1.1434 T est: 𝐻0 ∶ 𝛽 1 = 0 vs 𝐻1 ∶ 𝛽 1 > 0 Use 𝑡 distribution with 𝑑𝑓 = 𝑛 − 2 = 20 − 2 = 18 F or𝛼 = 0.05, the c"
5010,unknown,"F or𝛼 = 0.05, the critical value 𝑡∗ = 1.734 𝑡𝑠𝑡𝑎𝑡𝑠 = (7.7 − 0)/1.1434 = 6.734 > 𝑡∗ Reject 𝐻0, there is suﬀicient evidence to conclude 𝛽1 is positive, i.e. the maximum temperature and bowling activity between twelve noon and 6:00 pm have a positive association. • b. F or𝑥∗ = 90, ̂ 𝑦∗ = −432 + 7.7(90) = 261 𝑠𝑐𝑖 = 𝑠𝑒√ 1 𝑛 + (𝑥∗ − ̄ 𝑥)2 𝑆𝑥 𝑥 = 28.17√ 1 20 + (90−87.5)2 607 = 6.9172 251 The 95% confiden"
5011,unknown,"̂ 𝑦∗ ± 𝑡𝛼/2𝑠𝑐𝑖 = 261 ± 2.101(6.9172)− > (246.4670, 275.5330) • c. 𝑠𝑝𝑖 = 𝑠𝑒√1 + 1 𝑛 + (𝑥∗ − ̄ 𝑥)2 𝑆𝑥 𝑥 = 28.17√1 + 1 20 + (90−87.5)2 607 = 29.0068 The 95% prediction interval for 𝑦∗ is ̂ 𝑦∗ ± 𝑡𝛼/2𝑠𝑝𝑖 = 261 ± 2.101(29.0068)− > (200.0567, 321.9433) • d. 𝑦∗ = –432 + 7.7(100) = 338 lines Our regression line is only valid for the range of x values in our sample ( 77∘𝐹 to 95∘𝐹 ). W e should interpret thi"
5012,unknown,"Problem Sheet 3 Q1 Suppose we estimate the regression coeﬀicients in a linear regression model by minimizing 𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝛽0 − 𝑝 ∑ 𝑗=1 𝛽𝑗𝑥𝑖𝑗) 2 + 𝜆 𝑝 ∑ 𝑗=1 𝛽2 𝑗 for a particular value of 𝜆. Which of the following statement is TRUE? Select all that apply: A. As we increase 𝜆 from 0, the training RSS will steadily increase. B. As we increase 𝜆 from 0, the test RSS will steadily decrease. C. As we i"
5013,unknown,"C. As we increase 𝜆 from 0, the (squared) bias will steadily decrease. D. As we increase 𝜆 from 0, the variance of fitted values will steadily decrease. Q2 Which of the following is a benefit of the sparsity imposed by the Lasso? Select all that apply: A. Sparse models are generally more easy to interpret. B. The Lasso does variable selection by default. C. Using the Lasso penalty helps to decreas"
5014,unknown,"D. Using the Lasso penalty helps to decrease the variance of the fits. 252 Q3 Y ou perform a regression on a problem where your second predictor, x_2, is measured in meter. Y ou decide to refit the model after changing x_2 to be measured in kilometer. Which of the following is true? Select all that apply: A. If the model you performed is standard linear regression, then ̂𝛽2 will change but ̂ 𝑦will"
5015,unknown,"remain the same. B. If the model you performed is standard linear regression, then neither ̂𝛽2 nor ̂ 𝑦will change. C. If the model you performed is ridge regression, then ̂𝛽2 and ̂ 𝑦will both change. D. If the model you performed is Lasso regression, then neither ̂𝛽2 nor ̂ 𝑦will change. Q4 Y ou are working on a regression problem with many variables, so you decide to do Principal Components Analys"
5016,unknown,Components Analysis first and then fit the regression to the first 2 principal components. Which of the following would you expect to happen? Select all that apply: A. A subset of the features will be selected. B. Model Bias will decrease relative to the full least squares model. C. V ariance of fitted values will decrease relative to the full least squares model. D. Model interpretability will im
5017,unknown,D. Model interpretability will improve relative to the full least squares model. Q5 W e compute the principal components of our p predictor variables. The RSS in a simple linear regression of Y onto the largest principal component will always be no larger than the RSS in a simple regression of Y onto the second largest principal component. T rue or F alse? Q6 Suppose we estimate the regression coe
5018,unknown,"dictor 𝑥 for by minimizing 𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝛽0 − 𝛽1𝑥𝑖) 2 subject to |𝛽1| ≤ 𝑐. F or a given𝑐, the fitted coeﬀicient for predictor 𝑥 is ̂𝛽1 = 𝛼. Suppose we now include an exact copy 𝑥∗ = 𝑥 and refit our lasso regression. Characterize the effect of this exact collinearity by describing the set of solutions for ̂𝛽1 and ̂𝛽∗ 1, using the same value of 𝑐. 253 Q7 It is well-known that ridge regression tends "
5019,unknown,"253 Q7 It is well-known that ridge regression tends to give similar coeﬀicient values to correlated variables, whereas the lasso may give quite different coeﬀicient values to correlated variables. W e will now explore this property in a very simple setting. Suppose that 𝑛 = 2 , 𝑝 = 2 , 𝑥11 = 𝑥 12, 𝑥21 = 𝑥 22. F urthermore, suppose that 𝑦1 + 𝑦2 = 0 and 𝑥11 + 𝑥21 = 0 and 𝑥12 + 𝑥22 = 0, so that the e"
5020,unknown,"lasso model is zero: ̂𝛽0 = 0. (a) W rite out the ridge regression optimization problem in this setting. (b) Argue that in this setting, the ridge coeﬀicient estimates satisfy ̂𝛽1 = ̂𝛽2. (c) W rite out the lasso optimization problem in this setting. (d) Argue that in this setting, the lasso coeﬀicients ̂𝛽1 and ̂𝛽2 are not unique—in other words, there are many possible solutions to the optimization "
5021,unknown,these solutions. Q8 A principal component analysis of a 4-dimensional data set (containing the annual power generations of four power stations over a 20 years period) has been carried out. The standard deviation of each of the principal component (PC) is listed below: The eigenvectors generated using principal component analysis are listed below: Let ̂Σ ∈ ℝ4×4 denote the sample variance matrix of 
5022,unknown,"• Outline the main goals of principal components analysis of data • Provide the ordered eigenvalues of ̂Σ, and given the total variance of the data cloud. • Draw a scree plot. How many components would you need in order to capture at least 95% of the total variance of the data cloud? • Provide the first and the second eigenvector of ̂Σ, which we denote by 𝛾1 and 𝛾2. Without carrying out calculatio"
5023,unknown,"𝑇 𝛾1, 𝛾2 𝑇 𝛾1 and 𝛾2 𝑇 𝛾2, and explain your answer. Problem Sheet 3 Solution 254 Q1 Suppose we estimate the regression coeﬀicients in a linear regression model by minimizing 𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝛽0 − 𝑝 ∑ 𝑗=1 𝛽𝑗𝑥𝑖𝑗) 2 + 𝜆 𝑝 ∑ 𝑗=1 𝛽2 𝑗 for a particular value of 𝜆. Which of the following statement is TRUE? Select all that apply: A. As we increase 𝜆 from 0, the training RSS will steadily increase. B. As we i"
5024,unknown,"B. As we increase 𝜆 from 0, the test RSS will steadily decrease. C. As we increase 𝜆 from 0, the (squared) bias will steadily decrease. D. As we increase 𝜆 from 0, the variance of fitted values will steadily decrease. Solution A, D Q2 Which of the following is a benefit of the sparsity imposed by the Lasso? Select all that apply: A. Sparse models are generally more easy to interpret. B. The Lasso "
5025,unknown,"B. The Lasso does variable selection by default. C. Using the Lasso penalty helps to decrease the bias of the fits. D. Using the Lasso penalty helps to decrease the variance of the fits. Solution: A, B, D Q3 Y ou perform a regression on a problem where your second predictor, x_2, is measured in meter. Y ou decide to refit the model after changing x_2 to be measured in kilometer. Which of the follo"
5026,unknown,"of the following is true? Select all that apply: A. If the model you performed is standard linear regression, then ̂𝛽2 will change but ̂ 𝑦will remain the same. B. If the model you performed is standard linear regression, then neither ̂𝛽2 nor ̂ 𝑦will change. C. If the model you performed is ridge regression, then ̂𝛽2 and ̂ 𝑦will both change. D. If the model you performed is Lasso regression, then n"
5027,unknown,"Solution: A, C 255 Q4 Y ou are working on a regression problem with many variables, so you decide to do Principal Components Analysis first and then fit the regression to the first 2 principal components. Which of the following would you expect to happen? Select all that apply: A. A subset of the features will be selected. B. Model Bias will decrease relative to the full least squares model. C. V "
5028,unknown,C. V ariance of fitted values will decrease relative to the full least squares model. D. Model interpretability will improve relative to the full least squares model. Solution: C Q5 W e compute the principal components of our p predictor variables. The RSS in a simple linear regression of Y onto the largest principal component will always be no larger than the RSS in a simple regression of Y onto 
5029,unknown,"Solution: F alse Q6 Suppose we estimate the regression coeﬀicients for a lasso regression model of a single pre- dictor 𝑥 for by minimizing 𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝛽0 − 𝛽1𝑥𝑖) 2 subject to |𝛽1| ≤ 𝑐. F or a given𝑐, the fitted coeﬀicient for predictor 𝑥 is ̂𝛽1 = 𝛼. Suppose we now include an exact copy 𝑥∗ = 𝑥 and refit our lasso regression. Characterize the effect of this exact collinearity by describing the se"
5030,unknown,"1, using the same value of 𝑐. Solution: Apply the Lagrange multiplier, we minimize for the Lasso optimization is 𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝛽0 − 𝛽1𝑥𝑖 − 𝛽∗ 1𝑥∗ 𝑖 ) 2 + 𝜆(|𝛽1| + |𝛽∗ 1|) = 𝑛 ∑ 𝑖=1 (𝑦𝑖 − 𝛽0 − (𝛽1 + 𝛽∗ 1)𝑥𝑖) 2 + 𝜆|𝛽1 + 𝛽∗ 1| + 𝜆 (|𝛽1| + |𝛽∗ 1| − |𝛽1 + 𝛽∗ 1|) Note that the first two terms represent the same minimization problem for the original lasso regression (before we added a duplicate feature) "
5031,unknown,"regression (before we added a duplicate feature) and because |𝛽1 + 𝛽∗ 1| ≤ |𝛽1| + |𝛽∗ 1|, 256 the third term is either positive or zero. It will be zero if 𝛽1 and 𝛽∗ 1 are either both positive or negative. W e know that the minimum of the first two terms of this objective function happens for the solution 𝛽1 + 𝛽∗ 1 = 𝑎 as mentioned in the problem. Therefore the solution must satisfy 𝛽1 + 𝛽∗ 1 = 𝑎 "
5032,unknown,"must satisfy 𝛽1 + 𝛽∗ 1 = 𝑎 and 𝛽1 ∗ 𝛽∗ 1 > 0. Q7 It is well-known that ridge regression tends to give similar coeﬀicient values to correlated variables, whereas the lasso may give quite different coeﬀicient values to correlated variables. W e will now explore this property in a very simple setting. Suppose that 𝑛 = 2 , 𝑝 = 2 , 𝑥11 = 𝑥 12, 𝑥21 = 𝑥 22. F urthermore, suppose that 𝑦1 + 𝑦2 = 0 and 𝑥11 "
5033,unknown,"𝑥11 = 𝑥 12, 𝑥21 = 𝑥 22. F urthermore, suppose that 𝑦1 + 𝑦2 = 0 and 𝑥11 + 𝑥21 = 0 and 𝑥12 + 𝑥22 = 0, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: ̂𝛽0 = 0. (a) W rite out the ridge regression optimization problem in this setting. (b) Argue that in this setting, the ridge coeﬀicient estimates satisfy ̂𝛽1 = ̂𝛽2. (c) W rite out the lasso optimizat"
5034,unknown,"(c) W rite out the lasso optimization problem in this setting. (d) Argue that in this setting, the lasso coeﬀicients ̂𝛽1 and ̂𝛽2 are not unique—in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions. Solution: (a) (𝑦1 − 𝛽1𝑥11 − 𝛽2𝑥12)2 + (𝑦2 − 𝛽1𝑥21 − 𝛽2𝑥22)2 + 𝜆(𝛽2 1 + 𝛽2 2 ) (b) Let 𝑥11 = 𝑥 12 = 𝑥 1, 𝑥21 = 𝑥 22 = 𝑥 2. T ake derivatives of t"
5035,unknown,respect to both 𝛽1 and 𝛽2 and setting them equal to zero gives −2(𝑦1𝑥1 + 𝑦2𝑥2) + 2( ̂𝛽1 + ̂𝛽2)𝑥2 1 + 2( ̂𝛽1 + ̂𝛽2)𝑥2 2 + 𝜆 ̂𝛽1 = 0 −2(𝑦1𝑥1 + 𝑦2𝑥2) + 2( ̂𝛽1 + ̂𝛽2)𝑥2 1 + 2( ̂𝛽1 + ̂𝛽2)𝑥2 2 + 𝜆 ̂𝛽2 = 0 subtract one equation from the other givens ̂𝛽1 = ̂𝛽2 (c) (𝑦1 − 𝛽1𝑥11 − 𝛽2𝑥12)2 + (𝑦2 − 𝛽1𝑥21 − 𝛽2𝑥22)2 + 𝜆(|𝛽1| + |𝛽2|) (d) Note that minimizing the function in (c) is equivalent to minimizing the LS 
5036,unknown,"the constraints |𝛽1| + |𝛽2| ≤ 𝑐 , which can be geometrically interpreted as a diamond centered at (0, 0). Also given that 𝑦1 + 𝑦2 = 0 and 𝑥11 + 𝑥21 = 0 and 𝑥12 + 𝑥22 = 0 , the LS function (𝑦1 − 𝛽1𝑥11 − 𝛽2𝑥12)2 + (𝑦2 − 𝛽1𝑥21 − 𝛽2𝑥22)2 = 2(𝑦1 − (𝛽1 + 𝛽2)𝑥1)2. Therefore the solution of minimizing LS is ̂𝛽1 + ̂𝛽2 = 𝑦1 𝑥1 , which is in fact a line parallel to the edge of “Lasso-diamond” ̂𝛽1 + ̂𝛽2 = 𝑐. "
5037,unknown,"to the edge of “Lasso-diamond” ̂𝛽1 + ̂𝛽2 = 𝑐. Note the solutions to the original Lasso optimization problem are contours of the function 2(𝑦1 − ( ̂𝛽1 + ̂𝛽2)𝑥1)2 that touch the Lasso-diamond ̂𝛽1 + ̂𝛽2 = 𝑐. 257 Since ̂𝛽1 and ̂𝛽2 are along the line ̂𝛽1 + ̂𝛽2 = 𝑦1 𝑥1 , the entire edge ̂𝛽1 + ̂𝛽2 = 𝑐 is a potential solution to the Lasso optimization problem! The same argument can be made for the opposit"
5038,unknown,opposite Lasso-diamond edge ̂𝛽1 + ̂𝛽2 = −𝑐. Therefore this Lasso problem does not have a unique solution. The general form of solution is given by two line segments: ̂𝛽1 + ̂𝛽2 = 𝑐; ̂𝛽1 ≥ 0 𝑎𝑛𝑑 ̂𝛽2 ≥ 0 ̂𝛽1 + ̂𝛽2 = −𝑐; ̂𝛽1 ≤ 0 𝑎𝑛𝑑 ̂𝛽2 ≤ 0 Q8 A principal component analysis of a 4-dimensional data set (containing the annual power generations of four power stations over a 20 years period) has been carr
5039,unknown,"deviation of each of the principal component (PC) is listed below: The eigenvectors generated using principal component analysis are listed below: Let ̂Σ ∈ ℝ4×4 denote the sample variance matrix of the data set. • Outline the main goals of principal components analysis of data • Provide the ordered eigenvalues of ̂Σ, and given the total variance of the data cloud. • Draw a scree plot. How many com"
5040,unknown,"95% of the total variance of the data cloud? • Provide the first and the second eigenvector of ̂Σ, which we denote by 𝛾1 and 𝛾2. Without carrying out calculations, give the numerical values of𝛾1 𝑇 𝛾1, 𝛾2 𝑇 𝛾1 and 𝛾2 𝑇 𝛾2, and explain your answer. Solution: • T o assess “true” dimensionality of multivariate data T o provide low-dimensional representations of data T o relate principal forms of varia"
5041,unknown,"T o provide principal components that are uncorrelated with each other. • 𝜆1 = 2.0562 = 4.227136 𝜆2 = 0.4922 = 0.242064 258 𝜆3 = 0.2792 = 0.077841 𝜆4 = 0.1542 = 0.023716 𝑇 𝑉 = 𝜆 1 + 𝜆2 + 𝜆3 + 𝜆4 = 4.570757 • W e need the first two components to capture at least 95% of the total variance. • 𝛾1 = (0.361, −0.084, 0.856, 0.358)𝑇 ; 𝛾2 = (−0.656, −0.730, 0.173, 0.075)𝑇 . eigenvectors are unit vectors, i"
5042,unknown,"1 𝛾1 = 𝛾 𝑇 2 𝛾2 = 1 and mutually orthogonal, i.e. 𝛾𝑇 2 𝛾1 = 0. Problem Sheet 4 Q1 Which of the following statement about smoothing splines is TRUE? Select all that apply: A. Smoothing splines are natural cubic splines with an infinite number of knots. B. Letting tuning parameter 𝜆 tend to infinity leads to the standard (first-order) linear model situation. C. T oo small values of 𝜆 could lead to o"
5043,unknown,"C. T oo small values of 𝜆 could lead to overfitting. D. 𝜆 can be tuned via cross-validation. Q2 Let 𝐼 {𝑥 ≤ 𝑐} denote a function which is 1 if 𝑥 ≤ 𝑐 and 0 otherwise. Which of the following is a basis for linear splines with a knot at 𝑐? Select all that apply: A. 1, 𝑥, (𝑥 − 𝑐)𝐼 {𝑥 > 𝑐} B. 1, 𝑥, (𝑥 − 𝑐)𝐼 {𝑥 ≤ 𝑐} C. 1, 𝑥, 𝐼 {𝑥 ≤ 𝑐} D. 1, (𝑥 − 𝑐)𝐼 {𝑥 > 𝑐}, (𝑥 − 𝑐)𝐼 {𝑥 ≤ 𝑐}. 259 Q3 Suppose we have a ran"
5044,unknown,"259 Q3 Suppose we have a random sample from 𝑛 individuals. F or each individual we have mea- surements on 𝑝 predictor variables (𝑋1, ..., 𝑋𝑝) and a binary response variable 𝑌 . A logistic regression model is fitted to the data. Choose all TRUE statements about logistic regression. A. Logistic regression is based on a conditional model for (𝑋1, ..., 𝑋𝑝)|𝑌 = 0 and (𝑋1, ..., 𝑋𝑝)|𝑌 = 1 B. If the coeﬀi"
5045,unknown,"B. If the coeﬀicient of the j-th predictor variable, 𝛽𝑗, is positive, the probability that the response variable 𝑌 is equal to 1 increases as 𝑥𝑗 increases whilst all other predictor variables remain the same. C. Logistic regression cannot use categorical variables as predictor variables. D. Logistic regression cannot be extended to handle categorical response variables which take more than two pos"
5046,unknown,"more than two possible outcomes. Q4 In terms of model complexity , which is more similar to a smoothing spline with 100 knots and 5 effective degrees of freedom? A. A natural cubic spline with 5 knots B. A natural cubic spline with 100 knots Q5 In the GAM 𝑦 ∼ 𝑓1(𝑋1) + 𝑓2(𝑋2) + 𝜖, as we make 𝑓1 and 𝑓2 more and more complex we can approximate any regression function to arbitrary precision. T rue or "
5047,unknown,"Q6 Is the following function a cubic spline? Why or why not? 𝑓 (𝑥) = ⎧{{{{ ⎨{{{{⎩ 0, 𝑥 < 0, 𝑥3, 0 ≤ 𝑥 < 1, 𝑥3 + (𝑥 − 1)3, 1 ≤ 𝑥 < 2, −(𝑥 − 3)3 − (𝑥 − 4)3, 2 ≤ 𝑥 < 3, −(𝑥 − 4)3, 3 ≤ 𝑥 < 4, 0, 4 ≤ 𝑥. Q7 A cubic regression spline with one knot at 𝜉 can be obtained using a basis of the form 𝑥, 𝑥2, 𝑥3, (𝑥 − 𝜉)3 +, where (𝑥 − 𝜉)3 + = (𝑥 − 𝜉)3 if 𝑥 > 𝜉 and equals 0 otherwise. Show that a function of the "
5048,unknown,"of the form 𝑓 (𝑥) = 𝛽0 + 𝛽1𝑥 + 𝛽2𝑥2 + 𝛽3𝑥3 + 𝛽4(𝑥 − 𝜉)3 + 260 is indeed a cubic regression spline, regardless of the values of 𝛽0, 𝛽1, 𝛽2, 𝛽3, 𝛽4. a) Find a cubic polynomial 𝑓1(𝑥) = 𝑎1 + 𝑏1𝑥 + 𝑐1𝑥2 + 𝑑1𝑥3 such that 𝑓 (𝑥) = 𝑓1(𝑥) for all 𝑥 ≤ 𝜉. Express 𝑎1, 𝑏1, 𝑐1, 𝑑1 in terms of 𝛽0, 𝛽1, 𝛽2, 𝛽3, 𝛽4. b) Find a cubic polynomial 𝑓2(𝑥) = 𝑎2 + 𝑏2𝑥 + 𝑐2𝑥2 + 𝑑2𝑥3 such that 𝑓 (𝑥) = 𝑓2(𝑥) for all 𝑥 > 𝜉. Expr"
5049,unknown,"c) Show that 𝑓1(𝜉) = 𝑓2(𝜉). d) Show that 𝑓 ′ 1(𝜉) = 𝑓 ′ 2(𝜉). e) Show that 𝑓 ″ 1 (𝜉) = 𝑓 ″ 2 (𝜉). Q8 Prove that 𝑝(𝑋) = exp(𝛽0 +𝛽1 𝑋) 1+exp(𝛽0 +𝛽1 𝑋) is equivalent to ln 𝑝(𝑋) 1−𝑝(𝑋) = 𝛽0 + 𝛽1𝑋. In other words, the logistic function representation and logit transformation are equivalent. Q9 Consider a simple logistic regression model for a binary classification problem,𝑝(𝑋) = 𝑃 (𝑌 = 1|𝑋) = exp(𝛽0 +𝛽"
5050,unknown,"1+exp(𝛽0 +𝛽1 𝑋) . Given the data of response {𝑦𝑖}, 𝑖 = 1, ..., 𝑛 and the corresponding data of single predictor {𝑥𝑖}, 𝑖 = 1, ..., 𝑛, derive the log-likelihood of the parameters 𝛽0, 𝛽1 and state any assumptions required for the derivation. Problem Sheet 4 Solution Q1 Which of the following statement about smoothing splines is TRUE? Select all that apply: A. Smoothing splines are natural cubic splin"
5051,unknown,"A. Smoothing splines are natural cubic splines with an infinite number of knots. B. Letting tuning parameter 𝜆 tend to infinity leads to the standard (first-order) linear model situation. C. T oo small values of 𝜆 could lead to overfitting. D. 𝜆 can be tuned via cross-validation. Solution: B, C, D 261 Q2 Let 𝐼 {𝑥 ≤ 𝑐} denote a function which is 1 if 𝑥 ≤ 𝑐 and 0 otherwise. Which of the following is"
5052,unknown,"A. 1, 𝑥, (𝑥 − 𝑐)𝐼 {𝑥 > 𝑐} B. 1, 𝑥, (𝑥 − 𝑐)𝐼 {𝑥 ≤ 𝑐} C. 1, 𝑥, 𝐼 {𝑥 ≤ 𝑐} D. 1, (𝑥 − 𝑐)𝐼 {𝑥 > 𝑐}, (𝑥 − 𝑐)𝐼 {𝑥 ≤ 𝑐}. Solution: A, B, D Q3 Suppose we have a random sample from 𝑛 individuals. F or each individual we have mea- surements on 𝑝 predictor variables (𝑋1, ..., 𝑋𝑝) and a binary response variable 𝑌 . A logistic regression model is fitted to the data. Choose all TRUE statements about logistic reg"
5053,unknown,"A. Logistic regression is based on a conditional model for (𝑋1, ..., 𝑋𝑝)|𝑌 = 0 and (𝑋1, ..., 𝑋𝑝)|𝑌 = 1 B. If the coeﬀicient of the j-th predictor variable, 𝛽𝑗, is positive, the probability that the response variable 𝑌 is equal to 1 increases as 𝑥𝑗 increases whilst all other predictor variables remain the same. C. Logistic regression cannot use categorical variables as predictor variables. D. Logis"
5054,unknown,"D. Logistic regression cannot be extended to handle categorical response variables which take more than two possible outcomes. Solution: B Q4 In terms of model complexity , which is more similar to a smoothing spline with 100 knots and 5 effective degrees of freedom? A. A natural cubic spline with 5 knots B. A natural cubic spline with 100 knots Solution: A Even though the smoothing spline has 100"
5055,unknown,"Even though the smoothing spline has 100 knots, it is penalized to be smooth, so it is about as complex as a model with 5 variables (effective degree of freedom). The natural cubic spline with 5 knots has 5 degrees of freedom is such a model. Note a cubic spline with 5 knots has 5+3+1 degrees of freedom, the natural cubic spline adds two more linear constraints which frees 2 df at each side (a cub"
5056,unknown,"frees 2 df at each side (a cubic polynomial model–>linear model) 262 Q5 In the GAM 𝑦 ∼ 𝑓1(𝑋1) + 𝑓2(𝑋2) + 𝜖, as we make 𝑓1 and 𝑓2 more and more complex we can approximate any regression function to arbitrary precision. T rue or F alse? Solution: F alse Note, this additive model can not capture interaction behaviors. Q6 Is the following function a cubic spline? Why or why not? 𝑓 (𝑥) = ⎧{{{{ ⎨{{{{⎩ 0"
5057,unknown,"⎧{{{{ ⎨{{{{⎩ 0, 𝑥 < 0, 𝑥3, 0 ≤ 𝑥 < 1, 𝑥3 + (𝑥 − 1)3, 1 ≤ 𝑥 < 2, −(𝑥 − 3)3 − (𝑥 − 4)3, 2 ≤ 𝑥 < 3, −(𝑥 − 4)3, 3 ≤ 𝑥 < 4, 0, 4 ≤ 𝑥. Solution: At 𝑥 = 2, 𝑓 ′ is discontinuous (15 on one side and -15 on the other side), so this is not a cubic spline. Q7 A cubic regression spline with one knot at 𝜉 can be obtained using a basis of the form 𝑥, 𝑥2, 𝑥3, (𝑥 − 𝜉)3 +, where (𝑥 − 𝜉)3 + = (𝑥 − 𝜉)3 if 𝑥 > 𝜉 and e"
5058,unknown,"of the form 𝑓 (𝑥) = 𝛽0 + 𝛽1𝑥 + 𝛽2𝑥2 + 𝛽3𝑥3 + 𝛽4(𝑥 − 𝜉)3 + is indeed a cubic regression spline, regardless of the values of 𝛽0, 𝛽1, 𝛽2, 𝛽3, 𝛽4. a) Find a cubic polynomial 𝑓1(𝑥) = 𝑎1 + 𝑏1𝑥 + 𝑐1𝑥2 + 𝑑1𝑥3 such that 𝑓 (𝑥) = 𝑓1(𝑥) for all 𝑥 ≤ 𝜉. Express 𝑎1, 𝑏1, 𝑐1, 𝑑1 in terms of 𝛽0, 𝛽1, 𝛽2, 𝛽3, 𝛽4. b) Find a cubic polynomial 𝑓2(𝑥) = 𝑎2 + 𝑏2𝑥 + 𝑐2𝑥2 + 𝑑2𝑥3 such that 𝑓 (𝑥) = 𝑓2(𝑥) for all 𝑥 > 𝜉. Express "
5059,unknown,"c) Show that 𝑓1(𝜉) = 𝑓2(𝜉). d) Show that 𝑓 ′ 1(𝜉) = 𝑓 ′ 2(𝜉). e) Show that 𝑓 ″ 1 (𝜉) = 𝑓 ″ 2 (𝜉). Solution: 263 a) F or𝑥 ≤ 𝜉, 𝑓1(𝑥) has coeﬀicients 𝑎1 = 𝛽0, 𝑏1 = 𝛽1, 𝑐1 = 𝛽2 and 𝑑1 = 𝛽3 b) F or𝑥 > 𝜉, we have 𝑓 (𝑥) = 𝛽0 + 𝛽1𝑥 + 𝛽2𝑥2 + 𝛽3𝑥3 + 𝛽4(𝑥 − 𝜉)3 = (𝛽0 − 𝛽4𝜉3) + (𝛽1 + 3𝜉2𝛽4)𝑥 + (𝛽2 − 3𝛽4𝜉)𝑥2 + (𝛽3 + 𝛽4)𝑥3 so we take 𝑎2 = 𝛽0 − 𝛽4𝜉3, 𝑏2 = 𝛽1 + 3𝜉2𝛽4,𝑐2 = 𝛽2 − 3𝛽4𝜉, 𝑑2 = 𝛽3 + 𝛽4. c) 𝑓1(𝜉) = 𝛽0 +"
5060,unknown,c) 𝑓1(𝜉) = 𝛽0 + 𝛽1𝜉 + 𝛽2𝜉2 + 𝛽3𝜉3 and 𝑓2(𝜉) = (𝛽0 −𝛽4𝜉3)+(𝛽 1 +3𝜉 2𝛽4)𝜉 +(𝛽2 −3𝛽4𝜉)𝜉 2 +(𝛽3 +𝛽4)𝜉3 = 𝛽0 +𝛽1𝜉 +𝛽2𝜉2 +𝛽3𝜉3 d) 𝑓 ′ 1(𝜉) = 𝛽1 + 2𝛽2𝜉 + 3𝛽3𝜉2 and 𝑓 ′ 2(𝜉) = 𝛽1 + 3𝜉2𝛽4 + 2(𝛽2 − 3𝛽4𝜉)𝜉 + 3(𝛽3 + 𝛽4)𝜉2 = 𝛽1 + 2𝛽2𝜉 + 3𝛽3𝜉2 e) 𝑓 ″ 1 (𝜉) = 2𝛽2 + 6𝛽3𝜉 and 𝑓 ″ 2 (𝜉) = 2(𝛽2 − 3𝛽4𝜉) + 6(𝛽3 + 𝛽4)𝜉 = 2𝛽2 + 6𝛽3𝜉 Q8 Prove that 𝑝(𝑋) = exp(𝛽0 +𝛽1 𝑋) 1+exp(𝛽0 +𝛽1 𝑋) is equivalent to ln 𝑝(𝑋) 1−𝑝(𝑋) = 𝛽0 
5061,unknown,"1+exp(𝛽0 +𝛽1 𝑋) is equivalent to ln 𝑝(𝑋) 1−𝑝(𝑋) = 𝛽0 + 𝛽1𝑋. In other words, the logistic function representation and logit transformation are equivalent. Solution: 𝑝(𝑋) 1 − 𝑝(𝑋) = exp(𝛽0 +𝛽1 𝑋) 1+exp(𝛽0 +𝛽1 𝑋) 1 − exp(𝛽0 +𝛽1 𝑋) 1+exp(𝛽0 +𝛽1 𝑋) = exp(𝛽0 +𝛽1 𝑋) 1+exp(𝛽0 +𝛽1 𝑋) 1+exp(𝛽0 +𝛽1 𝑋) 1+exp(𝛽0 +𝛽1 𝑋) − exp(𝛽0 +𝛽1 𝑋) 1+exp(𝛽0 +𝛽1 𝑋) = exp(𝛽0 +𝛽1 𝑋) 1+exp(𝛽0 +𝛽1 𝑋) 1 1+exp(𝛽0 +𝛽1 𝑋) = exp(𝛽0 +"
5062,unknown,"1+exp(𝛽0 +𝛽1 𝑋) 1 1+exp(𝛽0 +𝛽1 𝑋) = exp(𝛽0 + 𝛽1𝑋) (13.5) therefore ln 𝑝(𝑋) 1−𝑝(𝑋) = 𝛽0 + 𝛽1𝑋 264 Q9 Consider a simple logistic regression model for a binary classification problem,𝑝(𝑋) = 𝑃 (𝑌 = 1|𝑋) = exp(𝛽0 +𝛽1 𝑋) 1+exp(𝛽0 +𝛽1 𝑋) . Given the data of response {𝑦𝑖}, 𝑖 = 1, ..., 𝑛 and the corresponding data of single predictor {𝑥𝑖}, 𝑖 = 1, ..., 𝑛, derive the log-likelihood of the parameters 𝛽0, 𝛽1 a"
5063,unknown,"and state any assumptions required for the derivation. Solution: The probability of the observed class was either 𝑝, if 𝑦𝑖 = 1 , or 1 − 𝑝, if 𝑦𝑖 = 0 . The likelihood is then 𝐿(𝛽0, 𝛽1) = 𝑛 ∏ 𝑖=1 𝑝(𝑥𝑖)𝑦𝑖 (1 − 𝑝(𝑥𝑖))1−𝑦𝑖 . The log-likelihood turns products into sums: 𝑙(𝛽0, 𝛽1) = 𝑛 ∑ 𝑖=1 𝑦𝑖 log 𝑝(𝑥𝑖) + (1 − 𝑦𝑖) log(1 − 𝑝(𝑥𝑖)) = 𝑛 ∑ 𝑖=1 log(1 − 𝑝(𝑥𝑖)) + 𝑛 ∑ 𝑖=1 𝑦𝑖 log 𝑝(𝑥𝑖) 1 − 𝑝(𝑥𝑖) = 𝑛 ∑ 𝑖=1 log(1 − "
5064,unknown,"𝑛 ∑ 𝑖=1 𝑦𝑖(𝛽0 + 𝛽1𝑥𝑖) = 𝑛 ∑ 𝑖=1 − log(1 + exp(𝛽0 + 𝛽1𝑥𝑖)) + 𝑛 ∑ 𝑖=1 𝑦𝑖(𝛽0 + 𝛽1𝑥𝑖) (13.6) Note the assumptions required are • independence of observations, in order to use the ∏ • 𝑝(𝑋) = 𝑃 (𝑌 = 1|𝑋) = exp(𝛽0 +𝛽1 𝑋) 1+exp(𝛽0 +𝛽1 𝑋) suggests that we assume a linear relationship between the predictor 𝑋 and the logit transformation of the response variable 𝑌 . Part II Practical Classes 265 Practical Cl"
5065,unknown,"Part II Practical Classes 265 Practical Class Sheets 1 In this practical class, we will use R to fit linear regression models by the method of least squares (LS). W e will make use of R’s lm function simplify the process of model fitting and parameter estimation, and use information from its output to construct confidence intervals. Finally , we will assess the quality of the regression and use re"
5066,unknown,"the validity of the regression assumptions. T o start, create a new R script by clicking on the corresponding menu button (in the top left); and save it somewhere in your computer. Y ou can now write al l your code into this file, and then execute it in R using the menu button Run or by simple pressing ctrl + enter. The faithful data set The dataset we will be using in this practial class concerns"
5067,unknown,"The dataset we will be using in this practial class concerns the behaviour of the Old F aithful geyser in Y ellowstone National Park, W yoming, USA. F or a natural phenomenon, the Old F aithful geyser is highly predictable and has erupted every 44 to 125 minutes since 2000. There are two variables in the data set: 1. the waiting time between eruptions of the geyser, and 2. the duration of the erup"
5068,unknown,"1. the waiting time between eruptions of the geyser, and 2. the duration of the eruptions. Before we start fitting any models, the first step should always be to perform some exploratory data analysis of the data to gain some insight into the behaviour of the variables and suggest any interesting relationships or hypotheses to explore further. Exercise 1.1 • Load the faithful dataset by typing dat"
5069,unknown,"• T o save typing later, let us extract the columns into two variables. Create a vector w that contains the vector of waiting times, and a second vector d that contains the durations of eruptions. • Plot the waiting times (y-axis) against the durations (x-axis). • Compute the Pearson correlation coeﬀicient between w and d using the cor function. • Is there evidence of a linear relationship? Click "
5070,unknown,"Click for solution ## SOLUTION data(faithful) 267 268 w <- faithful$waiting d <- faithful$eruptions cor(w, d) ## [1] 0.9008112 plot(y=w,x=d) 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 50 60 70 80 90 d w Fitting the simple linear regression model The model we’re interested in fitting to these data is a simple linear relationship between the waiting times, 𝑤, and the eruption durations, 𝑑: 𝑤 = 𝛽0 + 𝛽1𝑑 + 𝜀. R "
5071,unknown,"R provides the lmfunction to compute the least-squares linear regression for us, and it returns an object which summarises all aspects of the regression fit. T echnique In R, we can fit linear models by the method of least squares using the function lm. Suppose our response variable is 𝑌 , and we have a predictor variable 𝑋, with observations stored in R vectors y and x respectively . T o fit 𝑦 as"
5072,unknown,"command: model <- lm(y ~ x) Alternatively , if we have a data frame calleddataset with variables in columns a and b then we could fit the linear regression of a on b without having to extract the columns into vectors by specifying the data argument 269 model <- lm(a ~ b, data=dataset) The ~(tilde) symbol in this expression should be read as “is modelled as” . Note thatR always automatically includ"
5073,unknown,"automatically include a constant term, so we only need to specify the 𝑋 and 𝑌 variables. W e can inspect the fitted regression object returned from lm to get a simple summary of the estimated model parameters. T o do this, simply evaluate the variable which contains the information returned from lm: model Exercise 1.2 • Use lm to fit waiting times w as a linear function of eruption durations d. • "
5074,unknown,• Save the result of the regression function to model. • Inspect the value of model to find the fitted least-squares coeﬀicients. Click for solution ## SOLUTION # lm(y ~ x) use only x variable # lm(y ~ .) use all predictors # lm(y ~ 1) intercept only model <- lm(w~d) model ## ## Call: ## lm(formula = w ~ d) ## ## Coefficients: ## (Intercept) d ## 33.47 10.73 T echnique R also has a number of funct
5075,unknown,"R also has a number of functions that, when applied to the results of a linear regression, will return key quantities such as residuals and fitted values. • coef(model) and coefficicents(model) – returns the estimated model coeﬀicients as a vector (𝑏0, 𝑏1) • fitted(model) and fitted.values(model) – returns the vector of fitted values, ̂ 𝑦𝑖 = 𝑏0 + 𝑏1𝑥𝑖 • resid(model) and residuals(model) – returns "
5076,unknown,"𝑦𝑖 − ̂ 𝑦𝑖 Exercise 1.3 a. Use the coef function to extract the coeﬀicients of the fitted linear regression model as a vector. 270 b. Extract the vector of residuals from model, and use this to compute the sum of squares of the residuals as lsq.Q. Click for solution ## SOLUTION # (a) coef(model) ## (Intercept) d ## 33.47440 10.72964 beta0hat <- coef(model)[1] beta0hat ## (Intercept) ## 33.4744 beta"
5077,unknown,"## 33.4744 beta1hat <- coef(model)[2] beta1hat ## d ## 10.72964 # (b) lsq.resid <- resid(model) lsq.Q <- sum(lsq.resid^2) lsq.Q ## [1] 9443.387 The regression summary T echnique W e can easily extract and inspect the coeﬀicients and residuals of the linear model, but to obtain a more complete statistical summary of the model we use the summary function.: summary(model) There is a lot of informatio"
5078,unknown,"There is a lot of information in this output, but the key quantities to focus on are: • Residuals: simple summary statistics of the residuals from the regression. • Coeﬀicients: a table of information about the fitted coeﬀicients. Its columns are: – The label of the fitted coeﬀicient : The first will usually be (Intercept), and subsequent rows will be named after the other predictor variables in t"
5079,unknown,"– The Estimate column gives the least squares estimates of the coeﬀicients. – The Std. Error column gives the corresponding standard error for each coeﬀi- cient. – The t value column contains the 𝑡 test statistics. 271 – The Pr(>|t|) column is then the 𝑝-value associated with each test, where a low p-value indicates that we have evidence to reject the null hypothesis. • Residual standard error: Th"
5080,unknown,the residual variance. • Multiple R-Squared: This is the 𝑅2 value defined in lectures as the squared corre- lation coeﬀicient and is a measure of goodness of fit. W e can also use the summary to access the individual elements of the regression summary output. If we save the results of a call to the summary function of a lm object as summ: • summ$residuals – extracts the regression residuals as res
5081,unknown,"• summ$coefficients – returns the 𝑝 × 4 coeﬀicient summary table with columns for the estimated coeﬀicient, its standard error, t-statistic and corresponding (two-sided) p-value. • summ$sigma – extracts the regression standard error • summ$r.squared – returns the regression 𝑅2 Exercise 1.4 a. Apply the summary function to model to produce the regression summary for our ex- ample. b. Make sure you "
5082,unknown,"ample. b. Make sure you are able to locate: • The coeﬀicient estimates, and their standard errors, • The residual standard error, 𝑠𝑒; • The regression 𝑅2. c. Extract the regression standard error from the regression summary , and save it as se. (W e’ll need this later!) d. What do the entries in the t value column correspond to? What significance test (and what hypotheses) do these statistics corr"
5083,unknown,e. Are the coeﬀicients significant or not? What does this suggest about the regression? f. What is the 𝑅2 value for the regression and what is its interpretation? Can you extract its numerical value from the output as a new variable? g. Does the 𝑅2 value ‘agree’ with the Pearson correlation coeﬀicient between w and d you calculated early . Click for solution ## SOLUTION # (a) & (b) summary(model) 
5084,unknown,summary(model) ## ## Call: ## lm(formula = w ~ d) ## ## Residuals: 272 ## Min 1Q Median 3Q Max ## -12.0796 -4.4831 0.2122 3.9246 15.9719 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 33.4744 1.1549 28.98 <0.0000000000000002 *** ## d 10.7296 0.3148 34.09 <0.0000000000000002 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standa
5085,unknown,"## Multiple R-squared: 0.8115, Adjusted R-squared: 0.8108 ## F-statistic: 1162 on 1 and 270 DF, p-value: < 0.00000000000000022 # (c) se <- summary(model)$sigma se ## [1] 5.914009 # (d) & (e) # The column t value shows you the t-test associated with testing the significance # of the parameter listed in the first column. # Both coefficients are significant, as p-values are very small. # (f) rsq <- s"
5086,unknown,"# (f) rsq <- summary(model)$r.squared rsq ## [1] 0.8114608 # (g) cor(w, d)^2 ## [1] 0.8114608 Inference on the coeﬀicicents Given a fitted regression model, we can perform various hypothesis tests and construct confi- dence intervals to perform the usual kinds of statistical inference. T o do this, we require the Normality assumption to hold for our inference to be valid. Theory tells us that the "
5087,unknown,"𝑠𝑏0 = 𝑠𝑒√ 1 𝑛 + ̄ 𝑥2 𝑆𝑥𝑥 , 𝑠𝑏1 = 𝑠𝑒 √𝑆𝑥𝑥 , where 𝑠𝑏0 and 𝑠𝑏1 are unbiased estimates for 𝜎𝑏0 and 𝜎𝑏0 , the variances of 𝑏0 and 𝑏1. 273 Given these standard errors and the assumption of normality , we can say for coeﬀicient 𝛽𝑖, 𝑖 = 0, 1 that: 𝑏𝑖 − 𝛽𝑖 𝑠𝑏𝑖 ∼ 𝑡𝑛−2, and hence an 𝛼 level confidence interval for 𝛽𝑖 can be written as 𝑏𝑖 ± 𝑡∗ 𝑛−2,𝛼/2 𝑠𝑏𝑖 . Exercise 1.5 a. Consider the regression slope. Use "
5088,unknown,. Exercise 1.5 a. Consider the regression slope. Use the regression summary to extract the standard error of 𝑏1. Assign its value to the variable se.beta1. b. Compute the test statistic defined above using beta1hat and se.beta1 for a test under the null hypothesis 𝐻0 ∶ 𝛽1 = 0. Does it agree with the summary output from R? c. Use the pt function to find the probability of observing a test statistic
5089,unknown,"c. Use the pt function to find the probability of observing a test statistic as extreme as this. d. Referring to your 𝑡-test statistic and the t values in the regression summary , what do you conclude about the significance of the regression coeﬀicients? e. Find a 95% confidence interval for the slope 𝛽1. Y ou’ll need to use the qt function to find the critical value of the 𝑡-distribution. f. An e"
5090,unknown,"Click for solution ## SOLUTION # (a) # extract the standard error from the second column of the coefficient summary table se.beta1 <- summary(model)$coefficients[2,2] se.beta1 ## [1] 0.3147534 # (b) ## compute the t statistic t.beta1 <- (beta1hat-0)/se.beta1 t.beta1 ## d ## 34.08904 t.beta1<-unname(t.beta1) t.beta1 ## [1] 34.08904 # (c) ## p-value n <- length(w) 2*(1-pt(t.beta1, n-2)) # df =n-2 ##"
5091,unknown,"## [1] 0 274 # (e) ## confidence interval beta1hat + c(-1,1) * qt(0.975,n-2) * se.beta1 ## [1] 10.10996 11.34932 # (f) confint(model, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 31.20069 35.74810 ## d 10.10996 11.34932 # for the slope parameter confint(model, parm = ""d"", level = 0.95) ## 2.5 % 97.5 % ## d 10.10996 11.34932 # or confint(model, level = 0.95)[2,] ## 2.5 % 97.5 % ## 10.10996 11.34932"
5092,unknown,"confint(model, level = 0.95)[2,] ## 2.5 % 97.5 % ## 10.10996 11.34932 Estimation and prediction Suppose now that we are interested in some new point 𝑋 = 𝑥∗, and at this point we want to predict: (a) the location of the regression line, and (b) the value of 𝑌 . In the first problem, we are interested in the value 𝜇∗ = 𝛽0 + 𝛽1𝑥∗, i.e. the location of the regression line, at some new point 𝑋 = 𝑥∗. In"
5093,unknown,"In the second problem, we are interested in the value of 𝑌 ∗ = 𝛽0 + 𝛽1𝑥∗ + 𝜀, i.e. the actual observation value, at some new 𝑋 = 𝑥∗. The difference between the two is that the first simply concerns the location of the regression line, whereas the inference for the new observation 𝑌 ∗ concerns both the position of the regression line and the regression error about that point. Exercise 1.6 • Find a "
5094,unknown,"Exercise 1.6 • Find a 95% confidence interval for the estimated waiting time ̂𝑊 when the eruption duration is 3 minutes using the predict function. • Find a 95% prediction interval for the actual waiting time, 𝑊 . • Compare the two intervals. Click for solution 275 ## SOLUTION newdata1<- data.frame(d = 3) predict(model, newdata = newdata1, interval = ""confidence"", level = 0.95) ## fit lwr upr ## 1"
5095,unknown,"## fit lwr upr ## 1 65.66332 64.89535 66.4313 predict(model, newdata = newdata1, interval = ""prediction"", level = 0.95) ## fit lwr upr ## 1 65.66332 53.99458 77.33206 Residual Analysis The residuals from our linear regression are the values 𝑒𝑖 = ̂𝜀𝑖 = 𝑦𝑖 − ̂ 𝑦𝑖 for 𝑖 = 1, … , 𝑛, where ̂ 𝑦𝑖 = 𝑏0 − 𝑏1𝑥𝑖. Analysis of the residuals allow us to perform diagnostic tests to investigate the goodness of fi"
5096,unknown,"the goodness of fit of our regression under our modelling assumptions. Under the assumption of linearity , given the LS estimates, the observed values of 𝑋 are uncorrelated with the residuals. Which, in the case of simple linear regression, implies that the residuals are uncorrelated with any linear combination of𝑋, in particular the fitted values ̂ 𝑦𝑖 = 𝑏 0 + 𝑏1𝑥𝑖. Therefore, our diagnostics are "
5097,unknown,"̂ 𝑦𝑖 = 𝑏 0 + 𝑏1𝑥𝑖. Therefore, our diagnostics are based on the scatterplots of 𝑒 against ̂ 𝑦. In the case of simply linear regression will look the same as plotting 𝑒 against 𝑥. T o assess the SLR assumptions, we inspect the residual plot for particular features: • Even and random scatter of the residuals about 0 is the expected behaviour when our SLR assumptions are satisfied. • Residuals shown e"
5098,unknown,"• Residuals shown evidence of a trend or pattern — The presence of a clear pattern or trend in the residuals suggests that 𝔼 [𝜖]𝑖 ≠ 0 and ℂ𝑜𝑣 [𝜖𝑖, 𝜖𝑗] ≠ 0. There is clearly structure in the data that is not explained by the regression model, and so a simple linear model is not adequate for explaining the behaviour of 𝑌 . • Spread of the residuals is not constant — if the spread of the residuals ch"
5099,unknown,"substantially as 𝑥 (or ̂ 𝑦) changes then clearly our assumption of constant variance is not upheld. • A small number of residuals are very far from the others and 0 — obser- vations with very large residuals are known as outliers. Sometimes these points can be explained through points with particularly high measurement error, or the effect of another variable which should be included in the model."
5100,unknown,"of another variable which should be included in the model. Their presence could sig- nal problems with the data, a linear regression being inadequate, or a violation of Normality . Exercise 1.7 • Plot the residuals against the fitted values, and the residuals against the eruption durations side-by-side. • What do you see? Why should these plots be similar? 276 • Use your residual plot to assess wh"
5101,unknown,"Click for solution par(mfrow=c(1,2)) plot(y=resid(model), x=fitted(model)) plot(y=resid(model), x=d) 50 60 70 80 −10 −5 0 5 10 15 fitted(model) resid(model) 1.5 2.5 3.5 4.5 −10 −5 0 5 10 15 d resid(model) In order to state confidence intervals for the coeﬀicients and for predictions made using this model, in addition to the assumptions tested above, we also require that the regression errors are n"
5102,unknown,"are normally distributed. W e can check the normality assumption using quantile plots. Exercise 1.8 • Produce a side-by-side plot of the histogram of the residuals and a normal quantile plot of the residuals. • Do the residuals appear to be normally distributed? Click for solution par(mfrow=c(1,2)) hist(resid(model)) qqnorm(resid(model)) 277 Histogram of resid(model) resid(model) Frequency −15 −5 "
5103,unknown,"−15 −5 5 15 0 20 40 60 80 −3 −1 0 1 2 3 −10 −5 0 5 10 15 Normal Q−Q Plot Theoretical Quantiles Sample Quantiles 278 Practical Class Sheets 2 In this practical class, we will use R to fit linear regression models, make inference of model parameters, construct confidence intervals, assess the quality of the model, use residual diag- nostic plots to assess the validity of the regression assumptions a"
5104,unknown,"for multiple linear regression. T o start, create a new R script by clicking on the corresponding menu button (in the top left); and save it somewhere in your computer. Y ou can now write al l your code into this file, and then execute it in R using the menu button Run or by simple pressing ctrl + enter. Body F at and Body Measurements Data In this practical class we will investigate the dataset f"
5105,unknown,"In this practical class we will investigate the dataset fat from the library faraway, which consists of 252 observations concerning percentage of body fat and body measurements. W e are interested in predicting body fat (variable brozek) using the other variables available, except for siri (another way of computing body fat), density (it is used in the brozek and siri formula) and free (it is comp"
5106,unknown,"siri formula) and free (it is computed using the brozek formula). First, we need to install thefarawaypackage, using the commandinstall.packages(""faraway""). W e only need to do this once. Now, load the package and the data set and read the help file for more information about the variables: library(""faraway"") Exploratory data analysis T o get more information about the data set: names(fat) ## [1] "
5107,unknown,"names(fat) ## [1] ""brozek"" ""siri"" ""density"" ""age"" ""weight"" ""height"" ""adipos"" ## [8] ""free"" ""neck"" ""chest"" ""abdom"" ""hip"" ""thigh"" ""knee"" ## [15] ""ankle"" ""biceps"" ""forearm"" ""wrist"" ?fat str(fat) 279 280 ## 'data.frame': 252 obs. of 18 variables: ## $ brozek : num 12.6 6.9 24.6 10.9 27.8 20.6 19 12.8 5.1 12 ... ## $ siri : num 12.3 6.1 25.3 10.4 28.7 20.9 19.2 12.4 4.1 11.7 ... ## $ density: num 1.07 "
5108,unknown,## $ density: num 1.07 1.09 1.04 1.08 1.03 ... ## $ age : int 23 22 22 26 24 24 26 25 25 23 ... ## $ weight : num 154 173 154 185 184 ... ## $ height : num 67.8 72.2 66.2 72.2 71.2 ... ## $ adipos : num 23.7 23.4 24.7 24.9 25.6 26.5 26.2 23.6 24.6 25.8 ... ## $ free : num 135 161 116 165 133 ... ## $ neck : num 36.2 38.5 34 37.4 34.4 39 36.4 37.8 38.1 42.1 ... ## $ chest : num 93.1 93.6 95.8 101.8
5109,unknown,## $ abdom : num 85.2 83 87.9 86.4 100 94.4 90.7 88.5 82.5 88.6 ... ## $ hip : num 94.5 98.7 99.2 101.2 101.9 ... ## $ thigh : num 59 58.7 59.6 60.1 63.2 66 58.4 60 62.9 63.1 ... ## $ knee : num 37.3 37.3 38.9 37.3 42.2 42 38.3 39.4 38.3 41.7 ... ## $ ankle : num 21.9 23.4 24 22.8 24 25.6 22.9 23.2 23.8 25 ... ## $ biceps : num 32 30.5 28.8 32.4 32.2 35.7 31.9 30.5 35.9 35.6 ... ## $ forearm: num 
5110,unknown,## $ forearm: num 27.4 28.9 25.2 29.4 27.7 30.6 27.8 29 31.1 30 ... ## $ wrist : num 17.1 18.2 16.6 18.2 17.7 18.8 17.7 18.8 18.2 19.2 ... # Look at the first few rows head(fat) ## brozek siri density age weight height adipos free neck chest abdom hip ## 1 12.6 12.3 1.0708 23 154.25 67.75 23.7 134.9 36.2 93.1 85.2 94.5 ## 2 6.9 6.1 1.0853 22 173.25 72.25 23.4 161.3 38.5 93.6 83.0 98.7 ## 3 24.6 25
5111,unknown,## 3 24.6 25.3 1.0414 22 154.00 66.25 24.7 116.0 34.0 95.8 87.9 99.2 ## 4 10.9 10.4 1.0751 26 184.75 72.25 24.9 164.7 37.4 101.8 86.4 101.2 ## 5 27.8 28.7 1.0340 24 184.25 71.25 25.6 133.1 34.4 97.3 100.0 101.9 ## 6 20.6 20.9 1.0502 24 210.25 74.75 26.5 167.0 39.0 104.5 94.4 107.8 ## thigh knee ankle biceps forearm wrist ## 1 59.0 37.3 21.9 32.0 27.4 17.1 ## 2 58.7 37.3 23.4 30.5 28.9 18.2 ## 3 59
5112,unknown,## 4 60.1 37.3 22.8 32.4 29.4 18.2 ## 5 63.2 42.2 24.0 32.2 27.7 17.7 ## 6 66.0 42.0 25.6 35.7 30.6 18.8 # Summary statistics summary(fat) ## brozek siri density age ## Min. : 0.00 Min. : 0.00 Min. :0.995 Min. :22.00 ## 1st Qu.:12.80 1st Qu.:12.47 1st Qu.:1.041 1st Qu.:35.75 ## Median :19.00 Median :19.20 Median :1.055 Median :43.00 ## Mean :18.94 Mean :19.15 Mean :1.056 Mean :44.88 ## 3rd Qu.:24.
5113,unknown,## Max. :45.10 Max. :47.50 Max. :1.109 Max. :81.00 281 ## weight height adipos free ## Min. :118.5 Min. :29.50 Min. :18.10 Min. :105.9 ## 1st Qu.:159.0 1st Qu.:68.25 1st Qu.:23.10 1st Qu.:131.3 ## Median :176.5 Median :70.00 Median :25.05 Median :141.6 ## Mean :178.9 Mean :70.15 Mean :25.44 Mean :143.7 ## 3rd Qu.:197.0 3rd Qu.:72.25 3rd Qu.:27.32 3rd Qu.:153.9 ## Max. :363.1 Max. :77.75 Max. :48.9
5114,unknown,## neck chest abdom hip ## Min. :31.10 Min. : 79.30 Min. : 69.40 Min. : 85.0 ## 1st Qu.:36.40 1st Qu.: 94.35 1st Qu.: 84.58 1st Qu.: 95.5 ## Median :38.00 Median : 99.65 Median : 90.95 Median : 99.3 ## Mean :37.99 Mean :100.82 Mean : 92.56 Mean : 99.9 ## 3rd Qu.:39.42 3rd Qu.:105.38 3rd Qu.: 99.33 3rd Qu.:103.5 ## Max. :51.20 Max. :136.20 Max. :148.10 Max. :147.7 ## thigh knee ankle biceps forearm
5115,unknown,## thigh knee ankle biceps forearm ## Min. :47.20 Min. :33.00 Min. :19.1 Min. :24.80 Min. :21.00 ## 1st Qu.:56.00 1st Qu.:36.98 1st Qu.:22.0 1st Qu.:30.20 1st Qu.:27.30 ## Median :59.00 Median :38.50 Median :22.8 Median :32.05 Median :28.70 ## Mean :59.41 Mean :38.59 Mean :23.1 Mean :32.27 Mean :28.66 ## 3rd Qu.:62.35 3rd Qu.:39.92 3rd Qu.:24.0 3rd Qu.:34.33 3rd Qu.:30.00 ## Max. :87.30 Max. :49.1
5116,unknown,"## wrist ## Min. :15.80 ## 1st Qu.:17.60 ## Median :18.30 ## Mean :18.23 ## 3rd Qu.:18.80 ## Max. :21.40 # Check for missing values sum(is.na(fat)) ## [1] 0 # zero here, so no missing values Create a new dataframe called fat1 which is equivalent to fat, but with the variables siri, density and free removed. # Remove variables 2, 3 and 8. fat1 <- fat[,-c(2,3,8)] # siri is variable 2, density variab"
5117,unknown,"fat1 <- fat[,-c(2,3,8)] # siri is variable 2, density variable 3 and free variable 8. Exercise 2.1 a. How many rows are in this data set? How many columns? What do the rows and columns represent? b. Make some pairwise scatterplots of the predictors (columns) in this data set. Describe 282 your findings. c. Are any of the predictors associated with percentage of body fat? If so, explain the relatio"
5118,unknown,"relationship. Click for solution ## SOLUTION # (a) dim(fat1) ## [1] 252 15 # 252 observations and 15 variables # (b) pairs(fat1) brozek 20303580502022 0 40 20 80 age weight 150 30 height adipos 20 35 neck chest 80 80 abdom hip 90 50 thigh knee 35 20 ankle biceps 25 22 forearm 01502080903525 16 16 wrist # (c) # Add correlation coefficients # Correlation panel panel.cor <- function(x, y){ usr <- par"
5119,unknown,"usr <- par(""usr""); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) r <- round(cor(x, y), digits=2) txt <- paste0("" "", r) text(0.5, 0.5, txt, cex = 0.8) } 283 # Customize upper panel upper.panel<-function(x, y){ points(x,y, pch = 19) } # Create the plots pairs(fat1, lower.panel = panel.cor, upper.panel = upper.panel) brozek 20 0.29 0.61 30 −0.09 0.73 35 0.49 0.7 80 0.81 0.63 50 0.56 0.51 20 0.27 0.49 22"
5120,unknown,0.27 0.49 22 0.36 0 40 0.35 20 80 age −0.01 −0.17 0.12 0.11 0.18 0.23 −0.05 −0.2 0.02 −0.11 −0.04 −0.09 0.21 weight 0.31 0.89 0.83 0.89 0.89 0.94 0.87 0.85 0.61 0.8 0.63 150 0.73 30 height −0.02 0.25 0.13 0.09 0.17 0.15 0.29 0.26 0.21 0.23 0.32 adipos 0.78 0.91 0.92 0.88 0.81 0.71 0.5 0.75 0.56 20 0.63 35 neck 0.78 0.75 0.73 0.7 0.67 0.48 0.73 0.62 0.74 chest 0.92 0.83 0.73 0.72 0.48 0.73 0.58 80 
5121,unknown,"0.62 0.74 chest 0.92 0.83 0.73 0.72 0.48 0.73 0.58 80 0.66 80 abdom 0.87 0.77 0.74 0.45 0.68 0.5 0.62 hip 0.9 0.82 0.56 0.74 0.55 90 0.63 50 thigh 0.8 0.54 0.76 0.57 0.56 knee 0.61 0.68 0.56 35 0.66 20 ankle 0.48 0.42 0.57 biceps 0.68 25 0.63 22 forearm 0.59 01502080903525 16 16 wrist # A nicer plot using the corrplot package. #install.packages(""corrplot"") library(corrplot) ## corrplot 0.92 loaded"
5122,unknown,"library(corrplot) ## corrplot 0.92 loaded ## ## Attaching package: 'corrplot' ## The following object is masked from 'package:pls': ## ## corrplot #checking correlation between variables corrplot(cor(fat1), method = ""number"", type = ""upper"", diag = FALSE) 284 0.29 0.61 −0.01 −0.09 −0.17 0.31 0.73 0.12 0.89 −0.02 0.49 0.11 0.83 0.25 0.78 0.70 0.18 0.89 0.13 0.91 0.78 0.81 0.23 0.89 0.09 0.92 0.75 0"
5123,unknown,0.09 0.92 0.75 0.92 0.63 −0.05 0.94 0.17 0.88 0.73 0.83 0.87 0.56 −0.20 0.87 0.15 0.81 0.70 0.73 0.77 0.90 0.51 0.02 0.85 0.29 0.71 0.67 0.72 0.74 0.82 0.80 0.27 −0.11 0.61 0.26 0.50 0.48 0.48 0.45 0.56 0.54 0.61 0.49 −0.04 0.80 0.21 0.75 0.73 0.73 0.68 0.74 0.76 0.68 0.48 0.36 −0.09 0.63 0.23 0.56 0.62 0.58 0.50 0.55 0.57 0.56 0.42 0.68 0.35 0.21 0.73 0.32 0.63 0.74 0.66 0.62 0.63 0.56 0.66 0.57 
5124,unknown,"0.35 0.21 0.73 0.32 0.63 0.74 0.66 0.62 0.63 0.56 0.66 0.57 0.63 0.59 −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 age weight height adipos neck chest abdom hip thigh knee ankle biceps forearm wrist brozek age weight height adipos neck chest abdom hip thigh knee ankle biceps forearm corrplot.mixed(cor(fat1), upper = ""ellipse"", lower = ""number"",number.cex = .7) −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0."
5125,unknown,0 0.2 0.4 0.6 0.8 1 brozek age weight height adipos neck chest abdom hip thigh knee ankle biceps forearm wrist 0.29 0.61 −0.09 0.73 0.49 0.70 0.81 0.63 0.56 0.51 0.27 0.49 0.36 0.35 −0.01 −0.17 0.12 0.11 0.18 0.23 −0.05 −0.20 0.02 −0.11 −0.04 −0.09 0.21 0.31 0.89 0.83 0.89 0.89 0.94 0.87 0.85 0.61 0.80 0.63 0.73 −0.02 0.25 0.13 0.09 0.17 0.15 0.29 0.26 0.21 0.23 0.32 0.78 0.91 0.92 0.88 0.81 0.71 
5126,unknown,"0.23 0.32 0.78 0.91 0.92 0.88 0.81 0.71 0.50 0.75 0.56 0.63 0.78 0.75 0.73 0.70 0.67 0.48 0.73 0.62 0.74 0.92 0.83 0.73 0.72 0.48 0.73 0.58 0.66 0.87 0.77 0.74 0.45 0.68 0.50 0.62 0.90 0.82 0.56 0.74 0.55 0.63 0.80 0.54 0.76 0.57 0.56 0.61 0.68 0.56 0.66 0.48 0.42 0.57 0.68 0.63 0.59 285 Simple Linear Regression F ollowing practical class 01, conduct simple linear regression modelling. Linear regr"
5127,unknown,"Linear regression model fitting Exercise 2.2 a. Use the lm() function to fit a simple linear regression model, with brozek as the response variable and adiposas the predictor variable. Save the result of the regression function to reg1. b. Use summary(reg1) command to get information about the model. This gives us the estimated coeﬀicients, t-tests, p-values and standard errors as well as the R-sq"
5128,unknown,"F-statistic for the model. c. Use names(reg1) function to find what types of information are stored in reg1. F or example, we can get or extract the estimated coeﬀicients using the functioncoef(reg1) or reg1$coefficients. Click for solution ## SOLUTION # (a) reg1 <- lm(brozek~adipos,data=fat1) # (b) summary(reg1) ## ## Call: ## lm(formula = brozek ~ adipos, data = fat1) ## ## Residuals: ## Min 1Q "
5129,unknown,## Residuals: ## Min 1Q Median 3Q Max ## -21.4292 -3.4478 0.2113 3.8663 11.7826 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -20.40508 2.36723 -8.62 0.000000000000000778 *** ## adipos 1.54671 0.09212 16.79 < 0.0000000000000002 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 5.324 on 250 degrees of freedom ## M
5130,unknown,"## Multiple R-squared: 0.53, Adjusted R-squared: 0.5281 ## F-statistic: 281.9 on 1 and 250 DF, p-value: < 0.00000000000000022 286 # (c) names(reg1) ## [1] ""coefficients"" ""residuals"" ""effects"" ""rank"" ## [5] ""fitted.values"" ""assign"" ""qr"" ""df.residual"" ## [9] ""xlevels"" ""call"" ""terms"" ""model"" coef(reg1) ## (Intercept) adipos ## -20.405082 1.546712 reg1$coefficients ## (Intercept) adipos ## -20.405082 "
5131,unknown,"## (Intercept) adipos ## -20.405082 1.546712 Confidence and prediction intervals Exercise 2.3 a. Obtain a 95% confidence interval for the coeﬀicient estimates. Hint: use the confint() command. b. Obtain a 95% confidence and prediction intervals of brozek for a given value of adipos. Assume this given value of adipos is equal to 22. Click for solution ## SOLUTION # (a) confint(reg1, level = 0.95) #"
5132,unknown,"## 2.5 % 97.5 % ## (Intercept) -25.067331 -15.74283 ## adipos 1.365275 1.72815 # (b) newdata= data.frame(adipos=22) predict(reg1,newdata=newdata, interval=""confidence"", level = 0.95) ## fit lwr upr ## 1 13.62259 12.71416 14.53101 predict(reg1,data.frame(adipos=22), interval=""prediction"", level = 0.95) ## fit lwr upr ## 1 13.62259 3.096772 24.14841 287 Regression diagnostics Exercise 2.4 a. Create "
5133,unknown,"a. Create a scatterplot of adipos and brozek. Is there a relationship between the two variables? b. Plot brozek and adipos along with the least squares regression line using the plot() and abline() functions. c. Use residual plots to check the assumptions of the model. Click for solution ## SOLUTION # (a) & (b) plot(fat1$adipos, fat1$brozek, pch=16, col=""cornflowerblue"") abline(reg1,lwd=3,col=""red"
5134,unknown,"abline(reg1,lwd=3,col=""red"") 20 25 30 35 40 45 50 0 10 20 30 40 fat1$adipos fat1$brozek # (c) par(mfrow=c(1,2)) # to have two plots side-by-side plot(reg1, which=1, pch=16, col=""cornflowerblue"") plot(reg1, which=2, pch=16, col=""cornflowerblue"") 288 10 20 30 40 50 −20 −10 0 10 Fitted values Residuals Residuals vs Fitted 39 9 81 −3 −1 0 1 2 3 −4 −3 −2 −1 0 1 2 Theoretical Quantiles Standardized resi"
5135,unknown,"Standardized residuals Normal Q−Q 39 9 81 par(mfrow=c(1,1)) # to return to one plot per window Multiple Linear Regression Linear regression model fitting Exercise 2.5 a. Use the lm()function again to fit a linear regression model, withbrozekas the response variable and both adipos and age as predictors. Save the result of the regression function to reg2. b. Now use the lm() function again to fit a"
5136,unknown,"b. Now use the lm() function again to fit a linear regression model, with brozek as the response variable and all other 14 variables as predictors. Save the result of the regression function to reg3. Hint: Use lm(brozek~., data=fat1). c. Fit the same linear regression as in (b) but now exclude the age. Save the result of the regression function to reg4. Hint: Use lm(brozek~.-age, data=fat1). d. Co"
5137,unknown,d. Compare the R-squares of these models. Notice we can extract the R-square value from the summary object as summary(reg)$r.sq. Type ?summary.lm to see what else is available. e. Use V ariance Inflation F actor (VIF) to assess if there is any relationship between the independent variables. Hint: use the vif() function from the car package. Click for solution ## SOLUTION # (a) reg2 <- lm(brozek~ad
5138,unknown,"## SOLUTION # (a) reg2 <- lm(brozek~adipos+age,data=fat1) 289 summary(reg2) ## ## Call: ## lm(formula = brozek ~ adipos + age, data = fat1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.3523 -3.6914 -0.0805 3.5328 11.6982 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -24.75937 2.43119 -10.184 < 0.0000000000000002 *** ## adipos 1.49481 0.08875 16.842 < 0.0000000000000"
5139,unknown,"## age 0.12643 0.02569 4.921 0.00000157 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 5.093 on 249 degrees of freedom ## Multiple R-squared: 0.5716, Adjusted R-squared: 0.5682 ## F-statistic: 166.1 on 2 and 249 DF, p-value: < 0.00000000000000022 # (b) reg3 <- lm(brozek~.,data=fat1) summary(reg3) ## ## Call: ## lm(formula = brozek ~ ., da"
5140,unknown,## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2573 -2.5919 -0.1031 2.9040 9.2754 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -15.1901414 16.1089106 -0.943 0.3467 ## age 0.0568795 0.0300278 1.894 0.0594 . ## weight -0.0813022 0.0498853 -1.630 0.1045 ## height -0.0530707 0.1034409 -0.513 0.6084 ## adipos 0.0610056 0.2779614 0.219 0.8265 ## neck -0.4449644 0.2184007 -
5141,unknown,## neck -0.4449644 0.2184007 -2.037 0.0427 * ## chest -0.0308670 0.0977939 -0.316 0.7526 ## abdom 0.8789769 0.0854535 10.286 <0.0000000000000002 *** ## hip -0.2030661 0.1370728 -1.481 0.1398 ## thigh 0.2273768 0.1355589 1.677 0.0948 . ## knee -0.0009927 0.2298076 -0.004 0.9966 290 ## ankle 0.1572066 0.2075680 0.757 0.4496 ## biceps 0.1485112 0.1600277 0.928 0.3543 ## forearm 0.4296681 0.1848602 2.
5142,unknown,"## forearm 0.4296681 0.1848602 2.324 0.0210 * ## wrist -1.4792526 0.4966669 -2.978 0.0032 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 3.996 on 237 degrees of freedom ## Multiple R-squared: 0.749, Adjusted R-squared: 0.7342 ## F-statistic: 50.52 on 14 and 237 DF, p-value: < 0.00000000000000022 # (c) reg4 <- lm(brozek~.-age,data=fat1) sum"
5143,unknown,"summary(reg4) ## ## Call: ## lm(formula = brozek ~ . - age, data = fat1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.5851 -2.7683 -0.0017 2.8892 9.0632 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -16.44809 16.18249 -1.016 0.3105 ## weight -0.10069 0.04909 -2.051 0.0413 * ## height -0.07693 0.10323 -0.745 0.4568 ## adipos 0.05355 0.27944 0.192 0.8482 ## neck -0.406"
5144,unknown,## neck -0.40682 0.21865 -1.861 0.0640 . ## chest -0.02395 0.09826 -0.244 0.8076 ## abdom 0.94825 0.07765 12.212 <0.0000000000000002 *** ## hip -0.22302 0.13741 -1.623 0.1059 ## thigh 0.13510 0.12719 1.062 0.2892 ## knee 0.10451 0.22417 0.466 0.6415 ## ankle 0.11744 0.20762 0.566 0.5722 ## biceps 0.17082 0.16046 1.065 0.2881 ## forearm 0.37262 0.18338 2.032 0.0433 * ## wrist -1.15979 0.46969 -2.46
5145,unknown,"## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 4.018 on 238 degrees of freedom ## Multiple R-squared: 0.7452, Adjusted R-squared: 0.7313 ## F-statistic: 53.55 on 13 and 238 DF, p-value: < 0.00000000000000022 # (d) summary(reg1)$r.sq 291 ## [1] 0.5299755 summary(reg2)$r.sq ## [1] 0.5716312 summary(reg3)$r.sq ## [1] 0.7490309 summary(reg4)$r.sq"
5146,unknown,"## [1] 0.7452313 summary(reg4)$r.squared ## [1] 0.7452313 # or the adjusted R Squared summary(reg4)$adj.r.squared ## [1] 0.7313154 # (e) #install.packages(""car"") # you need to do this once library(car) vif(reg3) ## age weight height adipos neck chest abdom hip ## 2.250902 33.786851 2.256593 16.163444 4.430734 10.684562 13.346689 15.158277 ## thigh knee ankle biceps forearm wrist ## 7.961508 4.8288"
5147,unknown,"## 7.961508 4.828828 1.945514 3.674508 2.193390 3.379612 Interaction between variables (Additional) It is straightforward to include the interaction between variables in the linear model using the lm() function. F or example, the syntax chest*abdom will include chest, abdom and the interaction term chest x abdom as predictors. summary(lm(brozek~chest*abdom, data=fat1)) ## ## Call: ## lm(formula = "
5148,unknown,"## lm(formula = brozek ~ chest * abdom, data = fat1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.9568 -3.2831 0.1953 2.8313 11.0567 ## ## Coefficients: 292 ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -95.206407 18.306007 -5.201 0.00000041538850 *** ## chest 0.404148 0.187669 2.154 0.032243 * ## abdom 1.498560 0.204429 7.330 0.00000000000323 *** ## chest:abdom -0.006936 0.001820 -3.81"
5149,unknown,"## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 4.332 on 248 degrees of freedom ## Multiple R-squared: 0.6913, Adjusted R-squared: 0.6876 ## F-statistic: 185.1 on 3 and 248 DF, p-value: < 0.00000000000000022 W e can also include a non-linear transformation of the predictors, using the function I(). F or example, we can regress the response var"
5150,unknown,"F or example, we can regress the response variable brozek on both adipos and adipos^2 as follows. summary(lm(brozek~adipos+I(adipos^2), data=fat1)) ## ## Call: ## lm(formula = brozek ~ adipos + I(adipos^2), data = fat1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.8991 -3.4058 -0.1397 3.7930 11.3518 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -55.688716 7.963036 -"
5151,unknown,"## adipos 4.111738 0.561764 7.319 0.00000000000343 *** ## I(adipos^2) -0.045378 0.009814 -4.624 0.00000605491752 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 5.12 on 249 degrees of freedom ## Multiple R-squared: 0.5671, Adjusted R-squared: 0.5637 ## F-statistic: 163.1 on 2 and 249 DF, p-value: < 0.00000000000000022 W e can also apply ot"
5152,unknown,"summary(lm(brozek~log(adipos), data=fat1)) ## ## Call: ## lm(formula = brozek ~ log(adipos), data = fat1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.5263 -3.3776 0.0751 3.8273 11.4306 293 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -119.233 7.813 -15.26 <0.0000000000000002 *** ## log(adipos) 42.820 2.419 17.70 <0.0000000000000002 *** ## --- ## Signif. codes: 0 '"
5153,unknown,"## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 5.174 on 250 degrees of freedom ## Multiple R-squared: 0.5562, Adjusted R-squared: 0.5544 ## F-statistic: 313.3 on 1 and 250 DF, p-value: < 0.00000000000000022 V ariable Selection Please review the practical demonstration 5.4) presented in the lecture. F orward Stepwise Regression Exercise 2.6 a)"
5154,unknown,"Exercise 2.6 a) Use regsubsets() to perform forward stepwise selection, assigning the result to a variable called fwd. Note that you will need to (install and) load library leaps. b) Look at the summary of the resulting object fwd and extract the following statistics for the model of each size: RSS, 𝑅2, 𝐶𝑝, BIC and adjusted 𝑅2. Combine them into a single matrix. c) How many predictors are included"
5155,unknown,"single matrix. c) How many predictors are included in the models with i) minimum 𝐶𝑝, ii) minimum BIC, and iii) maximum adjusted 𝑅2? Which predictors are included in each case? d) In a single plotting window, generate the following three plots, showing the relation- ship between the two variables as lines (as seen in the lecture practical demonstration 5.4): i) 𝐶𝑝 against number of predictors, ii) "
5156,unknown,"5.4): i) 𝐶𝑝 against number of predictors, ii) BIC against number of predictors, and iii) adjusted 𝑅2 against number of predictors. Make sure to add relevant labels to the plots. Highlight the optimal model appropriately on each plot using a big red point. e) Use the special plot() function for regsubsets to visualise the models obtained by 𝐶𝑝. Click for solution ## SOLUTION # (a) library(""leaps"") "
5157,unknown,"Click for solution ## SOLUTION # (a) library(""leaps"") # required library for regsubsets() function. fwd = regsubsets(brozek~., data = fat1, method = 'forward', nvmax = 14) # (b) 294 results = summary(fwd) results ## Subset selection object ## Call: regsubsets.formula(brozek ~ ., data = fat1, method = ""forward"", ## nvmax = 14) ## 14 Variables (and intercept) ## Forced in Forced out ## age FALSE FAL"
5158,unknown,## weight FALSE FALSE ## height FALSE FALSE ## adipos FALSE FALSE ## neck FALSE FALSE ## chest FALSE FALSE ## abdom FALSE FALSE ## hip FALSE FALSE ## thigh FALSE FALSE ## knee FALSE FALSE ## ankle FALSE FALSE ## biceps FALSE FALSE ## forearm FALSE FALSE ## wrist FALSE FALSE ## 1 subsets of each size up to 14 ## Selection Algorithm: forward ## age weight height adipos neck chest abdom hip thigh kne
5159,unknown,"## 1 ( 1 ) "" "" "" "" "" "" "" "" "" "" "" "" ""*"" "" "" "" "" "" "" "" "" "" "" ## 2 ( 1 ) "" "" ""*"" "" "" "" "" "" "" "" "" ""*"" "" "" "" "" "" "" "" "" "" "" ## 3 ( 1 ) "" "" ""*"" "" "" "" "" "" "" "" "" ""*"" "" "" "" "" "" "" "" "" "" "" ## 4 ( 1 ) "" "" ""*"" "" "" "" "" "" "" "" "" ""*"" "" "" "" "" "" "" "" "" "" "" ## 5 ( 1 ) "" "" ""*"" "" "" "" "" ""*"" "" "" ""*"" "" "" "" "" "" "" "" "" "" "" ## 6 ( 1 ) ""*"" ""*"" "" "" "" "" ""*"" "" "" ""*"" "" "" "" "" "" "" "" "" "" "" ## 7 ( 1 ) ""*"" ""*"" "" "" "" "" ""*"" "" "" ""*"" "" "" ""*"""
5160,unknown,"## 8 ( 1 ) ""*"" ""*"" "" "" "" "" ""*"" "" "" ""*"" ""*"" ""*"" "" "" "" "" "" "" ## 9 ( 1 ) ""*"" ""*"" "" "" "" "" ""*"" "" "" ""*"" ""*"" ""*"" "" "" "" "" ""*"" ## 10 ( 1 ) ""*"" ""*"" "" "" "" "" ""*"" "" "" ""*"" ""*"" ""*"" "" "" ""*"" ""*"" ## 11 ( 1 ) ""*"" ""*"" ""*"" "" "" ""*"" "" "" ""*"" ""*"" ""*"" "" "" ""*"" ""*"" ## 12 ( 1 ) ""*"" ""*"" ""*"" "" "" ""*"" ""*"" ""*"" ""*"" ""*"" "" "" ""*"" ""*"" ## 13 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" "" "" ""*"" ""*"" ## 14 ( 1 ) ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ""*"" ""*"
5161,unknown,"## forearm wrist ## 1 ( 1 ) "" "" "" "" ## 2 ( 1 ) "" "" "" "" ## 3 ( 1 ) "" "" ""*"" ## 4 ( 1 ) ""*"" ""*"" ## 5 ( 1 ) ""*"" ""*"" ## 6 ( 1 ) ""*"" ""*"" ## 7 ( 1 ) ""*"" ""*"" 295 ## 8 ( 1 ) ""*"" ""*"" ## 9 ( 1 ) ""*"" ""*"" ## 10 ( 1 ) ""*"" ""*"" ## 11 ( 1 ) ""*"" ""*"" ## 12 ( 1 ) ""*"" ""*"" ## 13 ( 1 ) ""*"" ""*"" ## 14 ( 1 ) ""*"" ""*"" RSS = results$rss r2 = results$rsq Cp = results$cp BIC = results$bic Adj_r2 = results$adjr2 # Combine the ca"
5162,unknown,"Adj_r2 = results$adjr2 # Combine the calculated criteria values above into a single matrix. criteria_values <- cbind(RSS, r2, Cp, BIC, Adj_r2) criteria_values ## RSS r2 Cp BIC Adj_r2 ## [1,] 5094.931 0.6621178 71.075501 -262.3758 0.6607663 ## [2,] 4241.328 0.7187265 19.617727 -303.0555 0.7164672 ## [3,] 4108.183 0.7275563 13.279341 -305.5638 0.7242606 ## [4,] 3994.311 0.7351080 8.147986 -307.1180 "
5163,unknown,"## [5,] 3950.628 0.7380049 7.412297 -304.3597 0.7326798 ## [6,] 3913.377 0.7404753 7.079431 -301.2177 0.7341196 ## [7,] 3853.214 0.7444651 5.311670 -299.5925 0.7371342 ## [8,] 3819.985 0.7466688 5.230664 -296.2456 0.7383287 ## [9,] 3805.075 0.7476576 6.296894 -291.7018 0.7382730 ## [10,] 3793.873 0.7484005 7.595324 -286.9153 0.7379607 ## [11,] 3786.200 0.7489093 9.114827 -281.8960 0.7374010 ## [12"
5164,unknown,"## [12,] 3785.179 0.7489771 11.050872 -276.4346 0.7363734 ## [13,] 3784.367 0.7490309 13.000019 -270.9592 0.7353225 ## [14,] 3784.367 0.7490309 15.000000 -265.4298 0.7342058 # (c) which.min(Cp) # i) 8 ## [1] 8 which.min(BIC) # ii) 4 ## [1] 4 which.max(Adj_r2) # iii) 8 ## [1] 8 coef(fwd, 4) 296 ## (Intercept) weight abdom forearm wrist ## -31.2967858 -0.1255654 0.9213725 0.4463824 -1.3917662 # Look"
5165,unknown,"# Looking at the summary above, the model with 4 predictors (minimum BIC) # includes the predictors weight, abdom, forearm and wrist. coef(fwd, 8) ## (Intercept) age weight neck abdom hip ## -20.06213373 0.05921577 -0.08413521 -0.43189267 0.87720667 -0.18641032 ## thigh forearm wrist ## 0.28644340 0.48254563 -1.40486912 # The model with 8 predictors (minimum Cp and maximum adjusted R-squared) # in"
5166,unknown,"# includes additionally the predictors age, neck, hip and thigh. # (d) par(mfrow = c(1, 3)) plot(Cp, xlab = ""Number of Predictors"", ylab = ""Cp"", type = 'l', lwd = 2) points(8, Cp[8], col = ""red"", cex = 2, pch = 8, lwd = 2) plot(BIC, xlab = ""Number of Predictors"", ylab = ""BIC"", type = 'l', lwd = 2) points(4, BIC[4], col = ""red"", cex = 2, pch = 8, lwd = 2) plot(Adj_r2, xlab = ""Number of Predictors"","
5167,unknown,"plot(Adj_r2, xlab = ""Number of Predictors"", ylab = ""Adjusted RSq"", type = ""l"", lwd = 2) points(8, Adj_r2[8], col = ""red"", cex = 2, pch = 8, lwd = 2) 2 4 6 8 10 14 10 20 30 40 50 60 70 Number of Predictors Cp 2 4 6 8 10 14 −300 −290 −280 −270 Number of Predictors BIC 2 4 6 8 10 14 0.66 0.68 0.70 0.72 0.74 Number of Predictors Adjusted RSq par(mfrow = c(1, 1)) plot(fwd, scale = ""Cp"") 297 Cp (Interce"
5168,unknown,"age weight height adipos neck chest abdom hip thigh knee ankle biceps forearm wrist 71 15 13 9.1 7.6 7.1 5.3 Best Subset and Backward Selection Exercise 2.7 Do the best models (as determined by 𝐶𝑝, BIC and adjusted- 𝑅2) from best subset and backward stepwise selections have the same number of predictors? If yes, are the predictors the same as those from forward selection? Hint: if it is not obviou"
5169,unknown,"selection type ?regsubsets and see information about argument method. Click for solution ## SOLUTION best = regsubsets(brozek~., data = fat1, nvmax = 14) bwd = regsubsets(brozek~., data = fat1, method = 'backward', nvmax = 14) which.min(summary(best)$cp) ## [1] 8 which.min(summary(best)$bic) ## [1] 4 which.max(summary(best)$adjr2) ## [1] 8 which.min(summary(bwd)$cp) ## [1] 8 which.min(summary(bwd)"
5170,unknown,"## [1] 8 which.min(summary(bwd)$bic) ## [1] 4 which.max(summary(bwd)$adjr2) 298 ## [1] 8 # Yes, the three optimal models (under each of the criteria Cp, BIC and # adj-R-squared) for each of forward stepwise, backward stepwise and # best subset selections all have the same number of predictors. # Cp coef(fwd,8) ## (Intercept) age weight neck abdom hip ## -20.06213373 0.05921577 -0.08413521 -0.43189"
5171,unknown,"## thigh forearm wrist ## 0.28644340 0.48254563 -1.40486912 coef(best,8) ## (Intercept) age weight neck abdom hip ## -20.06213373 0.05921577 -0.08413521 -0.43189267 0.87720667 -0.18641032 ## thigh forearm wrist ## 0.28644340 0.48254563 -1.40486912 coef(bwd,8) ## (Intercept) age weight neck abdom hip ## -20.06213373 0.05921577 -0.08413521 -0.43189267 0.87720667 -0.18641032 ## thigh forearm wrist ##"
5172,unknown,"## 0.28644340 0.48254563 -1.40486912 # BIC coef(fwd,4) ## (Intercept) weight abdom forearm wrist ## -31.2967858 -0.1255654 0.9213725 0.4463824 -1.3917662 coef(best,4) ## (Intercept) weight abdom forearm wrist ## -31.2967858 -0.1255654 0.9213725 0.4463824 -1.3917662 coef(bwd,4) ## (Intercept) weight abdom forearm wrist ## -31.2967858 -0.1255654 0.9213725 0.4463824 -1.3917662 # In addition, the pred"
5173,unknown,"# adjusted R squared is not needed because it has also 8 predictors Practical Class Sheets 3 In this practical class, we will apply ridge and lasso regression for multiple linear regression and explore principal component analysis. T o start, create a new R script by clicking on the corresponding menu button (in the top left); and save it somewhere in your computer. Y ou can now write al l your co"
5174,unknown,"and save it somewhere in your computer. Y ou can now write al l your code into this file, and then execute it in R using the menu button Run or by simple pressing ctrl + enter. In this practical class we will investigate the dataset seatpos from the library faraway. Initial Data Analysis Exercise 3.1 This exercise focuses on generally improving your data analytical skills which you have been pract"
5175,unknown,"practicing since the first practical class. a) Load library faraway and check the help file for dataset seatpos. b) Looking at the help file, which variable might the researchers at the HuMoSim labora- tory consider to be a good response variable? c) Looking at the help document only for the moment (that is, without performing any analysis on the dataset), are there any predictor variables that we"
5176,unknown,to be highly correlated? d) What is the dimension of the dataset? Check whether there is any missing data. e) Use exploratory data analysis commands and plots to investigate the correlation of hipcenterwith the other (possible predictor) variables. With which predictor variables is hipcentermost correlated? Are those predictor variables correlated with each other? Does this exploratory analysis su
5177,unknown,"Does this exploratory analysis support your initial thoughts from part (c)? What do your findings here suggest about the fixed location referred to in the help document for description of hipcenter (is it in front or behind the driver)? Click for solution ## SOLUTION # (a) library(faraway) ?seatpos 299 300 # (b) # hipcenter would be a good response variable, as it is the only variable # in the dat"
5178,unknown,"# in the dataset that considers the location of a person within the car. # This can be used as a proxy for where different drivers will position the # seat, which is the quantity of interest for car designers. # (c) # Many of the predictors could be highly correlated. # Examples include: # - height and weight (a whole analysis could be done on this # pairing), # - lower leg length and thigh length"
5179,unknown,"# longer lower legs also have longer thighs, but maybe I'm wrong. # - Ht and HtShoes - these two variables measure height bare foot and in # shoes - given that people's heights are in general not going to increase # much by putting on a pair of shoes (and possibly by roughly a similar # amount for each person), these variables are likely to be (very) highly # correlated. I would question including"
5180,unknown,"# in an analysis, although our computational analyses will confirm whether # this is the case. # # Perhaps Age is unlikely to be too highly correlated with any of the other # variables. # Note that the aim of this question is to get you to think about the # dataset before we go headlong into applying computational methods, since # this is what a statistician/data scientist/machine learner should d"
5181,unknown,"# (d) dim( seatpos ) # 38 samples, 9 variables. ## [1] 38 9 sum(is.na(seatpos)) # no missing data. ## [1] 0 # (e) # We can calculate the correlation of `hipcenter` to the other variables. # Note that the response is the ninth variable in this dataset. cor(seatpos[,9], seatpos[,-9]) ## Age Weight HtShoes Ht Seated Arm Thigh ## [1,] 0.2051722 -0.640333 -0.7965964 -0.7989274 -0.7312537 -0.585095 -0.5"
5182,unknown,"## Leg ## [1,] -0.7871685 301 # From this we see that `hipcenter` has a reasonable negative correlation # with most of the predictors, apart from age. # We display the correlation matrix across the eight predictors as follows. cor( seatpos[,-9] ) ## Age Weight HtShoes Ht Seated Arm ## Age 1.00000000 0.08068523 -0.07929694 -0.09012812 -0.1702040 0.3595111 ## Weight 0.08068523 1.00000000 0.82817733 "
5183,unknown,## Weight 0.08068523 1.00000000 0.82817733 0.82852568 0.7756271 0.6975524 ## HtShoes -0.07929694 0.82817733 1.00000000 0.99814750 0.9296751 0.7519530 ## Ht -0.09012812 0.82852568 0.99814750 1.00000000 0.9282281 0.7521416 ## Seated -0.17020403 0.77562705 0.92967507 0.92822805 1.0000000 0.6251964 ## Arm 0.35951115 0.69755240 0.75195305 0.75214156 0.6251964 1.0000000 ## Thigh 0.09128584 0.57261442 0.
5184,unknown,## Thigh 0.09128584 0.57261442 0.72486225 0.73496041 0.6070907 0.6710985 ## Leg -0.04233121 0.78425706 0.90843341 0.90975238 0.8119143 0.7538140 ## Thigh Leg ## Age 0.09128584 -0.04233121 ## Weight 0.57261442 0.78425706 ## HtShoes 0.72486225 0.90843341 ## Ht 0.73496041 0.90975238 ## Seated 0.60709067 0.81191429 ## Arm 0.67109849 0.75381405 ## Thigh 1.00000000 0.64954120 ## Leg 0.64954120 1.0000000
5185,unknown,"# Many of the predictors are reasonably highly correlated with each other, # except for Age. # As suspected the pairing with greatest correlation is Ht and HtShoes. # We could obtain a pairs plot across the dataset. pairs( seatpos, pch = 16, col = 2 ) 302 Age 10015026 4030 20 50 100 250 Weight HtShoes 160 200 150 190 Ht Seated 80 95 26 34 Arm Thigh 35 45 30 38 Leg 20 701608035 −250 −50 −250 hipcen"
5186,unknown,"Leg 20 701608035 −250 −50 −250 hipcenter # These support the comments and correlations seen above. # I would hazard a guess that the fixed location in the car is behind the # driver, since the distance from it to the drivers hips gets smaller as # the driver's various dimensions (in general) get larger. Larger drivers # are likely to move their seat back, so to make the distance shorter, the # fix"
5187,unknown,"# fixed position should be behind the driver. # Note: these are my interpretations of the dataset, but again the purpose # of this question is about getting you to think logically about the data # you have been presented with. # This can sometimes be harder than the computational side of things. # In particular it may help to catch any coding errors (and potential # illogical reasonings and conclu"
5188,unknown,"# illogical reasonings and conclusions) later! Ridge Regression Exercise 3.2 a) Define a vector y containing all the values of the response variable hipcenter. b) Stack the predictors column-wise into a matrix, and define this matrix as x. c) Load library glmnet. Fit a model of all of the remaining variables for hipcenter using ridge regression over a grid of 200 𝜆 values. Hint: type ?glmnet and l"
5189,unknown,"ridge regression over a grid of 200 𝜆 values. Hint: type ?glmnet and look at argument nlambda. d) Plot the regularisation paths of the ridge regresssion coeﬀicients as a function of log 𝜆. Click for solution 303 ## SOLUTION #(a) y = seatpos$hipcenter #(b) x = model.matrix(hipcenter~., seatpos)[,-1] #(c) library(""glmnet"") ridge = glmnet(x, y, alpha = 0, nlambda = 200) #(d) plot(ridge, xvar = 'lambd"
5190,unknown,2 4 6 8 10 −5 −4 −3 −2 −1 0 Log Lambda Coefficients 8 8 8 8 8 Lasso Regression Exercise 3.3 Fit a model of all of the remaining variables for hipcenter using lasso regression. Plot regularisation paths against log𝜆 and ℓ1 norm values. Investigate which variables are included under various values of 𝜆. Do you notice anything interesting about the trends for the predictors Ht and HtShoes? Do the fin
5191,unknown,"3.1? Click for solution ## SOLUTION lasso = glmnet(x, y) 304 par(mfrow = c(1,2)) plot(lasso, xvar = 'lambda') plot(lasso) −3 −1 1 2 3 4 −6 −4 −2 0 Log Lambda Coefficients 7 7 5 6 5 3 2 0 0 2 4 6 8 10 −6 −4 −2 0 L1 Norm Coefficients 0 2 2 2 3 6 7 # We can see which values of lambda were in the grid. lasso$lambda ## [1] 47.02272566 42.84535631 39.03909294 35.57096752 32.41094081 29.53164215 ## [7] 2"
5192,unknown,## [7] 26.90813246 24.51768813 22.33960429 20.35501542 18.54673195 16.89909140 ## [13] 15.39782269 14.02992256 12.78354291 11.64788819 10.61312191 9.67028141 ## [19] 8.81120026 8.02843751 7.31521325 6.66534987 6.07321856 5.53369056 ## [25] 5.04209274 4.59416712 4.18603397 3.81415825 3.47531895 3.16658119 ## [31] 2.88527084 2.62895133 2.39540254 2.18260158 1.98870527 1.81203418 ## [37] 1.65105806 1
5193,unknown,## [37] 1.65105806 1.50438261 1.37073740 1.24896487 1.13801027 1.03691258 ## [43] 0.94479612 0.86086304 0.78438634 0.71470362 0.65121132 0.59335950 ## [49] 0.54064708 0.49261748 0.44885470 0.40897969 0.37264706 0.33954212 ## [55] 0.30937814 0.28189383 0.25685116 0.23403321 0.21324235 0.19429849 ## [61] 0.17703754 0.16131002 0.14697968 0.13392241 0.12202511 0.11118474 ## [67] 0.10130739 0.09230752 
5194,unknown,"## [73] 0.05797181 0.05282176 0.04812922 # We can see which variables were included under each of those. lasso$beta ## 8 x 75 sparse Matrix of class ""dgCMatrix"" ## [[ suppressing 75 column names 's0', 's1', 's2' ... ]] ## ## Age . . . . . . . ## Weight . . . . . . . ## HtShoes . . . . . . . ## Ht . -0.3788888 -0.71690000 -0.8815363 -1.031066 -1.168389 -1.292464 ## Seated . . . . . . . 305 ## Arm ."
5195,unknown,## Thigh . . . . . . . ## Leg . . -0.03001398 -0.5709448 -1.065255 -1.512438 -1.923026 ## ## Age . . . . . . . ## Weight . . . . . . . ## HtShoes . . . . . . . ## Ht -1.406576 -1.509502 -1.604347 -1.689716 -1.768569 -1.839363 -1.904943 ## Seated . . . . . . . ## Arm . . . . . . . ## Thigh . . . . . . . ## Leg -2.293971 -2.635099 -2.942743 -3.226199 -3.481281 -3.716848 -3.928280 ## ## Age . . . . 0
5196,unknown,## ## Age . . . . 0.03026313 0.07914329 ## Weight . . . . . . ## HtShoes . . . . . . ## Ht -1.963641 -2.017036 -2.066746 -2.111083 -2.13956040 -2.15657249 ## Seated . . . . . . ## Arm . . . . . . ## Thigh . . . . . . ## Leg -4.124080 -4.302752 -4.462389 -4.610703 -4.77565830 -4.94857151 ## ## Age 0.123651 0.164252 0.2011991 0.2349099 0.2656318 0.29557296 ## Weight . . . . . . ## HtShoes . . . . . 
5197,unknown,## Ht -2.172738 -2.186422 -2.1999306 -2.2112247 -2.2213847 -2.21537008 ## Seated . . . . . . ## Arm . . . . . . ## Thigh . . . . . -0.04904616 ## Leg -5.104146 -5.249008 -5.3779072 -5.4983751 -5.6085302 -5.71786751 ## ## Age 0.3254539 0.3526374 0.3774918 0.4000568 0.42215302 0.4425705 ## Weight . . . . . . ## HtShoes . . . . -0.08747275 -0.2410930 ## Ht -2.1919535 -2.1710512 -2.1509204 -2.1336034 
5198,unknown,## Seated . . . . . . ## Arm . . . . . . ## Thigh -0.1550600 -0.2512560 -0.3396447 -0.4194801 -0.50353183 -0.5808560 ## Leg -5.8252121 -5.9220281 -6.0129224 -6.0932133 -6.18702261 -6.2568841 ## ## Age 0.4611742 0.4781344 0.4935876 0.507660431 0.5325412 0.5538978 ## Weight . . . . . . ## HtShoes -0.3810765 -0.5094711 -0.6264423 -0.732315652 -0.8541160 -0.9612630 ## Ht -1.6933835 -1.5502583 -1.41986
5199,unknown,## Ht -1.6933835 -1.5502583 -1.4198654 -1.301776165 -1.1523513 -1.0218632 ## Seated . . . . . . ## Arm . . . -0.003724438 -0.1356874 -0.2445963 ## Thigh -0.6513109 -0.7155662 -0.7741120 -0.825232452 -0.8617522 -0.8945363 306 ## Leg -6.3205372 -6.3785468 -6.4314027 -6.478380251 -6.4798559 -6.4847698 ## ## Age 0.5733715 0.5910974 0.6072458 0.6219709 0.6353911 0.6476155 ## Weight . . . . . . ## HtSho
5200,unknown,## Weight . . . . . . ## HtShoes -1.0599721 -1.1485746 -1.2290941 -1.3032866 -1.3710911 -1.4325446 ## Ht -0.9018626 -0.7938899 -0.6957256 -0.6054368 -0.5229599 -0.4481416 ## Seated . . . . . . ## Arm -0.3438619 -0.4342699 -0.5166409 -0.5917202 -0.6601388 -0.7224752 ## Thigh -0.9244800 -0.9516743 -0.9764387 -0.9990584 -1.0196829 -1.0384553 ## Leg -6.4892510 -6.4933288 -6.4970433 -6.5004312 -6.50352
5201,unknown,## ## Age 0.6587657 0.6689108 0.6781749 0.6865932 0.6942714 0.7012737 ## Weight . . . . . . ## HtShoes -1.4893806 -1.5398858 -1.5875253 -1.6288279 -1.6673273 -1.7029099 ## Ht -0.3791066 -0.3175022 -0.2597176 -0.2091840 -0.1622689 -0.1190003 ## Seated . . . . . . ## Arm -0.7793014 -0.8310560 -0.8782503 -0.9212114 -0.9603579 -0.9960311 ## Thigh -1.0556179 -1.0711783 -1.0854622 -1.0983601 -1.1101614 
5202,unknown,## Leg -6.5089124 -6.5112740 -6.5134288 -6.5154515 -6.5172794 -6.5189702 ## ## Age 0.70765527 0.71346592 0.7187507 0.7228005 0.7250685 0.7283275 ## Weight . . . . . . ## HtShoes -1.73553063 -1.76517488 -1.7918480 -1.8012793 -1.8042393 -1.8036976 ## Ht -0.07936592 -0.04332879 -0.0108381 . . . ## Seated . . . . . . ## Arm -1.02852223 -1.05809285 -1.0849764 -1.1006536 -1.1107097 -1.1284351 ## Thigh -
5203,unknown,## Thigh -1.13080691 -1.13978543 -1.1479486 -1.1525898 -1.1535925 -1.1565048 ## Leg -6.52053931 -6.52200063 -6.5233662 -6.5216161 -6.5131065 -6.5068898 ## ## Age 0.731506 0.734511 0.737307 0.739810222 0.741186412 0.742529970 ## Weight . . . 0.001075362 0.003693442 0.006652873 ## HtShoes -1.802075 -1.800008 -1.797816 -1.798603166 -1.803557433 -1.808553585 ## Ht . . . . . . ## Seated . . . . . 0.002
5204,unknown,## Arm -1.145882 -1.162368 -1.177701 -1.194759877 -1.207604764 -1.224531973 ## Thigh -1.159510 -1.162575 -1.165548 -1.166929365 -1.167273424 -1.167232842 ## Leg -6.503318 -6.501111 -6.499640 -6.497567833 -6.499442317 -6.503591032 ## ## Age 0.745354326 0.74742495 0.7490595 0.75051743 0.75188196 0.75309161 ## Weight 0.008513292 0.01013531 0.0117728 0.01326849 0.01463865 0.01587222 ## HtShoes -1.8272
5205,unknown,## Ht . . . . . . ## Seated 0.040515474 0.08294766 0.1225952 0.15754489 0.19096305 0.21992109 ## Arm -1.239936898 -1.24773622 -1.2530833 -1.25803785 -1.26232015 -1.26641490 ## Thigh -1.163875042 -1.15825820 -1.1526247 -1.14780561 -1.14306697 -1.13909162 ## Leg -6.502812684 -6.49475353 -6.4872853 -6.48122990 -6.47512351 -6.47010118 ## 307 ## Age 0.75422793 0.75592912 0.75628200 0.75774078 0.7587048
5206,unknown,## Weight 0.01700937 0.01736582 0.01888635 0.01919551 0.01980897 0.02057163 ## HtShoes -1.95075883 -1.95798264 -1.97774056 -1.98463485 -1.99295752 -2.00426952 ## Ht . . . . . . ## Seated 0.24771599 0.26649454 0.29335360 0.31078418 0.32692789 0.34634309 ## Arm -1.26997742 -1.27709493 -1.27745387 -1.28286904 -1.28666067 -1.28881721 ## Thigh -1.13514900 -1.13506802 -1.12899125 -1.12874195 -1.12739244
5207,unknown,"## Leg -6.46501672 -6.46519803 -6.45672861 -6.45697888 -6.45593029 -6.45313255 ## ## Age 0.76009998 ## Weight 0.02118573 ## HtShoes -2.01275176 ## Ht . ## Seated 0.36049535 ## Arm -1.29085662 ## Thigh -1.12283674 ## Leg -6.45077390 # In particular, high values of lambda leads to inclusion of Ht only. # Other variables start to come in; leg, followed by Age, # followed by Thigh, as lambda gets smal"
5208,unknown,"# It is interesting that Age comes in third, despite having a reasonably # small correlation with the response. # However, this may be as expected, since the other variables also have # high correlations with each other (collinearity), whereas Age may # contain alternative predictive information. # Interestingly, HtShoes enters only for small enough values of lambda, at # which point its coefficie"
5209,unknown,"# which point its coefficients increase whilst the Ht coefficients # decrease, eventually to zero. # This is to do with the ridiculously high correlation between Ht # and HtShoes (spotted in Exercise 3.1) - even for really low values of lambda, # we don't need both of these variables. Principal Component Analysis Exercise 3.4 a) Conduct Principal Component Analysis to all the predictors x as in Ex"
5210,unknown,"all variables being scaled to have unit variance. b) Report the variance of each of the principal component and the eigenvectors that cor- responding to the first principal component. c) Draw a scree plot. How many components would you need in order to capture at least 95% of the total variance of the data cloud? d) Compress the data using the number of principal components as in (c), then reconst"
5211,unknown,"the data and compare the reconstruct data with the original data by looking at Age 308 and weight. Click for solution ## SOLUTION #(a) s <- apply(x, 2, sd) # calculates the column standard deviations x.s <- sweep(x, 2, s, ""/"") # divides all columns by their standard deviations seatpos.pr <- prcomp(x.s) #(b) seatpos.pr ## Standard deviations (1, .., p=8): ## [1] 2.38184501 1.11210881 0.68098711 0.4"
5212,unknown,## [8] 0.03985271 ## ## Rotation (n x k) = (8 x 8): ## PC1 PC2 PC3 PC4 PC5 PC6 ## Age -0.007219379 0.8763467 0.16383976 -0.16522774 -0.3349932 -0.25464449 ## Weight -0.366979122 0.0448877 0.42981137 -0.60025209 0.5537489 0.09798202 ## HtShoes -0.411460536 -0.1055831 0.03375209 0.02577245 -0.2204816 -0.05101900 ## Ht -0.412057421 -0.1119799 0.01116858 0.02294603 -0.1887759 -0.04369735 ## Seated -0.
5213,unknown,## Seated -0.381270226 -0.2178995 0.17138740 -0.15033847 -0.6171009 0.23019712 ## Arm -0.348771387 0.3742641 -0.01670980 0.55358297 0.2380225 0.60781701 ## Thigh -0.327523319 0.1251793 -0.86246173 -0.31151283 0.1038969 -0.06344739 ## Leg -0.389747512 -0.0555930 0.11688322 0.43024468 0.2205229 -0.70326773 ## PC7 PC8 ## Age 0.02269849 -0.015528966 ## Weight -0.04435483 0.008082356 ## HtShoes 0.53650
5214,unknown,"## HtShoes 0.53650776 0.691876699 ## Ht 0.50884054 -0.721493879 ## Seated -0.56689080 -0.002844309 ## Arm -0.07347462 0.007539785 ## Thigh -0.14492761 0.018814564 ## Leg -0.32092126 0.005274142 seatpos.pr$rotation[,1] ## Age Weight HtShoes Ht Seated Arm ## -0.007219379 -0.366979122 -0.411460536 -0.412057421 -0.381270226 -0.348771387 ## Thigh Leg ## -0.327523319 -0.389747512 #(c) plot(seatpos.pr) 3"
5215,unknown,309 seatpos.pr Variances 0 1 2 3 4 5 summary(seatpos.pr) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.3818 1.1121 0.68099 0.49088 0.44070 0.3731 0.22438 ## Proportion of Variance 0.7091 0.1546 0.05797 0.03012 0.02428 0.0174 0.00629 ## Cumulative Proportion 0.7091 0.8638 0.92171 0.95183 0.97611 0.9935 0.99980 ## PC8 ## Standard deviation 0.03985 ## Proportion 
5216,unknown,"## Cumulative Proportion 1.00000 # 4 components are needed to capture at least # 95% of the total variance of the data cloud #(d) T <- t(seatpos.pr$x[,c(1,2,3,4)]) #compressed using 4 PCs ms <- colMeans(x.s) R <- t(ms + seatpos.pr$rot[,c(1,2,3,4)]%*% T) #reconstruction plot(rbind(x.s[,1:2], R[,1:2]), col=c(rep(1,38),rep(3,38))) 1.5 2.0 2.5 3.0 3.5 4.0 4.5 3 4 5 6 7 8 Age Weight 310 Principal Compo"
5217,unknown,"1.5 2.0 2.5 3.0 3.5 4.0 4.5 3 4 5 6 7 8 Age Weight 310 Principal Component Regression Exercise 3.5 a) Load library pls(required for PCR). Use the commandpcrto fit a principal component regression model to hipcenter based on the remaining variables. Select the number of principal components using cross-validation. Remember to standardise the variables. How many principal components are recommended,"
5218,unknown,"error? b) Appropriately plot the mean squared validation error. c) Use the command coef() to find corresponding coeﬀicient estimates in terms of the original ̂𝛽’s. How do these estimates coincide with the exploratory analysis of Exercise 3.1? Click for solution ## SOLUTION #(a) library(""pls"") pcr.fit=pcr(hipcenter~., data=seatpos, scale = TRUE, validation = ""CV"" ) summary(pcr.fit) ## Data: X dimen"
5219,unknown,## Y dimension: 38 1 ## Fit method: svdpc ## Number of components considered: 8 ## ## VALIDATION: RMSEP ## Cross-validated using 10 random segments. ## (Intercept) 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps ## CV 60.45 37.89 36.83 38.04 37.68 38.05 39.64 ## adjCV 60.45 37.80 36.69 37.84 37.42 37.81 39.28 ## 7 comps 8 comps ## CV 43.57 43.93 ## adjCV 42.96 43.30 ## ## TRAINING: % variance expl
5220,unknown,"## ## TRAINING: % variance explained ## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps ## X 70.91 86.37 92.17 95.18 97.61 99.35 99.98 ## hipcenter 61.89 66.34 66.48 67.82 68.02 68.58 68.63 ## 8 comps ## X 100.00 ## hipcenter 68.66 # The summary suggests use of 2 principal components. # Notice this is different from the number in Exercise 3.4c 311 #(b) validationplot( pcr.fit, val.type = '"
5221,unknown,"0 2 4 6 8 1500 2500 3500 hipcenter number of components MSEP #(c) coef(pcr.fit, ncomp = 2) ## , , 2 comps ## ## hipcenter ## Age 9.779064 ## Weight -6.721580 ## HtShoes -9.301406 ## Ht -9.385585 ## Seated -9.978190 ## Arm -2.633940 ## Thigh -5.035274 ## Leg -8.307696 # Since Age had a positive correlation with hipcenter, the coefficient # is positive. For the remaining variables the correlation wa"
5222,unknown,# so the coefficients are negative. # Notice how the size of the coefficients is relatively similar across # all predictor variables. # This is to do with how Principal Component Analysis works. Predictive Performance of Methods Exercise 3.6 Use 50 repetitions of data-splitting with 28 samples for training (10 for testing) to compare the predictive performance (e.g. Correlation and MSE) of the fol
5223,unknown,"predictors for hipcenter: 312 • Best subset selection with 𝐶𝑝 • Ridge with min-CV 𝜆 (5 folds) • Lasso with min-CV 𝜆 (5 folds) • PCR with min-CV number of principal components. Click for solution # Load R libraries. library(leaps) # Predict function for regsubsets predict.regsubsets = function(object, newdata, id, ...){ form = as.formula(object$call[[2]]) mat = model.matrix(form, newdata) coefi = c"
5224,unknown,"xvars = names(coefi) mat[, xvars]%*%coefi } repetitions = 50 cor.bss = c() cor.ridge = c() cor.lasso = c() cor.pcr = c() set.seed(1) for(i in 1:repetitions){ # Step (i) data splitting training.obs = sample(1:38, 28) y.train = seatpos$hipcenter[training.obs] x.train = model.matrix(hipcenter~., seatpos[training.obs, ])[,-1] y.test = seatpos$hipcenter[-training.obs] x.test = model.matrix(hipcenter~.,"
5225,unknown,"x.test = model.matrix(hipcenter~., seatpos[-training.obs, ])[,-1] # Step (ii) training phase bss.train = regsubsets(hipcenter~., data=seatpos[training.obs,], nvmax=8) min.cp = which.min(summary(bss.train)$cp) ridge.train = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 5) lasso.train = cv.glmnet(x.train, y.train, nfold = 5) pcr.train = pcr(hipcenter~., data =seatpos[training.obs,], scale = TRUE, "
5226,unknown,"scale = TRUE, validation=""CV"") min.pcr = which.min(MSEP(pcr.train)$val[1,1, ] ) - 1 # Step (iii) generating predictions predict.bss = predict.regsubsets(bss.train, seatpos[-training.obs, ], min.cp) predict.ridge = predict(ridge.train, x.test, s = 'lambda.min') predict.lasso = predict(lasso.train, x.test, s = 'lambda.min') predict.pcr = predict(pcr.train,seatpos[-training.obs, ], ncomp = min.pcr ) "
5227,unknown,"313 # Step (iv) evaluating predictive performance cor.bss[i] = cor(y.test, predict.bss) cor.ridge[i] = cor(y.test, predict.ridge) cor.lasso[i] = cor(y.test, predict.lasso) cor.pcr[i] = cor(y.test, predict.pcr) } # Plot the resulting correlations as boxplots. boxplot(cor.bss, cor.ridge, cor.lasso, cor.pcr, names = c('BSS','Ridge', 'Lasso', 'PCR'), ylab = 'Test correlation', col = 2:5) BSS Ridge Las"
5228,unknown,"BSS Ridge Lasso PCR 0.3 0.5 0.7 0.9 Test correlation 314 Practical Class Sheets 4 In this practical class, we will explore polynomial regression, step-wise functions, splines, generalised additive models and logistic regression. T o start, create a new R script by clicking on the corresponding menu button (in the top left); and save it somewhere in your computer. Y ou can now write al l your code "
5229,unknown,"and save it somewhere in your computer. Y ou can now write al l your code into this file, and then execute it in R using the menu button Run or by simple pressing ctrl + enter. In this practical class we will investigate two datasets for modelling. Firstly the dataset seatpos from the library faraway, followed by the Boston data analysed in the lecture prac- tical demonstrations. Lastly we will ap"
5230,unknown,"problem. F ollowing practical class 3, we continue investigating the dataset seatpos. library(""MASS"") library(""faraway"") Polynomial and Step-wise F unction Regression Exercise 4.1 This exercise models the seatpos data by polynomial and step-wise function regression. W e will analyse the effects of predictor variable Ht on the response variable hipcenter. Assign the hipcenter values to a vector y, "
5231,unknown,"y = seatpos$hipcenter x = seatpos$Ht a) Plot variable Ht and hipcenter against each other. Remember to include suitable axis labels. F rom visual inspection of this plot, what sort of polynomial might be appropriate for this data? b) Fit a first and second order polynomial to the data using the commands lm and poly. Look at the corresponding summary objects. Do these back up your answer to part (a"
5232,unknown,"(a)? c) Plot the first and second polynomial fits to the data, along with ±2 standard deviation confidence intervals, similar to those generated in the lecture practical demonstrations. What do you notice about the degree-2 polynomial plot? 315 316 d) Use step function regression with 5 cut-points to model hipcenter based on Ht. Plot the results. Click for solution ## SOLUTION # (a) y.lab = 'hip c"
5233,unknown,"Click for solution ## SOLUTION # (a) y.lab = 'hip center (mm)' x.lab = 'Height (bare foot) in cm' plot( x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = """", bty = 'l', pch = 16 ) 150 160 170 180 190 200 −250 −150 −50 Height (bare foot) in cm hip center (mm) # mean trend in the data looks almost linear - perhaps only a # first-order polynomial model is needed. # (b) poly1 = l"
5234,unknown,"summary(poly1) ## ## Call: ## lm(formula = y ~ poly(x, 1)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -99.956 -27.850 5.656 20.883 72.066 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -164.88 5.90 -27.95 < 0.0000000000000002 *** ## poly(x, 1) -289.87 36.37 -7.97 0.00000000183 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 317 ## ## Residua"
5235,unknown,"317 ## ## Residual standard error: 36.37 on 36 degrees of freedom ## Multiple R-squared: 0.6383, Adjusted R-squared: 0.6282 ## F-statistic: 63.53 on 1 and 36 DF, p-value: 0.000000001831 poly2 = lm(y ~ poly(x, 2)) summary(poly2) ## ## Call: ## lm(formula = y ~ poly(x, 2)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -96.068 -25.018 4.418 22.790 75.322 ## ## Coefficients: ## Estimate Std. Error t val"
5236,unknown,"## (Intercept) -164.885 5.919 -27.855 < 0.0000000000000002 *** ## poly(x, 2)1 -289.868 36.490 -7.944 0.00000000242 *** ## poly(x, 2)2 31.822 36.490 0.872 0.389 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 36.49 on 35 degrees of freedom ## Multiple R-squared: 0.646, Adjusted R-squared: 0.6257 ## F-statistic: 31.93 on 2 and 35 DF, p-value: 0."
5237,unknown,"# p-value for second order term is high, suggesting that it is not # necessary in the model. This backs up our conclusion to part (a). # (c) sort.x = sort(x) # sorted values of x. # Predicted values. pred1 = predict(poly1, newdata = list(x = sort.x), se = TRUE) pred2 = predict(poly2, newdata = list(x = sort.x), se = TRUE) # Confidence interval bands. se.bands1 = cbind( pred1$fit - 2*pred1$se.fit, "
5238,unknown,"se.bands1 = cbind( pred1$fit - 2*pred1$se.fit, pred1$fit + 2*pred1$se.fit ) se.bands2 = cbind( pred2$fit - 2*pred2$se.fit, pred2$fit + 2*pred2$se.fit ) # Plot both plots on a single graphics device. par(mfrow = c(1,2)) # Degree-1 polynomial plot. plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Degree-1 polynomial"", bty = 'l') 318 lines(sort.x, pred1$fit, lwd = 2, col "
5239,unknown,"matlines(sort.x, se.bands1, lwd = 2, col = ""red"", lty = 3) # Degree-2 polynomial plot. plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Degree-2 polynomial"", bty = 'l') lines(sort.x, pred2$fit, lwd = 2, col = ""red"") matlines(sort.x, se.bands2, lwd = 2, col = ""red"", lty = 3) 150 170 190 −250 −150 −50 Degree−1 polynomial Height (bare foot) in cm hip center (mm) 150 170 1"
5240,unknown,"150 170 190 −250 −150 −50 Degree−2 polynomial Height (bare foot) in cm hip center (mm) # Degree-2 polynomial plot looks relatively close to a straight line as # well - perhaps this suggests the second order component isn't necessary? # (d) # Remember to define the number of intervals (one more than the number of # cut-points). step6 = lm(y ~ cut(x, 6)) pred6 = predict(step6, newdata = list(x = sor"
5241,unknown,"se.bands6 = cbind(pred6$fit + 2*pred6$se.fit, pred6$fit-2*pred6$se.fit) # Plot the results. plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""5 cutpoints"", bty = 'l') lines(sort(x), pred6$fit, lwd = 2, col = ""red"") matlines(sort(x), se.bands6, lwd = 1.4, col = ""red"", lty = 3) 319 150 160 170 180 190 200 −250 −150 −50 5 cutpoints Height (bare foot) in cm hip center (mm) "
5242,unknown,"hip center (mm) # Note that the slopes in this plot are an artifact of the way in which we # have plotted the results. # Use summary to see where the different intervals start and finish. summary(step6) ## ## Call: ## lm(formula = y ~ cut(x, 6)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.209 -25.615 0.936 22.425 70.623 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept)"
5243,unknown,"## (Intercept) -87.76 15.25 -5.753 0.00000222 *** ## cut(x, 6)(158,166] -44.11 19.29 -2.286 0.029 * ## cut(x, 6)(166,174] -102.35 19.69 -5.197 0.00001119 *** ## cut(x, 6)(174,182] -100.91 20.18 -5.001 0.00001983 *** ## cut(x, 6)(182,190] -142.44 24.12 -5.906 0.00000143 *** ## cut(x, 6)(190,198] -191.39 40.36 -4.742 0.00004197 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' "
5244,unknown,"## ## Residual standard error: 37.36 on 32 degrees of freedom ## Multiple R-squared: 0.6606, Adjusted R-squared: 0.6076 ## F-statistic: 12.46 on 5 and 32 DF, p-value: 0.0000009372 # The fitted model can be more accurately fitted over the data as follows: newx <- seq(from = min(x), to = max(x), length = 100) pred6 = predict(step6, newdata = list(x = newx), se = TRUE) 320 se.bands6 = cbind(pred6$fit"
5245,unknown,"320 se.bands6 = cbind(pred6$fit + 2*pred6$se.fit, pred6$fit-2*pred6$se.fit) plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""5 cutpoints"", bty = 'l') lines(newx, pred6$fit, lwd = 2, col = ""red"") matlines(newx, se.bands6, lwd = 1.4, col = ""red"", lty = 3) 150 160 170 180 190 200 −250 −150 −50 5 cutpoints Height (bare foot) in cm hip center (mm) # Whilst there is still so"
5246,unknown,"hip center (mm) # Whilst there is still some sloping artifact to the plot, it is more # negligible now - that is, the plot of the fitted values more accurately # represents the step-wise nature of the model. # Note that this was also present in the examples in # Section 9.1 and 9.2 - but less noticable due to the amount of data. # Change length = 1000 in the command seq for newx and you will see t"
5247,unknown,"# steps become more step-like still! Splines In this exercise we continue modeling the seatpos data by splines. W e will again analyse the effects of predictor variable Ht on the response variable hipcenter. Exercise 4.2 a) Find the 25th, 50th and 75th percentiles of x, storing them in a vector cuts. b) Use a linear spline to model hipcenter as a function of Ht, putting knots at the 25th, 50th and"
5248,unknown,"50th and 75th percentiles of x. c) Plot the fitted linear spline from part (b) over the data, along with ±2 standard devi- ation confidence intervals. d) Use a smoothing spline to model hipcenter as a function of Ht, selecting 𝜆 with cross-validation, and generate a relevant plot. What do you notice? 321 Click for solution ## SOLUTION #(a) summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 150"
5249,unknown,"## 150.2 163.6 169.5 169.1 175.7 198.4 cuts <- summary(x)[c(2,3,5)] #(b) library(""splines"") spline1 = lm(y ~ bs(x, degree = 1, knots = cuts)) #(c) # Sort the values of x from low to high. sort.x <- sort(x) # Obtain predictions for the fitted values along with confidence intervals. pred1 = predict(spline1, newdata = list(x = sort.x), se = TRUE) se.bands1 = cbind(pred1$fit + 2 * pred1$se.fit, pred1$"
5250,unknown,"# Plot the fitted linear spline model over the data. plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Linear Spline"", bty = 'l') lines(sort.x, pred1$fit, lwd = 2, col = ""red"") matlines(sort.x, se.bands1, lwd = 2, col = ""red"", lty = 3) 150 160 170 180 190 200 −250 −150 −50 Linear Spline Height (bare foot) in cm hip center (mm) 322 #(d) smooth1 = smooth.spline(x, y, df ="
5251,unknown,"plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Smoothing Spline (3df)"", bty = 'l') lines(smooth1, lwd = 2, col = ""brown"") 150 160 170 180 190 200 −250 −150 −50 Smoothing Spline (3df) Height (bare foot) in cm hip center (mm) # The fitted model is almost completely linear. GAMs In this exercise we consider multiple predictors for modeling the seatpos data by GAMs. Exer"
5252,unknown,"Exercise 4.3 a) Fit a GAM for hipcenter that consists of three terms: • a natural spline with 5 degrees of freedom for Age, • a smoothing spline with 3 degrees of freedom for Thigh, and • a simple linear model term for Ht. b) Plot the resulting contributions of each term to the GAM, and compare them with plots of hipcenter against each of the three variables Age, Thigh and Ht. c) Does the contribu"
5253,unknown,"c) Does the contribution of each term of the GAM make sense in light of these pair-wise plots. Is the GAM fitting the data well? Click for solution ## SOLUTION #(a) # Require library gam. 323 library(gam) # Fit a GAM. gam = gam( hipcenter ~ ns( Age, df = 5 ) + s( Thigh, df = 3 ) + Ht, data = seatpos ) #(b) # Plot the contributions. par( mfrow = c(2,3) ) plot( gam, se = TRUE, col = ""blue"" ) # Compa"
5254,unknown,"# Compare with the following plots. plot( seatpos$Age, seatpos$hipcenter, pch = 16, col = 2, ylab = y.lab, xlab = ""Age (years)"" ) plot( seatpos$Thigh, seatpos$hipcenter, pch = 16, col = 2, ylab = y.lab, xlab = ""Thigh length (cm)"" ) plot( seatpos$Ht, seatpos$hipcenter, pch = 16, col = 2, ylab = y.lab, xlab = ""Ht (bare foot) (cm)"" ) 20 30 40 50 60 70 −50 0 50 100 Age ns(Age, df = 5) 35 40 45 −40 0 2"
5255,unknown,"−40 0 20 40 60 80 Thigh s(Thigh, df = 3) 150 170 190 −100 0 50 Ht partial for Ht 20 30 40 50 60 70 −250 −150 −50 Age (years) hip center (mm) 35 40 45 −250 −150 −50 Thigh length (cm) hip center (mm) 150 170 190 −250 −150 −50 Ht (bare foot) (cm) hip center (mm) #(c) # The linear fit of Ht is quite clear. Perhaps the fits for Age and Thigh # are less clear, although I think we can see that the curves"
5256,unknown,"# are less clear, although I think we can see that the curves are fitting # the data somewhat. # Perhaps they are overfitting, capturing variation across the predictor 324 # ranges (in the sample) that isn't really there (in the population). Boston Data In this exercise we will investigate the Boston Housing Data. library(""MASS"") library(splines) Exercise 4.4 a) Investigate modelling medv using re"
5257,unknown,"indus as a single predictor. b) Choose a set of predictor variables to use in order to fit a GAM for medv. F eel free to experiment with different modelling options. Click for solution ## SOLUTION #(a) y = Boston$medv x = Boston$indus y.lab = 'Median Property Value' x.lab = 'Non-retail business acres per town' # Use summary on x to find 25th, 50th and 75th percentiles. We will use # these for the "
5258,unknown,"# these for the positions of the knots for regression and natural splines. summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.46 5.19 9.69 11.14 18.10 27.74 cuts <- summary(x)[c(2,3,5)] cuts ## 1st Qu. Median 3rd Qu. ## 5.19 9.69 18.10 # sort x for later. sort.x = sort(x) # Fit a cubic spline spline.bs = lm(y ~ bs(x, knots = cuts)) pred.bs = predict(spline.bs, newdata = list(x = sort.x), se "
5259,unknown,"se.bands.bs = cbind(pred.bs$fit + 2 * pred.bs$se.fit, pred.bs$fit - 2 * pred.bs$se.fit) # Fit a natural cubic spline. 325 spline.ns = lm(y ~ ns(x, knots = cuts)) pred.ns = predict(spline.ns, newdata = list(x = sort.x), se = TRUE) se.bands.ns = cbind(pred.ns$fit + 2 * pred.ns$se.fit, pred.ns$fit - 2 * pred.ns$se.fit) # Fit a smoothing spline, with 3 effective degrees of freedom. spline.smooth = smo"
5260,unknown,"spline.smooth = smooth.spline(x, y, df = 3) # Split the plotting device into 3. par(mfrow = c(1,3)) # Plot the cubic spline. plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Cubic Spline"", bty = 'l') lines(sort.x, pred.bs$fit, lwd = 2, col = ""red"") matlines(sort.x, se.bands.bs, lwd = 2, col = ""red"", lty = 3) # Plot the natural cubic spline. plot(x, y, cex.lab = 1.1, co"
5261,unknown,"main = ""Natural Cubic Spline"", bty = 'l') lines(sort.x, pred.ns$fit, lwd = 2, col = ""darkred"") matlines(sort.x, se.bands.ns, lwd = 2, col = ""darkred"", lty = 3) # Plot the smoothing spline. plot(x, y, cex.lab = 1.1, col=""darkgrey"", xlab = x.lab, ylab = y.lab, main = ""Smoothing Spline (3 df)"", bty = 'l') lines(spline.smooth, lwd = 2, col = ""brown"") 0 5 10 15 20 25 10 20 30 40 50 Cubic Spline Non−ret"
5262,unknown,Non−retail business acres per town Median Property Value 0 5 10 15 20 25 10 20 30 40 50 Natural Cubic Spline Non−retail business acres per town Median Property Value 0 5 10 15 20 25 10 20 30 40 50 Smoothing Spline (3 df) Non−retail business acres per town Median Property Value #(b) # Of course we discussed one possible GAM in lecture practical # demonstration. We try another one here. 326 # Let's 
5263,unknown,"# if used). Boston1 = Boston Boston1$chas = factor(Boston1$chas) # Let's fit a GAM - can you see how each predictor is contributing to # modelling the response? gam1 = gam( medv ~ ns( lstat, df = 5 ) + ns( nox, df = 7 ) + s( indus, df = 7 ) + poly( age, 5 ) + chas, data = Boston1 ) par(mfrow = c(2,3)) plot(gam1, se = TRUE, col = ""blue"") 10 20 30 −10 0 10 20 30 lstat ns(lstat, df = 5) 0.4 0.5 0.6 0"
5264,unknown,"−6 −4 −2 0 2 4 6 nox ns(nox, df = 7) 0 5 10 15 20 25 −5 0 5 indus s(indus, df = 7) 0 20 40 60 80 100 −6 −4 −2 0 2 age poly(age, 5) 0 1 2 3 4 partial for chas chas 0 1 Logistic Regression In this exercise we will apply logistic regression to a binary classification problem. Suppose we are interested in how variables, such as GRE (Graduate Record Exam scores), GP A (grade point average) and prestige"
5265,unknown,"GP A (grade point average) and prestige of the undergraduate institution, effect admission into graduate school. The response variable, admit/don’t admit, is a binary variable. This admit dataset has a binary response (outcome, dependent) variable called admit. There 327 are three predictor variables: gre, gpa and rank. W e will treat the variables gre and gpa as continuous. The variable rank take"
5266,unknown,"continuous. The variable rank takes on the values 1 through 4. Institutions with a rank of 1 have the highest prestige, while those with a rank of 4 have the lowest. admit <- read.csv(""https://www.maths.dur.ac.uk/users/hailiang.du/data/admit.csv"") head(admit) ## admit gre gpa rank ## 1 0 380 3.61 3 ## 2 1 660 3.67 3 ## 3 1 800 4.00 1 ## 4 1 640 3.19 4 ## 5 0 520 2.93 4 ## 6 1 760 3.00 2 Exercise 4"
5267,unknown,"Exercise 4.5 a) Get basic descriptives for the entire data set by using summary. Explore the data graphically by producing a pairs plot of the predictors and, additionally , colouring the points according to whether admit/don’t admit. b) Convert rank to a factor to indicate that rank should be treated as a categorical variable. c) Fit a logistic regression model in order to predict admit using gre"
5268,unknown,"variable. c) Fit a logistic regression model in order to predict admit using gre, gpa and rank. The glm() function can be used to fit many types of generalized linear models , including logistic regression. The syntax of the glm() function is similar to that of lm(), except that we must pass in the argument family = binomial in order to tell R to run a logistic regression rather than some other ty"
5269,unknown,"logistic regression rather than some other type of generalized linear model. d) Use the predict() function to predict the probability of admit, given values of the predictors for the training data. Hint: [The type = ""response"" option tells R to output probabilities of the form P(Y=1|X), as opposed to other information such as the logit. If no data set is supplied to the predict() function, then th"
5270,unknown,"logit. If no data set is supplied to the predict() function, then the probabilities are computed for the training data that was used to fit the logistic regression model. ] e) Make a prediction as to whether admit or not by converting these predicted probabilities into binary values, 0 or 1, based on whether the predicted probability of admit is is greater than or less than 0.5. f) Given these pre"
5271,unknown,"f) Given these predictions, use the table() function to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified. The confusion matrix is a two by two table with counts of the number of times each combi- nation occurred e.g. predicted admit and the student is admitted, predicted admit and the student is not admitted etc. Click for solution ## "
5272,unknown,Click for solution ## SOLUTION #(a) summary(admit) 328 ## admit gre gpa rank ## Min. :0.0000 Min. :220.0 Min. :2.260 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:520.0 1st Qu.:3.130 1st Qu.:2.000 ## Median :0.0000 Median :580.0 Median :3.395 Median :2.000 ## Mean :0.3175 Mean :587.7 Mean :3.390 Mean :2.485 ## 3rd Qu.:1.0000 3rd Qu.:660.0 3rd Qu.:3.670 3rd Qu.:3.000 ## Max. :1.0000 Max. :800.0 Max. :4.000
5273,unknown,"pairs(admit[,2:4], col=admit[,1]+1) gre 2.5 3.5 200 400 600 800 2.5 3.0 3.5 4.0 gpa 200 500 800 1.0 1.5 2.0 2.5 3.0 3.5 4.0 1.0 2.5 4.0 rank #it appears that higher gre score and gpa score tend to lead to admit. #(b) admit$rank <- factor(admit$rank) #(c) glm.fit = glm(admit ~ ., data=admit, family=""binomial"") summary(glm.fit) ## ## Call: ## glm(formula = admit ~ ., family = ""binomial"", data = admi"
5274,unknown,## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6268 -0.8662 -0.6388 1.1490 2.0790 ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -3.989979 1.139951 -3.500 0.000465 *** ## gre 0.002264 0.001094 2.070 0.038465 * ## gpa 0.804038 0.331819 2.423 0.015388 * 329 ## rank2 -0.675443 0.316490 -2.134 0.032829 * ## rank3 -1.340204 0.345306 -3.881 0.000104 *** ## rank4 -1.5
5275,unknown,"## rank4 -1.551464 0.417832 -3.713 0.000205 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 499.98 on 399 degrees of freedom ## Residual deviance: 458.52 on 394 degrees of freedom ## AIC: 470.52 ## ## Number of Fisher Scoring iterations: 4 #(d) glm.probs <- predict(glm.fit, type = ""respo"
5276,unknown,"#(d) glm.probs <- predict(glm.fit, type = ""response"") glm.probs[1:10] ## 1 2 3 4 5 6 7 8 ## 0.1726265 0.2921750 0.7384082 0.1783846 0.1183539 0.3699699 0.4192462 0.2170033 ## 9 10 ## 0.2007352 0.5178682 #(e) glm.pred=rep(0, 400) glm.pred[glm.probs > .5] = 1 #(f) table(glm.pred, admit$admit) ## ## glm.pred 0 1 ## 0 254 97 ## 1 19 30 (254 + 30) / 400 ## [1] 0.71 mean(glm.pred == admit$admit) ## [1] "
5277,unknown,"## [1] 0.71 #The diagonal elements of the confusion matrix indicate correct predictions, # while the off-diagonals represent incorrect predictions. # Hence our model correctly predicted 254+30 out of 400 cases. #The `mean()` function can be used to compute the fraction of the prediction was correct. 330 References 331 332 Bibliography S. Boyd and L. V andenberghe. Convex Optimization . Cambridge U"
5278,unknown,"bridge, 7 edition, 2009. J. J. F araway .Linear Models with R . CRC press, London, 2009. J. Hurwitz and D. Kirsch. Machine Learning for Dummies. John Wiley & Sons, New Jersey , 2018. G. James, D. Witten, T. Hastie, and R. Tibshirani. An Introduction to Statistical Learning. Springer, New Y ork, 2013. W.J. Krzanowski. Principles of Multivariate Analysis: A User’s Perspective . Oxford Univer- sity P"
5279,unknown,"sity Press, Oxford, 2000. K.V. Mardia, J.T. Kent, and J.M. Bibby . Multivariate Analysis. Academic Press, London, 1979. T. Mitchell. Machine Learning. McGraw Hill, 1997. Kevin P . Murphy .Machine Learning : A Probabilistic Perspective . MIT Press, Cambridge, 2012. Kevin P . Murphy .Probabilistic Machine Learning: An Introduction. MIT Press, Cambridge, 2022. Karl Pearson. On lines and planes of clo"
5280,unknown,"Magazine, Series 6 , 11(2):559–572, 1901. N.A. W eiss. Introductory Statistics. Addison-W esley Pearson Inc., Boston, 2012. 333 Newsom Psy 522/622 Multiple Regression and Multivariate Quantitative Methods, Winter 2024 1 Multivariate Analysis of Variance Multivariate analysis of variance (MANOVA) compares groups on a set of dependent variables simultaneously. Rather than test group differences usin"
5281,unknown,"increased familywise error (probability of one or more Type I errors), the MANOVA approach makes a single comparison.1 The MANOVA is appropriate only when the several dependent variables are related to one another and the pattern of group differences expected for all of the dependent variables is in the same direction. The multiple measures can be several scale scores, individual items, or other r"
5282,unknown,"An example might be a researcher's interest in which of several psychotherapy approaches (independent variable) differ in their ability to reduce psychological distress, where several measures of psychological distress, including depression, anxiety, and perceived stress are analyzed together (as dependent variables). Alternatively, one might analyze several subscales of depression, such as positi"
5283,unknown,"negative affect, and somatic symptoms. MANOVA provides a convenience with a different type of omnibus test of all of the measures at once. The null hypothesis tested with MANOVA is that all of the dependent variable means are equal. Because the algebraic equations become increasingly complex with multiple dependent variables, multivariate analysis are usually described in terms of matrices that su"
5284,unknown,"the null hypothesis is also a test of whether the vectors (columns) of means are equal across groups. A significant result indicates that one or more of the dependent variable means differ among groups. Although usually a set of univariate ANOVA comparisons will be consistent with the MANOVA, there are some circumstances in which the results may be at odds with one another, typically because the M"
5285,unknown,"circumstances in which the results may be at odds with one another, typically because the MANOVA differences were somewhat weak and did not quite reach the significance level and because separate ANOVAs were more sensitive, focused tests of differences. A typical example would be when only one of several dependent measures shows a pattern of differences among the groups, resulting in an overall no"
5286,unknown,"nonsignificant multivariate test but a significant univariate test for one specific measure. Hotelling's T 2 There are several test statistics that are used with MANOVA (we had a brief introduction to them with repeated measures ANOVA). In the first of the multivariate test statistics, Hotelling (1931) developed a generalization of Gosset's t-test for the univariate case, now referred to as Hotell"
5287,unknown,"the univariate t-test is the following: ( ) ( ) 12 22 1212 1212 11 11 2 yyt ssnn nnnn −= +−−  ++−  The difference between two group means, 1y and 2y , in the numerator is evaluated relative to the standard error given in the denominator. In words, the standard error estimate is the square root of the weighted pooled variance divided by the group sample sizes and represents an estimate of sam"
5288,unknown,"difference between the means. Pituch and Stevens (2016) show that, with a little algebra, the square of the t-test can be restated as ( )( ) ( ) 12 12 2 12 12 12 nnt yy yy snn − = −−+ 1 Kesselman et al. (1998) make the argument that reducing Type I error is not good reason for conducting multivariate analysis of variance “There is very limited empirical support for this strategy. A counter positio"
5289,unknown,"multivariate effects that are of substantive interest. If the univariate effects are those of interest, then it is suggested that the researcher go directly to the univariate analyses and bypass MANOVA.” (pp. 361-362). Note also that the rationale to reduce familywise error and the need to use measures that are related and expected to show similar pattern of results are somewhat at odds with one a"
5290,unknown,"measures are related) familywise error will be lower—it is maximal when tests are orthogonal. Newsom Psy 522/622 Multiple Regression and Multivariate Quantitative Methods, Winter 2024 2 Where s2 is the pooled variance that combines 2 1s and 2 2s from above, and the superscript -1 is the inverse (which is just 1/s2 here). Ignoring the first quantity on the right (the ratio of ns) for a minute, the "
5291,unknown,"quantities on the right represent a square of the mean differences ( )12yy− divided by the pooled variance. This restatement of the t-test is statistically equivalent to the Gosset equation for the first t equation given above, but it is convenient for expanding and expressing in terms of matrices that can contain more than one dependent variable. ( ) ( ) 21 12 12 nnT nn −′= + 12 12 Sy -y y -y"
5292,unknown,"one dependent variable. ( ) ( ) 21 12 12 nnT nn −′= + 12 12 Sy -y y -y The bolded values of 1y and 2y represent column vectors of means (the ' symbol indicates the first parenthetical term is restated as a row), which contain the multiple dependent variables, and the S matrix is the variance-covariance matrix, containing variances for all of the y values on the diagonal and covariances"
5293,unknown,"on the off-diagonal. Hotelling's T2 can be transformed to be evaluated against an F-distribution for significance and conceptualized in terms of a ratio of between-group to within-group mean squares. Multivariate Statistical Tests The Hotelling's T2 is stated above in terms of two groups, but MANOVA can be used with any number of groups (i.e., levels of the independent variable), k, and any number"
5294,unknown,"several highly related tests that are typically produced by MANOVA software procedures. Each of them can be stated in terms of the between-group and within-group variance. Below each is shown using W and B, which are matrices of the sums of squares of y and their cross-products (the variance and covariance matrices without dividing by df). All of these measures are tested against an F-distribution"
5295,unknown,"When the result is significant, it indicates that differences existing among the groups on the dependent variables taken together. Wilks' lambda is the simplest and most straightforward in terms of its analogous relation to (the opposite of the) F ratio or eta-squared in univariate ANOVA, involving the ratio of mean square within to mean square total. Wilks' lambda Λ= = + WW BW T Hotelling's Trace"
5296,unknown,"square within to mean square total. Wilks' lambda Λ= = + WW BW T Hotelling's Trace Hotelling’s Trace (or Lawley-Hotelling trace) is a generalization of Hotelling's T2, applying to k groups. The term “trace” comes from the matrix function that sums the diagonals of the matrix. 2T Nk− or ( )1trace −EH with N as total sample size, k as number of groups, H is the matrix of sum of squares cross-product"
5297,unknown,"for the hypothesis (explained) and E is the matrix sum of squares cross-products of errors. Pillai's Trace ( ) 1 trace − +H HE Roy's Largest Root Roy's largest root (or sometimes Roy's greatest root) is the largest eigenvalue (see the ""Principal Components"" handout, covered next) from 1−EH Newsom Psy 522/622 Multiple Regression and Multivariate Quantitative Methods, Winter 2024 3"
5298,unknown,"Newsom Psy 522/622 Multiple Regression and Multivariate Quantitative Methods, Winter 2024 3 With only two groups, Pillai's Trace, Wilks' Lambda, Hotellings Trace, and Roy’s largest root are all equal. Pillai's Trace, Wilks' Lambda, Hotelling's Trace are asymptotically equivalent and will converge with larger samples. Olson (1976) suggests that Roy's largest root is too likely to produce Type I err"
5299,unknown,"avoided, that Wilks' lambda and Hotelling's trace are sensitive to violations of equal covariances in smaller samples, and he recommends Pillai's trace for general use. Variance Accounted For Owing to the close connection between Wilks' lambda and F from univariate ANOVA, we can compute eta- squared for the variance accounted for in the multivariate dependent variable composite by groups"
5300,unknown,"squared for the variance accounted for in the multivariate dependent variable composite by groups (Tabachnick & Fidell, 2013), is simply 2 1η = −Λ . Partial eta-squared is then 2 1/1 sη = −Λ , where s is based on the number of dependent variables, p, and degrees of freedom ( ) ( ) 22 22 4 5 effect effect dfps dfp −= +− Assumptions The assumptions for MANOVA overlap with those of standard ANOVA (an"
5301,unknown,"The assumptions for MANOVA overlap with those of standard ANOVA (and regression), such as independence of observations. There are somewhat stricter assumptions, however, and violations can lead to fairly severe increases in Type I or Type II errors (Christensen & Rencher, 1997; Coombs, Algina, & Oltman, 1996). Instead of the assumption that the single dependent variable is normally distributed in "
5302,unknown,"population, MANOVA assumes the dependent variables together are multivariately normally distributed. This assumption is stricter in that, even if all the individual dependent variables are normally distributed, they may not be multivariate normally distributed. The analogy to the equal variance assumption is that the covariance matrices are equal across groups. Bartlett and Box tests can be used t"
5303,unknown,"covariance matrices are equal across groups. Bartlett and Box tests can be used to make these comparisons, but they may be highly sensitive to minor departures with large N and insensitive to larger departures with small N (Huberty & Petoskey, 2000). There are a variety of proposed solutions to violations of these assumptions (see Coombs & Algina, 1996). Other Possible Approaches It is possible to"
5304,unknown,"Sidak-Bonferroni correction; see the ""Post Hoc Tests"" handout for my univariate course). When the dependent variables are uncorrelated or weakly correlated, there may be little logical benefit to using MANOVA. MANOVA, in general, is a less powerful and a less focused test than univariate ANOVA (Tabachnick & Fidell, 2013). Although MANOVA does not quite make an assumption that there is a single und"
5305,unknown,"authors have cautioned against using MANOVA when the dependent variables are not related and are not expected to show similar results (Huberty & Petoskey, 2000). Otherwise, when the pattern of group differences is not the same across dependent variables, multivariate group differences will be weakened. The MANOVA presumption that the dependent variables are assessing related or the same construct "
5306,unknown,The MANOVA presumption that the dependent variables are assessing related or the same construct is never tested explicitly and could be in error. One simple alternative to MANOVA is to create a composite of the dependent variable and test group differences with a single (univariate) ANOVA. Composites should generally only be created when the variables are at least moderately correlated and are hyp
5307,unknown,"assess the same underlying construct. 2 Such an analysis would differ from the MANOVA in two ways: (a) mean or summed composite indexes are usually equally weighted (e.g., measure 4 is not given more weight in the analysis than measure 2), whereas the MANOVA unequally weights the measures; and (b) in the single ANOVA of the composite, the composite is formed for the entire sample, whereas the MANO"
5308,unknown,"weights the measures maximizing the group differences. When the relative importance or weights of the items do not differ strongly and the correlation among the measures is high, one should not expect to find substantial differences between the composite ANOVA approach and the MANOVA approach. 2 Negatively correlated dependent variables will work (Tabachnick & Fidell, 2013), so the dependent varia"
5309,unknown,"to be positively correlated. Interpretation will be much simpler if the dependent variables are in the same direction, however. Newsom Psy 522/622 Multiple Regression and Multivariate Quantitative Methods, Winter 2024 4 A second, related approach would be to form an unequally weighted composite for the dependent variables based on an initial principal components analysis or factor analysis, approa"
5310,unknown,"the underlying construct is unidimensional or not. Results from these initial analyses could then be used to form an unequally weighted composite and then analyzed with a univariate ANOVA (see Jackson, 1991, for a detailed discussion). This factor-scores approach usually results in a measure that is highly correlated with the unweighted composite (Fava & Velicer, 1992), and the univariate ANOVA re"
5311,unknown,"lead to the same result whether factor scores are used or not. An advantage of the initial principal components or factor analysis is that the presumption about the unidimensionality of the set of dependent variables is explicitly tested. A third approach would be to use structural equation modeling (SEM), where latent variables are formed from three or more measures assessing the same construct ("
5312,unknown,"are compared. A regression-based approach (e.g., with one or more grouping variables predicting the latent variable) or group mean comparisons of the latent variables can be tested (Thompson & Green, 2013). Confirmatory factor analysis may have some advantages over the principal components or exploratory factor analysis method in its sensitivity for selecting quality items and arriving at the most"
5313,unknown,"structure (Brown, 2014), and the SEM approach has advantages in its capability to compare measurement properties across groups (testing for measurement “invariance”), incorporate more complex error structures, and expand to more complex models. The SEM approach allows the researcher to explore the equality of variances and covariances between groups for all variables individually or together but a"
5314,unknown,"require that either be equal across groups. Extensions The assigned reading (Pituch & Stevens, 2016) and the discussion above focus on a simple case in which two groups are compared, but MANOVA can be generalized to include more groups or multiple factors and to include covariates (MANCOVA). It is even possible to test multiple repeated measures with MANOVA (doubly multivariate), random effects, o"
5315,unknown,"analysis (see Timm, 2002, for proofs), so multivariate regression is also possible. Follow-up post hoc tests can also be conducted (see Pituch & Stevens, 2016 for a more complete discussion). One test is the Roy-Bose simultaneous confidence interval approach. This is a generalization of the Scheffé test and is therefore equivalent to the overly conservative adjustments made by the Bonferroni corre"
5316,unknown,"or step-up procedure. Several of the alternative corrections, such as the Sidak-Bonferroni (discussed in the ""Post Hoc Tests"" handout in my univariate course) or step approaches (e.g., Hochberg’s) can be adapted for specific-group for multiple unplanned multivariate tests or if follow-ups involve univariate comparisons planned contrasts or Tukey pairwise comparisons can be used. References"
5317,unknown,"planned contrasts or Tukey pairwise comparisons can be used. References Brown, T. A. (2014). Confirmatory factor analysis for applied research, second edition. New York: Guilford Publications. Christensen, W. F., & Rencher, A. C. (1997). A comparison of Type I error rates and power levels for seven solutions to the multivariate Behrens- Fisher problem. Communications in Statistics-Simulation and C"
5318,unknown,"Fisher problem. Communications in Statistics-Simulation and Computation, 26, 1251-1273. Coombs, W. T., & Algina, J. (1996). New test statistics for MANOVA/descriptive discriminant analysis. Educational and Psychological Measurement, 56(3), 382-402. Coombs, W. T., Algina, J., & Oltman, D. O. (1996). Univariate and multivariate omnibus hypothesis tests selected to control Type I error rates when pop"
5319,unknown,"population variances are not necessarily equal. Review of Educational Research, 66, 137-179. Fava, J. L., & Velicer, W. F. (1992). An empirical comparison of factor, image, component, and scale scores. Multivariate Behavioral Research, 27, 301-322. Jackson, J. E. (2005). A user's guide to principal components. New York: John Wiley & Sons. Huberty, C. J., & Petoskey, M. D. (2000). Multivariate anal"
5320,unknown,"Handbook of applied multivariate statistics and mathematical modeling (pp. 183-208). Academic Press. Keselman, H. J., Huberty, C. J., Lix, L. M., Olejnik, S., Cribbie, R. A., Donahue, B., ... & Levin, J. R. (1998). Statistical practices of educational researchers: An analysis of their ANOVA, MANOVA, and ANCOVA analyses. Review of Educational Research, 68(3), 350-386. Olson, C. L. (1976). On choosi"
5321,unknown,"Pituch, K. A., & Stevens, J. P. (2015). Applied multivariate statistics for the social sciences: Analyses with SAS and IBM’s SPSS. New York: Routledge. Tabachnick, B.G. and Fidell, L.S. (2013). Using multivariate statistics, sixth edition. Boston: Pearson. Thompson, M.S., & Green, S.B. (2013). Evaluating between-group differences in latent variable means. In G.R. Hancock & R.O. Mueller (Eds.),"
5322,unknown,"Structural equation modeling: A second course, 2nd edition (pp. 163-218). Charlotte, NC: Information Age Publishing. Psychology Science, Volume 46, 2004 (2), p. 243-258 Testing the assumption of multivariate normality ALEXANDER VON EYE 1 , G. ANNE BOGAT Abstract Methods of assessing the degree to which multivariate data deviate from multinormality are discussed. The best known of these methods, Ma"
5323,unknown,"are discussed. The best known of these methods, Mardia’s tests of multivariate skewness and kurtosis, allow one to test null hypotheses that are compatible with the assumption of multi- normality. However, if these null hypotheses are rejected, researchers do not know whether particular sectors carry inordinate amounts of the violations. A sector test and an omnibus test are proposed. The sector t"
5324,unknown,"test are proposed. The sector test allows one to identify subspaces that contain different num- bers of cases than expected on the assumption of multinormality. The omnibus test allows one to test whether, overall, the hypothesis of a multinormal distribution is tenable. Mardia’s tests and the new sector and omnibus tests are applied to data from a project on parenting abilities of adolescents. Ke"
5325,unknown,"Key words: multivariate normal distribution; multivariate skewness and kurtosis; multi- variate sector tests; multinormal distribution test 1 Alexander von Eye, Michigan State University, Department of Psychology, 107D Psychology Building, East Lansing, MI 48824-1116, USA; E-mail: voneye@msu.edu A. von Eye, G. A. Bogat 244"
5326,unknown,"Lansing, MI 48824-1116, USA; E-mail: voneye@msu.edu A. von Eye, G. A. Bogat 244 Statistical hypotheses in such methods as the multivariate analysis of variance (MANOVA) are usually tested under three assumptions: 1) the observations are independently distributed; 2) the observations are normally distributed; and 3) the observations have a common variance-covariance matrix. The second assumption im"
5327,unknown,"The second assumption implies that the observations are randomly drawn from a multi- variate normal (multinormal) population. Tests are called robust if their significance level (Type I error probability) and its power (1 - T ype II error probability) are insensitive to vio- lations of the assumptions based on which these tests are derived (Ito, 1980). To be able to determine whether a test is app"
5328,unknown,"To be able to determine whether a test is applicable and robust, one needs to know whether the sample at hand was drawn from a multinormal distribution. There exists a large number of tests for multinormality. These tests allow one to examine hypotheses that are consistent with the assumption of multinormality. They do not allow one to test the distribu- tion directly. In this article, we review a"
5329,unknown,"tion directly. In this article, we review a selection of existing tests, and present and apply a new test of multinormality (von Eye & Gardiner, 2004). Many data analysts tend to believe that such methods as MANOVA are robust against violations of multinormality or against heteroscedasticity. Indeed, Ito (1980) states that rather strong evidence exists that the ANOVA F-test which is derived under "
5330,unknown,"strong evidence exists that the ANOVA F-test which is derived under the assumptions of normality and homoscedasticity is „extremely r obust under violations of these assumptions“ (p. 220). This applies accordingly to the tests used for MANOVA. However, Ito states also that the „major exception to this statement occurs for small and unequal sample sizes“ (p. 220). We note that this is the typical c"
5331,unknown,"220). We note that this is the typical case of a pplications of such methods in the empirical social sciences. The major exception is thus the norm, which makes it even more important to be able to diagnose multinormality violations. 1. A brief review of tests of multinormality The behavior of tests under violations of normality assumptions is a classic topic in the theoretical and the applied sta"
5332,unknown,"theoretical and the applied statistical literature. Early work was performed by Pearson (1931). Current textbooks either cite the results that suggest robustness, just mention the assumption of multinormality (Bartholomew, St eele, Moustaki, & Galbright, 2002), or de- scribe a selection of tests of multinormality (Jobson, 1992). Two of the more frequently employed methods are briefly reviewed in t"
5333,unknown,"employed methods are briefly reviewed in this section. The first method plots individual data points against points that are expected under the assumption of multinormality. Specifically, consider the squared Mahalanobis distance of data vector x i from its sample mean, x , ),x - x( S )x - x( = m i -1 i 2 i ′ where xi is the data vector of case i, and S is the sample covariance matrix. The N multi"
5334,unknown,"ate distances 2 im can be ordered and plotted against the χ 2 distribution percentiles Testing the assumption of multivariate normality 245 2 (1- );dαχ with (1 - αi) = ( i - 0.5)/ N, and i = 1, 2, ..., N. The scatterplot of the points i 2 (1- );di[, ]m αχ should show a straight line. Most popular, and available in a number of the general purpose statistical software pack- ages, e.g., in S+, are Ma"
5335,unknown,"ages, e.g., in S+, are Mardia’s (1970, 1980) m easures of multivariate skewness and kurtosis. The sample measure of multivariate skewness is , ˆ ,γ = ∑ 3 1d ij 2 ij 1 m N where -1ij i j = ( - x) ( - x)mx S x ′ . The statistic ˆ1dN / 6γ follows a χ 2 distribution with df = d(d + 1)(d + 2)/6. Mardia’s multivariate kurtosis is , ˆ ,= ∑ 4 2d i ij 1 mNγ where 2- 1ii i = ( - x ) ( - x )mx S x , and i = "
5336,unknown,"with mean d(d + 2) and variance 8 d(d + 2)/ N, where d is the number of variables under study. An early, univariate precursor of this te st was proposed by David, Hartley, and Pear- son (1954). These two measures allow one to test two hypotheses that are compatible with the as- sumption of multinormality. Specifically, if a sample was randomly drawn from a multinor- mal population, there should be"
5337,unknown,"mal population, there should be no significant skew, and the measure of kurtosis should deviate from expectancy, that is from 0, only randomly 2 . If these measures suggest signifi- cant deviations, one can conclude with reasonable certainty that the data come from a non- normal population (other implications are discussed below). However, it is unknown whether particular variable patterns or sect"
5338,unknown,"particular variable patterns or sectors in the multivariate space exist that carry an inordinate amount of the deviation. In the following section, χ 2 -tests are presented that allow one to test hypotheses about particular deviations. 2. χ 2 -tests of deviations from normality In this section, we first illustrate the well-known χ 2 -test for univariate normality. When only one variable is studied"
5339,unknown,"only one variable is studied, this test is mo st useful. When multiple variables are studied, multinormality tests are typically preferred. The reason for this choice is that individual variables from a set of variables that are jointly multinormally distributed, are also normally distributed. In contrast, if a number of variable s is normally distributed individually, they are not necessarily als"
5340,unknown,"not necessarily also multinormal, unless they are independent of each other. Second, we present the new sector test of multinormality and its companion omnibus test. 2 It should be noted, however, that even if skewness and kurtosis are 0, one cannot be sure that normality"
5341,unknown,"exists. There exist (1) an extremely non-normal three poi nt distribution at (0, 0), and (2) infinitely many other distributions that are non-normal. A. von Eye, G. A. Bogat 246 2.1 χ 2 test of univariate normality The χ 2 goodness-of-fit test of univariate normality is well known and does not need to be described in detail. The test involves the following steps: 1) segmenting the variable under s"
5342,unknown,"1) segmenting the variable under study; 2) counting the number of cases per segment; 3) calculating z-scores for the boundaries of the segments; 4) estimating the probability for each segment (for more detail, see Point 3 in Section 2.2, below); 5) calculating the estimated expected segment frequencies; 6) comparing the observed and the estimated expected segment frequencies using the Pearson good"
5343,unknown,"Pearson goodness-of-fit χ 2 -test. In the following paragraphs, we illustrate this test using a data set that was collected as part of a larger project on parenting (Bogat et al., 1998). The participants in this project were 175 pregnant adolescents in an alternative school in Lansing, Michigan. The average age of the adolescents was about 15 years (range: 12 - 20). Their ethnical backgrounds were"
5344,unknown,"Caucasian, 43% African American, 14% Latina, 3% Native American, 1% Asian American, and 10% other/multiracial. The school served predominantly low income families (approxi- mately 60% received public assistance). Most referrals to the program were made by school counselors primarily because they deemed the adolescents at-risk of dropping out of school (e.g., poor grades, sporadic school attendance"
5345,unknown,"(e.g., poor grades, sporadic school attendance). The study itself had several goals including assessing the influence of mental health (e.g., anxiety and depression), developmental history (e.g., autonomy from parents), academic achievement, and social support on the adolescent’s parenting abilities as well as her confidence to cope with the vicissitudes of parenting. The data presented here and i"
5346,unknown,"data presented here and in the second example, below, were collected while the adolescent was pregnant and within 10 days of entering the alternative school. One element of this project involved testing hypotheses on the relationship between men- tal health and parenting. One of the indicators of mental health was T1GHQ. To be suitable for multivariate data analysis, this variable ha s to be norma"
5347,unknown,"for multivariate data analysis, this variable ha s to be normally distributed by itself. To exam- ine this variable in regard to its distributiona l characteristics, we first present two graphical representations and then perform the χ 2 goodness-of-fit test. The graphical representations appear in Figure 1. The bar chart of T1GHQ in Figure 1 shows the bars and a normal distribution blended in. Th"
5348,unknown,"The normal distribution indicates how the variable would be distributed were its distribution normal. Clearly, there is a skew. This skew is significantly different than zero: skewness = 0.582; ASE = 0.184; z = 3.16; p < 0.01. This result is presented in a slightly different way in the second panel of Figure 1: the observed distribution appears on the top border of the box, the expected distributi"
5349,unknown,"the expected distribution appears on the right hand border, and the scatterplot appears inside the box. The line deviates from the straight line that describes a normally distributed vari- able. The following paragraphs present the χ 2 analysis to answer the question whether variable T1GHQ is normally distributed. The variable is scaled to have a range from 1.5 through 4.0 with a mean of 2.48, and"
5350,unknown,"with a mean of 2.48, and sd = 0.526. Five segments were created, the first ending at 2.00 (p = 0.149), the second ending at 2.50 ( p = 0.334), the third ending at 3.00 ( p = 0.323), the fourth Testing the assumption of multivariate normality 247 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 T1GHQ 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 T1GHQ 0 10 20 30 40 50 60 Count 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 T1GHQ -3 -2 -1 0 1 2"
5351,unknown,"T1GHQ -3 -2 -1 0 1 2 3 Expected Value for Normal Distribution Figure 1: Bar chart (left panel) and q-plot (right panel) of the variable T1GHQ ending at 3.50 (p = 0.135), and the last ending at 4.0 (p = 0.024). These segments are equidis- tant on the raw score scale. The probabilities reflect again that the variable is skewed, be- cause otherwise the segments would mirror at the mean and would be e"
5352,unknown,displays the χ 2 analysis. Table 1: χ 2 analysis to test the hypothesis that variable T1GHQ is normally distributed Segment mi ˆim X 2 p 1 34 26.145 2.360 .1245 2 70 58.485 2.267 .1321 3 42 56.595 3.764 .0524 4 22 23.695 .121 .7277 5 7 4.27 1.745 .1865 sums 175 175 10.257 The overall Pearson X 2 has 3 degrees of freedom and comes with p = 0.0165. This small probability suggests that variable T1GHQ
5353,unknown,"probability suggests that variable T1GHQ is not normally distributed. For the distributional characteristics of this variable, see Figure 1. In the next section, we describe the new sector test for multinormality. A. von Eye, G. A. Bogat 248 2.2 A sector test of multinormality The following sector test can be seen as a multivariate extension of the univariate χ 2 test that was illustrated in the l"
5354,unknown,"that was illustrated in the last section. Specifically, the multivariate version of this test pro- ceeds in the following steps: 1) Segment each of the d variables under study . For the jth variable, we obtain c j seg- ments, with j = 1, ..., d. 2) Crossing the segmented variables . Crossing all segmented variables yields a d- dimensional cross-classification with d j j=1 cΠ sectors. In the next s"
5355,unknown,"dimensional cross-classification with d j j=1 cΠ sectors. In the next step, the probability needs to be calculated for a case to be located in one of these d-dimensional sectors. 3) Calculating the probability of each sector. Consider the univariate case first. Let the first segment to have boundaries - ∞ and z1. Then, the probability for an element to be located in this segment is ()( ) ( ) , 1z "
5356,unknown,"be located in this segment is ()( ) ( ) , 1z 1pz pz 0 zd z−∞ −∞ −= − Ψ ∫ that is, the area under the normal curve from - ∞ to z1. In more general terms, let the boundaries of a segment be zk and zk+1. The area under the normal curve for this seg- ment is ( ) ( ) () () k1 kzz k1 kpz pz zd z zd z + + −∞ −∞ −= Ψ − Ψ∫∫ Now, let j kp denote the probability that an element sits in segment k of the jth v"
5357,unknown,"Now, let j kp denote the probability that an element sits in segment k of the jth vari- able. In the multivariate case, cases sit in the sectors that were created by crossing the segmented variables. Each of these sectors has boundaries 1 iz and 1 i+1z on the first variable, 2 jz and 2 j+1z on the second variable, ..., and d k z and d k+1 z on the dth variable, where the subscripts indicate the se"
5358,unknown,"variable, where the subscripts indicate the segments and the superscripts indicate the variables. The probability of being located in the sector with these boundaries is 2 d1 j+1i+1 k+1 12 dij k z zz 12 d11 22 d d 1 2 d ij i+1 j+1 kk + 1 zz z p( - , - , ..., - ) = ... ( , , ..., ) ... dz dz dzzz zz z z z z z Ψ∫∫ ∫ Until 1992, there was no easily tractable solution for this equation. Genz (1992) pr"
5359,unknown,"Until 1992, there was no easily tractable solution for this equation. Genz (1992) pro- posed such a solution (cf. Gupta, 1963; for solutions for bivariate analyses see May- deu & Olivares, 2001; Seidler & Formann, 1980; the software packages S plus and Testing the assumption of multivariate normality 249 Mathematica can be used to solve this equation; a Fortran subroutine is available from Genz, 1"
5360,unknown,"from Genz, 1992). Somerville (1998) proposed a solution for convex, that is, ellip- soid sectors. For the test that is describe d in this article, we use Genz’ solution. We label the individual sector s i, j,...,k , and abbreviate the probability of sitting in a d- dimensional right-angle sector with pi, j,...,k. 4) Estimating expected sector frequencies. The expected frequency of objects in Secto"
5361,unknown,"4) Estimating expected sector frequencies. The expected frequency of objects in Sector si,j,...,k is ei,j,...,k = Np i,j,...,k. The next step involves performing tests for each individual sector. 5) Sector-specific tests. To identify locations of violations of multinormality, one com- pares for each Sector si,j,...,k the observed frequency of objects, oi,j,...,k with the corre- sponding expected f"
5362,unknown,"comparison suggests that a sector contains significantly more or fewer objects than expected based on the joint density function of the d variables under study, this sec- tor evinces a violation of multivariate normality. Therefore, the assumption of mul- tivariate normality must be rejected at least for this sector. There is a large number of tests that are suitable for the present purpose (von E"
5363,unknown,"well-known Pearson X 2 -component test, for three reasons. First, the X 2 -components are known to have desirable properties for the analysis of individual cells of a multi- variate cross-classification (von Eye, 2002; von Weber, Lautsch, & von Eye, 2004). Second, the component test is directly parallel to the univariate χ 2 -test. Third, the components sum up to an omnibus test statistic. The tes"
5364,unknown,"sector is 2 i, j,...,k i, j,...,k2 i, j,...,k i, j,...,k ( - )oe = X e with df = 1. Because of the possibly large number of tests, it is advisable to protect the significance threshold α. Most popular is the Bonferroni procedure which takes only the total number of tests into account. The Bonferroni-protected threshold is α* = α/ d j j=1 cΠ . This threshold becomes rapidly impractical as the numbe"
5365,unknown,"= α/ d j j=1 cΠ . This threshold becomes rapidly impractical as the number of sectors in- creases. Therefore, more efficient pro cedures such as the ones proposed by Holm (1979) or Keselman, Cribbie, and Holland (1999) may be preferable, because they yield protected α-thresholds that are less prohibitive. 6) Performing an omnibus test . The sum of the X 2 -components yields the omnibus test statis"
5366,unknown,"statistic 2 i, j,...,k i, j,...,k2 i, j,...,ki, j,...,k ( - )oe = X e∑ This statistic can be used as a test of whether, overall, the cross-classification of segments follows a multinormal distribution. The test has A. von Eye, G. A. Bogat 250 df =   −− −  Π d j cov j=1 2d d 1c , where cj is the number of segments of the jth variable. The term dcov indicates the number of correlations (or c"
5367,unknown,"number of correlations (or covariances) taken into account. Typically, dcov = d 2    , that is, all covariances are taken into account. Determining the spacing of segments. The spacing of segments is an issue of concern in applications of the present test of multinormality. Two concepts of spacing have been dis- cussed (von Eye & Gardiner, 2004), equidistant spacing and equiprobability spaci"
5368,unknown,"Equidistant spacing involves creating segments that span the same distance on a raw score scale. In other words, equidistant spacing splits the distance between the two extreme scores in equal segments. The advantage of this procedure is that the segments have a natural interpretation. However, there is a big drawback. The extreme segments come with small probabilities. Consider a study in which t"
5369,unknown,"probabilities. Consider a study in which the thr ee variables neuroticism, schizophrenia, and use of leisure drugs are studied. Now suppose that the lowest segments of each of these three scales have an upper bound of T = 20, that is, separate the bottom 13% of the population from the top 87%. The joint probability of these three segments is p = 0.0022 (possible corre- lations among the three vari"
5370,unknown,"lations among the three variables not taken into account). This applies accordingly to the segments close to the upper end of the scale. The sample needed for the X 2 -tests to perform well is so large that the test may become inapplicable. Therefore, equiprobability spacing has been proposed. Here, the probability scale is seg- mented instead of the raw score scale. Splitting the probability scal"
5371,unknown,"mented instead of the raw score scale. Splitting the probability scale in equally-spaced seg- ments comes with the advantage over equidistant segmenting that the segments have equal a priori-determined probabilities. Thus, the segments at the extremes of the scales have the same probability as the segments close to th e midpoints of the scales. This characteristic simplifies calculation. On the do"
5372,unknown,"simplifies calculation. On the downside is that the segments will differ in length on the raw score scale. Specifically, the segments close to the midpoint of the raw score scale will be short, whereas the segments towards the ends of the raw score scales will be long. Examples follow below. Determining the number of segments . When determining the number of sectors, two ar- guments are of importa"
5373,unknown,"guments are of importance, sample size and statistical power. When the sample size is given, one can use power calculations to determine the number of sectors. For a given effect size, one can calculate the number of cases needed for each test, and then determine the number of sectors. For example, to be able to detect small deviations from multinormality ( w = 0.1) using a χ 2 -test , one needs 1"
5374,unknown,"2 -test , one needs 1000 cases per sector for p = 0.8 (Cohen, 1988). If one trusts in the robustness of statistical methods and looks for large effects only ( w = 0.6), the number of 30 cases per sector may be sufficient for p = 0.8. Here again, large numbers of segments, and segments that are equidistant on the raw score scale may be problematic because of possibly burdensome sample size requirem"
5375,unknown,"Testing the assumption of multivariate normality 251 3. Data examples For the following data examples, we use the same data as for the first example, above. One question that the project asked concerned the relationship among indicators of autonomy and coping. Specifically, it was asked whether autonomy information allows one to predict the ability to cope. For the following illustration, we exami"
5376,unknown,"the ability to cope. For the following illustration, we examine the two autonomy indicators, T1ATTAUT (attitudinal autonomy) and T1EMOAUT (emotional autonomy), and the sum- mary scale of coping, T1COPALL. We proceed in three steps. First, we calculate descriptive information for each of these three variables. Second, we perform uni- and three-variate graphical analyses to gain visual insight into "
5377,unknown,"graphical analyses to gain visual insight into the uni- and three-variate distributions of the three variables, and third, we ask whether th e three-dimensional distribution of the three variables is multinormal. The descriptive statistics for the three variables appear in Table 2. Table 2: Descriptive statistics for three variables of autonomy and coping T1EMOAUT T1ATTAUT T1COPALL N of cases 175 "
5378,unknown,"N of cases 175 175 175 Minimum 1.071 1.125 1.273 Maximum 5.000 5.000 3.182 Mean 3.260 3.037 2.244 Standard Dev 0.919 0.815 0.400 Skewness(G1) -0.264 -0.017 -0.110 SE Skewness 0.184 0.184 0.184 Kurtosis(G2) -0.568 -0.520 -0.382 SE Kurtosis 0.365 0.365 0.365 The statistics in Table 2 suggest that the three variables may follow a univariate normal distribution. Specifically, their skewness and kurtos"
5379,unknown,"distribution. Specifically, their skewness and kurtosis scores do not differ significantly from expectancy. This conclusion is supported by the graphs displayed in Figure 2. Figure 2 suggests that the three variables are not perfectly well behaved. However, devia- tions from the univariate normal distribution are certainly not dramatic and we conclude, again, that these variables are normally dist"
5380,unknown,"again, that these variables are normally distributed. We now ask whether this conclusion holds when we inspect the 3D scatterplot, in Figure 3. The visual inspection of Figure 3 suggests no obvious deviations from the ellipsoid form of the data cloud that one would expect from three multinormal, moderately to weakly corre- lated variables (r T1EMOAUT-T1ATTAUT = 0.48; r T1EMOAUT-T1COPALL = 0.09; r "
5381,unknown,"T1EMOAUT-T1ATTAUT = 0.48; r T1EMOAUT-T1COPALL = 0.09; r T1ATTAUT-T1COPALL = 0.09). Thus, to find out whether these three variables deviate significantly from a three-variate normal distribution, we have to use statistical tests. We use four tests, Mardia’s test of multivariate kurtosis, Mardia’s test of multivariate skew ness, the sector tests proposed here, and the χ 2 omnibus test also proposed "
5382,unknown,"2 omnibus test also proposed in this article. In addition, we perform the sector and omnibus tests using the two methods of segmenting discussed in the last section, that is, segmenting that is equidistant on the raw score scale, and segmenting that is equidistant on the probabil- ity scale. In both cases, we take the correlations among the three variables into account, to A. von Eye, G. A. Bogat "
5383,unknown,1 2 3 4 5 6 T1EMOAUT -3 -2 -1 0 1 2 3 Expected Value for Normal Distribution 1 2 3 4 5 6 T1ATTAUT -3 -2 -1 0 1 2 3 Expected Value for Normal Distribution 1.0 1.5 2.0 2.5 3.0 3.5 T1COPALL -3 -2 -1 0 1 2 3 Expected Value for Normal Distribution Figure 2: q-plots of two autonomy and one coping variable make sure significant deviations indicate sectors of non-normality rather than variable corre-
5384,unknown,"make sure significant deviations indicate sectors of non-normality rather than variable corre- lations (as would be of interest in applicati ons of Configural Frequency Analysis; von Eye, 2002). Mardia’s tests. For the three variables, T1EMOAUT, T1ATTAUT, and T1COPALL, we calculate the value of 0.184 for the multivariate skewness (p = 0.865) and the value of 12.834 for the multivariate skewness ( "
5385,unknown,"for the multivariate skewness ( z = -2.616; p = 0.0045). We thus conclude that whereas there is no significant skew, the three-variate distribution is slightly heavy around the belt line (deCarlo, 1997). The assumption of multinormality can thus be rejected. We now ask whether individual sectors exist that carry an inordinate portion of this deviation from multi- normality. Testing the assumption "
5386,unknown,"normality. Testing the assumption of multivariate normality 253 Figure 3: 3D scatterplot of two autonomy and one coping variable Testing multinormality based on segments that are equidistant on the raw score scale . For both runs of the sector test, we created th ree segments on each of the variables. This led to 27 sectors in the three-dimensional data space and an average of 175/27 = 6.48 cases "
5387,unknown,"to 27 sectors in the three-dimensional data space and an average of 175/27 = 6.48 cases per sector (under a null model). The power of each indi vidual sector test will thus be small. Still, there may be strong deviations. Table 3 displays the boundaries of the segments for each variable and the segment probabilities. The boundaries at the lower ends of the scales are not repeated (see minima in Ta"
5388,unknown,"repeated (see minima in Table 2). The Pearson X 2 -component test was used to test each sec- tor hypothesis, and the significance level was Bonferroni-adjusted to be α* = .05/27 = 0.00185. Table 4 displays the results of the sector tests. Table 3: Segment boundaries and segment probability estimates for segments that are equidistant on the raw score scales Variable Segment boundaries Segment Proba"
5389,unknown,"Variable Segment boundaries Segment Probabilities T1EMOAUT 2.38 3.69 5.00 .161 .511 .291 T1ATTAUT 2.42 3.71 5.00 .214 .572 .197 T1COPALL 1.91 2.55 3.18 .194 .573 .216 A. von Eye, G. A. Bogat 254 Table 4: Sector X 2 -tests based on segments that are equidistant on the raw score scale, correlations taken into account Sector Index Frequencies X2-statistic p Observed Expected 111 6 1.975 8.204 .004179"
5390,unknown,Observed Expected 111 6 1.975 8.204 .00417976 112 7 4.394 1.545 .21380364 113 8 2.271 14.451 .00014382* 121 3 4.373 .431 .51148416 122 7 9.730 .766 .38151161 123 4 5.029 .210 .64638712 131 0 1.881 1.881 .17024146 132 1 4.185 2.424 .11950667
5391,unknown,132 1 4.185 2.424 .11950667 133 0 2.163 2.163 .14137414 211 3 4.443 .469 .49349759 212 8 9.887 .360 .54849609 213 2 5.110 1.893 .16889434 221 11 9.839 .137 .71128922 222 33 21.892 5.636 .01759101 223 10 11.315 .153 .69587554
5392,unknown,223 10 11.315 .153 .69587554 231 4 4.232 .013 .91026967 232 6 9.416 1.239 .26562870 233 4 4.867 .154 .69444052 311 3 3.182 .010 .91885760 312 4 7.079 1.339 .24713630 313 1 3.659 1.932 .16451013 321 5 7.045 .594 .44098138
5393,unknown,321 5 7.045 .594 .44098138 322 12 15.676 .862 .35321836 323 8 8.102 .001 .97141188 331 5 3.030 1.280 .25781107 332 11 6.742 2.689 .10105231 333 9 3.485 8.729 .00313193 Testing the assumption of multivariate normality 255 The results in Table 4 suggest that Sector 113 contains more cases than compatible with
5394,unknown,"The results in Table 4 suggest that Sector 113 contains more cases than compatible with the assumption of a three-variate normal distribu tion. These are individuals with low scores on both attitudinal variables and a high score on coping. Two additional sectors may carry more cases than expected (Sectors 111 and 333), but the conservative Bonferroni adjustment in combination with the low power fo"
5395,unknown,cies from becoming significant. The omnibus X 2 -score is 59.567 (df = 27 - 6 - 3 - 1 = 17; p < 0.01); this suggests that the expected distribution differs significantly from the observed one. Testing multinormality based on segments that are equidistant on the probability scale. We now present the results that are based on equiprobable segments. The testing itself used the same procedures as for 
5396,unknown,"from the sector tests. The results in Table 5 differ from the ones in Table 4 in two important respects. First, the estimated expected cell frequencies are closer to a uniform distribution. In fact, these fre- quencies are uniform as follows from the specification of the equiprobable segments on the probability scale. For correlations equal to zer o, each of the expected frequencies would be e ijk"
5397,unknown,"e ijk = 175/27 = 6.48. The deviations from this valu e that are obvious in Table 5, reflect the variable intercorrelations. Second, there is not a single sector that evinces a significant devia- tion from multinormality. All together, however, the deviations amount to a significant dis- crepancy (X 2 = 55.271; df = 19; p < 0.01). From these results, we conclude that 1) the sector tests, in tandem "
5398,unknown,"1) the sector tests, in tandem with the X 2 omnibus test, are a powerful tool for the detec- tion of deviations from multinormality; 2) the sector tests allow one to search for sectors in the multivariate space that carry an inordinate portion of the overall deviation from multinormality; 3) even if no single sector stands out as indi cating particularly blatant violations of multinormality, the o"
5399,unknown,"multinormality, the omnibus test can still suggest that, overall, the assumption of multinormality must be rejected; 4) the selection of segments may influence the appraisal of the multinormality hypothe- ses. 4. Discussion Multinormality is a condition that often must be met to obtain parameter estimates that are efficient and unbiased. In many social science applications, robustness cannot be as"
5400,unknown,"are efficient and unbiased. In many social science applications, robustness cannot be as- sumed because sample sizes tend to be small. Therefore, it is recommended to perform tests of multinormality before applying parametric mu ltivariate statistical procedures. In this article, we discuss known tests of multinormality, specifically, Mardia’s tests of multivariate skewness and multivariate kurtos"
5401,unknown,"skewness and multivariate kurtosis. These test s allow one to test hypotheses that conform with the assumption of multinormality. However, these tests do not allow one to test hy- potheses about multinormal density directly. In addition, these tests provide no information about whether violations are prominent specifica lly in particular sectors of the multivariate space. The newly proposed sector"
5402,unknown,"space. The newly proposed sector tests and the companion omnibus test do allow one to test hypotheses that address these issues. What are the implications of the results of such tests? A. von Eye, G. A. Bogat 256 Table 5: Sector X 2 -tests based on segments that are equidistant on the probability scale, correlations taken into account Sector Index Frequencies X2-statistic p Observed Expected 111 9"
5403,unknown,Observed Expected 111 9 5.409 2.384 .12256823 112 9 5.129 2.921 .08741991 113 12 5.782 6.687 .00971149 121 3 6.182 1.638 .20066252 122 6 5.862 .003 .95450639 123 1 6.608 4.759 .02914093 131 4 5.312 .324 .56910070 132 3 5.038 .824 .36397446
5404,unknown,132 3 5.038 .824 .36397446 133 4 5.679 .496 .48115553 211 7 7.000 .000 .99993107 212 1 6.638 4.788 .02865255 213 4 7.483 1.621 .20297573 221 16 8.000 8.001 .00467571 222 12 7.586 2.568 .10901820 223 12 8.551 1.391 .23828623
5405,unknown,223 12 8.551 1.391 .23828623 231 3 6.875 2.184 .13945990 232 4 6.519 .973 .32381419 233 7 7.349 .017 .89759298 311 5 6.151 .215 .64250081 312 5 5.833 .119 .73012480 313 4 6.576 1.009 .31519005 321 3 7.030 2.310 .12852033
5406,unknown,321 3 7.030 2.310 .12852033 322 6 6.666 .067 .79631524 323 5 7.515 .842 .35893172 331 8 6.041 .635 .42555684 332 9 5.729 1.868 .17174763 333 13 6.458 6.627 .01004601 Testing the assumption of multivariate normality 257 The implications are relatively clear when null hypotheses concerning multinormality are
5407,unknown,"The implications are relatively clear when null hypotheses concerning multinormality are rejected. In this case, one can assume that the data at hand have a very small probability to have been drawn from a multinormal population. If the sector test identifies one or several sectors as carrying inordinate amounts of the deviations from multinormality, one can ask whether there may be a reason for t"
5408,unknown,"whether there may be a reason for this imbalance. Possible reasons include sampling prob- lems, selective sampling, and mis-specification of populations. Researchers may then con- sider remedial steps, for instance, completing a sample by searching for cases under- represented in the first data collection. In the data example given in the previous section, one may ask why too many cases with low s"
5409,unknown,"may ask why too many cases with low scores on both attitudinal variables and a high score on coping are members of the sample (Pattern 113; Table 4). If inadvertent selective proce- dures led to oversampling of such cases, resampling may be considered. This last issue highlights that the proposed te sts are useful from two perspectives. First, they allow one to test multinormality assumptions. Sec"
5410,unknown,"they allow one to test multinormality assumptions. Second, they allow one to determine where in the sample there may be under- or ove rrepresented groups of cases. Thus, even if one assumes that certain tests are robust, the test s proposed here, in particular the sector test, provide the means of identifying those sectors of the data space that may need to be resam- pled to obtain a representativ"
5411,unknown,"pled to obtain a representative sample from a multinormal population. Interesting is the fact that in those cases in which deviations from multinormality are most deleterious, the tests proposed here (as well as a number of other tests of multinormality (see Mardia, 1980)) may have the least power. These are the cases in which sample sizes are small. In other words, when needed the most, the exist"
5412,unknown,"small. In other words, when needed the most, the existing tests may perform the worst. There are several ways out of this problem. First, th e sample can be increased. Second, one can use more powerful tests. Examples of such tests have been discussed in the context of Configural Frequency Analysis (von Eye, 2002; see also von Eye & Mun, 2003; von Weber, von Eye, & Lautsch, 2004). A most powerful "
5413,unknown,"Lautsch, 2004). A most powerful test was pr oposed by Lehmacher (1981). This test, how- ever, can be applied only when variable correlations are not taken into account, a rather unrealistic situation for the present purposes. A third way involves employing exact tests. Finally, if certain sectors are suspected to ca rry the brunt of non-normality, testing can focus on these sectors. As a consequen"
5414,unknown,"α-level becomes less extreme. If, based on the proposed tests, researchers conclude that patterns with too many or too few cases indicate that the concept of multinormality may not apply, alternative statistical methods need to be found. Alternatives incl ude robust methods, methods that are based on more flexible distributional assumptions, and weighted least squares methods. Unfortunately, alter"
5415,unknown,"alternative methods are not always available. In the case in which none of the tests leads one to reject null hypotheses concerning mul- tinormality, researchers need to be cautious nevert heless. In this case, the results of the tests only suggest that the null hypotheses concerning multinormality remain un-rejected. These results provide no proof that multinormality exists in the population. The"
5416,unknown,"results provide no proof that multinormality exists in the population. The same sampling errors that can result in selective samples that contradict multinormality although it exists in the population, can lead to collecting data that suggest multinormality because the sample is biased. If this is the case, tests of multinorma lity may be useless. The same applies to the parameters estimated under"
5417,unknown,"parameters estimated under such conditions. S till, if multinormality hypotheses can be re- tained, parametric statistical procedures are typically applied without much hesitation. A. von Eye, G. A. Bogat 258 References 1. Bartholomew, D.J., Steele, F., Moustaki, I., & Galbraith, J.I. (2002): The analysis and interpretation of multivariate data for social scientists. Boca Raton: Chapman & Hall."
5418,unknown,"interpretation of multivariate data for social scientists. Boca Raton: Chapman & Hall. 2. Bogat, G.A., Caldwell, R.A., Guzmán, B., Galasso, L., & Davidson, W.S.II (1998): Structure and stability of maternal support among pregnant and parenting adolescents. Journal of Community Psychology, 26, 549-568. 3. Cohen, J. (1988): Statistical power analysis for the behavioral sciences. Hillsdale, NJ: Lawre"
5419,unknown,"Erlbaum. 4. David, H.A., Hartley, H.O., & Pearson, E.S. (1954): The distribution of the ratio, in a single normal sample, of range to standard deviation. Biometrika, 41, 482-493. 5. DeCarlo, L.T. (1997): On the meaning and use of kurtosis. Psychological Methods, 2, 292-307. 6. Genz, A. (1992): Numerical computation of multivariate normal probabilities. Journal of Computational and Graphical Statis"
5420,unknown,"Computational and Graphical Statistics, 1, 141-149. 7. Gupta, S.S. (1963): Probability integrals of multivariate normal and multivariate t. The Annals of Mathematical Statistics, 34, 792-828. 8. Holm, S. (1979): A simple sequentially rejective multiple test procedures based on a modified Bonferroni test. Scandinavian Journal of Statistics, 6, 65-70. 9. Ito, P.K. (1980): Robustness of ANOVA and M A"
5421,unknown,"9. Ito, P.K. (1980): Robustness of ANOVA and M ANOVA test procedures. In P.R. Krishnaiah (ed.), Handbook of statistics 1. Analysis of variance (pp. 199-236). Amsterdam: North Holland. 10. Jobson, J.D. (1992): Applied multivariate data analysis. Volume II: Categorical and multivariate analysis. New York: Springer. 11. Keselman, H.J., Cribbie, R., & Holland, B. (1999): The pairwise multiple comparis"
5422,unknown,"11. Keselman, H.J., Cribbie, R., & Holland, B. (1999): The pairwise multiple comparison multiplicity problem: an alternative approach to familywise and comparisonwise Type I error control. Psychological Methods, 4, 58-69. 12. Lehmacher, W. (1981): A more powerful simultaneous test procedure in Configural Frequency Analysis. Biometrical Journal, 23, 429-436. 13. Mardia, K.V. (1970): Measures of mul"
5423,unknown,"Biometrika, 57, 519-530. 14. Mardia, K.V. (1980): Tests of univariate and multivariate normality. In P.R. Krishnaiah (ed.), Handbook of statistics (vol. 1; pp 279-320). Amsterdam: North Holland. 15. Maydeu-Olivares, A. (2001): Testing categorized bivariate normality with two-stage polychoric correlations. Dept. of Psychology. University of Barcelona. http://www.ub.es/personal/amaydeu/ testing%20ca"
5424,unknown,"testing%20categorized%20bivariate%20normality.pdf 16. Pearson, E.S. (1931): The analysis of variance in cases of non-normal variation. Biometrika, 23, 114-133. 17. Seidler, H., & Formann, A.K. (1980): Eini ge Bemerkungen zur statistischen Auswertung anthropometrischer Daten [Some comments on the statistical analysis of anthropometric data]. Anthropologischer Anzeiger, 38, 270-285. 18. Somerville, "
5425,unknown,"18. Somerville, P.N. (1998): Numerical computati on of multivariate normal and multivariate-t probabilities over convex regions. Journal of Computational and Graphical Statistics, 7, 529-544. 19. von Eye, A. (2002): Configural Frequency An alysis: methods, models , and applications. Mahwah, NJ: Lawrence Erlbaum. 20. von Eye, A., & Gardiner, J.C. (2004): Loca ting deviations from multivariate norma"
5426,unknown,"Understanding statistics (in press). 21. von Eye, A., & Mun, E.-Y. (2003): Characteristics of measures for 2 x 2 tables. Understanding Statistics, 2, 243-266. 22. von Weber, S., von Eye, A., & Lautsch, E. (2004): The Type II error of measures for the analysis of 2 x 2 tables. Understanding Statistics. (In press) Neural Likelihoods for Multi-Output Gaussian Processes Martin Jankowiak∗ Uber AI Labs "
5427,unknown,"Uber AI Labs San Francisco, CA, USA Jacob Gardner Uber AI Labs San Francisco, CA, USA Abstract We construct ﬂexible likelihoods for multi- output Gaussian process models that leverage neural networks as components. We make use of sparse variational inference methods to en- able scalable approximate inference for the re- sulting class of models. An attractive feature of these models is that they ca"
5428,unknown,of these models is that they can admit analytic predictive means even when the likelihood is non-linear and the predictive distributions are non-Gaussian. We validate the modeling po- tential of these models in a variety of experi- ments in both the supervised and unsupervised setting. We demonstrate that the ﬂexibility of these ‘neural’ likelihoods can improve predic- tion quality as compared to 
5429,unknown,"tion quality as compared to simpler Gaussian process models and that neural likelihoods can be readily combined with a variety of underly- ing Gaussian process models, including deep Gaussian processes. 1 Introduction Signiﬁcant effort has gone into developing ﬂexible, tractable probabilistic models, especially for the super- vised settings of regression and classiﬁcation. These include, among oth"
5430,unknown,"(Bonilla et al., 2008), Gaussian process regression net- works (Wilson et al., 2011), deep Gaussian processes (Damianou and Lawrence, 2013), Gaussian processes with deep kernels (Wilson et al., 2016; Calandra et al., 2016), as well as various approaches to Bayesian neu- ral networks (Graves, 2011; Blundell et al., 2015; Hern´andez-Lobato and Adams, 2015). While neural net- works promise considerab"
5431,unknown,"works promise considerable ﬂexibility, scalable learning ∗Correspondence to: jankowiak@uber.com algorithms for Bayesian neural networks that can deliver robust uncertainty estimates remain elusive. For this reason Gaussian processes (GPs) are an impor- tant class of models in cases where predictive uncertainty estimates are important. Gaussian processes offer sev- eral key advantages over other pr"
5432,unknown,"the covariance functions they employ typically have a se- mantic meaning that is natural for practitioners to reason about; ii) they facilitate incorporating prior knowledge; and iii) they tend to yield high-quality uncertainty esti- mates, even for out-of-sample data. These strengths mir- ror corresponding weaknesses of current approaches to Bayesian neural networks, weaknesses that become es- pe"
5433,unknown,"pecially evident in the small data regime, where Bayesian neural networks often struggle to deliver meaningful un- certainties. Despite these strengths, the simplest variants of GP models often fall short of the ﬂexibility of their neural network counterparts. In recent years, a number of researchers have formulated more ﬂexible Gaussian process models by modifying the GP prior itself. One approac"
5434,unknown,"classes of kernels. This approach is exempliﬁed by deep kernels, which use a deep neural network to deﬁne a rich parametric family of kernels (Wilson et al., 2016; Calan- dra et al., 2016). Another complementary approach is the use of deep Gaussian processes, which compose multi- ple layers of latent functions to build up more ﬂexible— in particular non-Gaussian—function priors (Damianou and Lawre"
5435,unknown,"and Lawrence, 2013). Surprisingly little attention, how- ever, has been paid to the ﬂexibility of likelihoods in this setting. This is likely because, historically, the likeli- hoods used in the multi-output setting have often been constrained for computational reasons. However, with recent advances in stochastic gradient methods, some of these structural assumptions are no longer required to en- "
5436,unknown,"able efﬁcient inference. 1We refer the reader to (Rasmussen, 2003) for a general in- troduction to GPs. arXiv:1905.13697v1 [stat.ML] 31 May 2019 With this opportunity in mind, our aim in this work is to make multi-output Gaussian process models more ﬂex- ible by equipping them with more ﬂexible likelihoods. We employ two simple modeling patterns to construct richer likelihoods. To make these model"
5437,unknown,"more concrete, let F(x) denote the latent vector of func- tion values drawn from a multi-output GP prior evalu- ated at an input x. In the ﬁrst approach, we pass F(x) through what is in effect a single-layer neural network before adding Gaussian observation noise. Alternatively, in a second approach we multiply F(x) by a matrix of coefﬁcients controlled by a deterministic neural net- work that dep"
5438,unknown,"approach—which can be viewed as a semi-Bayesian ver- sion of the Gaussian Process Regression Network (Wil- son et al., 2011)—is more ﬂexible but is potentially more prone to overﬁtting. We ﬁnd that both modeling patterns result in ﬂexible models that admit efﬁcient inference using sparse vari- ational methods. Furthermore, we demonstrate empiri- cally that this added ﬂexibility can lead to conside"
5439,unknown,"gains in predictive performance. Importantly, these neu- ral likelihoods are complementary to other approaches for making GP models more ﬂexible, including, for ex- ample, deep Gaussian processes and deep kernels. The rest of this paper is organized as follows. In Sec. 2 we place our work in the context of related work. In Sec. 3 we describe the models with neural likelihoods that are the focus of"
5440,unknown,"scalable inference algorithms for this class of models. In Sec. 5 we demonstrate the modeling potential of neural likelihoods with a series of experiments. 2 Related Work As discussed in the introduction, a large body of work aims to make GP priors more ﬂexible, including deep Gaussian processes (Damianou and Lawrence, 2013), GPs with deep kernels (Wilson et al., 2016; Calan- dra et al., 2016), re"
5441,unknown,"tos et al., 2015), spectral mixture kernels (Wilson and Adams, 2013), and compositional kernels (Sun et al., 2018). In the same spirit, a variety of GP models have been proposed that model correlations between multi- ple outputs (Alvarez and Lawrence, 2009; ´Alvarez and Lawrence, 2011). In particular these include a number of models that have been formulated in the multi-task set- ting (Bonilla et"
5442,unknown,"et al., 2014), including models for Bayesian optimization (Swersky et al., 2013). As mentioned in the introduction, much of this work makes particular structural assump- tions about the covariance structure and/or likelihood for computational convenience; this limits the ﬂexibility of these models. In this context see (Dezfouli and Bonilla, 2015) for an application of sparse variational methods to"
5443,unknown,"to a broader class of likelihoods. Finally, Snelson et al. (2004) construct ﬂexible likelihoods in the GP setting by warping the observed outputs with a learned deter- ministic bijection.2 A similar model, where the warping function is modeled by a GP, is considered in (L ´azaro- Gredilla, 2012). Our N-MOGP model is closest to this latter setup, with the difference that we work in the multi- outpu"
5444,unknown,"output setting and our warping function is provided by a Bayesian neural network. A number of researchers have explored models that com- bine various aspects of GPs and neural networks. For example, Cutajar et al. (2017) use random feature expan- sions to formulate a link between deep GPs and Bayesian neural networks that then enables efﬁcient inference in the resulting class of models. In (Ma et "
5445,unknown,"authors propose implicit stochastic processes as a frame- work for deﬁning ﬂexible function priors; similarly, Neu- ral Processes are a recent class of models that com- bine aspects of stochastic processes with neural networks (Garnelo et al., 2018a,b). Both these classes of models generally do not employ explicit kernel functions as is characteristic of GPs. Finally, another work with close analo"
5446,unknown,"analogs to deep kernels is (Huang et al., 2015). 3 Models In this section we deﬁne the class of models that is the focus of this work. First, in Sec. 3.1 we equip Gaussian process regression models with neural likelihoods. Next, in Sec. 3.2 we repurpose a subset of the same models for the unsupervised setting. Throughout we use the following notation. In the re- gression setting we suppose we are "
5447,unknown,"{(xi,yi)}N i=1 of size N with each input xi ∈RDX and each output yi ∈RDY . We use X and Y to refer to the full set of inputs and outputs, respectively. In the unsu- pervised setting we assume a dataset D= {yi}N i=1. 3.1 Models for Regression In Sec. 3.1.1-3.1.3 we specify three baseline GP models. Then in Sec. 3.1.4-3.1.7 we modify and/or extend these baseline models to obtain four models with neu"
5448,unknown,"hoods that will form the basis of our experiments. 2We experimented with similar constructions but found them to perform poorly in the multi-output setting, suffering from a tendency to get stuck in bad local optima. 3.1.1 Multi-Output Gaussian Processes We begin by deﬁning our simplest baseline model, a basic multi-output Gaussian process ( MOGP).3 We de- ﬁne Lindependent Gaussian processes {fℓ(x"
5449,unknown,"1,...,L and each with kernel Kℓ.4 For a given input x we use F(x) to denote the L-dimensional latent vector of GP function values at x. The marginal probability of the MOGP is then speciﬁed as follows p(Y|X) = ∫ dMp(M) L∏ ℓ=1 dfℓp(fℓ|X)× N∏ i=1 N(yi|MF(xi),β) (1) where M is a DY×Lmixing matrix, MF denotes matrix multiplication, and p(M) is a unit Normal prior on M. Here and throughout β is a DY-di"
5450,unknown,"precisions that controls the (diagonal) observation noise Note that for ﬁxedM the covariance structure of theDY- dimensional vector MF(x) is that of the ‘linear model of coregionalization’ (LMC) (Alvarez et al., 2012). While other covariance structures for the Gaussian process prior are possible, for uniformity—and since our primary in- terest is to investigate modiﬁcations to the likelihood— all "
5451,unknown,"all our models employ this basic pattern. For the same reason we use RBF kernels throughout. 3.1.2 Gaussian Process Regression Networks A natural generalization of the model in Eqn. 1 is the Gaussian Process Regression Network (GPRN) (Wilson et al., 2011). In effect we promote M to a x-dependent matrix of Gaussian processes to obtain a model P(Y|X) = ∫ ΠL ℓ=1dfℓp(fℓ|X)ΠDY mℓ=1dmmℓp(mmℓ|X)× N∏ i=1 "
5452,unknown,"mℓ=1dmmℓp(mmℓ|X)× N∏ i=1 N(yi|M(xi)F(xi),β) (2) where M(xi) is now aDY×Lrandom variable governed by a GP prior.5 Note that this model utilizes(DY+1)×L Gaussian processes and so we generally expect inference to be expensive for this class of models. 3Compare to the model in (Seeger et al., 2005). 4In general we assume that each Kℓ has its own kernel hy- perparameters; we specify when this is not th"
5453,unknown,"5Following (Wilson et al., 2011) we share kernel hyperpa- rameters among the Gaussian processes {mmℓ} but maintain individual kernels for the LGaussian processes {fℓ}. In addi- tion each kernel Kℓ for the latent function fℓ includes a diago- nal noise component. 3.1.3 Two Layer Deep Gaussian Processes We consider a deep multi-output GP with two layers of latent functions (Damianou and Lawrence, 20"
5454,unknown,"P(Y|X)= ∫ dMp(M) L∏ ℓ=1 dfℓp(fℓ|X) L′ ∏ ℓ′=1 d˜fℓ′ p(˜fℓ′ |f1:L)× N∏ i=1 N(yi|M˜F(xi),β) (3) where ˜F(x) is the L′-dimensional vector of Gaussian process function values at x. Here M is a DY ×L′ma- trix and p(M) denotes a unit Normal prior.6 We refer to this model as DGP. 3.1.4 Semi-Bayesian Gaussian Process Regression Networks We now introduce the ﬁrst model of interest in this work, namely a sem"
5455,unknown,"by Eqn. 2. We simply ‘demote’ M(xi) in Eqn. 2 to a (deterministic) neural network:7 P(Y|X)= ∫ ΠL ℓ=1dfℓp(fℓ|X) N∏ i=1 N(yi|M(xi)F(xi),β) (4) Below we refer to this model as SBGPRN; it can be viewed as occupying an intermediate position between the MOGP and GPRN. 3.1.5 Neural Multi-Output Gaussian Processes A natural extension to the MOGP speciﬁed by Eqn. 1 is to pass the vector of Gaussian process"
5456,unknown,"a layer of non-linearities before using F to compute a mean function for the likelihood, i.e. we consider a model speciﬁed by its marginal likelihood as ∫ dMp(M) L∏ ℓ=1 dfℓp(fℓ|X) N∏ i=1 N(yi|Mσ(˜MF(xi)),β) (5) 6Another alternative would be to choose L′= DY and set M → 1. Since, however, we are particularly interested in the regime where DY could be quite high-dimensional—and because inference qui"
5457,unknown,"models as we increase Land L′—we would like to avoid deep GP models with a very large number of latent functions. 7For simplicity we regularize the neural network M(x) with L2-regularization on the weights, although other schemes are possible as well. where ˜M is a DH ×DL matrix8 and M is a DY ×DH matrix, where DH is a new hyperparameter that con- trols the number of ‘hidden units.’ Here σ(·) is a"
5458,unknown,"point-wise non-linearity (e.g. ReLU) and we place a unit Normal prior on M. Below we refer to this model as N-MOGP. Since this model does not contain a (deter- ministic) neural network conditioned on the inputs as a subcomponent, we generally expect it to be less suscepti- ble to overﬁtting than the SBGPRN. Note that, as is com- monly done in the case of neural networks, we include a (stochastic) "
5459,unknown,"(stochastic) bias for each of theDH hidden units; see the supplementary materials for details. 3.1.6 Neural Semi-Bayesian Gaussian Process Regression Networks In analogy to the Neural MOGP, a natural extension to the SBGPRN is to pass the vector of Gaussian processes F through a layer of non-linearities before applying the mixing matrix M(x), yielding a model speciﬁed via its marginal probability "
5460,unknown,"marginal probability as ∫ L∏ ℓ=1 dfℓp(fℓ|X) N∏ i=1 N(yi|M(xi)σ(˜MF(xi)),β) (6) Here σ(·) is a ﬁxed non-linearity, ˜M is a DH ×Lmatrix and M(x) denotes a DY ×DH matrix controlled by a neural network. Here, again, DH is a hyperparameter that controls the number of ‘hidden units.’ Below we refer to this model as N-SBGPRN. 3.1.7 Neural Deep Gaussian Processes We equip the deep Gaussian process in Sec."
5461,unknown,"neural likelihood: P(Y|X)= ∫ dMp(M) L∏ ℓ=1 dfℓp(fℓ|X) L′ ∏ ℓ′=1 d˜fℓ′ p(˜fℓ′ |f1:L) N∏ i=1 N(yi|Mσ(˜M˜F(xi)),β) (7) where ˜M and M are DH×L′and DY ×DH-sized matri- ces, respectively. As above p(M) denotes a unit Normal prior. We refer to this model as N-DGP. 3.2 Models with Latent Inputs Each of the models in Sec. 3.1.1-3.1.7 can be repurposed as a model with latent inputs by adding a prior onX. F"
5462,unknown,"8Throughout we treat ˜M as a learnable parameter that is regularized via L2-regularization, i.e. we place a Normal prior on ˜M and perform MAP estimation on it. example, for the MOGP in Sec. 3.1.1 we have p(Y) = ∫ dXdMp(X)p(M) L∏ ℓ=1 dfℓp(fℓ|X)× N∏ i=1 N(yi|MF(xi),β) (8) where p(X) is a unit Normal prior on the inputs. We in- vestigate a subset of these models empirically in Sec. 5.4. 4 Inference "
5463,unknown,4 Inference In this section we describe how we perform approximate inference for the models described in Sec. 3. In all cases we make use of variational inference due to its favor- able computational properties and because it enables data subsampling during training. 4.1 Sparse Variational Methods In order to scale inference to large datasets we make use of sparse variational methods for Gaussian 
5464,unknown,"which we now brieﬂy review (Titsias, 2009; Hensman et al., 2013). For every GP we introduce inducing vari- ables u with dim(u) = Nind, which are conditioned on Nind variational parameters {zk},9 with each zk of the same dimension as the inputs to the GP. We then aug- ment the GP prior with the inducing variables u p(f|X) →p(f|u,X,Z)p(u|Z) and introduce a multivariate Normal variational distri- but"
5465,unknown,"bution q(u). We parameterize the covariance matrix of q(u) with a cholesky factor L. For the variational distri- bution overf we choose the priorp(f|u,X,Z) so that the variational distribution over (f,u) is p(f|u,X,Z)q(u). By introducing the auxiliary variable u we obtain a vari- ational objective that supports data subsampling, thus allowing us to scale to large datasets. Before we dis- cuss appl"
5466,unknown,"cuss applying sparse methods to any particular model in Sec. 3, we ﬁrst take a step back and discuss variational inference for models with Normal likelihoods. 4.2 Variational Inference for Normal Likelihoods We consider a regression model whose marginal likeli- hood is given by p(Y|X) = ∫ dWp(W) N∏ i=1 N(yi|Φ(xi,W),β) (9) 9Unless noted otherwise, if there are multiple GPs we share the inducing poi"
5467,unknown,"the inducing points. where Φ(x,W) is an arbitrary regressor function and W denotes all the latent variables in the model. Note that all the models in Sec. 3.1 can be expressed in this form.10 Introducing a variational distribution q(W) the variational objective—i.e. the evidence lower bound (ELBO)—can be written as ELBO = Eq(W) [log p(Y|X,W)]   ELL −KL(q(W)||p(W)) where the ﬁrst term is the ex"
5468,unknown,"We henceforth assume that the KL term is analytically tractable—as is the case for all the models we consider— and focus on the expected log likelihood (ELL). At this point there are two possibilities: i) we approximate the ELL with Monte Carlo samples; or ii) we compute the ELL analytically. As we will make use of both ap- proaches in our experiments, let us consider each pos- sibility in turn. 4"
5469,unknown,"4.2.1 Stochastic Gradient Variational Bayes For all the models in Sec. 3 we choose exclusively Normal variational distributions, which are amenable to the ‘reparameterization trick’ (Price, 1958; Kingma and Welling, 2013; Rezende et al., 2014). Consequently we can maximize the ELBO using stochastic gradient meth- ods. At high level each iteration of training proceeds as follows: 1. subsample a min"
5470,unknown,"2. form the variational distribution q(fmb) ≡ ∫ dup(fmb|u,Xmb,Z)q(u) 3. sample fmb ∼q(fmb) and compute a MC estimate of the expected log likelihood ELLmb = Eq(fmb) log p(Ymb|fmb) 4. rescale ELLmb to account for data subsampling 5. compute gradients of the ELBO with respect to model and variational parameters and take a gradi- ent step We refer the reader to (Salimbeni and Deisenroth, 2017) for an "
5471,unknown,"particular case of deep GPs. 4.2.2 Analytic ELBOs For all the models in Sec. 3.1 apart from the deep Gaus- sian process models the expected log likelihood can ei- ther be computed analytically or—for those models with 10For example, for the MOGP in Eqn. 1 W correspond to the Llatent function values {fℓ} = {Fi} and the latent matrix M and Φ(x,W) = MF(x). a non-linearity σ—almost analytically for a "
5472,unknown,"non-linearities σ. Here by ‘almost’ analytically we mean that everything can be computed analytically up to one- dimensional quadrature. We include a brief summary of this approach and refer the reader to the supplementary materials for details. The expected log likelihood for a single datapoint ican be rewritten as ELL(i) = Eq(W) [log p(yi|X,W)] = 1 2 DY∑ k=1 log βk 2π − DY∑ k=1 βk 2 { (yi,k −m(x"
5473,unknown,"DY∑ k=1 βk 2 { (yi,k −m(xi)k)2 + v(xi)k } (10) where the DY-dimensional mean and variance functions m(x) and v(x) are deﬁned as m(x)k = Eq(W) [Φ(x,W)k] v(x)k = Eq(W) [ (Φ(x,W)k −m(x)k)2] (11) For the MOGP and GPRN in Sec. 3.1.1-3.1.2 as well as the SBGPRN in Sec. 3.1.4 both of these quantities can be computed analytically. For the N-MOGP and N- SBGPRN in Sec. 3.1.5-3.1.6 the mean function m(x) can"
5474,unknown,"can be computed analytically for a wide class of non- linearities that includes, e.g., ReLU and the error func- tion (erf).11 Note that this implies that all of these mod- els admit analytic predictive means. For this same class of non-linearities the variance function v(x) can be re- duced to O(D2 H) univariate Gaussian integrals, each of which can be efﬁciently computed using Gauss-Hermite quadr"
5475,unknown,"quadrature; see the supplementary materials for details. Inference for the Neural MOGP To make the proceeding overview more concrete, we pro- vide a more detailed discussion of inference for the Neu- ral MOGP in 3.1.5, focusing on the case where the ex- pected log likelihood is computed analytically. We place a diagonal Normal variational distribution q(M) = N(M|M0,σM) on M. We form Lmultivari- at"
5476,unknown,"ate Normal variational distributions {q(uℓ)}for the cor- responding LGPs. Assuming we have analytic control over the non-linearity σ, we compute the mean function m(x): m(xi)k = Eq(M) ∏ ℓq(fℓ,i) [ (Mσ(˜MF(xi)))k ] = ΣhM0,khE∏ ℓq(fℓ,i) [ (σ(˜MF(xi)))h ] = ΣhM0,khmσ h(xi) (12) 11More broadly, it includes all piecewise polynomial non- linearities as well as non-linearities of the form σ(x) = poly(x)e"
5477,unknown,"poly(x)erf(x), where poly(x) is polynomial. Here q(fℓ,i) = ∫ duℓp(fℓ,i|uℓ,xi,Z)q(uℓ) and we have implicitly introduced the mean activation function mσ(x) on the last line. This quantity can be computed analytically as a function of ˜M and the means and vari- ances of the marginal (Normal) distributions {q(fℓ,i)}; see the supplementary materials for details. Similarly we compute the variance functi"
5478,unknown,"v(x) = v1(x) + v2(x) + v3(x) with v1(x)k ≡ΣhΣh′ M0,khvσ hh′ (x)M0,kh′ v2(x)k ≡Σhσ2 M,khmσ h(x)2 v3(x)k ≡Σhσ2 M,khvσ hh(x) (13) Here vσ(x) is the DH ×DH covariance matrix corre- sponding to mσ(x). This quantity can be computed ef- ﬁciently using univariate quadrature, at a cost that scales quadratically in the number of hidden units DH; see the supplementary materials for details. 4.3 Variational I"
5479,unknown,"Inputs Inference for the models in Sec. 3.2 proceeds analo- gously to the models in Sec. 3.1, with the difference that we now need to infer the latent inputsX. We introduce a factorized variational distribution q(X) = ∏N i=1 qi(xi), where each qi(xi) is a Normal distribution with a diag- onal covariance matrix. During training we sample a mini-batch of latent inputs Xmb ∼q(Xmb) and make use of the"
5480,unknown,"use of the reparameterization trick to compute gradients with respect to the variational parameters for the latent inputs. Since we do not make use of an amortized vari- ational distribution for the local latent variables {xi}, at test time we need to ﬁt a variational distribution q(X∗) corresponding to test data Y∗. For more details on the inference procedure, see the supplementary details. 4.4 F"
5481,unknown,"The primary bottleneck for the inference procedures out- lined above arises from dealing with the (potentially) large number of Gaussian processes. In particular, some of the most expensive subcomputations involved in com- puting the variational objective include: 1. computing KL divergences KL(q(uℓ)|p(uℓ)) 2. sampling from q(fℓ) when doing inference via SGVB as in Sec. 4.2.1 3. computing the mean"
5482,unknown,"distributions q(fℓ,i) as required to compute analytic expected log likelihoods, c.f. Sec. 4.2.2 For this reason we leverage modern conjugate gradient methods as implemented in GPyTorch (Gardner et al., 0.0 0.2 0.4 0.6 0.8 1.0 1.2 ||x|| 2 0 2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 ||x|| 2 0 2 Figure 1: Predictions for the MOGP (top) and N- MOGP (bottom) for the synthetic regression experiment in Sec. 5.1. We "
5483,unknown,"ted line, the mean model predictions with dashed lines, and colored uncertainty bands that extend from the 10th to the 90th percentile. Note that predictions for ||x||>1 are extrapolations. 2018), which reduce the computational costs of 1-3 above from O(N3 ind) to O(N2 ind). 5 Experiments In this section we conduct a series of experiments to il- lustrate the modeling potential of the models descri"
5484,unknown,"in Sec. 3. First, in Sec. 5.1 we conduct a simple experi- ment with synthetic data. Next, in Sec. 5.2 we describe the robotics data that we use in all our remaining experi- ments. In Sec. 5.3 we consider regression models, while in Sec. 5.4 we consider the unsupervised setting. In ad- dition in Sec. 5.5 we consider the effect of varying the number of ‘hidden units’ in the neural likelihood, while "
5485,unknown,"in Sec. 5.6-5.7 we examine the small data regime and missing outputs, respectively. All our experiments are implemented using GPyTorch (Gardner et al., 2018) and PyTorch (Paszke et al., 2017). 5.1 Synthetic Regression Experiment We conduct a simple experiment using synthetic data to explore the modeling capacity of neural likelihoods. We consider the function g : R5 →R8 given by gk(x) = cos(4||x||"
5486,unknown,"of x. We sample N = 1000 inputs {xi}from the unit ball in R5 and generate a dataset with noisy outputs via D= {(xi,g(xi) +σ0ϵi)}with ϵi ∼N(0,1) and where σ0 = 0.1. We then compare the quality of ﬁt obtained by a MOGP versus a Neural MOGP. For both models we set the number of GPs to L = 3, use Nind = 200 inducing points, and choose DH = 8 for the N-MOGP. To assess the quality of the ﬁt visually, we"
5487,unknown,"random line segment in R5 originating at the origin as well as a random output dimension k ∈[1,8] and de- pict model predictions yk(x∗) along the line segment, see Fig. 1. While both models are able to learn reason- able mean functions, the MOGP exhibits a higher test MRMSE (0.166) than the N-MOGP (0.102). More strik- ingly, the N-MOGP is able to learn better calibrated un- certainties and thus ob"
5488,unknown,"likelihood: 6.92 versus -0.72. One reason for this dif- ference may be due to our choice of a bounded 12 non- linearity σ(·), which gives the N-MOGP more ﬂexibility in learning a suitable variance function.13 5.2 Data We use ﬁve robotics datasets for our main set of exper- iments, four of which were collected from real-world robots and one of which was generated using the Mu- JoCo physics simulato"
5489,unknown,"datasets have been used in a number of papers, including references (Vijayakumar and Schaal, 2000; Meier et al., 2014; Cheng and Boots, 2017). In all ﬁve datasets the input and output dimensions correspond to various joint positions/velocities/etc. of the robot. These datasets form a good testbed for our proposed models, since the com- plex dynamics recorded in these data is highly non-linear and "
5490,unknown,and inherently multi-dimensional. See Table 1 for a sum- mary of the different datasets. Dataset Ntrain Ntest DX DY R-Baxter 6918 2000 21 7 F-Baxter 14295 5000 21 14 Kuka 15068 5000 21 14 Sarcos 43933 5000 21 7 MuJoCo 105 104 23 9 Table 1: Datasets used in our experiments. 5.3 Regression In this section we compare the performance of the vari- ous regression models deﬁned in Sec. 3.1. To facilitate
5491,unknown,"a fair comparison we choose the same number of GPs Lin all models. In particular we choose L = ⌈DY/2⌉, where ⌈·⌉denotes the ceiling function. For the (N-)DGP models we choose L′ = ⌈3 4 DY⌉so that the DGP prior is expected to be quite ﬂexible. For the N-MOGP, N- SBGPRN, and N-DGP models, each of which includes a 12Speciﬁcally we chose the (shifted) error function σ(x) = erf(x) + 1. 13Recall from Eq"
5492,unknown,"erf(x) + 1. 13Recall from Eqn. 10 that in order for a regression model to obtain a large expected log likelihood it must learn a high- quality mean function and a high-quality variance function. 14MRMSE is the RMSE along each output dimension aver- aged across all output dimensions. hyperparameter controlling the number of hidden units, we set DH = 2 DY, DH = DY, and DH = 2 DY, respectively. For s"
5493,unknown,"results with a deep kernel (denoted by ‘DK’). For the models with neural likelihoods, we experiment with the following set of non-linearities σ(·): 1. ReLU: relu(x) ≡max(0,x) 2. Leaky ReLU: leaky(x) ≡max(ϵx,x) with15 ϵ> 0 3. Error function: erf(x) 4. Shifted error function: sherf(x) ≡1 + erf(x) For a partial set of results see Table 2, which is organized to facilitate comparison between baseline m"
5494,unknown,their neural counterparts (e.g. MOGP versus N-MOGP). For additional details on the models and for additional results see the supplementary materials. For most of the models and datasets predictive perfor- mance improves substantially with the addition of a neu- ral likelihood; this is especially pronounced for the N- MOGP and (N-)SBGPRN. For the ﬂexible DGP prior the gain in performance tends to b
5495,unknown,"R-Baxter and F-Baxter datasets). This smaller gain in performance, however, is largely a result of our choice of L′. Indeed if we choose L = L′(so that the DGP prior is less powerful) the performance jump from DGP to N-DGP is substantial; see the supplementary materials. Note that in one case (F-Baxter) the N-MOGP has bet- ter predictive performance than the DGP and in three out of ﬁve datasets N-"
5496,unknown,"likelihood, even though the GPRN achieves higher LLs than the MOGP on all ﬁve datasets. The N-SBGPRN and DK-N-SBGPRN perform particularly well across all ﬁve datasets; this is encouraging because we found these models easy and fast to train. Note as well that with the DK-N-SBGPRN we demonstrate that neural likelihoods can be successfuly combined with deep kernels. 5.4 Unsupervised Learning In this"
5497,unknown,"unsupervised models deﬁned by the general recipe in Sec. 3.2. In particular we consider unsupervised ver- sions of the following models: MOGP, N-MOGP, and N- SBGPRN. We also trained unsupervised versions of the GPRN, DPG, and SBGPRN, but we do not report any re- sults, since we found these models to perform poorly. 16 15We choose ϵ= 0.35 in our experiments. 16In order to get a deep GP with latent "
5498,unknown,"performance we would presumably need to implement a cus- tom inference procedure more along the lines of the one used in (Dai et al., 2015). The sampling-based approach we used Dataset R-Baxter F-Baxter Kuka Sarcos MuJoCo Model LL MRMSE LL MRMSE LL MRMSE LL MRMSE LL MRMSE Baseline Models MOGP 1.38±0.18 0.297±0.007 13.16±0.46 0.285±0.004 15.64±0.40 0.174±0.006 1.63±0.04 0.250±0.001 −3.39±0.05 0.388"
5499,unknown,DK-MOGP 1.46±0.14 0.294±0.005 14.02±0.80 0.290±0.007 16.32±1.06 0.177±0.007 1.76±0.05 0.250±0.002 −1.97±0.13 0.347±0.004 GPRN 3.80±0.22 0.144±0.007 23.74±0.79 0.042±0.002 17.57±0.64 0.091±0.002 4.79±0.05 0.125±0.001 −2.50±0.11 0.336±0.004 DK-GPRN 2.78±0.38 0.179±0.009 24.77±0.79 0.040±0.001 19.14±0.64 0.089±0.002 4.52±0.11 0.133±0.003 −1.83±0.35 0.315±0.011 DGP 6.34±0.12 0.200±0.004 23.81±0.31 0.0
5500,unknown,Neural Likelihood Models N-MOGP 4.83±0.22 0.186±0.018 25.50±0.24 0.068±0.001 25.32±0.26 0.095±0.002 2.43±0.21 0.194±0.011 −3.06±0.17 0.383±0.014 SBGPRN 6.94±0.37 0.116±0.006 33.73±0.36 0.040±0.001 29.65±0.55 0.087±0.001 5.78±0.05 0.113±0.001 0.96±0.07 0.231±0.002 N-SBGPRN 7.55±0.18 0.105±0.002 35.23±0.33 0.039±0.001 31.19±0.33 0.087±0.001 6.02±0.07 0.109±0.001 1.54±0.08 0.219±0.002
5501,unknown,DK-N-SBGPRN 7.80±0.14 0.107±0.003 36.01±0.34 0.038±0.001 31.58±0.39 0.086±0.001 6.14±0.09 0.107±0.001 1.87±0.12 0.212±0.003 N-DGP 7.30±0.23 0.110±0.004 27.36±0.42 0.057±0.002 25.41±0.37 0.088±0.001 3.69±0.07 0.158±0.003 −2.41±0.15 0.348±0.009 Table 2: Results for the regression experiments in Sec. 5.3. We report test log likelihoods per datapoint (LL) and mean root mean squared errors 14(MRMSE) av
5502,unknown,materials for additional results and model details. . Dataset R-Baxter F-Baxter Kuka Sarcos MuJoCo Model LL MRMSE LL MRMSE LL MRMSE LL MRMSE LL MRMSE MOGP −18.29±2.75 0.555±0.007 −34.93±10.12 0.583±0.015 −19.22±11.37 0.416±0.012 −32.58±6.48 0.654±0.011 −40.09±4.16 0.748±0.009 N-MOGP −12.72±2.45 0.461±0.011 −18.88±5.10 0.402±0.025 −11.52±5.78 0.301±0.017 −24.43±2.17 0.500±0.018 −32.94±2.05 0.626±0.
5503,unknown,N-SBGPRN 2.45±1.19 0.355±0.005 19.49±6.64 0.094±0.008 22.82±1.89 0.100±0.006 −5.50±1.81 0.257±0.014 −18.06±0.57 0.408±0.007 Table 3: Results for the unsupervised learning experiments in Sec. 5.4. We report test log likelihoods per datapoint (LL) and mean root mean squared errors (MRMSE) averaged over ten random train/test splits of the data. See the supplementary materials for additional results a
5504,unknown,". Note that we turn the supervised datasets described in Sec. 5.2 into unsupervised datasets by concatenating the inputs and outputs: yi ←(xi,yi). For all models we set the latent dimension to DX = 4 and the number of GPs to L= 4. For the N-MOGP and N-SBGPRN we set DH = 14 and DH = 7, respectively. For a partial set of results see Table 3. For additional details on the models and for additional re"
5505,unknown,"supplementary materials. Analogous to the regression models in the previous sec- tion, we ﬁnd that the models with neural likelihoods sub- stantially outperform the baseline MOGP. The perfor- mance gain is especially striking for the N-SBGPRN, which is the clear winner on all ﬁve datasets. This re- sult is somewhat surprising, in that one might worry that the N-SBGPRN—which employs a deterministic"
5506,unknown,"ral network to mix the latent Gaussian processes in the struggled to learn anything, probably at least in part due to high variance gradients. likelihood—could be especially susceptible to overﬁtting in the unsupervised setting. In fact, while we do see evi- dence17 of moderate overﬁtting on these datasets, we ﬁnd that the increased ﬂexibility of the likelihood easily com- pensates for any loss in"
5507,unknown,"ting. This result is encouraging because (as above) we generally found the N-SBGPRN easy and fast to train. 5.5 Varying the Number of Hidden Units To explore the effect of varying the number of hidden units DH we train N-MOGP, N-SBGPRN, and N-DGP regression models on the Kuka and R-Baxter datasets for a range of DH ∈[4,20]. See Fig. 2 for the results. As we would expect, we ﬁnd that the performanc"
5508,unknown,"terms of the test log likelihood and the test MRMSE— tends to improve for all three models as we increase the number of hidden units. However, the effect is much 17We typically ﬁnd a difference of about 1 nat between train and test log likelihoods (here normalized per output dimen- sion). more pronounced for the N-MOGP and N-DGP, where the likelihoods are not as ﬂexible as in the N-SBGPRN, which i"
5509,unknown,"which includes a (deterministic) neural network as a sub- component. Notably, as we increase the number of hid- den units in the N-MOGP and N-DGP we close the ma- jority or all of the performance gap between these two models and the N-SBGPRN. This is encouraging, since we expect the N-MOGP and N-DGP to be less prone to overﬁtting. 5 10 15 20 Number of hidden units DH 0 10 20 30Test LL N-MOGP N-SBG"
5510,unknown,5 10 15 20 Number of hidden units DH 0.1 0.2 0.3 0.4Test MRMSE N-MOGP N-SBGPRN N-DGP 5 10 15 20 Number of hidden units DH 0.0 2.5 5.0 7.5Test LL N-MOGP N-SBGPRN N-DGP 5 10 15 20 Number of hidden units DH 0.1 0.2 0.3 0.4Test MRMSE N-MOGP N-SBGPRN N-DGP Figure 2: Test LLs and MRMSEs as a function of the number of hidden units on the Kuka (top) and R-Baxter (bottom) datasets for three neural GP model
5511,unknown,"parison we include results for the MOGP (dashed line) and DGP (dotted line). Results are averaged over ten random train/test splits. 5.6 Small Data Regime Here we explore the extent to which the models deﬁned in Sec. 3.1 are susceptible to overﬁtting. Among the models with neural likelihoods, we choose the N-MOGP, since, as discussed above, we expect it to be robust in the 1000 2000 3000 4000 5000"
5512,unknown,1000 2000 3000 4000 5000 6000 7000 Ndata 0 2 4 6LL Train MOGP Test MOGP Train N-MOGP Test N-MOGP Figure 3: Training and Test LLs for models trained on varying amounts of training data Ndata for the R- Baxter dataset. Results are averaged over ﬁfteen random train/test splits for each value of Ndata. 0 20 40 60 80 100 Missing Outputs (%) 15 20 25Test LL MOGP N-MOGP 0 20 40 60 80 100 Missing Outputs 
5513,unknown,0.10 0.15 0.20Test MRMSE MOGP N-MOGP Figure 4: Test LLs (top) and MRMSEs (bottom) for the MOGP and N-MOGP trained with varying amounts of missing outputs {yi}for the Kuka dataset. Results are averaged over ten random train/test splits. See Sec. 5.7 for details. small data regime.18 We then compare the N-MOGP to the MOGP and depict train and test log likelihoods ob- tained on the R-Baxter dataset a
5514,unknown,"training data, see Fig. 3. Although, as expected, we tend to observe lower log likelihoods as the number of train- ing datapoints decreases, there is no evidence for over- ﬁtting. We observe similar results for an analogous ex- periment performed with the N-DGP. We thus expect the N-MOGP and the N-DGP to retain the (relative) robust- ness against overﬁtting that is characteristic of Gaussian proce"
5515,unknown,process models. 5.7 Missing Outputs Here we explore the extent to which the models deﬁned in Sec. 3.1 can handle missing data. In particular we con- sider the case of missing outputs (i.e. each output yi has some number of output dimensions missing). We com- pare the N-MOGP to the MOGP and report test log like- lihoods and MRMSEs obtained with the Kuka dataset as we vary the number of missing outp
5516,unknown,"see Fig. 4. We ﬁnd that, as is characteristic of Gaus- sian process models, both models maintain good perfor- mance in the presence of missing outputs. Moreover, the N-MOGP maintains its considerable performance ad- vantage over the MOGP over the entire percentage range of missing outputs. See the supplementary materials for similar results obtained with the F-Baxter dataset. 6 Discussion Neural l"
5517,unknown,"augment multi-output GP models and make them more ﬂexible. We expect this class of likelihoods to be most useful in scenarios where the output dimension DY is large. In these cases it may be impractical to consider models constructed with L= DY Gaussian processes so that it becomes necessary to choose L ≪DY. In order to form a likelihood, we then need to transform the L- dimensional latent vector "
5518,unknown,"DY dimensions. While this can be done with a sim- ple linear transformation, as is done in the MOGP in Sec. 3.1.1, it is natural to consider more ﬂexible alter- natives as represented by the N-MOGP and SBGPRN. Empirically, we have seen that this added ﬂexibility can result in substantial gains in predictive performance. Im- portantly, this added ﬂexibility comes at little additional computational "
5519,unknown,"lihood tend to be negligible when compared to the costs associated with the Gaussian process prior. Moreover, neural likelihoods are complementary to other methods for making GP priors ﬂexible, as we demonstrated em- pirically in Sec. 5 by combining our approach with both deep GPs and deep kernels. There are several interesting avenues for future research. In our experiments we have focused on reg"
5520,unknown,"unsupervised learning. However, it could be of particu- lar interest to apply neural likelihoods to the multi-task setting—for example to tasks that do not share a common set of inputs—where the additional ﬂexibility offered by a neural likelihood could be especially beneﬁcial. Fi- nally, for the deterministic neural network used to de- 18In addition we ﬁnd that the SBGPRN and (N-)SBGPRN are actua"
5521,unknown,"are actually susceptible to underﬁtting in this regime because of a tendency to get stuck in bad local minima. ﬁne the SBGPRN in Sec. 3.1.4, we have relied on weight decay for regularization. It could be fruitful to explore variants of the SBGPRN that employ other techniques for regularizing neural networks, including for example dropout (Srivastava et al., 2014). Acknowledgements We cordially tha"
5522,unknown,"of the datasets we used in our experiments. MJ would like to thank Felipe Petroski Such for help with infras- tructure for efﬁcient distribution of experiments. References Mauricio Alvarez and Neil D Lawrence. Sparse con- volved gaussian processes for multi-output regression. In Advances in neural information processing systems, pages 57–64, 2009. Mauricio A ´Alvarez and Neil D Lawrence. Computa- "
5523,unknown,"tionally efﬁcient convolved multiple output gaussian processes. Journal of Machine Learning Research, 12 (May):1459–1500, 2011. Mauricio A Alvarez, Lorenzo Rosasco, Neil D Lawrence, et al. Kernels for vector-valued functions: A review. Foundations and Trends R⃝in Machine Learning, 4(3):195–266, 2012. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in ne"
5524,unknown,"and Daan Wierstra. Weight uncertainty in neural net- works. arXiv preprint arXiv:1505.05424, 2015. Edwin V Bonilla, Kian M Chai, and Christopher Williams. Multi-task gaussian process prediction. In Advances in neural information processing systems , pages 153–160, 2008. Roberto Calandra, Jan Peters, Carl Edward Rasmussen, and Marc Peter Deisenroth. Manifold gaussian pro- cesses for regression. In2"
5525,unknown,"ference on Neural Networks (IJCNN) , pages 3338– 3345. IEEE, 2016. Ching-An Cheng and Byron Boots. Variational inference for gaussian process models with linear complexity. In Advances in Neural Information Processing Systems , pages 5184–5194, 2017. Kurt Cutajar, Edwin V Bonilla, Pietro Michiardi, and Maurizio Filippone. Random feature expansions for deep gaussian processes. In Proceedings of the"
5526,unknown,"34th International Conference on Machine Learning- Volume 70, pages 884–893. JMLR. org, 2017. Zhenwen Dai, Andreas Damianou, Javier Gonz´alez, and Neil Lawrence. Variational auto-encoded deep gaus- sian processes. arXiv preprint arXiv:1511.06455 , 2015. Andreas Damianou and Neil Lawrence. Deep gaus- sian processes. In Artiﬁcial Intelligence and Statistics, pages 207–215, 2013. Amir Dezfouli and Ed"
5527,unknown,"ence for gaussian process models with black-box like- lihoods. In Advances in Neural Information Process- ing Systems, pages 1414–1422, 2015. Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu ac- celeration. In Advances in Neural Information Pro- cessing Systems, pages 7587–7597, 2018. Marta Gar"
5528,unknown,"Marta Garnelo, Dan Rosenbaum, Chris J Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo J Rezende, and SM Es- lami. Conditional neural processes. arXiv preprint arXiv:1807.01613, 2018a. Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018b. Alex Graves. Practica"
5529,unknown,"networks. In Advances in neural information process- ing systems, pages 2348–2356, 2011. James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian processes for big data. arXiv preprint arXiv:1309.6835, 2013. Jos´e Miguel Hern´andez-Lobato and Ryan Adams. Prob- abilistic backpropagation for scalable learning of bayesian neural networks. InInternational Conference on Machine Learning, pages 1861–18"
5530,unknown,"on Machine Learning, pages 1861–1869, 2015. Wenbing Huang, Deli Zhao, Fuchun Sun, Huaping Liu, and Edward Chang. Scalable gaussian process regres- sion using deep neural networks. In Twenty-Fourth In- ternational Joint Conference on Artiﬁcial Intelligence, 2015. Martin Jankowiak. Closed form variational objectives for bayesian neural networks with a single hidden layer. arXiv preprint arXiv:1811.0"
5531,unknown,"arXiv preprint arXiv:1811.00686, 2018. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 , 2013. Miguel L´azaro-Gredilla. Bayesian warped gaussian pro- cesses. In Advances in Neural Information Processing Systems, pages 1619–1627, 2012. "
5532,unknown,"Systems, pages 1619–1627, 2012. Chao Ma, Yingzhen Li, and Jos ´e Miguel Hern ´andez- Lobato. Variational implicit processes. arXiv preprint arXiv:1806.02390, 2018. C´esar Lincoln C Mattos, Zhenwen Dai, Andreas Dami- anou, Jeremy Forth, Guilherme A Barreto, and Neil D Lawrence. Recurrent gaussian processes. arXiv preprint arXiv:1511.06644, 2015. Franziska Meier, Philipp Hennig, and Stefan Schaal. I"
5533,unknown,"cremental local gaussian regression. In Advances in Neural Information Processing Systems , pages 972– 980, 2014. Edward W Ng and Murray Geller. A table of integrals of the error functions. Journal of Research of the Na- tional Bureau of Standards B, 73(1):1–20, 1969. Trung V Nguyen, Edwin V Bonilla, et al. Collaborative multi-output gaussian processes. In UAI, pages 643– 652, 2014. Adam Paszke, S"
5534,unknown,"Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Au- tomatic differentiation in pytorch. 2017. Robert Price. A useful theorem for nonlinear devices having gaussian inputs. IRE Transactions on Informa- tion Theory, 4(2):69–72, 1958. Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer"
5535,unknown,"learning. In Summer School on Machine Learning , pages 63–71. Springer, 2003. Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approx- imate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014. Hugh Salimbeni and Marc Deisenroth. Doubly stochas- tic variational inference for deep gaussian processes. In Advances in Neural Information Pr"
5536,unknown,"tems, pages 4588–4599, 2017. Matthias Seeger, Yee-Whye Teh, and Michael Jordan. Semiparametric latent factor models. Technical report, 2005. Edward Snelson, Zoubin Ghahramani, and Carl E Ras- mussen. Warped gaussian processes. In Advances in neural information processing systems, pages 337– 344, 2004. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dr"
5537,unknown,"a simple way to prevent neural networks from overﬁt- ting. The Journal of Machine Learning Research , 15 (1):1929–1958, 2014. Shengyang Sun, Guodong Zhang, Chaoqi Wang, Wenyuan Zeng, Jiaman Li, and Roger Grosse. Dif- ferentiable compositional kernel learning for gaussian processes. arXiv preprint arXiv:1806.04326, 2018. Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimizatio"
5538,unknown,"ral information processing systems, pages 2004–2012, 2013. Michalis Titsias. Variational learning of inducing vari- ables in sparse gaussian processes. In Artiﬁcial Intel- ligence and Statistics, pages 567–574, 2009. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mu- joco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelli- gent Robots and Systems , pages 5"
5539,unknown,"2012. Sethu Vijayakumar and Stefan Schaal. Locally weighted projection regression: An o (n) algorithm for incre- mental real time learning in high dimensional space. In Proceedings of the Seventeenth International Con- ference on Machine Learning (ICML 2000), volume 1, pages 288–293, 2000. Christopher Williams, Stefan Klanke, Sethu Vijayaku- mar, and Kian M Chai. Multi-task gaussian process learni"
5540,unknown,"Neural Information Processing Systems , pages 265– 272, 2009. Andrew Wilson and Ryan Adams. Gaussian process ker- nels for pattern discovery and extrapolation. In In- ternational Conference on Machine Learning , pages 1067–1075, 2013. Andrew Gordon Wilson, David A Knowles, and Zoubin Ghahramani. Gaussian process regression networks. arXiv preprint arXiv:1110.4411, 2011. Andrew Gordon Wilson, Zhiti"
5541,unknown,"nov, and Eric P Xing. Deep kernel learning. In Artiﬁ- cial Intelligence and Statistics, pages 370–378, 2016. 7 Appendix 7.1 Additional Experimental Results 7.1.1 Regression See Table 4 for additional results for the (N-)MOGP models. Here as elsewhere the preﬁx ‘DK’ indicates that the given model is equipped with a deep kernel. See Table 5 for additional results for the GPRN family of models. Perha"
5542,unknown,"which contain neural networks in both the kernel and the likelihood, is the best performing model across the board. See Table 6 for additional results for the (N-)DGP mod- els. The models with L′= Lemploy less ﬂexible prios than the models for which we report results in the main text. Note that, as mentioned in the main text, the per- formance gain from adding neural likelihoods is signiﬁ- cantly "
5543,unknown,"cantly larger for these models. 7.1.2 Unsupervised Learning See Table 7 for additional results for the unsupervised learning experiments in Sec. 5.4. For the N-MOGP mod- els we consider both DH = 7 and DH = 14 hidden units. 7.2 Experimental Details For all experiments we use the Adam optimizer (Kingma and Ba, 2014). 7.2.1 Synthetic Experiment We follow the training protocol discussed in the next s"
5544,unknown,"tion. 7.2.2 Regression We specify some of the details of our regression mod- els and their corresponding inference procedures omit- ted in the main text. Our RBF kernels use separate length scales for each input dimension. For all mod- els we choose Nind = 400 inducing points, except for the GPRN (where we choose Nind = 100 ) and for the N-DGP models (where we choose Nind = 400 for the ﬁrst layer "
5545,unknown,"ﬁrst layer of GPs and Nind = 100 for the second layer of GPs). We ﬁnd that these models can struggle to take ad- vantage of more inducing points and become susceptible to getting stuck in bad local optima when the number of inducing points is too large. For the results in Table 2 we choose the shifted erf non-linearity for the N-MOGP, the leaky ReLU non-linearity for the N-SBGPRN and DK- N-SBGPRN,"
5546,unknown,"We train all models for 250 epochs. We use mini-batch sizes of 1000, 500, 500, 500, and 250 for the MuJoCo, Kuka, F-Baxter, Sarcos, and R-Baxter datasets, respec- tively, except for the (N-)DGP models, where we double the mini-batch size. For each training/test split we use 5 random parameter initializations and only train the best performing model (in terms of training LL) to comple- tion. Depend"
5547,unknown,"rates in the range [0.01,0.05], which we reduce stepwise over the course of training. For some of the (N-)DGP models we also found it useful to employ KL anneal- ing during the ﬁrst 20 epochs of training. Except for the inducing points for the second layer of GPs in the DGP (which we initialize randomly and where we do not share the inducing points across the L′GPs), we initialize in- ducing point"
5548,unknown,"ducing points using k-means clustering on the training data inputs {xi}. For uniformity we train all models except for the GPRN 19 using the SGVB approach outlined in Sec. 4.2.1. That is, we compute stochastic gradient estimates of the expected log likelihood by drawing Nsamples = 250 samples from the variational distribu- tions {qℓ(fmb)}for each GP. For the (N-)DGP models, where the nesting of mu"
5549,unknown,"cesses makes sampling particularly expensive, we in- stead use Nsamples = 5 samples.20 Note that for models that make use of a mixing matrix M we simply integrate M out and never sample M. Throughout we report test log likelihoods using an esti- mator of the form log p(Y∗|X∗) ≈ 1 Nouter ΣNouter k=1 log ( 1 Ninner ΣNinner j=1 p(Y∗|X∗,F∗ jk) ) where each sample F∗ jk is from the relevant variational"
5550,unknown,"distribution and Nouter = 25 and Ninner = 50. Bias in Hidden Units For the N-MOGP, N-SBGPRN, and N-DGP we use DH bias terms in the non-linearityσthat appears in the likeli- hood. That is, the expression σ(˜MF) in Eqn. 5 is in fact shorthand for σ(˜MF+b), where bis a DH-dimensional vector of bias terms with a unit Normal prior. During inference, we use mean ﬁeld Normal variational distri- butions f"
5551,unknown,"butions for each bias term bh. For these same models, because of the ﬂexibility of the bias units, we use Gaus- 19We train the GPRN by computing analytic expected log likelihoods as outlined in Sec. 4.2.2. 20Our inference procedure for the (N-)DGP models closely follows that of (Salimbeni and Deisenroth, 2017). Dataset R-Baxter F-Baxter Kuka Sarcos MuJoCo Model LL MRMSE LL MRMSE LL MRMSE LL MRMSE "
5552,unknown,Dataset R-Baxter F-Baxter Kuka Sarcos MuJoCo Model LL MRMSE LL MRMSE LL MRMSE LL MRMSE LL MRMSE MOGP 1.38±0.18 0.297±0.007 13.16±0.46 0.285±0.004 15.64±0.40 0.174±0.006 1.63±0.04 0.250±0.001 −3.39±0.05 0.388±0.003 DK-MOGP 1.46±0.14 0.294±0.005 14.02±0.80 0.290±0.007 16.32±1.06 0.177±0.007 1.76±0.05 0.250±0.002 −1.97±0.13 0.347±0.004 N-MOGP (erf) 4.59±0.55 0.183±0.028 18.37±0.77 0.128±0.016 22.90±0
5553,unknown,N-MOGP (relu) 0.88±1.76 0.365±0.089 18.53±0.61 0.103±0.011 21.64±0.79 0.107±0.007 1.99±0.10 0.220±0.007 −3.02±0.05 0.377±0.003 N-MOGP (sherf) 4.83±0.22 0.186±0.018 25.50±0.24 0.068±0.001 25.32±0.26 0.095±0.002 2.43±0.21 0.194±0.011 −3.06±0.17 0.383±0.014 N-MOGP (leaky) 4.80±0.34 0.174±0.016 23.00±0.50 0.075±0.002 23.50±0.48 0.097±0.002 2.16±0.15 0.212±0.008 −2.92±0.05 0.373±0.003
5554,unknown,Table 4: Full results for the (N-)MOGP models in Sec. 5.3. We report test log likelihoods per datapoint (LL) and mean root mean squared errors (MRMSE) averaged over ten random train/test splits of the data. Dataset R-Baxter F-Baxter Kuka Sarcos MuJoCo Model LL MRMSE LL MRMSE LL MRMSE LL MRMSE LL MRMSE GPRN 3.80±0.22 0.144±0.007 23.74±0.79 0.042±0.002 17.57±0.64 0.091±0.002 4.79±0.05 0.125±0.001 −2
5555,unknown,DK-GPRN 2.78±0.38 0.179±0.009 24.77±0.79 0.040±0.001 19.14±0.64 0.089±0.002 4.52±0.11 0.133±0.003 −1.83±0.35 0.315±0.011 SBGPRN 6.94±0.37 0.116±0.006 33.73±0.36 0.040±0.001 29.65±0.55 0.087±0.001 5.78±0.05 0.113±0.001 0.96±0.07 0.231±0.002 DK-SBGPRN 6.84±0.47 0.116±0.007 34.26±0.37 0.039±0.001 29.53±0.52 0.088±0.002 5.93±0.06 0.110±0.001 1.56±0.10 0.218±0.002 N-SBGPRN (erf) 7.47±0.26 0.105±0.004 3
5556,unknown,N-SBGPRN (relu) 7.24±0.35 0.110±0.006 35.75±0.26 0.038±0.001 31.30±0.46 0.087±0.002 5.90±0.13 0.111±0.002 1.52±0.08 0.220±0.003 N-SBGPRN (sherf) 6.72±0.16 0.119±0.002 35.69±0.40 0.039±0.001 31.70±0.36 0.087±0.002 6.00±0.07 0.110±0.001 1.24±0.10 0.226±0.002 N-SBGPRN (leaky) 7.55±0.18 0.105±0.002 35.23±0.33 0.039±0.001 31.19±0.33 0.087±0.001 6.02±0.07 0.109±0.001 1.54±0.08 0.219±0.002
5557,unknown,DK-N-SBGPRN (erf) 7.55±0.21 0.107±0.003 35.41±0.32 0.041±0.001 30.81±0.54 0.088±0.001 5.93±0.10 0.110±0.001 1.41±0.08 0.222±0.002 DK-N-SBGPRN (relu) 7.52±0.29 0.104±0.003 36.66±0.30 0.037±0.001 31.91±0.44 0.086±0.001 6.04±0.10 0.108±0.001 1.86±0.08 0.211±0.002 DK-N-SBGPRN (sherf) 6.66±0.40 0.118±0.008 36.24±0.36 0.038±0.001 32.30±0.48 0.086±0.001 6.13±0.06 0.107±0.001 1.60±0.10 0.217±0.003
5558,unknown,"DK-N-SBGPRN (leaky)7.80±0.14 0.107±0.003 36.01±0.34 0.038±0.001 31.58±0.39 0.086±0.001 6.14±0.09 0.107±0.001 1.87±0.12 0.212±0.003 Table 5: Full results for the GPRN, SBGPRN, and N-SBGPRN models in Sec. 5.3. We report test log likelihoods per datapoint (LL) and mean root mean squared errors (MRMSE) averaged over ten random train/test splits of the data. sian priors with (ﬁxed) zero mean functions,"
5559,unknown,"other Gaussian process priors we use (trainable) constant mean functions. Deep Kernels All our deep kernels make use of neural networks with two hidden layers, each with 50 hidden units, and have the same number of output dimensions as input dimen- sions. We use tanh non-linearities. We also use a mul- tiplicative parameterization of the deep kernels such that at initialization each is close to th"
5560,unknown,"SBGPRN Neural Networks Throughout the neural network M(x) that appears in the SBGPRN and N-SBGPRN has two hidden layers, each with 50 units, and utilizes tanh non-linearities. 7.2.3 Unsupervised Learning We specify some of the details of our unsupervised mod- els and their corresponding inference procedures omitted in the main text. We use Nind = 200 inducing points for all models. For the results"
5561,unknown,"ReLU non-linearity for the N-MOGP and the ReLU non- linearity for the N-SBGPRN. The training procedure and modeling setup generally follows that of the regression models, with the important difference that (except for the latent inputs, which we always sample) we compute ana- lytic expected log likelihoods as described in Sec. 4.2.2. We use Nqp = 100 quadrature points. During training we sample a "
5562,unknown,"point. We ﬁnd that using analytic ELBOs during training leads to better stability and performance. In contrast to the regression models, the RBF kernels in our unsuper- vised learning experiments use the same length scale for all dimensions. During test time, we introduce a new variational distribution q(X∗) for the unseen data and ﬁt q(X∗) by maximizing the ELBO. That is, ﬁtting q(X∗) proceeds an"
5563,unknown,"proceeds analogously to training, except that now every- thing except forq(X∗) is kept ﬁxed (i.e. the kernel hyper- parameters, the variational distributions q(uℓ), etc.). We initialize q(X∗) by using a nearest neighbor algorithm to ﬁnd points in the training data that are close to points in Dataset R-Baxter F-Baxter Kuka Sarcos MuJoCo Model LL MRMSE LL MRMSE LL MRMSE LL MRMSE LL MRMSE"
5564,unknown,Dataset R-Baxter F-Baxter Kuka Sarcos MuJoCo Model LL MRMSE LL MRMSE LL MRMSE LL MRMSE LL MRMSE DGP 6.34±0.12 0.200±0.004 23.81±0.31 0.083±0.001 25.08±0.22 0.089±0.001 3.45±0.11 0.166±0.003 −2.46±0.16 0.354±0.013 N-DGP (erf) 7.30±0.23 0.110±0.004 27.36±0.42 0.057±0.002 25.41±0.37 0.088±0.001 3.69±0.07 0.158±0.003 −2.41±0.15 0.348±0.009 N-DGP (sherf) 3.27±2.58 0.232±0.082 27.42±0.49 0.058±0.003 25.
5565,unknown,"N-DGP (leaky) 7.42±0.17 0.107±0.003 27.93±0.21 0.053±0.001 25.66±0.29 0.089±0.001 3.65±0.10 0.161±0.005 −2.99±0.96 0.390±0.052 DGP (L′ = L) 2.91±0.29 0.315±0.029 12.89±0.69 0.284±0.005 15.60±0.36 0.190±0.007 2.31±0.05 0.243±0.002 −2.81±0.16 0.384±0.021 N-DGP (L′ = L, erf) 5.98±0.20 0.145±0.017 21.77±0.65 0.081±0.005 23.69±0.23 0.103±0.004 2.82±0.16 0.214±0.010 −2.78±0.14 0.388±0.015"
5566,unknown,"N-DGP (L′ = L, sherf) 3.21±2.51 0.238±0.084 24.63±0.97 0.068±0.004 24.73±0.46 0.093±0.002 2.93±0.21 0.201±0.010 −2.68±0.17 0.374±0.018 N-DGP (L′ = L, leaky) 5.83±0.18 0.169±0.016 24.43±0.41 0.067±0.001 24.61±0.32 0.092±0.001 2.97±0.24 0.197±0.012 −3.26±0.54 0.405±0.027 Table 6: Full results for the (N-)DGP models in Sec. 5.3. We report test log likelihoods per datapoint (LL) and mean root mean squ"
5567,unknown,"L′= ⌈3 4 DY⌉. Dataset R-Baxter F-Baxter Kuka Sarcos MuJoCo Model LL MRMSE LL MRMSE LL MRMSE LL MRMSE LL MRMSE MOGP −18.29±2.75 0.555±0.007 −34.93±10.12 0.583±0.015 −19.22±11.37 0.416±0.012 −32.58±6.48 0.654±0.011 −40.09±4.16 0.748±0.009 N-MOGP (DH= 7, erf) −14.51±3.81 0.483±0.020 −25.87±4.43 0.506±0.022 −20.36±5.65 0.379±0.015 −27.80±2.52 0.634±0.042 −39.63±2.57 0.762±0.025"
5568,unknown,"N-MOGP (DH= 7, relu) −30.60±3.41 0.782±0.061 −30.92±3.77 0.654±0.064 −20.73±5.90 0.504±0.088 −30.06±2.05 0.735±0.054 −36.53±1.14 0.765±0.029 N-MOGP (DH= 7, sherf) −13.98±3.14 0.500±0.035 −27.04±6.49 0.522±0.038 −23.59±12.63 0.388±0.024 −28.51±3.76 0.633±0.039 −42.71±6.23 0.768±0.033 N-MOGP (DH= 7, leaky) −13.45±1.78 0.489±0.021 −23.65±4.46 0.483±0.028 −17.00±9.57 0.373±0.027 −26.75±5.33 0.569±0.03"
5569,unknown,"N-MOGP (DH= 14, erf) −18.65±7.67 0.490±0.030 −20.00±4.69 0.428±0.028 −23.47±10.88 0.349±0.024 −24.08±2.62 0.536±0.034 −34.69±1.96 0.679±0.037 N-MOGP (DH= 14, relu) −26.91±3.42 0.726±0.064 −28.87±5.06 0.602±0.082 −18.33±5.13 0.479±0.086 −27.36±2.23 0.652±0.038 −32.32±0.88 0.670±0.021 N-MOGP (DH= 14, sherf) −15.87±6.49 0.461±0.016 −23.20±9.07 0.435±0.042 −28.77±16.68 0.363±0.028 −23.58±2.81 0.520±0."
5570,unknown,"N-MOGP (DH= 14, leaky) −12.72±2.45 0.461±0.011 −18.88±5.10 0.402±0.025 −11.52±5.78 0.301±0.017 −24.43±2.17 0.500±0.018 −32.94±2.05 0.626±0.018 N-SBGPRN (erf) 1.88±1.36 0.348±0.004 17.69±4.49 0.089±0.007 22.18±2.07 0.089±0.004 −7.74±3.63 0.259±0.021 −19.10±2.22 0.412±0.017 N-SBGPRN (relu) 2.45±1.19 0.355±0.005 19.49±6.64 0.094±0.008 22.82±1.89 0.100±0.006 −5.50±1.81 0.257±0.014 −18.06±0.57 0.408±0."
5571,unknown,N-SBGPRN (sherf) −1.16±3.95 0.353±0.007 13.09±11.64 0.100±0.011 15.60±5.30 0.105±0.011 −9.63±5.32 0.263±0.027 −18.41±2.46 0.405±0.016 N-SBGPRN (leaky) 1.33±2.13 0.347±0.005 11.14±8.37 0.091±0.007 15.65±6.65 0.091±0.007 −9.44±3.26 0.258±0.016 −21.45±1.93 0.422±0.013 Table 7: Full results for the unsupervised learning experiments in Sec. 5.4. We report test log likelihoods per datapoint (LL) and mea
5572,unknown,the test data; we then initialize the means ofq(x∗ i) by us- ing the mean of the variational distribution q(xj) for the training datapoint xj that is closest to x∗ i. 7.2.4 Varying DH The experimental protocol for these experiments closely follows that of the regression experiments in Sec. 5.3. 7.2.5 Small Data Regime The experimental protocol for these experiments closely follows that of the regr
5573,unknown,with the difference that we only use Nind = 250 induc- ing points and that as we reduce the training set size we reduce the mini-batch size proportionally. 7.2.6 Missing Outputs The experimental protocol for these experiments closely follows that of the regression experiments in Sec. 5.3. For each missing output percentage the number of miss- ing output dimensions for each output yi is identical (
5574,unknown,(e.g. 3/14 output dimensions are missing). The particular missing output dimensions for each datapoint are differ- ent between each train/test split. For additional results obtained with the F-Baxter dataset see Fig. 5. 7.3 Expectations of Non-linearities We discuss how we compute the expectations required to form analytic expected log likelihoods as described for the N-MOGP in Sec. 4.2.2. Here we
5575,unknown,"error function non-linearity g(x) = erf( x). An analytic expression for the mean function can be obtained from a table of integrals:21 EN(x|µ,σ) [erf(x)] = erf ( µ√ 1 + 2σ2 ) (14) 21See e.g. the integrals listed in (Ng and Geller, 1969) 0 20 40 60 80 100 Missing Outputs (%) 10 15 20 25Test LL MOGP N-MOGP 0 20 40 60 80 100 Missing Outputs (%) 0.0 0.1 0.2 0.3Test MRMSE MOGP N-MOGP Figure 5: Test LLs"
5576,unknown,"MOGP and N-MOGP trained with varying amounts of missing outputs {yi}for the F-Baxter dataset. Results are averaged over ten random train/test splits. An analytic expression for the corresponding second mo- ment is not readily available but can be computed efﬁ- ciently using quadrature: EN(x|µ,σ) [ erf(x)2] = EN(x|0,1/ √ 2) [ erf( √ 2σx+ µ)2 ] = 1√π ∫ dxe−x2 erf( √ 2σx+ µ)2 ≈ 1√π Nq∑ i=1 wierf( √ 2"
5577,unknown,"≈ 1√π Nq∑ i=1 wierf( √ 2σxi + µ)2 (15) where {(xi,wi)}are the sample points and weights from a Gauss-Hermite quadrature rule of order Nq (conven- tionally deﬁned w.r.t. the weighting function e−x2 ). Fi- nally, we would like to compute the bivariate expectation EN(x|µ,Σ) [g(x1)g(x2)] = EN(x|µ,Σ) [erf(x1)erf(x2)] (16) An analytic expression is not readily available but we can do “half” of the integ"
5578,unknown,"maining univariate integral using quadrature. Changing variables so that the Normal distribution has a diagonal covariance matrix, we obtain: 1√π ∫ dx1e−x2 1 g( √ 2L11x1 + µ1)h(x1) (17) where Lis the Cholesky decomposition22 of Σ with Σ = LLT and h(x1) is given by the mean function h(x1) = EN(x2|0,1/ √ 2) [ g( √ 2L21x1 + √ 2L22x2 + µ2) ] = EN(˜x2| √ 2L21x1+µ2,L22) [g(˜x2)] (18) Consequently whenev"
5579,unknown,"able for this inner expectation—as is the case for the error function, recall Eqn. 14—the bivariate expectation in Eqn. 16 can be efﬁciently computed with univariate Gauss-Hermite quadrature. The identity Eqn. 14 can be manipulated to yield all ex- pectations of the formEN(x|µ,σ) [xnerf(x)], thus making all the mean functions for all nonlinearities of the form g(x) = poly(x)erf(x) for some polynom"
5580,unknown,"alytically tractable. For example we have EN(x|µ,σ) [xerf(x)] =µerf ( µ√ 1 + 2σ2 ) + 2σ2 √π(1 + 2σ2)1/2 e− µ2 1+2σ2 (19) and EN(x|µ,σ) [ x2erf(x) ] =(µ2 + σ2)erf ( µ√ 1 + 2σ2 ) + 4µσ2(1 + σ2)√π(1 + 2σ2)3/2 e− µ2 1+2σ2 (20) For the analytic expressions needed to deal with piece- wise polynomial non-linearities like ReLU we refer the reader to (Jankowiak, 2018). Finally, note that above (e.g. in Eqn"
5581,unknown,"analytic expressions for expectations with respect to 1- dimensional Normal random variables. However, in Sec. 4.2.2 the mean and variance functions for the ac- tivations, mσ(x) and vσ(x), are expressed as expecta- tions with respect to the product distribution ∏ ℓq(fℓ,i). Since, however, the argument of the non-linearity σ(·) in these equations (see e.g. Eqn. 12) is a linear combi- nation of 1-di"
5582,unknown,"argument is in fact a 1-dimensional Normal random vari- able, and so our analytic results are directly applicable. We simply appeal to the fact that if ai ∼N(µi,σi) and x≡∑ ibiai for some constants {bi}, then x∼N(µ,σ) with µ= ∑ ibiµi and σ2 = ∑ ib2 iσ2 i. 22Since Σ is two-dimensional this decomposition is trivial to compute: L11 = √Σ11, etc. Focus Article Multivariate random forests Mark Segal∗ an"
5583,unknown,"Random forests have emerged as a versatile and highly accurate classiﬁcation and regression methodology, requiring little tuning and providing interpretable outputs. Here, we brieﬂy outline the genesis of, and motivation for, the random forest paradigm as an outgrowth from earlier tree-structured techniques. We elaborate on aspects of prediction error and attendant tuning parameter issues. However"
5584,unknown,"However, our emphasis is on extending the random forest schema to the multiple response setting. We provide a simple illustrative example from ecology that showcases the improved ﬁt and enhanced interpretation afforded by the random forest framework. C⃝ 2011 John Wiley & Sons, Inc.WIREs Data Mining Knowl Discov2011 1 80–87 DOI: 10.1002/widm.12 INTRODUCTION S ince the mid-1980s, tree-structured (or"
5585,unknown,"ince the mid-1980s, tree-structured (or recursive partitioning) classiﬁcation and regression meth- ods have enjoyed widespread popularity. This fol- lowed the publication of the Classiﬁcation and Re- gression Trees (CART) monograph 1 that established a rigorous framework for such techniques, and con- vincingly illustrated one of their greatest virtues: inter- pretability. Tree-structured methods ("
5586,unknown,"interpretable prediction rules by subdividing data into subgroups that are homogenous with respect to both predictors and response. For continuous responses, as considered here, simple (terminal) subgroup sum- maries (typically means) serve as predictions. The in- terpretability of the attendant prediction rules derives from (1) the natural, recursive fashion by which pre- dictors are employed in "
5587,unknown,"accessibility of companion tree diagram schematics, and (3) the availability of predictor importance sum- maries. However, by the mid/late-1990s a serious de- ﬁciency of TSM was evident: modest predictive per- formance, especially in comparison with emerging, ﬂexible competitors such as support vector machines (SVM). 6 In a series of papers, Breiman developed a strategy for remedying this shortcom"
5588,unknown,"ensemble of trees, where each tree in the ensemble is grown in accordance with the realization of a ran- dom vector and obtain predictions by aggregating (voting) over the ensemble. Bagging 2 represents an early example whereby each tree is constructed from a bootstrap 10 sample drawn with replacement from the training data. The simple mechanism whereby bag- ∗Correspondence to: mark@biostat.ucsf.e"
5589,unknown,"Department of Epidemiology and Biostatistics, UCSF DOI: 10.1002/widm.12 ging reduces prediction error for unstable predictors, such as trees, is well understood in terms of variance reduction resulting from averaging.3 Such variance gains can be enhanced by reducing the correlation be- tween the quantities being averaged. It is this principle that motivates random forests (RF). Random forests seek"
5590,unknown,"Random forests seek to effect such correlation reduction by a further injection of randomness. In- stead of determining the optimal subdivision of a given subgroup of a (constituent) tree by evaluat- ing all allowable partitions on all predictors, as is done with single-tree methods or bagging, a subset of the predictors drawn at random, is employed. The size of this subset, designated mtry, is th"
5591,unknown,"tuning parameter for the forest procedure. Breiman 4 argues, on the basis of a comprehensive empiric eval- uation employing numerous benchmark datasets ex- cerpted from the University of California at Irvine (UCI) repository, that RF enjoy exceptional predic- tion accuracy, and that this accuracy is attained for a wide range of settings of the key tuning parameter mtry. Here, we provide a very bri"
5592,unknown,"there now being excellent accounts in the literature. 11 Rather, we detail (1) the extension of regression trees to multivariate response settings, and the related gen- eralization of RF, (2) an underappreciated aspect of RF pertaining to overﬁtting and tuning parameters, and (3) an illustrative example of multivariate RF (MRF). Throughout, we concentrate on regression, rather than classiﬁcation p"
5593,unknown,"MULTIVARIATE REGRESSION TREES The regression tree framework, as developed by Breiman et al.1 involves four components: (1) A set of 80 Volume 1, January/February 2011c⃝ 2011 John Wiley & Sons, Inc. WIREs Data Mining and Knowledge Discovery Multivariate random forests binary (yes/no) questions, or splits, phrased in terms of the predictors that serve to partition the predic- tor space. The subsampl"
5594,unknown,"according to these splits are termed nodes. A node that does not have any descendant nodes is a terminal node or leaf. (2) A node impurity measure, typically relating to response variance in the regression context. (3) A split function,φ(s,t), that can be evaluated for each allowable splits, of each nodet. The best split, which optimizesφ, is such that the response distribu- tions in the resultant"
5595,unknown,"nous among all competing splits, with homogeneity assessed via the impurity measure. (4) A means for determining the appropriate tree size. In the single (univariate) response setting, let y i and xij (i = 1,..., n; j = 1,..., p) designate re- sponse and predictors respectively. Consider a node t containing a sub-sample of cases. We aim to par- tition t into two child nodes, a ‘left’ nodet L, and "
5596,unknown,"L, and a ‘right’ nodetR.L e tj be the index of a continuous or ordered categorical predictor. Then (default) allow- able splits are order-preserving binary cuts of the form t L = i ∈ t : xij ≤ c, tR = i ∈ t : xij > c as the cut-point c ranges over all possible values resulting in distinct tL,tR. For unordered categorical predictors all splits into disjoint subsets of the categories are allowed. Th"
5597,unknown,"L2 node impurity measure is just the sum-of-squares SS (t) = ∑ i∈t(yi − μ(t))2 where μ(t) is the mean ofyi in nodet. Then the corresponding split function is φ(s,t) = SS (t) − SS (tL) − SS (tR). (1) Now consider multiple response data yik (i = 1,..., n; k = 1,..., m). The formulation includes time course and clustered outcomes. In view of anticipated dependencies between the responses, in- terpret"
5598,unknown,"realized by analyzing all responses simultaneously. Examples illustrating attainment of such gains are provided elsewhere. 20 For simplicity, we assume that each individual has the same number of responses (m) and that the predictors are ‘baseline’ variables; i.e., do not vary withk. Segal17 discusses means for removing these restrictions. All that is required to extend regres- sion trees to multi"
5599,unknown,"split function. A natural formulation is to replace the node impurity measure with a ‘covariance’ weighted analog: SS (t) = ∑ i∈t (yi − μ(t))′V−1(t,η )(yi − μ(t)). (2) Here η represents parameters characterizing pre- scribed covariance structures (e.g., auto regressive, compound symmetry). Using (2) a multiresponse split function is created as per (1). The prediction for each leaf of a multirespon"
5600,unknown,"response means for cases reaching that leaf. RANDOM FORESTS An RF, as deﬁned by Breiman,5 is a collection of tree predictors h(x; θk),k = 1,..., K where x represents the observed input (predictor) vector of lengthp with associated random vectorX and the θk are indepen- dent and identically distributed (iid ) random vectors. The observed data is assumed to be independently drawn from the joint dist"
5601,unknown,"prises n (p + 1)-tuples (x1, y1),..., (xn, yn). The RF prediction is the unweighted average over the collec- tion: ¯h(x) = (1/K) ∑K k=1 h(x; θk). As k →∞ the Law of Large Numbers ensures EX,Y(Y − ¯h(X))2 → EX,Y(Y − Eθ h(X; θ))2. (3) The quantity on the right is the prediction (or gen- eralization) error for the RF, designated PE ∗ f .T h e average prediction error for an individual treeh(X; θ) is "
5602,unknown,"is PE ∗ t = Eθ EX,Y(Y − h(X; θ))2. Assume that for allθ the tree is unbiased, i.e.,EY = EXh(X; θ). Then PE ∗ f ≤ ¯ρPE ∗ t (4) where ¯ρ is the weighted correlation between residuals Y − h(X; θ) andY − h(X; θ′) for independentθ,θ′. The inequality (4) pinpoints what is required for accurate RF regression: (1) low correlation be- tween residuals of differing tree members of the for- est, and (2) low p"
5603,unknown,"trees. Further, the RF will, in expectation, decrease the individual tree error,PE ∗ t , by the factor ¯ρ.T h e strategy employed to achieve these objectives is: (1) to keep individual error low, grow trees to maximal depth, and (2) to keep residual correlation low, ran- domize via (a) growing each tree on a bootstrap 10 sample from the training data, and (b) prespecifying mtr y≪ p (the number of "
5604,unknown,"node of every tree, randomly selectmtr ypredictors and pick the best split of that node using only these predictors. Note that limiting step (2) to component (a) or, equivalently, usingmtr y= p reduces the RF procedure to bagging. The use of bootstrap resampling in (2)(a) pro- vides for a built-in mechanism for unbiased estima- tion of PE without requiring recourse to cross vali- dation. Simply, t"
5605,unknown,"each bootstrap sample serve as test data for the tree constructed using that sample. These cases, and the associated PE estimate, are termed out-of-bag (OOB). Volume 1, January/February 2011 81c⃝ 2011 John Wiley & Sons, Inc."
5606,unknown,"Volume 1, January/February 2011 81c⃝ 2011 John Wiley & Sons, Inc. 19424795, 2011, 1, Downloaded from https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.12 by <Shibboleth>-member@dur.ac.uk, Wiley Online Library on [03/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable "
5607,unknown,"Focus Article wires.wiley.com/widm We revisit issues surrounding the ﬁrst part of the prescription: growing trees to maximal depth. This represents a notable departure from the strategy ad- vocated under the original1 (single) tree paradigm. There, while a large tree was initially grown, con- siderable effort was dedicated to iteratively pruning (upward collapsing) this tree. Subsequent selection "
5608,unknown,"a ﬁnal right-sized tree from the set of pruned subtrees made recourse to cross validation based assessment of predictive performance. This emphasis on determin- ing tree size derived from the recognition that overly large trees, although unbiased, would incur a predic- tion variance cost resulting in degraded performance. In the RF context the hope is that by averaging over the (large) ensemble of"
5609,unknown,"nent is reduced. Indeed, the dominant performance of RF in comparative benchmarking studies would support such a notion. However, as we indicate next, the benchmark datasets employed featured idiosyn- crasies that may mislead on this issue. Now consider Figures 1(a) and (b), which dis- play two very distinct cross validatedPE proﬁles. The data used in Figure 1(a) comes from a study of RNA splice s"
5610,unknown,"minimum PE is attained at about 100 splits. The min- imum occurs in a plateau region, after which there is an appreciable rise in error. This increase is such that the PE at the maximal number of splits is ‘sig- niﬁcantly’ greater than the minimumPE; the verti- cal segments (contained within each plotting symbol) represent ±1 standard error. Such proﬁles where, as a function of increasing model si"
5611,unknown,"initially decreases, plateaus, and then increases are common. Indeed, prototypic depictions of the rela- tionship betweenPE and model complexity have this form; see, Ref 1 pp. 87 and Ref 11 pp. 38. The spi- der data, analyzed subsequently via multivariate trees and forests, also exhibits this pattern; see, Figure 2(a). The presence of noise and/or redundant predictors are factors that can contribu"
5612,unknown,"factors that can contribute to such proﬁles, the impact of the latter being discussed in Ref 18. Figure 1(b) differs in that thePE at the maximal number of splits is the global minimum. That is to say, no matter how large a (single) tree-structured predic- tor we ﬁt, we don’t overﬁt the data. This behavior is unusual. The (letter recognition) data used to generate Figure 1(b) were obtained from th"
5613,unknown,"Machine Learning Databases as converted to R, 12 and available from themlbench package. What is remark- able, and seemingly not appreciated, is that almost every dataset in themlbench package exhibits this be- havior. And, it was this compendium that was used in establishing the superior predictive performance of RF under the strategy of growing (individual) trees to maximal depth. Our central con"
5614,unknown,"Our central concern, then, is that this strategy yields maximal trees which may be highly unstable. This instability will be reﬂected in inﬂated prediction errors, and the variance reduction achieved by averag- ing over the ensemble may not sufﬁciently counteract this inﬂation. Precisely, this behavior was observed for the splice site identiﬁcation study. That this behav- ior was not observed in t"
5615,unknown,"RF using the UCI repository is potentially attributable to the above mentioned property of the repository constituents. The R package RF 15 includes a tun- ing parametermaxnodes that can be used to override growing maximal trees. Judiciously setting this pa- rameter is anticipated to be beneﬁcial in large sample size settings, and/or in situations where maximal trees severely overﬁt. Such control "
5616,unknown,"pecially for large sample sizes, than trying to govern tree size by the (related) parameternodesize which determines the minimum size for terminal nodes: pre- scribing maxnodes more readily allows for penaliza- tion of tree complexity whereas not precluding vari- ably sized nodes. Lin and Jeon 16 provide a theoretic perspective that is also contrary to use of maximal trees. MULTIVARIATE RF To cons"
5617,unknown,"To construct MRF, that accommodate multivariate outcomes, we simply generate an ensemble of MRTs via bootstrap resampling and predictors subsampling as for univariate RF. Here we employ MRTs as de- scribed above, but note that other formulations have been advanced. 9,14,21 Under this straightforward ex- tension, we inherit two byproducts of univariate RF useful for enhanced interpretation. Proximi"
5618,unknown,"Proximity Matrix The proximity matrix captures how cases relate to each other, and so provides the underpinnings for (supervised) clustering. For each tree in the ensem- ble, all data (training and OOB) are run down to their assigned terminal node, as dictated by the split sequence. If cases i and j are assigned to the same terminal node, then the proximity value, pv i,j , be- tween i and j is inc"
5619,unknown,"repeated for each tree in the forest, with subsequent normalization by dividing by the number of trees. The proximity matrix is then × n matrix of pv’s, and is symmetric, positive deﬁnite, and bounded by 1. The 82 Volume 1, January/February 2011c⃝ 2011 John Wiley & Sons, Inc."
5620,unknown,"82 Volume 1, January/February 2011c⃝ 2011 John Wiley & Sons, Inc. 19424795, 2011, 1, Downloaded from https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.12 by <Shibboleth>-member@dur.ac.uk, Wiley Online Library on [03/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable "
5621,unknown,WIREs Data Mining and Knowledge Discovery Multivariate random forests (a) Number of splits Number of splits (b) 0 0.0 0.2 0.4 0.6 Cross-validated error 0.8 1.0 1.2 0.0 0.2 0.4 0.6 Cross-validated error 0.8 1.0 1.2 500 1000 1500 2000 0 1000 2000 3000 4000 5000 FIGURE 1 | Cross-validated prediction error proﬁles for trees grown to maximal size on the (a) splice site identiﬁcation data and (b) letter
5622,unknown,"recognition data frommlbench. Volume 1, January/February 2011 83c⃝ 2011 John Wiley & Sons, Inc. 19424795, 2011, 1, Downloaded from https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.12 by <Shibboleth>-member@dur.ac.uk, Wiley Online Library on [03/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles a"
5623,unknown,Focus Article wires.wiley.com/widm X-val relative error 0 2 04 06 08 0 0.4 0.6 0.8 1.0 1.2 No. of trees OOB relative error (a) (b) FIGURE 2 | PE proﬁles for (a) MRT and (b) MRF. (a) Cross-validated errors are plotted against tree size. The vertical bars representPE standard error. The orange spot indicates ﬁnal tree size as selected by the 1-SE rule or overallPE minimum. (b) OOB errors are plotted
5624,unknown,"trees in MRF. The dotted line is the minimum error rate. values 1 minus proximities can be treated as (squared) distances. Variable Importance For each tree in the ensemble, we computePE us- ing OOB cases. We thenpermute each variable (one at a time) and again computePE. The difference be- tween the original and permutedPE s, averaged over all trees, provides a variable importance summary. This ca"
5625,unknown,"This can be used to rank variables and identify those most inﬂuencing prediction. EXAMPLE: CO-OCCURRENCE OF SPIDER SPECIES We use a simple example pertaining to species abun- dance and environmental characteristics7 to illustrate MRF. A much more detailed treatment, pertaining to the cell cycle, is given in Ref 20. Multiple responses are the abundances of 12 spider species, cases are 28 sites, and"
5626,unknown,"tors potentially affecting spider habitat. De’ath 7 used MRT, and developed companion software,8 to pre- dict species co-occurrence based on the predictors. We deﬁne the impurity of a node as the sum of squared Euclidean distances of sites within the node to the node centroid; i.e.,V = I in Ref 2, as this was utilized by De’ath. 7 Typically, either the tree with the mini- mum cross validatedPE, or"
5627,unknown,Water< 5.5 8 ≥sgiwT5.3 <retaW Water ≥ 5.5 8 <sgiwT5.3 ≥retaW arct.lute pard.lugu zora.spin pard.nigr pard.pull aulo.albi troc.terr alop.cune pard.mont alop.acce alop.fabr arct.peri 7 5 88 FIGURE 3 | MRT for the spider data. Terminal nodes are indicated by colored dots. Barplots show average abundances of the 12 species at each terminal node. The number of sites in each terminal nodes is given belo
5628,unknown,"given below the barplots. one standard error of this minimum (1-SE rule)1 is se- lected. Once tree size is determined, the resulting ter- minal nodes can be interpreted as ‘habitats’,7 which are composed of sites that have similar species com- positions and environmental attributes. The application of MRT analysis on the spi- der data, built using the R packagemvpart, yields a 84 Volume 1, January"
5629,unknown,"84 Volume 1, January/February 2011c⃝ 2011 John Wiley & Sons, Inc. 19424795, 2011, 1, Downloaded from https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.12 by <Shibboleth>-member@dur.ac.uk, Wiley Online Library on [03/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable "
5630,unknown,"WIREs Data Mining and Knowledge Discovery Multivariate random forests Water Sand Moss Reft Twigs Herbs Variable importance measure 0 20 40 60 80 100 FIGURE 4 | MRF variable importance measures. tree with four terminal nodes as determined by cross validation using either the 1-SE rule or global PE minimum (of 50%); see, Figure 2(a) where predic- tion error is plotted against tree size. This tree is"
5631,unknown,"ﬁned by two environmental variables,water and twig (Figure 3). The barplot below each terminal node de- picts the species composition. The tree schematic in Figure 3 enables a ready description of habitats in terms of interplay of spider species composition and environmental features and thereby highlights the interpretability of (single) trees. However, considerable caution is needed in drawing s"
5632,unknown,"such interpretations. This is on account of the inher- ent instability of tree-structured predictors, caused in part by the greedy optimization of the split function (1). Consequently, small data changes can produce dramatically different tree topologies and attendant interpretations. Fitting MRTs to bootstrap samples of the original data afﬁrms this by yielding a range of tree sizes and associate"
5633,unknown,"As described, MRF seek to overcome this instability by aggregating over an ensemble of MRTs. Accord- ingly, we apply MRF to the spider data using an en- semble of 300 trees with parametersnodesize and mtry both set to 2. As shown in Figure 2 (B), MRF attain a PE of 38%, a substantial improvement over the 50% achieved by MRT. Interpretation of MRFs is compromised by the inability to simultaneously "
5634,unknown,"the inability to simultaneously view hundreds of tree schematics. However, by making recourse to the byproducts described above interpretability can be regained. Variable importance summaries for the six environmental predictors are presented in Figure 4. Perhaps not surprisingly, water has the highest importance value. The proximity matrix can be used for both clustering and identifying outliers."
5635,unknown,"We apply the partition around medioid (PAM) clus- tering algorithm 13 to group sites into four homoge- nous groups based on the proximity matrix. To visualize the clustering of the sites, we employ met- ric multidimensional scaling to project the data onto a two dimensional space (Figure 5) using the ﬁrst three principal coordinates which together explain 72% of the total variance. The groups are "
5636,unknown,"by convex hulls based on clustering membership by MRF and PAM. Sites 16 and 20, both of which have negative silhouette widths, cannot be clustered with conﬁdence. The derived habitats by MRF and PAM can be described in terms of their deﬁning predictor (a) (b) 2nd s c 1st s c 2nd 3rd sc s c FIGURE 5 | Metric multidimensional scaling of the spider data based on the MRF proximity matrix. Sites are de"
5637,unknown,"and convex hulls indicate PAM cluster membership; the two sites in gray have negative silhouette widths, suggesting low clustering conﬁdence. Letters, A-D, are located at the cluster means. Volume 1, January/February 2011 85c⃝ 2011 John Wiley & Sons, Inc."
5638,unknown,"Volume 1, January/February 2011 85c⃝ 2011 John Wiley & Sons, Inc. 19424795, 2011, 1, Downloaded from https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.12 by <Shibboleth>-member@dur.ac.uk, Wiley Online Library on [03/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable "
5639,unknown,Focus Article wires.wiley.com/widm DCBA Habitat species 0.0 0.5 1.0 1.5 2.0 2.5 3.0 arct.lute pard.lugu zora.spin pard.nigr pard.pull aulo.albi troc.terr alop.cune pard.mont alop.acce alop.fabr arct.peri DCBA Habitat environment 0.0 0.2 0.4 0.6 0.8 1.0 1 Water Twigs Reft Sand Herbs Moss (a) (b) FIGURE 6 | (a) species and (b) environment characteristics of the four habitats. (environmental factors)
5640,unknown,"sitions) variables. We present corresponding barplots for both sets of variables at each habitat in Figure 6. Integrating information from both panels reveals a dynamic relationship between environment and species. Habitat A is characterized by a complete lack of water and twigs, but with a plentitude of reft, moss, sand and herbs, and is populated by three spi- der species:Alopecosa accentuate, A"
5641,unknown,"and Arctosa perita. In contrast, habitat C has abun- dant water and twigs, but has a lack of moss, reft and sand, and it is dominated by the spider species Pardosa lugubris. CONCLUSION Random forests have emerged as a forefront classiﬁ- cation and regression technique, enjoying exceptional accuracy, and enabling interpretative insight for wide classes of problem, all with minimal tuning. Here, we "
5642,unknown,"have illustrated facets of the RF formulation that can beneﬁt from additional tuning: restricting the extent of individual trees in large sample or high noise set- tings. More importantly, we have demonstrated how readily RF can be generalized. The extension we de- tailed, to multiple outcomes, can be used to analyze clustered or longitudinal responses. REFERENCES 1. Breiman L, Friedman JH, Olshen"
5643,unknown,"Classiﬁcation and Regression Trees , Belmont, CA: Wadsworth; 1984. 2. Breiman L. Bagging predictors. Mach Learn 1996, 24:123–140. 3. Breiman L. Arcing classiﬁers.Ann Stat1998, 26:801– 849. 4. Breiman L. Statistical modeling: the two cultures.Stat Sci 2001a, 16:199–215. 5. Breiman L. Random forests.Mach Learn2001b, 45:5– 32. 6. Cristianini N, Shawe-Taylor J.An Introduction to Sup- port Vector Machi"
5644,unknown,"port Vector Machines. Cambridge, Cambridge Univer- sity Press; 2000. 7. De ´ath G. Multivariate regression trees: a new tech- nique for modeling species-environment relationships. Ecology 2002, 83:1005–1117. 8. De ´ath G. mvpart: Multivariate partitioning. R package version 1.3-1. (2010). 9. D ˇzeroski S, ˇZenko B. Stacking with multi-response model trees. In:Multiple Classiﬁer Systems, Proceed- i"
5645,unknown,"ings of the Third International Workshop, pp 201– 211. Berlin: Springer; 2002. 10. Efron B, Tibshirani RJ.An Introduction to the Boot- strap. New York: Chapman & Hall; 1993. 11. Hastie TJ, Tibshirani RJ, Friedman JH.The Elements of Statistical Learning. New York, NY: Springer; 2009. 12. Ihaka R, Gentleman R. R: A language for data analysis and graphics.J Comput Graph Stat1996, 5:299–314. 13. Kaufm"
5646,unknown,"introduction to cluster analysis. New York, NY: John Wiley & Sons; 1990. 14. Kim S-J, Lee K. Constructing decision trees with mul- tiple response variables.International Journal of Man- agement and Decision Making2003, 4:337–353. 86 Volume 1, January/February 2011c⃝ 2011 John Wiley & Sons, Inc."
5647,unknown,"86 Volume 1, January/February 2011c⃝ 2011 John Wiley & Sons, Inc. 19424795, 2011, 1, Downloaded from https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.12 by <Shibboleth>-member@dur.ac.uk, Wiley Online Library on [03/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable "
5648,unknown,"WIREs Data Mining and Knowledge Discovery Multivariate random forests 15. Liaw A, Wiener M. Classiﬁcation and regression by random Forest.RN e w s2002, 2:18–22. 16. Lin Y, Jeon Y. Random forests and adaptive nearest neighbors. J Am Stat Assoc2006, 97:578–590. 17. Segal MR. Tree-structured methods for longitudinal data. J Am Stat Assoc1992, 87:407–418. 18. Segal MR, Barbour JD, Grant RM. Relating H"
5649,unknown,"sequence variation to replication capacity via trees and forests. Stat Appl Genet Mol Biol2004, 3. 19. Segal MR. Prediction of RNA Splice Signals. In: Biswas A, Datta S, Fine J, Segal MR, eds.Statistical Advances in the Biomedical Sciences. New York: John Wiley & Sons; 2008, 443–463. 20. Xiao Y, Segal MR. Identiﬁcation of yeast transcrip- tional regulation networks using multivariate random forest"
5650,unknown,"forests. PLoS Comput Biol2009, 5:e1000414. 21. Zhang H. Classiﬁcation trees for multiple binary re- sponses. J Am Stat Assoc1998, 93:180–193. Volume 1, January/February 2011 87c⃝ 2011 John Wiley & Sons, Inc."
5651,unknown,"Volume 1, January/February 2011 87c⃝ 2011 John Wiley & Sons, Inc. 19424795, 2011, 1, Downloaded from https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.12 by <Shibboleth>-member@dur.ac.uk, Wiley Online Library on [03/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable "
5652,unknown,"Daniel/uni00A0Stockemer Quantitative Methods for the Social Sciences A Practical Introduction with Examples in SPSS and Stata Quantitative Methods for the Social Sciences Daniel Stockemer Quantitative Methods for the Social Sciences A Practical Introduction with Examples in SPSS and Stata Daniel Stockemer University of Ottawa School of Political Studies Ottawa, Ontario, Canada ISBN 978-3-319-99117"
5653,unknown,"ISBN 978-3-319-99117-7 ISBN 978-3-319-99118-4 (eBook) https://doi.org/10.1007/978-3-319-99118-4 Library of Congress Control Number: 2018957702 #Springer International Publishing AG 2019 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, speci ﬁcally the rights of translation, reprinting, reuse of illustrations, recit"
5654,unknown,"broadcasting, reproduction on micro ﬁlms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a speci ﬁc "
5655,unknown,"protective laws and regulations and therefore free for general use. The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omi"
5656,unknown,"claims in published maps and institutional af ﬁliations. This Springer imprint is published by the registered company Springer Nature Switzerland AG The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland Contents 1 Introduction.......................................... 1 2 The Nuts and Bolts of Empirical Social Science ................ 5 2.1 What Is Empirical Research in the "
5657,unknown,"2.2 Qualitative and Quantitative Research . .................. 8 2.3 Theories, Concepts, Variables, and Hypothesis . . ........... 10 2.3.1 Theories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.3.2 Concepts . ................................. 12 2.3.3 Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.3.4 Hypotheses ......................"
5658,unknown,2.4 The Quantitative Research Process . ..................... 18 References . . .......................................... 20 3 A Short Introduction to Survey Research .................... 23 3.1 What Is Survey Research? ............................ 23 3.2 A Short History of Survey Research . . . . . . . . . . . . . . . . . . . . . 24 3.3 The Importance of Survey Research in the Social Sciences a 
5659,unknown,a n dB e y o n d...................................... 26 3.4 Overview of Some of the Most Widely Used Surveys in the Social Sciences . . . ............................ 27 3.4.1 The Comparative Study of Electoral Systems (CSES) . . . 28 3.4.2 The World Values Survey (WVS) . . . . . . . . . . . . . . . . 29 3.4.3 The European Social Survey (ESS) . . . . . . . . . . . . . . . 30 3.5 Different Types 
5660,unknown,3.5 Different Types of Surveys . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3.5.1 Cross-sectional Survey . ....................... 31 3.5.2 Longitudinal Survey . . . . . . . . . . . . . . . . . . . . . . . . . . 32 References . . .......................................... 34 4 Constructing a Survey ................................... 37 4.1 Question Design . . .........................
5661,unknown,4.2 Ordering of Questions ............................... 38 4.3 Number of Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.4 Getting the Questions Right . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.4.1 Vague Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.4.2 Biased or Value-Laden Questions ................ 39 v vi Contents 4.4.3 Th
5662,unknown,v vi Contents 4.4.3 Threatening Questions ........................ 39 4.4.4 Complex Questions .......................... 40 4.4.5 Negative Questions . . . . . . . . . . . . . . . . . . . . . . . . . . 40 4.4.6 Pointless Questions . . . . . . . . . . . . . . . . . . . . . . . . . . 40 4.5 Social Desirability .................................. 41 4.6 Open-Ended and Closed-Ended Questions ...........
5663,unknown,4.7 Types of Closed-Ended Survey Questions . . . .............. 44 4.7.1 Scales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 4.7.2 Dichotomous Survey Questions . ................. 47 4.7.3 Multiple-Choice Questions . . . . . . . . . . . . . . . . . . . . . 47 4.7.4 Numerical Continuous Questions . . . .............. 48 4.7.5 Categorical Survey Questions . . . . . . . 
5664,unknown,4.7.6 Rank-Order Questions . . . . . . . . . . . . . . .......... 49 4.7.7 Matrix Table Questions ....................... 49 4.8 Different Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 4.9 Coding of Different Variables in a Dataset . ............... 51 4.9.1 Coding of Nominal Variables . . ................. 51 4.10 Drafting a Questionnaire: General Information ....
5665,unknown,"4.10.1 Drafting a Questionnaire: A Step-by-Step Approach . . . 53 4.11 Background Information About the Questionnaire . . . ........ 54 References . . .......................................... 55 5 Conducting a Survey .................................... 57 5.1 Population and Sample .............................. 57 5.2 Representative, Random, and Biased Samples .............. 58 5.3 Sampling Err"
5666,unknown,5.3 Sampling Error . . .................................. 62 5.4 Non-random Sampling Techniques . ..................... 62 5.5 Different Types of Surveys . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 5.6 Which Type of Survey Should Researchers Use? .......... . 67 5.7 Pre-tests . . . ...................................... 67 5.7.1 What Is a Pre-test? . . . . . . ..................... 67
5667,unknown,5.7.2 How to Conduct a Pre-test? . .................... 69 References . . .......................................... 69 6 Univariate Statistics .................................... 73 6.1 SPSS and Stata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.2 Putting Data into an SPSS Spreadsheet . . ................. 73 6.3 Putting Data into a Stata Spreadsheet . . . . . . .
5668,unknown,"6.3 Putting Data into a Stata Spreadsheet . . . . . . . . . . . . . . . . . . . . 75 6.4 Frequency Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 6.4.1 Constructing a Frequency Table in SPSS . . . . . . . . . . . 77 6.4.2 Constructing a Frequency Table in Stata . . . . . . . . . . . 78 6.5 The Measures of Central Tendency: Mean, Median, Mode, a n dR a n g e.........."
5669,unknown,"6.6 D isplaying Data Graphically: Pie Charts, Boxplots, and Histograms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 Contents vii 6.6.1 Pie Charts ................................. 80 6.6.2 Doing a Pie Chart in SPSS . . . . . . . . . . . . . . . . . . . . . 82 6.6.3 Doing a Pie Chart in Stata ...................... 83 6.7 Boxplots . . . .........................."
5670,unknown,"6.7.1 Doing a Boxplot in SPSS . . . . . . . . . . . . . . . . . . . . . . 86 6.7.2 Doing a Boxplot in Stata . . ..................... 86 6.8 Histograms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 6.8.1 Doing a Histogram in SPSS . ................... 88 6.8.2 Doing a Histogram in Stata ..................... 90 6.9 Deviation, Variance, Standard Deviation, Standar"
5671,unknown,"Sampling Error, and Con ﬁdence Interval . . . . . . . . . . . . . . . . . 91 6.9.1 Calculating the Con ﬁdence Interval in SPSS . . . .... . 95 6.9.2 Calculating the Con ﬁdence Interval in Stata . . ....... 96 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 7 Bivariate Statistics with Categorical Variables ................ 1 01 7.1 Independent Samp"
5672,unknown,7.1 Independent Sample t -Test . . .......................... 1 01 7.1.1 Doing an Independent Samples t -Test in SPSS . ...... 1 04 7.1.2 Interpreting an Independent Samples t -Test SPSS Output ............................... 1 06 7.1.3 Reading an SPSS Independent Samples t -Test Output Column by Column . . . . . . . . . . . . . . . . . . . . . . . . . . 107 7.1.4 Doing an Independent Samples t -T
5673,unknown,7.1.5 Interpreting an Independent Samples t -Test Stata O u t p u t.................................... 1 09 7.1.6 Reporting the Results of an Independent Samplest -Test .............................. 1 11 7.2F -Test or One-Way ANOVA . . . . . . . . . . . . . . . . . . . . . . . . . . 111 7.2.1 Doing an f -Test in SPSS ....................... 1 13 7.2.2 Interpreting an SPSS ANOVA Output . . . . . 
5674,unknown,7.2.3 Doing a Post hoc or Multiple Comparison Test in SPSS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 7.2.4 Doing an f -Test in Stata . ...................... 1 19 7.2.5 Interpreting an f -Test in Stata ................... 1 20 7.2.6 Doing a Post hoc or Multiple Comparison Test with Unequal Variance in Stata . . . ............... 1 21 7.2.7 Reporting the Results of a
5675,unknown,7.2.7 Reporting the Results of an f -Test . . . . . . . ......... 1 24 7.3 Cross-tabulation Table and Chi-Square Test ............... 1 25 7.3.1 Cross-tabulation Table . . . . . . . . . . . . . . . . . . . . . . . . 125 7.3.2 Chi-Square Test ............................. 1 26 7.3.3 Doing a Chi-Square Test in SPSS . . . . . . . . . . . . . . . . 127 7.3.4 Interpreting an SPSS Chi-Square Test ......
5676,unknown,7.3.5 Doing a Chi-Square Test in Stata ................. 1 30 7.3.6 Reporting a Chi-Square Test Result . . . . . . . . . . . . . . . 131 Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131 viii Contents 8 Bivariate Relationships Featuring Two Continuous Variables ..... 1 33 8.1 What Is a Bivariate Relationship Between Two Continuous Variables? ..
5677,unknown,Variables? ....................................... 1 33 8.1.1 Positive and Negative Relationships . . . . . . . . . . . . . . 133 8.2 Scatterplots ....................................... 1 34 8.2.1 Positive Relationships Displayed in a Scatterplot . . . . . 134 8.2.2 Negative Relationships Displayed in a Scatterplot . . . . . 134 8.2.3 No Relationship Displayed in a Scatterplot .......... 1 35 8.3
5678,unknown,8.3 Drawing the Line in a Scatterplot . ...................... 1 36 8.4 Doing Scatterplots in SPSS ........................... 1 36 8.5 Doing Scatterplots in Stata ............................ 1 39 8.6 Correlation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 8.6.1 Doing a Correlation Analysis in SPSS ............. 1 44 8.6.2 Interpreting an SPSS Correlation Output .
5679,unknown,8.6.3 Doing a Correlation Analysis in Stata . . . . . . . . . . . . . 147 8.7 Bivariate Regression Analysis . . . ...................... 1 48 8.7.1 Gauging the Steepness of a Regression Line . . . . ... . 1 48 8.7.2 Gauging the Error Term . . ..................... 1 50 8.8 Doing a Bivariate Regression Analysis in SPSS . ........... 1 52 8.9 Interpreting an SPSS (Bivariate) Regression Output ......
5680,unknown,8.9.1 The Model Summary Table . . . .................. 1 53 8.9.2 The Regression ANOVA Table .................. 1 54 8.9.3 The Regression Coef ﬁcient Table . . . . . . . . . . . . . . . . 155 8.10 Doing a (Bivariate) Regression Analysis in Stata . . . . . . . . . . . . 156 8.10.1 Interpreting a Stata (Bivariate) Regression Output . . . . 157 8.10.2 Reporting and Interpreting the Results of a Bivar
5681,unknown,Regression Model . .......................... 1 60 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 9 Multivariate Regression Analysis .......................... 1 63 9.1 The Logic Behind Multivariate Regression Analysis . . . . . . . . . 163 9.2 The Functional Forms of Independent Variables to Include in a Multivariate Regression Model ..........
5682,unknown,9.3 Interpretation Help for a Multivariate Regression Model . . . . . . 166 9.4 Doing a Multiple Regression Model in SPSS . ............. 1 66 9.5 Interpreting a Multiple Regression Model in SPSS . . . ...... . 1 66 9.6 Doing a Multiple Regression Model in Stata . . . . . . . . . . . . . . . 168 9.7 Interpreting a Multiple Regression Model in Stata . . . . . . ..... 1 68 9.8 Reporting the Results 
5683,unknown,9.8 Reporting the Results of a Multiple Regression Analysis . . .... 1 70 9.9 Finding the Best Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170 9.10 Assumptions of the Classical Linear Regression Model or Ordinary Least Square Regression Model (OLS) . . . . . ....... 1 71 Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174 Co
5684,unknown,Contents ix Appendix 1: The Data of the Sample Questionnaire ................ 1 7 5 Appendix 2: Possible Group Assignments That Go with This Course . . . 177 Index.. ... .............................................. 1 7 9 Introduction 1 Under what conditions do countries go to war? What is the in ﬂuence of the 2008–2009 economic crisis on the vote share of radical right-wing parties in Western Eu
5685,unknown,"Europe? What type of people are the most likely to protest and partake in demonstrations? How has the urban squatters ’movement developed in South Africa after apartheid? There is hardly any ﬁ eld in the social sciences that asks as many research questions as political science. Questions scholars are interested in can be speci ﬁc and reduced to one event (e.g., the development of the urban squatte"
5686,unknown,"squatter’s movement in South Africa post-apartheid) or general and systemic such as the occurrence of war and peace. Whether general or speci ﬁc, what all empirical research questions have in common is the necessity to use adequate research methods to answer them. For example, to effectively evaluate the in ﬂuence of the economic downturn in 2008 –2009 on the radical right-wing success in the elec"
5687,unknown,"downturn in 2008 –2009 on the radical right-wing success in the elections preceding the crisis, we need data on the radical right-wing vote before and after the crisis, a clearly de ﬁned operationalization of the crisis and data on confounding factors such as immigration, crime, and corruption. Through appropriate modeling techniques (i.e., multiple regression analysis on macro-level data), we can"
5688,unknown,"(i.e., multiple regression analysis on macro-level data), we can then assess the absolute and relative in ﬂuence of the economic crisis on the radical right-wing vote share. Research methods are the “ bread and butter ”of empirical political science. They are the tools that allow researchers to conduct research and detect empirical regularities, causal chains, and explanations of political and soc"
5689,unknown,"regularities, causal chains, and explanations of political and social phenomena. To use a practical analogy, a political scientist needs to have a toolkit of research methods at his or her disposal to build good empirical research in the same way as a mason must have certain tools to build a house. It is indispensable for a mason to not only have some rather simple tools (e.g., a hammer) but also "
5690,unknown,"sophisticated tools such as a mixer or crane. The same applies for a political scientist. Ideally, he or she should have some easy tools (such as descriptive statistics or means testing) at his or her disposal but also some more complex tools such as pooled time series analysis or maximum likelihood estimation. Having these tools allows # Springer International Publishing AG 2019 D. Stockemer, Qua"
5691,unknown,"https://doi.org/10.1007/978-3-319-99118-4_1 1 political scientists to both conduct their own research and judge and evaluate other peoples’work. This book will provide a ﬁ rst simple toolkit in the area of quantitative methods, survey research, and statistics. 2 1 Introduction There is one caveat in methods training: research methods can hardly be learnt by just reading articles and books. Rather,"
5692,unknown,"just reading articles and books. Rather, they need to be learnt in an applied fashion. Similar to the mixture of theoretical and practical training a mason acquires during her apprenticeship, political science students should be introduced to methods ’ training in a practical manner. In particular, this applies to quantitative methods and survey research. Aware that methods learning can only be fr"
5693,unknown,"and survey research. Aware that methods learning can only be fruitful if students learn to apply their theoretical skills in real-world scenarios, I have constructed this book on survey research and quantitative methods in a very practical fashion. Through my own experience as a professor of introductory courses into quantita- tive method, I have learnt over and over again that students only enjoy"
5694,unknown,"if they see the applicability of the techniques they learn. This book follows the structure as laid down in Fig. 1.1; it is structured so that students learn various statistical techniques while using their own data. It does not require students to have taken prior methods classes. To lay some theoretical groundwork, the ﬁ rst chapter starts with an introduction into the nuts and bolts of empirica"
5695,unknown,"starts with an introduction into the nuts and bolts of empirical social sciences (see Chap. 2). The book then shortly introduces students to the nuts and bolts of survey research (see Chap. 3). The following chapter then very brie ﬂy teaches students how they can construct and administer their own survey. At the end of Chap. 4, students also learn how to construct their own questionnaire. The ﬁ ft"
5696,unknown,"also learn how to construct their own questionnaire. The ﬁ fth chapter, entitled “Conducting a Survey, ”instructs students on how to conduct a survey in the ﬁ eld. During this chapter, groups of students test their survey in an empirical setting by soliciting answers from peers. Chapters 6 to 9 are then dedicated to analyzing the survey. In more detail, students learn how to input their responses "
5697,unknown,"SPSS or STATA dataset in the ﬁ rst part of Chap. 6. The second part covers univariate statistics and graphical representations of the data. In Chap. 7, I introduce different forms of means testing, and Chap. 8 is then dedicated to bivariate correla- tion and regression analysis. Finally, Chap. 9 covers multivariate regression analysis). The book can be used as a self-teaching device. In this case,"
5698,unknown,"analysis). The book can be used as a self-teaching device. In this case, students should redo the exercises with the data provided. In a second step, they should conduct all the tests with other data they have at their disposal. The book is also the perfect accompanying textbook for an introductory class to survey research and statistics. In the latter case, there is a built-in semester-long group"
5699,unknown,"In the latter case, there is a built-in semester-long group exercise, which enhances the learning process. In the semester-long group work that follows the sequence of the book, students are asked to conceive, conduct, and analyze survey. The survey that is analyzed throughout is a colloquial survey that measures the amount of money students spend partying. Actually, the survey is an original surv"
5700,unknown,"original data, which one of my student groups collected during their semester-long project. Using this “ colloquial”survey, the students in this study group had lots of fun collecting and analyzing their data, showing that learning statistics can (and should) be fun. I hope that the readers and users of this book experience the same joy in their ﬁ rst encounter with quantitative methods. 1 Introdu"
5701,unknown,1 Introduction 3 Step 1 Step 2 Step 3: Step 4: Step 5: Step 6: Determine the purpose and the design of the study. Define/select the questions Decide upon the population and sample Pre-test the questionnaire Conduct the survey Analyze the data Report the results Constructing a Survey Conducting a Survey Analyzing a Survey Fig. 1.1 Different steps in survey research The Nuts and Bolts of Empirical S
5702,unknown,"The Nuts and Bolts of Empirical Social Science 2 Abstract This chapte r covers the nuts and bolts of empirical political science. It gives an introduction into empirical research in the social sciences and statistics; explains the notion of concepts, theories, and hypotheses; as well as introduces students to the different steps in the quantitative research process. 2.1 What Is Empirical Research "
5703,unknown,"2.1 What Is Empirical Research in the Social Sciences? Regardless of the social science sub-discipline, empirical research in the social sciences tries to decipher how the world works around us. Be it development studies, economics, sociology, political science, or geography, just to name a few disciplines, researchers try to explain how some part of how the world is structured. For example, polit"
5704,unknown,"example, political scientists try to answer why some people vote, while others abstain from casting a ballot. Scholars in developmental studies might look at the inﬂuence of foreign aid on economic growth in the receiving country. Researchers in theﬁ eld of education studies might examine how the size of a school class impacts the learning outcomes of high school students, and economists might be "
5705,unknown,"the effect of raising the minimum wage on job growth. Regardless of the discipline they are in, social science researchers try to explain the behavior of individuals such as voters, protesters, and students; the behavior of groups such as political parties, companies, or social movement organizations; or the behavior of macro-level units such as countries. While the tools taught in this book are a"
5706,unknown,"While the tools taught in this book are applicable to all social science disciplines, I mainly cover examples from empirical political science, because this is the discipline in which I teach and research. In all social sciences and in political science, more generally, knowledge acquisition can be both normative and empirical. Nor- mative political science asks the question of how the world ought"
5707,unknown,"mative political science asks the question of how the world ought to be. For example, normative democratic theorists quibble with the question of what a democracy ought # Springer International Publishing AG 2019 D. Stockemer, Quantitative Methods for the Social Sciences , https://doi.org/10.1007/978-3-319-99118-4_2 5 to be. Is it an entity that allows free, fair, and regular elections, which, in "
5708,unknown,"democracy literature, is referred to as the “ minimum de ﬁnition of democracy ” (Bogaards 2007)? Or must a country, in addition to having a fair electoral process, grant a variety of political rights (e.g., freedom of religion, freedom of assembly), social rights (e.g., the right to health care and housing), and economic rights (e.g., the right to education or housing) to be “ truly”democratic? Th"
5709,unknown,"right to education or housing) to be “ truly”democratic? This more encompassing deﬁnition is currently referred to in the literature as the “ maximum de ﬁnition of democracy”(Beetham 1999). While normative and empirically oriented research have fundamentally different goals, they are nevertheless complementary. To high- light, an empirical democracy researcher must have a benchmark when she de ﬁne"
5710,unknown,"and codes a country as a democracy or nondemocracy. This benchmark can only be established through normative means. Normative political science must establish the “gold standard ”against which empirically oriented political scientists can empiri- cally test whether a country is a democracy or not. 6 2 The Nuts and Bolts of Empirical Social Science As such, empirical poli tical science is less inte"
5711,unknown,"As such, empirical poli tical science is less interested in what a democracy should be, but rather how a democracy behaves in the real world. For instance, an empirical researcher could ask the following questions: Do democracies have more women ’s representation in parliament than nondemocracies? Do democracies have less mili- tary spending than autocracies or hybrid regimes? Is the history curri"
5712,unknown,"schools different in democracies than in other regimes? Does a democracy spend more on social services than an autocracy? Answering these questions requires observation and empirical data. Whether it is collected at the individual level through interviews or surveys, at the meso-level through, for example, membership data of parties or social movements, or at the macro level through government/int"
5713,unknown,"parties or social movements, or at the macro level through government/international agencies or statistical of ﬁces, the collected data should be of high quality. Ideally, the measurement and data collection process of any study should be clearly laid down by the researcher, so that others can replicate the same study. After all, it is our goal to gain intersubjective knowledge. Intersubjective me"
5714,unknown,"gain intersubjective knowledge. Intersubjective means that if two individuals would engage in the same data collection process and would conduct the same empirical study, their results would be analogous. To be as intersubjective or “ facts based ”as possible, empirical political science should abide by the following criteria: FalsiﬁabilityThe falsi ﬁability paradigm implies that statements or hyp"
5715,unknown,"be proven or refuted. For example, the statement that democracies do not go to war with each other can be tested empirically. After de ﬁning what war and democracy is, we can get data that ﬁ ts our de ﬁnition for a country ’s regime type from a trusted source like the Polity IV data verse and data for con ﬂict/war from another high- quality source such as the UCDP/PRIO Armed Con ﬂictdataset. In se"
5716,unknown,"can then use statistics to test whether the statement that democracies refrain from engaging in warfare with each other is true or not. 1,2 2.1 What Is Empirical Research in the Social Sciences? 7 TransmissibilityThe process through which the transmissibility of research ﬁndings is achieved is called replication. Replication refers to the process by which prior ﬁ ndings can be retested. Retesting "
5717,unknown,"which prior ﬁ ndings can be retested. Retesting can involve either the same data or new data from the same empirical referents. For instance, the “ law-like”statement that democracies do not go to war with each other could be retested every 5 years with the most recent data from Polity IV and the UCDP/PRIO Armed Con ﬂictdataset covering these 5 years to see if it still holds. Replication involves "
5718,unknown,"covering these 5 years to see if it still holds. Replication involves high scienti ﬁc standards; it is only possible to replicate a study if the data collection, the data source, and the analytical tools are clearly explained and laid down in any piece of research. The replicator should then also use these same data and methods for her replication study. Cumulative Nature of Knowledge Empirical sc"
5719,unknown,"replication study. Cumulative Nature of Knowledge Empirical scienti ﬁc knowledge is cumulative. This entails that substantive ﬁ ndings and research methods are based upon prior knowledge. In short, researchers do not start from scratch or intuition when engaging in a research project. Rather, they try to con ﬁrm, amend, broaden, or build upon prior research and knowledge. For example, the statemen"
5720,unknown,"research and knowledge. For example, the statement that democracies avoid war with each other had been con ﬁrmed and recon ﬁrmed many times in the 1980s, 1990s, and 2000s (see Russett 1994; De Mesquita et al. 1999). After con ﬁrming that theDemocratic Peace Theory in its initial form is solid, researchers tried to broaden the democratic peace paradigm and examined, for example, if countries that s"
5721,unknown,"the same economic system (e.g., neoliberalism) also do not go to war with each other. Yet, for the latter relationship, tests and retests have shown that the empirical linkage for the economic system ’s peace is less strong than the democratic peace statement (Chandler 2010). The same applies to another possible expansion, which looks at if democracies, in general, are less likely to go to war tha"
5722,unknown,"looks at if democracies, in general, are less likely to go to war than nondemocracies. Here again the empirical evidence is negative or inconclusive at best (Daase 2006; Mansﬁeld and Snyder 2007). GeneralizabilityIn empirical social science, we are interested in general rather than speciﬁc explanations; we are interested in boundaries or limitations of empirical statements. Does an empirical state"
5723,unknown,"statements. Does an empirical statement only apply to a single case (e.g., does it only explain why the United States and Canada have never gone to war), or can it be generalized to explain many cases (e.g., does it explain why all pairs of democracies don’t go to war?) In other words, if it can be generalized, does the democratic peace 1The Polity IV database adheres to rather minimal de ﬁnition "
5724,unknown,"1The Polity IV database adheres to rather minimal de ﬁnition of democracy. In essence, the database gauges the fairness and competitiveness of the elections and the electoral process on a scale from –10 to +10. – 10 describes the “ worst”autocracy, while 10 describes a country that fully respects free, fair, and competitive elections (Marshall et al. 2011). 2The UCDP/PRIO Armed Con ﬂict Dataset de"
5725,unknown,"2The UCDP/PRIO Armed Con ﬂict Dataset de ﬁnes minor wars by a death toll between 25 and 1000 people and major wars by a death toll of 1000 people and above (see Gleditsch 2002). paradigm apply to all democracies, or only to neoliberal democracies, and does it apply across all (normative) de ﬁnitions of democracies, as well as all time periods?). Stated differently, we are interested in the number "
5726,unknown,"Stated differently, we are interested in the number of cases in which the statement is applicable. Of course, the broader the applicability of an explanation, the more weight it carries. In political science the Democratic Peace Theory is among the theories with the broadest applicability. While there a re some questionable cases of conﬂict between states such as the con ﬂict between Turkey and Gr"
5727,unknown,"conﬂict between states such as the con ﬂict between Turkey and Greece over Cyprus in 1974, there has, so far, been no case that clearly disproves the Democratic Peace Theory. In fact, the Democratic Peace Theory is one of the few law-like rules in political science. 8 2 The Nuts and Bolts of Empirical Social Science 2.2 Qualitative and Quantitative Research In the social sciences, we distinguish t"
5728,unknown,"In the social sciences, we distinguish two large strands of research: quantitative and qualitative research. The major difference between these two research traditions is the number of observations. Research that involves few observations (e.g., one, two, or three individuals or countries) is generally referred to as qualitative. Such research requires an in-depth examination of the cases at hand."
5729,unknown,"requires an in-depth examination of the cases at hand. In contrast, work that includes hundreds, thousands, or even hundred thousand observations is generally called quantitative research. Quantitative research works with statistics or numbers that allow researchers to quantify the world. In the twenty- ﬁrst century, statistics are nearly everywhere. In our daily lives, we encounter statistics in "
5730,unknown,"nearly everywhere. In our daily lives, we encounter statistics in approval ratings of TV shows, the measurement of consumer preferences, weather forecasts, and betting odds, just to name a few examples. In social and political science research, statistics are the bread and butter of much scienti ﬁc inquiry; statistics help us make sense of the world around us. For instance, in the political realm,"
5731,unknown,"the world around us. For instance, in the political realm, we might gauge turnout rates as a measurement of the percentage of citizens that turned out during an election. In economics, some of the most important indicators about the state of the economy are monthly growth rates and consumer price indexes. In the ﬁ eld of education, the average grade of a student from a speci ﬁc school gives an ind"
5732,unknown,"of the school ’s quality. By using statistics, quantitative methods not only allow us to numerically describe phenomena, they also help us determine relationships between two or more variables. Examples of these relationships are multifold. For example, in the ﬁeld of political science, statistics and quantitative methods have allowed us to detect that citizens who have a higher socioeconomic stat"
5733,unknown,"detect that citizens who have a higher socioeconomic status (SES) are more likely to vote than individuals with a lower socioeconomic status (Milligan et al. 2004). In theﬁ eld of economics, researchers have established with the help of quantitative analysis that low levels of corruption foster economic growth (Mo 2001). And in education research, there is near consensus in the quantitative resear"
5734,unknown,"students from racially segregated areas and poor inner-city schools, on average, perform less strongly in college entry exams than students from rich, white neighborhoods (Rumberger and Palardy 2005). 2.2 Qualitative and Quantitative Research 9 Quantitative research is the primary tool to establish empirical relationships. However, it is less well-suited to explain the constituents or causal mecha"
5735,unknown,"behind a statistical relationship. To highlight, quantitative research can illustrate that individuals with low education levels and below average income are less likely to vote compared to highly educated and rich citizens. Yet, it is less suitable to explain the reasons for their abstentions. Do they not feel represented? Are they fed up with how the system works? Do they not have the informatio"
5736,unknown,"up with how the system works? Do they not have the information and knowledge necessary to vote? Similarly, quantitative research robustly tells us that students in racially segregated areas tend to perform less strongly than students in predomi- nantly white and wealthy neighborhoods. However, it does not tell us how the disadvantaged students feel about these inequalities and what they think can "
5737,unknown,"done to reverse them. Are they enraged or fed up with the political regime and the politicians that represent it? Questions like these are better answered by qualitative research. The qualitative researcher wants to interpret the observational data (i.e., the fact that low SES individual has a higher likelihood to vote) and wants to grasp the opinions and attitudes of study subjects (i.e., how min"
5738,unknown,"opinions and attitudes of study subjects (i.e., how minority students feel in dis- advantaged areas, how they think the system perpetuates these inequalities, and under what circumstances they are ready to protest). To gather this in-depth infor- mation, the qualitative researcher uses different techniques than the quantitative researchers. She needs research tools to tap into the opinions, percep"
5739,unknown,"feelings of study subjects. Tools appropriate for these inquiries are ethnographic methods including qualitative interviewing, participant observations, and the study of focus groups. These tools help us understand how individuals live, act, think, and feel in their natural setting and give meaning to quantitative ﬁ ndings. In addition to allowing us to decipher meaning behind quantitative relatio"
5740,unknown,"qualitative research techniques are an important tool in theory building. In fact, many researchﬁ ndings originate in qualitative research and are tested in a later stage in a quantitative large-N study. To take a classic in social sciences, Theda Skocpol offers in her seminal work States and Social Revolutions: A comparative Analysis of Social Revolutions in Russia, France and China ( 1979), an e"
5741,unknown,"Revolutions in Russia, France and China ( 1979), an explanation for the occurrence of three important revolutions in the modern world, the French Revolution in 1789, the Chinese Revolution in 1911, and the Russian Revolution in 1917. Through historical analysis, Skocpol identi ﬁes three conditions for a revolution to happen: (1) a profound state crisis, (2) the emergence of a dominant class outsid"
5742,unknown,"(1) a profound state crisis, (2) the emergence of a dominant class outside of the ruling elites, and (3) a state of severe economic and/or security crisis. Skocpol ’s book is an important exercise in theory building. She identi ﬁes three causal conditions, conditions that are quanti ﬁable and that can be tested for other or all revolutions. By testing whether a profound state crisis, the emergence"
5743,unknown,"By testing whether a profound state crisis, the emergence of a dominant class outside of the ruling elites, or a state of crisis explains other or all revolutions, quantitative researchers can establish the boundary conditions of Skocpol ’s theory. It i s a lso important to note that not all research is quanti ﬁable. Some phenomena such as individual identities or ideologies are dif ﬁcult to reduc"
5744,unknown,"such as individual identities or ideologies are dif ﬁcult to reduce to numbers: What are ethnic identities, religious identities, or regional identities? Often these critical concepts are not only dif ﬁcult to identify but frequently also dif ﬁcult to grasp empirically. For example, to understand what the regional francophone identity of Quebecers is, we need to know the historical, social, and po"
5745,unknown,"Quebecers is, we need to know the historical, social, and political context of the province and the fact that the province is surrounded by English speakers. To get a complete grasp of this regional identity, we, ideally, also have to retrace the recent development that more and more English is spoken in the major cities of Québec such as Montréal, particularly in the business world. These complex"
5746,unknown,"such as Montréal, particularly in the business world. These complexities are hard to reduce to numbers and need to be studied in-depth. For other ev ents, there are just not enough observations to quantify them. For example, the Cold War is a unique event, an event that organized and shaped the world for 45 years in the twentieth century. Nearly, by de ﬁnition this even is important and needs to b"
5747,unknown,"Other events, like World War I and World War II, are for sure a subset of wars. However, these two wars have been so important for world history that, nearly by deﬁnition, they require in-depth study, as well. Both w ars have shaped who we as individuals are (regardless where we live), what we think, how we act, and what we do. Hence, any bits of additional knowledge we acquire from these events n"
5748,unknown,"help us understand the past but also help us move forward in the future. 10 2 The Nuts and Bolts of Empirical Social Science Quantitative and qualitative methods are complimentary ; students of the social sciences should master both techniques. However, it is hardly possible to do a thorough introduction into both. This book is about survey research, quantitative research tools, and statistics. It"
5749,unknown,"research tools, and statistics. It will teach you how to draft, conduct, and analyze a survey. However, before delving into the nuts and bolts of data analysis, we need to know what theories, hypotheses, concepts, and variables are. The next section will give you a short overview of these building blocks in social research. 2.3 Theories, Concepts, Variables, and Hypothesis 2.3.1 Theories We have a"
5750,unknown,"2.3.1 Theories We have already learnt that social science research is cumulative. We build current knowledge on prior knowledge. Normally, we summarize our prior knowledge in theories, which are parsimonious or simpli ﬁed explanations of how the world works. As such, a theory summarizes established knowledge in a speci ﬁcﬁ eld of study. Because the world around us is dynamic, a theory in the socia"
5751,unknown,"Because the world around us is dynamic, a theory in the social sciences is never a deterministic statement. Rather it is open to revisions and amendments. 3 Theories can cover the micro-, meso-, and macro-levels. Below are three powerful social sciences theories. Example of a Microlevel Theory: Relative Deprivation Relative deprivation is a powerful individual-level theory to explain and predict c"
5752,unknown,"citizens’participation in social movement activities. Relative deprivation starts with the premise that individuals do not protest, when they are happy with their lives. 3The idea behind parsimony is that scientists should rely on as few explanatory factors as possible while retaining a theory ’s generalizability. Rather grievance theorists (e.g., Gurr 1970; Runciman 1966) see a discrepancy betwee"
5753,unknown,"between value expectation and value capabilities as the root cause for protest activity. For example, according to Gurr ( 1970), individuals normally have no incentive to protest and voice their dissatisfaction if they are content with their daily lives. However, a deteriorating economic, social, or political situation can trigger frustrations, whether or real or perceived; the higher these frustr"
5754,unknown,"higher the likelihood that somebody will protest. 2.3 Theories, Concepts, Variables, and Hypothesis 11 Example of a Meso-level Theory: The Iron Law of Oligarchy The iron law of oligarchy is a political meso-level theory developed by German sociologist Robert Michels. His main argument is that over time all social groups, including trade unions and political parties, will develop hierarchical power"
5755,unknown,"structures or oligarchic tendencies. Stated differently, in any organization a “ leader- ship class ”consisting of paid administrators, spokespersons, societal elites, and organizers will prevail and centralize its power. And with power comes the possi- bility to control the laws and procedures of the organization and the information it communicates as well as the possibility to reward faithful me"
5756,unknown,"communicates as well as the possibility to reward faithful members; all these tendencies are accelerated by apathetic masses, which will allow elites to hierarchize an organization faster (see Michels 1915). Example of a Macro-level Theory: The Democratic Peace Theory As discussed earlier in this chapter, a famous example of a macro-level theory is the so-called Democratic Peace Theory, which date"
5757,unknown,"so-called Democratic Peace Theory, which dates back to Kant ’s treatise on Perpetual Peace ( 1795). The theory states that democracies will not go to war with each other. It explicitly tackles the behavior of some type of state (i.e., democracies) and has only applicability at the macro-level. Theory d evelopment is an iterative process. Because the world around us is dynamic (what is true today m"
5758,unknown,"dynamic (what is true today might no longer be true tomorrow), a theory must be perpetually tested and retested against reality. The more it is con ﬁrmed across time and space, the more it is robust. Theory building is a reiterative and lengthy process. Sometimes it takes years, if not decades to build and construct a theory. A famous example of how a theory can develop and re ﬁne is the simple ra"
5759,unknown,"of voting. In his 1957 famous book, An Economic Theory of Democracy , Anthony Downs tries to explain why some people vote, whereas others abstain from casting their ballots. Using a simple rational choice explanation, he concludes that voting is a “rational act”if the bene ﬁts of voting surpass the costs. To operationalize his theory, he de ﬁnes the bene ﬁts of voting by the probability that an in"
5760,unknown,"he de ﬁnes the bene ﬁts of voting by the probability that an individual vote counts. The costs include the physical costs of actually leaving one ’s house and casting a ballot, as well as the ideational costs of gathering the necessary information to cast an educated ballot. While Downs ﬁ nds his theory logical, he intuitively ﬁ nds that there is something wrong with it. That is, the theory would "
5761,unknown,"there is something wrong with it. That is, the theory would predict that in the overall majority of cases, citizens should not vote, because in almost every case, the probability that an individual ’s vote will count is close to 0. Hence, the costs of voting surpass the bene ﬁts of voting for nearly every individual. However, Downs ﬁnds that in the majority people still vote, but does not have an "
5762,unknown,"this paradox of voting. 12 2 The Nuts and Bolts of Empirical Social Science More than 10 years late r, in a reformulation of Downs ’theory, Riker and Ordeshook ( 1968) resolve Downs ’paradox by adding an important component to Downs’model: the intangible bene ﬁts. According to the authors, the bene ﬁts of voting are not reduced to pure materialistic evaluations (i.e., the chance that a person’s vo"
5763,unknown,"person’s vote counts) but also to some nonmaterialistic bene ﬁts such as citizens ’ willingness to support democracy or the democratic system. Adding this additional component makes Down ’s theory more realistic and in tune with reality. On the negative side, adding nonmaterial bene ﬁts makes Down ’s theory less parsimonious. However, all researchers would probably agree that this sacri ﬁce of par"
5764,unknown,"more than compensated for by the higher empirical applicability of the theory. Therefore, in this case the more complex theory is preferential to the more parsi- monious theory. More generally, a theory should be as simple or parsimonious as possible and as complex as necessary. 2.3.2 Concepts Theories are abstractions of objects, objects ’properties, or behavioral phenomena. Any theory normally c"
5765,unknown,"Any theory normally consists of at least two concepts, which de ﬁne a theory ’s content and attributes. For example, the Democratic Peace Theory consists of the two concepts: democracy and war. Some concepts are concise (e.g., wealth, edu- cation, women ’s representation) and easier to measure, whereas other concepts are abstract (democracy, equal opportunity, human rights, social mobility, politi"
5766,unknown,"culture) and more dif ﬁcult to gauge. Whether abstract or precise, concepts provide a common language for political science. For sure, researchers might disagree about the precise (normative) de ﬁnition of a concept. Nevertheless, they agree about its meaning. For example, if we talk about democracy, there is common understanding that we talk about a speci ﬁc regime type that allows free and fair "
5767,unknown,"that we talk about a speci ﬁc regime type that allows free and fair elections and some other freedoms. Nevertheless, there might be disagreement about the precise de ﬁ- nition of the concept in question; in this case disagreement about democracy might revolve the following questions: do we only look at elections, do we include political rights, social rights, economic rights, or all of the above? "
5768,unknown,"rights, social rights, economic rights, or all of the above? To avoid any confusion, researchers must be precise when de ﬁning the meaning of a concept. In particular, this applies for contested concepts such as democracy. As already mentioned, for some scholars, the existence of parties, free and fair elections, and a reasonable participation by the population might be enough to classify a countr"
5769,unknown,"cracy. For others, a country must have legally enforced guarantees for freedoms of speech, press, and religion and must guarantee social and economic rights. It can be either a normative or a practical question or both whether one or the other classi ﬁ- cation is more appropriate. It might also be a question of the speci ﬁc research topic or research question, whether one or the other de ﬁnition i"
5770,unknown,"research question, whether one or the other de ﬁnition is more appropriate. Yet, whatever de ﬁnition she chooses, a researcher must clearly identify and justify the choice of her de ﬁnition, so that the reader of a published work can judge the appropriateness of the chosen de ﬁnition. 2.3 Theories, Concepts, Variables, and Hypothesis 13 It is also w orth noting that the meaning of concepts can als"
5771,unknown,"It is also w orth noting that the meaning of concepts can also change over time. Take again the example of democracy. Democracy 2000 years ago had a different meaning than democracy today. In the Greek city-states (e.g., Athens), democracy was a system of direct decision-making, in which all men above a certain income threshold convened on a regular basis to decide upon important matters such as i"
5772,unknown,"on a regular basis to decide upon important matters such as international treaties, peace and war, as well as taxation. Women, servants, slaves, and poor citizens were not allowed to participate in these direct assemblies. Today, more than 2000 years after the Greek city-states, we commonly refer to democracy as a representative form of government, in which we elect members to parliament. In the e"
5773,unknown,"government, in which we elect members to parliament. In the elected assembly, these elected politicians should then represent the citizens that mandated them to govern. Despite the contention of how many political, civic, and social rights are necessary to consider a country a democracy, there is nevertheless agreement among academics and practitioners today that the Greek de ﬁnition of democracy "
5774,unknown,"practitioners today that the Greek de ﬁnition of democracy is outdated. In the twenty- ﬁrst century, no serious academic would disagree that suffrage must be universal, each vote must count equally, and elections must be free and fair and must occur on a regular basis such as in a 4- or 5-year interval. 2.3.3 Variables A variable refers to properties or attributes of a concept that can be measured"
5775,unknown,"way or another: in short, a variable is a measurable version of a concept. The process to transform a concept into a variable is called operationalization. To take an example, age is a variable, but the answer to the question how old you are is a variable. Some concepts in political or social science are rather easy to measure. For instance, on the individual level, somebody ’s education level can"
5776,unknown,"overall years of schooling somebody has achieved or by the highest degree some- body has obtained. On the macro-level, women ’s representation in parliament can be easily measured by the percentage of seats in the elected assembly which are occupied by women. Other concepts, such as someone ’s political ideology on the individual level or democracy on the macro-level, are more dif ﬁcult to measure"
5777,unknown,"example, operationalizations of political ideology range from the party one identi ﬁes with, to answers to survey questions about moral issues such as abortion or same-sex marriage, and to questions about whether somebody prefers more welfare state spending and higher taxes or less welfare state spending and lower taxes. For democracy, as already discussed, there is not only discussion of the prec"
5778,unknown,"of democracy but also on how to measure different regime types. For example, there is disagreement in the academic literature if we should adopt a dichotomous de ﬁ- nition that distinguishes a democracy from a nondemocracy (Przeworski et al. 1996), a d istinction in democracy, hybrid regime, or autocracy (Bollen 1990), or if we should use a graded measure, that is, democracy is not a question of k"
5779,unknown,"degree, and the gradation should capture sometimes partial processes of democratic institutions in many countries (Elkins 2000). Table 2.1Measuring Dahl ’s polyarchy Components of democracy – – – – 14 2 The Nuts and Bolts of Empirical Social Science Country Country Country 1 2 3 Elected of ﬁcials have control over government decisions x – x Free, fair, and frequent elections x – x Universal suffra"
5780,unknown,"Universal suffrage x – x Right to run for of ﬁce for all citizens x – x Freedom of expression x Alternative sources of information x Right to form and join autonomous political organizations x – x Polyarchy Yes No No When measuring a concept, it is important that a concept has high content validity; there should be a high degree of convergence between the measure and the concept it is thought to r"
5781,unknown,"the concept it is thought to represent. In other words, a high content validity is achieved if a measure represents all facets of a given concept. To highlight how this convergence can be achieved, I use one famous de ﬁnition of democracy, Dahl ’s polyarchy. Polyarchy, according to Dahl, is a form of representative democracy characterized by a particular set of political institutions. These includ"
5782,unknown,"ofﬁcials, free and fair elections, inclusive suffrage, the right to run for of ﬁce, freedom of expression, alternative information, and associational autonomy (see Dahl 1973). To achieve high content validity, any measurement of polyarchy must include the seven dimensions of democracy; that is, any of these seven dimensions must be explicitly measured. Sometimes a conceptual de ﬁnition predisposes"
5783,unknown,"researchers to use one operationalization of a concept over another one. In Dahl ’s classiﬁcation, the respect of the seven features is a minimum standard for demo- cracy; that is why his concept of polyarchy is best operationalized dichotomously. That is, a country is a polyarchy if it respects all of the seven features and is not if it doesn’t (i.e., it is enough to not qualify as a democracy if"
5784,unknown,"doesn’t (i.e., it is enough to not qualify as a democracy if one of the features is not respected). Table 2.1 graphically displays this logic. Only country 1 respects all features of a polyarchy and can be classi ﬁed as such. Countries 2 and 3 violate some or all of these minimum conditions of polyarchy and hence must be coded as nondemocracies. Achieving high content validity is not always easy. "
5785,unknown,"nondemocracies. Achieving high content validity is not always easy. Some concepts are dif ﬁcult to measure. Take the concept of political corruption. Political corruption, or the private (mis)use of public funds for illegitimate private gains, happens behind closed doors without the supervision of the public. Nearly by de ﬁnition this entails that nearly all proxy variables to measure corruption a"
5786,unknown,"measure corruption: 1. Large international efforts compiled by international organizations such as the World Bank or Transparency International try to track corruption in the public sector around the globe. For example, the Corruption Perceptions Index (CPI) 2.3 Theories, Concepts, Variables, and Hypothesis 15 focuses on corruption in the public sector. It uses expert surveys with country experts "
5787,unknown,"experts inside and outside the country under scrutiny on, among others, bribery of public ofﬁcials, kickbacks in public procurement, embezzlement of public funds, and the strength and effectiveness of public sector anti-corruption efforts. It then creates a combined measure from these surveys. 2. National agencies in several (Western) countries track data on the number of federal, state, and local"
5788,unknown,"federal, state, and local government of ﬁcials prosecuted and convicted for cor- ruption crimes. 3. I nternational public opinion surveys (e.g., the World Value Survey) ask citizens about their own experience with corruption (e.g., if they have paid or received a bribe to or for any public service within the past 12 months). Any of these three measurements is potentially problematic. First, percep"
5789,unknown,"indexes based on interviews/surveys with country experts can be deceiving, as there is no hard evidence to back up claims of high or low corruption, even if these assessments come from so-called experts. However, the hard evidence can be deceiving as well. Are many corruption charges and indictments a sign of high or low corruption? They might be a sign of high corruption, as it shows corruption i"
5790,unknown,"widespread; a certain percentage of the of ﬁcials in the public sector engage in the exchange of public goods for private promotion. Yet, many cases treated in court might also be a sign of low corruption. It might show that the system works, as it cracks down on corrupted of ﬁcials. For the third measure, citizens ’personal experi- ence with corruption is suboptimal, as well. Given that corruptio"
5791,unknown,"survey participants might not admit that they have participated in fraudulent activities. They might also fear repercussions by the public if they admit being part of a corrupt network. Finally, it might not be rational to admit corruption, particularly if you are one of the bene ﬁciaries of it. In particular, for dif ﬁcult to measure concepts such as corruption, it might be advisable to cross-val"
5792,unknown,"advisable to cross-validate any imperfect proxy with another measure. In other words, different measures must resemble each other if they tap into the same concept. If this is the case, we speak of high construct validity, and it is possibly safe to use one proxy or even better create a conjoint index of the proxy variables in question. If this is not the case, then there is a problem with one or "
5793,unknown,"measurements, something the researcher should assess in detail. One way to measure whether two measurements of the same variable are strongly related to each other is through correlation analysis (see Chap. 8). Sometimes it is not only dif ﬁcult to achieve high operational validity of dif ﬁcult concepts such as corruption but sometimes also for seemingly simple concepts such as voting or casting a"
5794,unknown,"as voting or casting a ballot for a radical right-wing party. In answering a survey, individuals might pretend they have voted or cast a ballot for a mainstream party to pretend that they abide by the societal norms. Yet it is very dif ﬁcult to detect the type of individuals, who either deliberately or undeliberately answer a survey question incorrectly (for a broader discussion of biases in surve"
5795,unknown,Sect. 5.2). 16 2 The Nuts and Bolts of Empirical Social Science 2.3.3.1 Types of Variables In empirical research we distinguish two main types of variables: dependent variable and independent variable. Dependent VariableThe dependent variable is the variable the researcher is trying to explain. It is the primary variable of interest and depends on other variables (so-called independent variables).
5796,unknown,"the notation y . Independent VariableIndependent variables are hypothesized to explain variation in the dependent variable. Because they are thought to explain variation or changes in the dependent variable, independent variables are sometimes also called expla- natory variables (as they should explain the dependent variable). In quantitative studies, the independent variable has the notation x . "
5797,unknown,"I use another famous theory, modernization theory, to explain the difference between independent and dependent variable. In essence, modernization theory states that countries with a higher degree of development are more likely to be democratic (Lipset 1959). In this example, the dependent variable is regime type (however measured). The independent variable is a country ’s level of development, wh"
5798,unknown,"which could, for instance, be measured by a country ’s GDP per capita. In the academic literature, independent variables that are not the focus of the study, but which might also have an in ﬂuence on the dependent variable, are sometimes referred to as control variables. To take an example from the turnout literature, a researcher might be interested in the relationship between electoral competiti"
5799,unknown,"competitiveness and voter turnout. Electoral competitiveness is the independent variable, and turnout is the dependent variable. However, turnout rates in countries or electoral districts are not only dependent on the competiveness of the election (which is often operationalized by the difference in votes between the winner and the runner-up) but also by a host of other factors including compulsor"
5800,unknown,"electoral system type, corruption, or income inequalities, to name a few factors. These other independent variables must also be accounted for and included in the study. In fact, researchers can only test the “ real impact ”of competitiveness on turnout, if they also take these other factors into consideration. 2.3.4 Hypotheses A hypothesis is a tentative, provisional, or uncon ﬁrmed statement der"
5801,unknown,"theory that can (in principle) be either veri ﬁed or falsi ﬁed. It explicitly states the expected relationship between an independent and dependent variable. Hypotheses must be empirically testable statements that can cover any level of analysis. In fact, a good hypothesis should specify the types or level of political actor to which the hypothesis will test (see also Table 2.2). Table 2.2Examples"
5802,unknown,"Table 2.2Examples of good and bad hypotheses Wrong Right Democracy is the best form of government The more democratic a country is, the better its government performance will be The cause of civil war is economic upheaval The more there is economic upheaval, the more likely a country will experience civil war Raising the US minimum wage will affect job growth Raising the minimum wage will create m"
5803,unknown,"relationship) Raising the minimum wage will cut jobs (negative relationship) Macro-levelAn example of a macro-level hypothesis derived from modernization theory would be: The more highly developed a country, the more likely it is a democracy. Meso-levelAn example of a meso-level hypothesis derived from the iron law of oligarchy would be: The longer a political or social organization is in existenc"
5804,unknown,"more hierarchical are its power structures. MicrolevelAn example of a microlevel hypothesis derived from the resource theory of voting would be: The higher somebody ’s level of education, the more likely they are to vote. Scientiﬁc hypotheses are always stated in the following form: The more [independent variable], the more [dependent variable] or the more [independent variable], the less [depende"
5805,unknown,"When researchers formulate hypotheses, they make three explicit statements: 2.3 Theories, Concepts, Variables, and Hypothesis 17 1. Xand Y covary. This implies that there is variation in the independent and dependent variable and that at least some of the variation in the dependent variable is explained by variation in the independent variable. 2. Change inX precedes change in Y .B yd e ﬁnition a "
5806,unknown,"2. Change inX precedes change in Y .B yd e ﬁnition a change in independent variable can only trigger a change in the dependent variable if this change happens before the change in the dependent variable. 3. The effect of the independent variable on the dependent variable is not coinci- dental or spurious (which means explained by other factors) but direct. To provide an example, the resource theor"
5807,unknown,"To provide an example, the resource theory of voting states that individuals with higher socioeconomic status (SES) are more likely to vote. From this theory, I can derive the microlevel hypothesis that the more educated a citizen is, the higher the chance that she will cast a ballot. To be able to test this hypothesis, I operationalize SES by a person ’s years of full-time schooling and voting by"
5808,unknown,"SES by a person ’s years of full-time schooling and voting by a survey question asking whether somebody voted or not in the last national election. By formulating this hypothesis, I make the implicit assumption that there is variation in the overall years of schooling and variation in voting. I also explicitly state that the causal 18 2 The Nuts and Bolts of Empirical Social Science explanatory ch"
5809,unknown,"explanatory chain goes from education to voting (i.e., that education precedes voting). Finally, I expect that changes in somebody ’s education trigger changes in somebody’s likelihood to vote (i.e., I expect the relationship to not be spurious). While for the resource hypothesis, there is probably consensus that the causal chain goes from more education to a higher likelihood to vote, and not the"
5810,unknown,"around, the same does not apply to all empirical relationships. Rather, in political science we do not always have a one-directional relationship. For example, regard- ing the modernization hypothesis, there is some debate in the scholarly community surrounding whether it is development that triggers the installation of democracy or if it is democracy that triggers robust economic development more"
5811,unknown,"if it is democracy that triggers robust economic development more than any other regime type. There are statistical methods to treat cases of reversed causation such as structural equation modelling. Because of the advanced nature of these techniques, I will not cover these techniques in this book. Nevertheless, what is important to take away from this discussion is that students of political scie"
5812,unknown,"away from this discussion is that students of political science and the social sciences, more generally, must think carefully about the direction of cause and effect before they formulate a hypothesis. It is also important that students know the difference between an alternative hypothesis and a null hypothesis. The alternative hypothesis, sometimes also called research hypothesis, is the hypothes"
5813,unknown,"research hypothesis, is the hypothesis you are going to test. The null hypothesis is the rival hypothesis — it assumes that there is no association between the independent and dependent variables. To give an example derived from the iron law of oligarchy, a researcher wanting to test this theory could postulate the hypothesis that “ the longer a political organization is in existence, the more hie"
5814,unknown,"longer a political organization is in existence, the more hierarchical it will get. ”In social science jargon, this hypothesis is called the alternative hypothesis. The corresponding null-hypothesis would be that length of existence of a political organization and its hierarchical structure are unrelated. 2.4 The Quantitative Research Process The quantitative research process is deductive (see Fig"
5815,unknown,"The quantitative research process is deductive (see Fig. 2.1). It is theory driven; it starts and ends with theory. Before the start of any research project, students of political science must know the relevant literatures. They must know the dominant theories and explanations of the phenomenon they want to study and identify controversies and holes or gaps in knowledge. The existing theory will t"
5816,unknown,"them to formulate some hypotheses that will ideally try to resolve some of the controversies or ﬁ ll one or several gaps in knowledge. Quantitative research might also test existing theories with new quantitative data, establish the boundaries or limitations of a theory, or establish the conditions under which a theory applies. Whatever its purpose, good research starts with a theoretically derive"
5817,unknown,"question and hypothesis. Ideally, the research question should address a politically relevant and important topic and make a potential theoretical contribution to the literature (it should potentially add to, alter, change, or refute the existing theory). The hypothesis should clearly identify the independent and dependent variable. It should be a plausible statement of how the researcher thinks t"
5818,unknown,"2.4 The Quantitative Research Process 19 Theory Hypotheses Statistical Analysis SamplingOperationalization Measurement Adapted from Walsh and Ollenburger 2001 Fig. 2.1Display of the quantitative research process variable behaves toward the dependent variable. In addition, the researcher must also identify potential control variables. In the next step, the researcher has to think about how to measu"
5819,unknown,"how to measure independent, dependent, and control variables. When operationalizing her variables, she must ensure that there is high content validity between the numerical representation and the conceptional de ﬁnition of any given concept. After having decided how to measure the variables, the researcher has to think about sampling. In other words, which empirical referents will she use to test "
5820,unknown,"her hypothesis? Measurement and sampling are often done concurrently, because the empirical referents, which the researchers study, might predispose her to use one operationalization of an indicator over another. Sometimes, also practical consider- ations such as the existence of empirical data determine the measurement of variables and the number and type of observations studied. Once the researc"
5821,unknown,"has her data, she can then conduct the appropriate statistical tests to evaluate research question and hypothesis. The results of her study will then ideally have an in ﬂuence on theory. Let us explain Fig. 2.1 with a concrete example. We assume that a researcher is interested in individuals ’participation in demonstrations. Reading the literature, she ﬁnds two dominant theories. On the one hand, "
5822,unknown,"ﬁnds two dominant theories. On the one hand, the resource theory of political action states that the more resources individuals have in the form of civic skills, network connections, time, and money, the more likely they are to engage in collective political activities including partaking in demonstrations. On the other hand, the relative deprivation approach states that individuals must be frustr"
5823,unknown,"economic, social, and political situation. The more they see a gap between value expectations and value capabilities, the more likely they are going to protest. Implicit in the second argument is that individuals in the bottom echelon of society such as the unemployed, those who struggle economically, or those who are deprived of equal chances in society such as minorities are more likely to demon"
5824,unknown,"identiﬁed this controversy, the researcher asks himself which, if either, of the two competing theories is more correct. Because the researcher does not know, a priori, which of the two theories is more likely to apply, she formulat es two competing hypotheses: 20 2 The Nuts and Bolts of Empirical Social Science Hypothesis 1: The higher somebody ’s SES, the higher somebody ’s likelihood to partake"
5825,unknown,"partake in a demonstration. Hypothesis 2:The higher somebody ’s dissatisfaction with her daily life, the higher the likelihood that this person will demonstrate. Having formulated her hypotheses, the researcher has to identify other potentially relevant variables that could explain one ’s decision to partake in a demonstration. From the academic literature on protest, she identi ﬁes gender, age, p"
5826,unknown,"From the academic literature on protest, she identi ﬁes gender, age, political sociali- zation, and place of residency as other potentially relevant variables which she also has to include/control for in her study. Once the hypotheses are formulated and control variables identi ﬁed, the researcher then has to determine the measurement of the main variables of interest and for the control variables"
5827,unknown,"appropriate study sample. To measure the ﬁ rst independent variable, a person ’s SES, the researcher decides to employ two very well-known proxy variables, education and income. For the second, independent variable, she thinks that the survey question “ how satis ﬁed are you with your daily life ”captures individuals ’ levels of frustrations pretty well. The dependent variable, partaking in a demo"
5828,unknown,"stration, could be measured by a survey question asking whether somebody has demonstrated within the past year. Because the researcher ﬁ nds that the European Social Survey (ESS) asks all these questions using a representative sample of individuals in about 20 European countries, she uses this sample as the study object or data source. She then engages in appropriate statistical techniques to gaug"
5829,unknown,"inﬂuence of her two main variables of interest on the dependent variable. As a preliminary test, she must also test her assumption that people with poor SES are more likely to be frustrated and dissatis ﬁed with their lives. Let us assume she ﬁ nds through appropriate statistical analysis that it is in fact less educated and lower- income individuals who are more likely to be dissatis ﬁed and who "
5830,unknown,"more. Finding this, the researcher would resolve some of the controversy around the two contradictory hypotheses for partaking in demonstrations, at least when it comes to the European countries under consideration. References Beetham, D. (1999). Democracy and human rights . Cambridge: Polity. Bogaards, M. (2007). Measuring democracy through election outcomes: A critique with African data.Comparat"
5831,unknown,"data.Comparative Political Studies, 40 (10), 1211 –1237. References 21 Bollen, K. A. (1990). Political democracy: Conceptual and measurement traps. Studies in Compar- ative International Development (SCID), 25 (1), 7 –24. Chandler, D. (2010). The uncritical critique of ‘ liberal peace ’.Review of International Studies, 36(1), 137 –155. Daase, C. (2006). Democratic peace — Democratic war: Three rea"
5832,unknown,"war-prone. In Democratic wars (pp. 74 –89). London: Palgrave Macmillan. Dahl, R. A. (1973). Polyarchy: Participation and opposition . Yale: Yale University Press. De Mesquita, B. B., Morrow, J. D., Siverson, R. M., & Smith, A. (1999). An institutional explanation of the democratic peace. American Political Science Review, 93 (4), 791 –807. Downs, A. (1957). An economic theory of political action i"
5833,unknown,"Economy, 65 (2), 135 –150. Elkins, Z. (2000). Gradations of democracy? Empirical tests of alternative conceptualizations. American Journal of Political Science, 44 (2), 293 –300. Gleditsch, N. E. (2002). Armed con ﬂict 1946 –2001: A new dataset. Journal of Peace Research, 39(5), 615 –637. Gurr, T. R. (1970). Why men rebel . Princeton: Princeton University Press. Kant, I. (1795) [2011]. Zum ewigen "
5834,unknown,"Kant, I. (1795) [2011]. Zum ewigen Frieden. (3rd ed.). Berlin: Akademie Verlag. Lipset, S. L. (1959). Some social requisites of democracy: Economic development and political legitimacy.American Political Science Review, 53 (1), 69 –105. Mansﬁeld, E. D., & Snyder, J. (2007). Electing to ﬁ ght: Why emerging democracies go to war . Boston: MIT Press. Marshall, M. G., Jaggers, K., & Gurr, T. R. (2011)"
5835,unknown,"Arlington: Polity IV Project. Michels, R. (1915). Political parties: A sociological study of the oligarchical tendencies of modern democracy . New York: The Free Press. Milligan, K., Moretti, E., & Oreopoulos, P. (2004). Does education improve citizenship? Evidence from the United States and the United Kingdom. Journal of Public Economics, 88 (9), 1667–1695. Mo, P. H. (2001). Corruption and econom"
5836,unknown,"Mo, P. H. (2001). Corruption and economic growth. Journal of Comparative Economics, 29 (1), 66–79. Przeworski, A., Alvarez, M., Cheibub, J. A., & Linongi, F. (1996). What makes democracies endure?Journal of Democracy, 7 (1), 39 –55. Riker, W. H., & Ordeshook, P. C. (1968). A theory of the calculus of voting. American Political Science Review, 62 (1), 25 –42. Rumberger, R. W., & Palardy, G. J. (200"
5837,unknown,"Rumberger, R. W., & Palardy, G. J. (2005). Does segregation still matter? The impact of student composition on academic achievement in high school. Teachers College Record, 107 (9), 1999. Runciman, W. G. (1966). Relative deprivation and social injustice. A study of attitudes to social inequality in twentieth century England . London: Routledge and Keagan Paul. Russett, B. (1994). Grasping the demo"
5838,unknown,"Princeton: Princeton University Press. Skocpol, T. (1979). States and social revolutions: A comparative analysis of France, Russia and China.Cambridge : Cambridge University Press. Walsh, A., & Ollenburger, J. C. (2001). Essential statistics for the social and behavioral sciences: A conceptual approach. Prentice Hall: Pearson Education. Further Reading Research Design Creswell, J. W., & Creswell, "
5839,unknown,"Creswell, J. W., & Creswell, J. D. (2017). Research design: Qualitative, quantitative, and mixed methods approaches . Thousand Oaks, CA: Sage. Nice introduction into the two main research 22 2 The Nuts and Bolts of Empirical Social Science traditions qualitative and quantitative research. The book also covers mixed methods ’ approaches (approaches that combine qualitative and quantitative methods)"
5840,unknown,"McNabb, D. E. (2015). Research methods for political science: Quantitative and qualitative methods. London: Routledge (Chap. 7). Nice introduction into the nuts and bolts of quantitative methods. Introduces basic concepts such as reliability and validity, as well as discusses different types of statistics (i.e. inferential statistics). Shively, W. P. (2016). The craft of political research . New Y"
5841,unknown,"introduction into the quantitative research process. Theories and Hypotheses Brians, C. L., Willnat, L., Manheim, J., & Rich, R. (2016). Empirical political analysis . London: Routledge (Chaps. 2, 4, 5). Comprehensive introduction into theories, hypothesis testing and operationalization of variables. Qualitative Research Elster, J. (1989). Nuts and bolts for the social sciences . Cambridge: Cambri"
5842,unknown,"nice introduction into causal explanations and causal mechanisms. The book explains what causal mechanisms are and what research steps the researcher can conduct to detect them. Gerring, J. (2004). What is a case study and what is it good for?. American political science review, 98(2), 341 –354. A nice introduction on what a case study is, what is good for in political science, and what different "
5843,unknown,"political science, and what different types of case studies exist. Lijphart, A. (1971). Comparative politics and the comparative method. American Political Science Review,65 (3), 682–693. Seminal work on the comparative case study. Explains what a compar- ative case study is, how it relates to the ﬁ eld of comparative politics, and how to conduct a comparative case study. A Short Introduction to S"
5844,unknown,"comparative case study. A Short Introduction to Survey Research 3 Abstract This chapter offers a brief introduction into survey research. In the ﬁ rst part of the chapter, students learn about the importance of survey research in the social and behavioral sciences, substantive research areas where survey research is fre- quently used, and important cross-national survey such as the World Values Su"
5845,unknown,"Survey and the European Social Survey. In the second, I introduce different types of surveys. 3.1 What Is Survey Research? Survey research has become a major, if not the main, technique to gather information about individuals of all sorts. To name a few examples: • Costumer surveysask individuals about their purchasing habits or their satisfac- tion with a product or service. Such surveys can gain"
5846,unknown,"and inform marketing strategies by companies. • Attitudinal surveys poll participants on social, economic, or cultural attitudes. These surveys are important for researchers and policy makers as they allow us to detect cultural values, political attitudes, and social preferences. • Election surveys ask citizens about their voting habits. As such they can, for example, in ﬂuence campaign strategies"
5847,unknown,"Regardless of its type, survey research involves the systematic collection of information from individuals using standardized procedures. When conducting survey research, the researcher normally uses a (random or representative) sample from the population she wants to study and asks the survey subjects one or several questions about attitudes, perceptions, or behaviors. In the ideal case, she want"
5848,unknown,"produce a set of data on a given phenomenon that captures the studied concept, as # Springer International Publishing AG 2019 D. Stockemer, Quantitative Methods for the Social Sciences , https://doi.org/10.1007/978-3-319-99118-4_3 23 well as relevant independent variables. She also wants to have a sample that describes the population she wants to study fairly well (Fowler 2009: 1). To provide a co"
5849,unknown,"a concrete example, if a researcher wants to gather information on the popularity of the German chancellor, she has to collect a suf ﬁciently large sample that is represen- tative of the German population (see Chap. 4 for a discussion of representativeness). She might ask individuals to rate the popularity of the German chancellor on a 0 –100 scale. She might also ask respondents about their gende"
5850,unknown,"scale. She might also ask respondents about their gender, age, income, education, and place of residency to determine what types of individuals like the head of the German government more and what groups like her less. If these data are collected on a regular basis, it also allows researchers to gain relevant information about trends in societies. F or example, so-called trend studies allow resear"
5851,unknown,popularity of the chancellor over time and possibly to associate increases and decreases in her popularity with political events such as the German reuni ﬁcation in 1990 or the refugee crisis in Germany in 2015. 24 3 A Short Introduction to Survey Research 3.2 A Short History of Survey Research The origins of survey research go back thousands of years. These origins are linked to the understanding
5852,unknown,"to the understanding that every society with some sort of bureaucracy, in order to function properly, needs some information about its citizens. For example, in order to set taxation levels and plan infrastructure, bureaucracies need to know basic information about their citizens such as how many citizens live in a geographical unit, how much money they earn, and how many acres of land they own. H"
5853,unknown,"ﬁrst data collection efforts date back to the great civilizations of antiquity, such as China, Egypt, Persia, Greece, or the Roman Empire. A famous example of early data collection is the census mentioned in the bible during the time of Jesus ’birth: In those days a decree went out from Emperor Augustus that all the world should be registered. This was the ﬁ rst registration and was taken while Qu"
5854,unknown,"registered. This was the ﬁ rst registration and was taken while Quirinius was governor of Syria. All went to their own towns to be registered. Joseph also went from the town of Nazareth in Galilee to Judea, to the city of David called Bethlehem, because he was descended from the house and family of David. He went to be registered with Mary, to whom he was engaged and who was expecting a child. (Lu"
5855,unknown,"While it is historically unclear whether the census by Emperor Augustus was actually held at the time of Jesus ’birth, the citation from the bible nevertheless shows that as early as in the ancient times, governments tried to retrieve information about their citizens. To do so, families had to register in the birth place of the head of the family and answer some questions which already resembled o"
5856,unknown,"the family and answer some questions which already resembled our census questions today. In the middle ages, data collection efforts and surveys became more sophisti- cated. England took a leading role in this process. The ﬁ rst Norman king, William the Conqueror, was a leading ﬁ gure in this quest. After his conquest of England in 1066, he strived to gather knowledge on the property conditions, a"
5857,unknown,"yearly income of the barons and cities in the seized territories. For example, he 3.2 A Short History of Survey Research 25 wanted to know how many acres of land the barons owned so that he could determine appropriate taxes. In the following centuries, the governing processes became increasingly centralized. To run their country ef ﬁciently and to defend the country against external threats, the a"
5858,unknown,"country against external threats, the absolutist English rulers depended on extensive data on labor, military capabilities, and trade (Hooper 2006). While some of these data were “ hard data ”collected directly from of ﬁcial books (e.g., the manpower of the army), other data, for example, on military capabilities, trade returns, or the development of the population, were, at least in part, retriev"
5859,unknown,"development of the population, were, at least in part, retrieved through survey questions or interviews. Regardless of its nature, the importance of data collection rose, in particularly, in the economic and military realms. London was the ﬁ rst city, where statistics were systematically applied to some collected data. In the seven- teenth century, economists including John Graunt, William Petty, "
5860,unknown,"Halley tried to estimate population developments on the basis of necrologies and birth records. These studies are considered to be the precursors of modern quanti- tative analysis with the focus on causal explanations (Petty and Graunt 1899). Two additional societal developments rendered the necessity for good data the more urgent. First, the adaption of a data-based capitalist economic system in "
5861,unknown,"eighteenth and nineteenth century accelerated data collection efforts in England and later elsewhere in Europe. The rationalization of administrative planning processes in many European countries further increased the need to gain valid data, not only about the citizens but also about the administrative processes. Again, some of these data could only be collected by asking others. The next boost t"
5862,unknown,"data could only be collected by asking others. The next boost then occurred in the early nineteenth century. The Industrial Revolution combined with urbanization had created high levels of poverty for many residents in large British cities such as Manchester or Birmingham. To get some “ valid picture”of the diffusion of poverty, journalists collected data by participating in poor people ’s lives, "
5863,unknown,"questions about their living standard and publishing their experiences. This devel- opment resulted in the establishment of “ statistical societies ”in most large English cities (Wilcox 1934). Although the government shut down most of these statistical societies, it was pressed to extend its own data gathering by introducing routine data collections on births, deaths, and crime. Another element of"
5864,unknown,"collections on births, deaths, and crime. Another element of these developments was the implementation of enquete commissions whose work looked at these abominable living conditions in some major English cities and whose conclusions were partly based on quantitative data gathered by asking people questions about their lives. Similar developments happened elsewhere, as well. A prominent example is "
5865,unknown,"empirical research of medical doctors in Germany in the nineteenth century, who primarily examined the living and working conditions of laborers and the health-care system (Schnell et al. 2011:1 3 –20). Despite these efforts, it was not until the early twentieth century until political opinion polling in the way we conduct it today was born. Opinion polling in its contemporary form has its roots i"
5866,unknown,"contemporary form has its roots in the United States of America (USA). It started in the early twentieth century, when journalists attempted to forecast the outcomes of presidential elections. Initially, the journalists just took the assessment of some citizens before newspapers came up with more systematic approaches to predict the election results. The Literary Digest was the ﬁ rst newspaper to "
5867,unknown,"26 3 A Short Introduction to Survey Research number of postal surveys among voters in 1916 (Converse 2011). The poll also correctly predicted the winner of the 1916 Presidential Elections, Woodrow Wilson. This survey was the ﬁ rst mass survey in the United States and the ﬁ rst systematic opinion poll in the country ’s history (Burnham et al. 2008: 99 f.). At about the same time, the British philan"
5868,unknown,"time, the British philanthropists Charles Booth and Seebohm Rowntree chose interview approaches to explain the causes of poverty. What distinguishes their works from former studies is the close link between social research and political application. To a get valid picture of poverty in the English city of York, Rowntree attempted to interview all working-class people living in York. Of course, thi"
5869,unknown,"long and tiring procedure that took several years. The Rowntree example rendered it very clear to researchers, journalists, and data collection organizations that collecting data on the population researchers want to study is very cumbersome and dif ﬁcult to do. Consequently, this method of data collection has become very exceptional (Burnham et al. 2008: 100 f.; Schnell et al. 2011:2 1 –23). Due "
5870,unknown,"costs associated with complete enumerations, only governments have the means to carry them out today (e.g., through the census). Researchers must rely mainly on samples, which they use to draw inferences on population statistics. Building on the work of The Literary Digest , in the USA and various efforts on the continent, the twentieth century has seen a re ﬁnement of survey and sampling techniqu"
5871,unknown,"broad application to many different scenarios, be they economic, social, or political. Today surveys are ubiquitous. There is probably not one adult individual in the Western world who has not been asked at least once in her lifetime to participate in a survey. 3.3 The Importance of Survey Research in the Social Sciences and Beyond Survey research is one of the pillars in social science research i"
5872,unknown,"century. Surveys are used to measure almost everything from voting behavior to public opinion and to sexual preferences (De Leeuw et al. 2008: 1). They are of interest to a wide range of constituents including citizens, parties, civil society organizations, and governments. Individuals might be interested in situating their beliefs and behavior in relation to those of their peers and societies. Pa"
5873,unknown,"want to know which party is ahead in the public preference at any given point in time and what the policy preferences of citizens are. Civil society organizations might use surveys to give credence to their lobbying points. Governments at various levels (i.e., the federal, regional, or local) may use surveys to ﬁ nd out how the public judges their performance or how popular speci ﬁc policy proposa"
5874,unknown,"their performance or how popular speci ﬁc policy proposals are among the general public. In short, surveys are ubiquitous in social and political life (for a good description of the importance of survey research, see Archer and Berdahl 2011). Opinion p olls help us to situate ourselves with regard to others in different social settings. On the one hand, survey research allows us to compare our soc"
5875,unknown,"and ideals in Germany, Western Europe, or the Americas to those in Japan, China, or Southeast Asia. For example, analyzing data from a general cross-national social 3.4 Overview of Some of the Most Widely Used Surveys in the Social Sciences 27 survey provides us with an opportunity to compare attitudes and social behaviors across countries; for instance, we can compare whether we eat more fast foo"
5876,unknown,"more television, have more pets, or believe more in extensive social welfare than citizens in Australia or Asia. Yet, not only does survey research allow us to detect between country variation in opinions, beliefs, and behaviors but also within a country, if the sample is large enough. In Germany, for example, large-scale surveys can detect if individuals in the East have stronger anti-immigrant a"
5877,unknown,"individuals in the West. In the United States, opinion polls can identify whether the approval rating of President Trump is higher in Texas than in Connecticut. Finally, opinion polls can serve to detect differences in opinion between different cohorts of the population. For example, we can compare how much trust young people (i.e., individuals in the age cohort 18 –25) have into the military comp"
5878,unknown,"senior citizens (i.e., individuals aged 60 and older) both for one country and for several countries. Survey research has also shaped the social- and political sciences. To illustrate, I will just introduce two classic works in political science, whose ﬁ ndings and conclusions are primarily based on survey research. First, one of the most outstand- ing political treatises based on survey research "
5879,unknown,"ing political treatises based on survey research is The Civic Culture by Gabriel Almond and Sidney Verba ( 1963). In their study, the authors use surveys on political orientations about the political systems (e.g., opinions, attitudes, and values) to detect that cultural norms must be congruent with the political system to ensure the stability of the system in question. Another classic using surve"
5880,unknown,"Robert Putnam ’sBowling Alone; The collapse and revival of American community (2001).Mainly through survey research, Putnam ﬁ nds that social engagement had weakened in the United States during the late twentieth century. He links the drop in all types of social and political activities to a decrease in membership in all kinds of organizations (e.g., social, political, or community organizations),"
5881,unknown,"organizations (e.g., social, political, or community organizations), declining contract among individuals (e.g., among neighbors, friends, and family), less volunteering, and less religious involvement. It should also be noted that survey research is not only a stand-alone tool to answer many relevant research questions, it can also be combined with other types of research such as qualitative case"
5882,unknown,"combined with other types of research such as qualitative case studies or the analysis of hard macro-level data. In a prime example of mixed methods, Wood ( 2003), aiming to understand the peasant ’s rationales in El Salvador to join revolutionary movements in the country ’s civil war, uses ﬁ rst ethnographic interviews of some peasants in a speci ﬁc region to tap into these motivations. In a late"
5883,unknown,"employs large national household surveys to con ﬁrm the conclusions derived from the interviews. 3.4 Overview of Some of the Most Widely Used Surveys in the Social Sciences Governments, governmental and non-governmental organizations, and social research centers spend millions of dollars per year to conduct cross-national surveys. These surveys (e.g., the World Values Survey or the European Social"
5884,unknown,"28 3 A Short Introduction to Survey Research representative or random samples of individuals in many countries to detect trends in individuals’social and political opinions, as well as their social and political behavior. We can distinguish different types of surveys. First, behavioral surveys measure individuals ’political-related, health-related, or job-related behavior. Prob- ably most prominen"
5885,unknown,"ably most prominent in the ﬁ eld of political science, election surveys gauge individuals’conventional and unconventional political activities in a regional, national, or international context (e.g., whether somebody participates in elections, engages in protest activity, or contacts a political of ﬁcial). Other behavioral surveys might capture health risk behaviors, employee habits, or drug use, "
5886,unknown,"Second, opinion surveys try to capture opinions and beliefs in a society; these questionnaires aim at gauging individual opinions on a variety of topics ranging from consumer behavior to public preferences, to political ideologies, and to pre- ferred free time activities and preferred vacation spots. Below, I present three of the most widely used surveys in political science and possibly the socia"
5887,unknown,"possibly the social sciences more generally: the Comparative Study of Electoral Systems (CSES), the World Values Survey (WVS), and the European Social Survey (ESS). Hundreds, if not thousands, of articles have emanated from these surveys. In these large-scale research projects, the researcher ’s duties include the composition of the questionnaire and the selection and training of the interviewers."
5888,unknown,The latter functions as the link between researcher and respondent. They run the interviews and should record the responses precisely and thoroughly (Loosveldt 2008: 201). 3.4.1 The Comparative Study of Electoral Systems (CSES) The Comparative Study of Electoral Systems (CSES) is a collaborative program of cross-national research among election studies conducted in over 50 states. The CSES is comp
5889,unknown,"CSES is composed of three tightly linked parts: ﬁ rst, a common module of public opinion survey questions is included in each participant country ’s post-election study. These“ microlevel”data include vote choice, candidate and party evaluations, current and retrospective economic evaluations, evaluations of the electoral system itself, and standardized sociodemographic measures. Second, district-"
5890,unknown,"itself, and standardized sociodemographic measures. Second, district-level data are reported for each respondent, including electoral returns, turnout, and the number of candidates. Finally, system- or “ macro-level”data report aggregate electoral returns, electoral rules and formulas, and regime characteristics. This design allows researchers to conduct cross-level and cross-national analyses, ad"
5891,unknown,"researchers to conduct cross-level and cross-national analyses, addressing the effects of electoral institutions on citizens ’attitudes and behavior, the presence and nature of social and political cleavages, and the evaluation of democratic institutions across different political regimes. The CSES is unique among comparative post-electoral studies because of the extent of cross-national collabora"
5892,unknown,"extent of cross-national collaboration at all stages of the project: the research agenda, the survey instrument, and the study design are developed by the CSES Planning Committee, whose members include leading scholars of electoral politics from around the world. This design is then implemented in each country by that country ’s 3.4 Overview of Some of the Most Widely Used Surveys in the Social Sc"
5893,unknown,"foremost social scientists, as part of their national post-election studies. Frequently, the developers of the survey decide upon a theme for any election cycle. For example, the initial round of collaboration focused on three general themes: the impact of electoral institutions on citizens ’political cognition and behavior (parlia- mentary versus presidential systems of government, the electoral "
5894,unknown,"mentary versus presidential systems of government, the electoral rules that govern the casting and counting of ballots and political parties), the nature of political and social cleavages and alignments, and the evaluation of democratic institutions and processes. The key theoretical question to be addressed by the second module is the contrast between the view that elections are a mechanism to ho"
5895,unknown,accountable and the view that they are a means to ensure that citizens ’views and interests are properly represented in the democratic process. It is the module ’s aim to explore how far this contrast and its embodiment in institutional structures in ﬂuences vote choice and satisfaction with democracy. The CSES can be accessed at http://www.isr.umich.edu/cps/project_cses.html. 3.4.2 The World Valu
5896,unknown,"The World Values Survey is a global research project that explores peoples ’values and beliefs, how they change over time, and what social and political impact they have. It emerged in 1981 and was mainly coined by the scientists Ronald Inglehart, Jan Kerkhofs, and Ruud de Moor. The survey ’s focus was initially on European countries, although since the late 1990s, however, non-European countries "
5897,unknown,"received more attention. Today, more than 80 independent countries representing 85% of the world ’s population are included in the survey (Hurtienne and Kaufmann 2015: 9 f.). The survey is carried out by a worldwide network of social scientists who, since 1981, have conducted representative national surveys in multiple waves in over 80 countries. The WVS measures, monitors, and analyzes a host of "
5898,unknown,"including support for democracy; tolerance of foreigners and ethnic minorities; support for gender equality; the role of religion and changing levels of religiosity; the impact of globalization; attitudes toward the environment, work, family, politics, national identity, culture, diversity, and insecurity; and subjective well-being on the basis of face-to-face interviews. The questionnaires are di"
5899,unknown,"1100–3500 interviewees per country. The ﬁ ndings are valuable for policy makers seeking to build civil society and democratic institutions in developing countries. The work is also frequently used by governments around the world, scholars, students, journalists, and international organizations and institutions such as the World Bank and the United Nations (UNDP and UN-Habitat). Thanks to the incre"
5900,unknown,"increasing number of participating countries and the growing time period that the WVS covers, the WVS satis ﬁes (some of) the demand for cross-sectional attitudinal data. The application of WVS data in hundreds of publications and in more than 20 languages stresses the crucial role that the WVS plays in scienti ﬁc research today (Hurtienne and Kaufmann 2015: 9 f.). The World Values Survey can be a"
5901,unknown,"3.4.3 The European Social Survey (ESS) The European Social Survey (ESS) is an academically driven cross-national survey that has been conducted every 2 years across Europe since 2001. It is directed by Rory Fitzgerald (City University London). The survey measures the attitudes, beliefs, and behavioral patterns of diverse populations in more than 30 European nations. As the largest data collection "
5902,unknown,"30 3 A Short Introduction to Survey Research 1. To chart stability and change in social structure, conditions, and attitudes in Europe and to interpret how Europe ’s social, political, and moral fabric is changing. 2. To achieve and spread higher standards of rigor in cross-national research in the social sciences, including, for example, questionnaire design and pre-testing, sampling, data collec"
5903,unknown,"sampling, data collection, reduction of bias, and the reliability of questions. 3. To introduce soundly based indicators of national progress, based on citizens ’ perceptions and judgements of key aspects of their societies. 4. To undertake and facilitate the training of European social researchers in com- parative quantitative measurement and analysis. 5. To improve the visibility and outreach of"
5904,unknown,"policy makers, and the wider public. Theﬁ ndings of the ESS are based on face-to-face interviews, and the question- naire is comprised of three sections, a core module, two rotating modules, and a supplementary questionnaire. The core module comprises questions on the media and social trust, politics, the subjective well-being of individuals, gender and household dynamics, sociodemographics, and s"
5905,unknown,"household dynamics, sociodemographics, and social values. As such, the core module should capture topics that are of enduring interest for researchers as well as provide the most comprehensive set of socio-structural variables in a cross- national survey worldwide. The two rotating modules capture “ hot”social science topics; for example, rotating modules in 2002 and 2014 focused on immigration, w"
5906,unknown,while the 2016 wave captures European citizens ’attitudes about welfare and opinions toward climate change. The purpose of the supplementary questionnaire at the end of the survey is to elaborate in more detail on human values and to test the reliability and validity of the items in the principal questionnaire on the basis of some advanced statistical techniques (ESS 2017). The European Social Sur
5907,unknown,"europeansocialsurvey.org/. 3.5 Different Types of Surveys For political science students, it is important to realize that one survey design does not necessarily resemble another survey design. Rather, in survey research, we generally distinguish between two types of surveys: cross-sectional surveys and longitudinal surveys (see Frees 2004: 2). 3.5 Different Types of Surveys 31 3.5.1 Cross-sectiona"
5908,unknown,"A cross-sectional survey is a survey that is used to gather information about individuals at a single point in time. The survey is conducted once and not repeated. An example of a cross-sectional survey would be a poll that asks respondents in the United States how much they donated toward the reconstruction efforts after Hurri- cane Katrina hit the Southern States of the United States. Surveys, s"
5909,unknown,"capturing donation patterns in the aftermath of Hurricane Katrina, are particularly interesting to seize attitudes and behaviors related to one event that probably will not repeat itself. Yet, cross-sectional surveys are not only used to capture one-time events. To the contrary, they are quite frequently used by researchers to tap into all types of research questions. Because, it is logistically c"
5910,unknown,"all types of research questions. Because, it is logistically complicated, time- consuming, and costly to conduct the same study at regular intervals with or without the same individuals, cross-sectional studies are frequently the fall-back option for many researchers. In many instances, the use of cross-sectional surveys can be justiﬁed from a theoretical perspective; frequently, a cross-sectional"
5911,unknown,"allows researchers to draw inferences about relationships between independent and dependent variables (Behnke et al. 2006: 70 f.). However, it is worth noting that the use of these types of surveys to detect empirical relationships can be tricky. Most importantly, because we only have data at one point for both independent and dependent variables, cross-sectional surveys cannot establish causality"
5912,unknown,"cannot establish causality (i.e., they cannot establish that a change in the independent variable precedes a change in the dependent variable) (De Vaus 2001: 51). There- fore, it is important that ﬁ ndings/conclusions derived from cross-sectional studies are supported by theory, logic, and/or intuition (Frees 2004: 286). In other words, a researcher should only use cross-sectional data to test the"
5913,unknown,"between independent and dependent variable is rather clear a priori. If w e h ave clear theoretical assumptions about a relationship, a cross-sectional survey can provide a good tool to test hypotheses. For example, a cross-sectional survey could be appropriate to test the linkage between formal education and casting a ballot at elections, as there is a strong theoretical argument in favor of the "
5914,unknown,"that higher formal education will increase somebody ’s propensity to vote. According to the resource model of voting (see Brady et al. 1995), higher educated individuals have the material and nonmaterial resources necessary to understand complex politi- cal scenarios, as well as the network connections, all of which should render some- body more likely to vote. Vice versa, uneducated individuals l"
5915,unknown,"body more likely to vote. Vice versa, uneducated individuals lack these resources and are frequently politically disenfranchised. Practically, it is also impossible that the sheer act of voting changes somebody ’s formal education. Hence, if we analyze a cross-sectional survey on voting and ﬁ nd that more educated individuals are more likely to vote, we can assume that this association re ﬂects an"
5916,unknown,"likely to vote, we can assume that this association re ﬂects an empirical reality. To take another example, if we want to study the in ﬂuence of age on protesting, data from a cross-sectional survey could be completely appropriate, as well, to study this rela- tionship, as the causal change clearly goes from age to protesting and not the other way round. By de ﬁnition, the fact that I protest does"
5917,unknown,"at least when we look at somebody ’s biological age. 32 3 A Short Introduction to Survey Research Nevertheless, empirical relationships are not always that clear-cut. Rather con- trary, sometimes it is tricky to derive causal explanations from cross-sectional studies. To highlight this dilemma, let us take an example from American Politics and look at the relationship between watching Fox News and"
5918,unknown,"Trump. For one, it makes theoretical sense that watching Fox News in the United States increases somebody ’s likelihood to vote for Donald Trump in the Presidential Election, because this TV chain supports this populist leader. Yet, the causal or correlational chain could also go the other way round. In other words, it might also be that somebody, who decided to vote for Trump, is actively looking"
5919,unknown,"outlet that follows her convictions. As a result, she might watch Fox News after voting for Trump. A slightly different example highlights even clearer that the correlational or causal direction between independent and dependent variable is not always clear. For example take the following example; it is theoretically unclear if the consump- tion of Fox News renders somebody more conservative or if"
5920,unknown,"tion of Fox News renders somebody more conservative or if more conservative individuals have a higher likelihood to watch Fox News . Rather than one variable inﬂuencing the other, both factors might mutually reinforce each other. Therefore, even if a researcher ﬁ nds support for the hypothesis that watching Fox News makes people more conservative, we cannot be sure of the direction of this associa"
5921,unknown,"because a cross-sectional survey would ask individuals the same question at the same time.1 Consequently, we cannot detect what comes ﬁ rst: watchingFox News or being conservative. Hence, cross-sectional surveys cannot resolve the aforemen- tioned temporal aspect. Rather than a cross-sectional survey, a longitudinal survey would be necessary to determine the causal chain between being conservative"
5922,unknown,"watchingFox News . Such a survey, in particular, if it is conducted over many years and if it solicits the same individuals in regular intervals, could tell researchers if respondentsﬁ rst become conservative and then watch Fox News or if the relation- ship is the other way round. 3.5.2 Longitudinal Survey In contrast to cross-sectional studies, which are conducted once, longitudinal surveys repea"
5923,unknown,"surveys repeat the same survey questions several times. This allows the researchers to analyze changing attitudes or behaviors that occur within the population over time. There are three types of longitudinal surveys: trend studies, cohort studies, and panel studies. 3.5.2.1 Trend Surveys A trend study, which is frequently also labeled a repeated cross-sectional survey, is a repeated survey that i"
5924,unknown,"repeated survey that is normally not composed of the same individuals in the different waves. Most of the main international surveys including the European 1In the literature, such reversed causation is often referred to as an endogeneity problem. Social Survey or the World Values Survey are trend studies. The surveys of the different waves are fully or partly comprised of the same questions. As s"
5925,unknown,"allow researchers to detect broad changes in opinions and behaviors over time. Nevertheless, and because the collected data covers different individuals in each wave of the study, the collected data merely allows for conclusions on the aggregate level such as the regional or the national level (Schumann 2012: 113). To highlight, most of the major surveys ask the question: How satis ﬁed are you wit"
5926,unknown,"democracy works in your country? Frequently, the answer choices range from 0 or not satisﬁed at all to 10 or very satis ﬁed. Since, citizens answer these questions every 2 years, researchers can track satisfaction rates with democracy over a longer period such as 10 years. Comparing the answers for several waves, a researcher can also establish if the same or different independent v ariables (e.g."
5927,unknown,"establish if the same or different independent v ariables (e.g., unemployment or economic insecurity, gender, age, or income) trigger higher rates of dissatisfaction with democracy. However, what such a question/study cannot do is to track down what altered an individual ’s assessment of the state of democracy in her country. Rather, it only allows researchers to draw conclusions on the macro- or "
5928,unknown,"level. 3.5 Different Types of Surveys 33 3.5.2.2 Cohort Surveys While trend studies normally focus on the whole population, cohort studies merely focus on a segment of the population. One common feature of a cohort study is that a central event or feature occurred approximately at the same time to all members of the group. Most common are birth cohorts. In that case, birth is the special event tha"
5929,unknown,"took place in the same year or in the same years for all members of the cohort (e.g., all Americans who were born in or after 1960). Analogous to trend studies, cohort studies use the same questions in several waves. In each wave, a sample is drawn from the cohort. This implies that the population remains the same over time, whereas the individuals in the sample change. A typical example of cohort"
5930,unknown,"is the “ British National Child Study ”(NCDS). In the course of this study, 11,400 British citizens born between March 3 and 9, 1958, were examined with respect to their health, education, income, and attitudes in eight waves in a time span of 50 years (Schnell et al. 2011: 237 f.). 3.5.2.3 Panel Surveys Panel studies normally ask the same questions to the same people in subsequent waves. These ty"
5931,unknown,"waves. These types of surveys are the most costly and most dif ﬁcult to implement, but they are the best suited to detect causal relationships or changes in individual behavior. For example, a researcher could ask questions on the consumption of Fox News and being conservative to the same individual over the period of several years. This could help her detect the temporal chain in the relationship"
5932,unknown,"This could help her detect the temporal chain in the relationship between a certain type of news consumption and political ideologies. Panel studies frequently have the problem of high attrition or mortality rates. In other words, people drop out during waves for multiple reasons, for example, they could move, become sick, or simply refuse further participation. Hence, it is likely that a sample t"
5933,unknown,"refuse further participation. Hence, it is likely that a sample that was representative from the outset becomes less and less representative for subsequent waves of the panel. To highlight, imagine that a researcher is conducting a panel on citizens ’ preference on which electoral system should be used in a country, and they ask this question every 2 years to the same individuals. Individuals who "
5934,unknown,"question every 2 years to the same individuals. Individuals who are interested in electoral politics, and/or have a strong opinion in favor of one or the other type of electoral system, might have a higher likelihood to stay in the sample than citizens who do not care. In contrast, those who are less interested wil l no longer participate in future waves. Others, like old-age citizens, might die o"
5935,unknown,"in future waves. Others, like old-age citizens, might die or move into an old people ’s home. A third group such as diplomats and consultants is more likely to move than manual workers. It is possible to continue the list. Therefore, there is the danger that many panels become less representative of the population for any of the waves covered. Nevertheless, in particular, if some representativenes"
5936,unknown,"subsequent waves or if the representativ eness is not an issue for the research question, panel studies can be a powerful tool to detect causal relationships. An early example of an in ﬂuential panel study is Butler and Stokes ’Political Change in Britain: Forces Shaping Electoral Choice. Focusing on political class as the key independent variable for the vote choice for a party, the authors condu"
5937,unknown,"waves of panels with the same randomly selected electors in the summer 1963, after the general elections 196 4 and after the general elections 1966 to determine habitual voting and vote switching. Among others, they ﬁ nd that voting patterns in favor of the three main parties (i.e., the Liberal Party, Labour Party, and the Conservative Party) are more complex to be fully captured by class. 34 3 A "
5938,unknown,"34 3 A Short Introduction to Survey Research References Almond, G., & Verba, S. (1963) [1989]. The civic culture: Political attitudes and democracy in ﬁve nations .Newbury Park, CA: Sage. Archer, K., & Berdahl, L. (2011). Explorations: Conducting empirical research in canadian poli- tical science . Toronto: Oxford University Press. Behnke, J., Baur, N., & Behnke, N. (2006). Empirische Methoden der"
5939,unknown,"Paderborn: Schöningh. Brady, H. E., Verba, S., & Schlozman, K. L. (1995). Beyond SES: A resource model of political participation. American Political Science Review, 89 (2), 271 –294. Burnham, P., Lutz, G. L., Grant, W., & Layton-Henry, Z. (2008). Research methods in politics (2nd ed.). Basingstoke, Hampshire: Palgrave Macmillan. Converse, J. M. (2011). Survey research in the United States: Roots "
5940,unknown,"Picataway: Transaction. De Leeuw, E. D., Hox, J. J., & Dillman, D. A. (2008). The cornerstones of survey research. In E. D. De Leeuw, J. J. Hox, & D. A. Dillman (Eds.), International handbook of survey method- ology. New York: Lawrence Erlbaum Associates. De Vaus, D. (2001). Research design in social research . London: Sage. ESS (European Social Survey). (2017). Source questionnaire . Retrieved Au"
5941,unknown,"ESS (European Social Survey). (2017). Source questionnaire . Retrieved August 7, 2017, from http://www.europeansocialsurvey.org/methodology/ess_methodology/source_questionnaire/ Fowler, F. J. (2009). Survey research methods (4th ed.). Thousand Oaks, CA: Sage. Frees, E. W. (2004). Longitudinal and panel data: Analysis and applications in the social sciences . Cambridge: Cambridge University Press. "
5942,unknown,"Hooper, K. (2006). Using William the conqueror ’s accounting record to assess manorial ef ﬁciency: A critical appraisal. Accounting History, 11 (1), 63 –72. References 35 Hurtienne, T., & Kaufmann, G. (2015). Methodological biases: Inglehart ’s world value survey and Q methodology . Berlin: Folhas do NAEA. Loosveldt, G. (2008). Face-to-face interviews . In E. D. De Leeuw, J. J. Hox, & D. A. Dillma"
5943,unknown,"(Eds.),International handbook of survey methodology . New York: Lawrence Erlbaum Associates. Petty, W., & Graunt, J. (1899). The economic writings of Sir William Petty (Vol. 1). London: University Press. Putnam, R. D. (2001). Bowling alone: The collapse and revival of American community . New York: Simon and Schuster. Schnell, R., Hill, P. B., & Esser, E. (2011). Methoden der empirischen Sozialfor"
5944,unknown,"München: Oldenbourg. Schumann, S. (2012). Repräsentative Umfrage: Praxisorientierte Einführung in empirische Meth- oden und statistische Analyseverfahren (6th ed.). München: Oldenbourg. Willcox, W. F. (1934). Note on the chronology of statistical societies. Journal of the American Statistical Association, 29 (188), 418 –420. Wood, E. J. (2003). Insurgent collective action and civil war in El Salva"
5945,unknown,"Cambridge University Press. Further Reading Why Do We Need Survey Research? Converse, J. M. (2017). Survey research in the United States: Roots and emergence 1890 –1960. New York: Routledge. This book has more of an historical ankle. It tackles the history of survey research in the United States. Davidov, E., Schmidt, P., & Schwartz, S. H. (2008). Bringing values back in: The adequacy of the Europ"
5946,unknown,"European Social Survey to measure values in 20 countries. Public Opinion Quarterly, 72 (3), 420 445. This rather short article highlights the importance of conducting a large pan-European– survey to measure European ’s social and political beliefs. Schmitt, H., Hobolt, S. B., Popa, S. A., & Teperoglou, E. (2015). European parliament election study 2014, voter study. GESIS Data Archive, Cologne. ZA"
5947,unknown,"study 2014, voter study. GESIS Data Archive, Cologne. ZA5160 Data ﬁ le Version ,2 (0). The European Voter Study is another important election study that researchers and students can access freely. It provides a comprehensive battery of variables about voting, political preferences, vote choice, demographics, and political and social opinions of the electorate. Applied Survey Research Almond, G. A."
5948,unknown,"Almond, G. A., & Verba, S. (1963). The civic culture: Political attitudes and democracy in ﬁ ve nations. Princeton: Princeton University Press. Almond ’s and Verba ’s masterpiece is a seminal work in survey research measuring citizens ’political and civic attitudes in key Western democracies. The book is also one of the ﬁ rst books that systematically uses survey research to measure political trai"
5949,unknown,"Inglehart, R., & Welzel, C. (2005). Modernization, cultural change, and democracy: The human development sequence . Cambridge: Cambridge University Press. This is an in ﬂuential book, which uses data from the World Values Survey to explain modernization as a process that changes individual ’s values away from traditional and patriarchal values and toward post- materialist values including environm"
5950,unknown,"Constructing a Survey 4 Abstract This chapter instructs students on how to conduct a survey. Topics covered include question design, question wording, the use of open- and closed-ended questions, measurement, pre-testing, and re ﬁning a survey. As part of this chapter, students construct their own survey. 4.1 Question Design In principle, in a survey a researcher can ask questions about what peopl"
5951,unknown,"they do, what attributes they have, and how much knowledge they have about an issue. Questions about opinions, attitudes, beliefs, and values —capture what people think about an issue, a person or an event These questions frequently give respondents some choices in their answers Example: Do you agree with the following statement? The European Union should create a crisis fund in order to be able t"
5952,unknown,"create a crisis fund in order to be able to rapidly bail out member countries, when they are in ﬁ nancial dif ﬁculties. (Possible answer choices: Agree, partly agree, partly disagree, and do not agree) Questions about individual behavior —capture what people do Example: Did you vote in the last election? (Possible answer choices, yes or no) Questions about attributes —what are peoples ’characteris"
5953,unknown,"Questions about attributes —what are peoples ’characteristics Example: age, gender, etc. Example: What is your gender? (Possible answers, male, female, no answer) Questions about knowledge —how much people know about political, social, or cultural issues Example: In your opinion, how much of its annual GDP per capita does Germany attribute toward development aid? # Springer International Publishin"
5954,unknown,"D. Stockemer, Quantitative Methods for the Social Sciences , https://doi.org/10.1007/978-3-319-99118-4_4 37 4.2 Ordering of Questions There are some general guidelines for constructing surveys that make it easy for participants to respond. 38 4 Constructing a Survey 1. The ordering of questions should be logical to the respondents and ﬂ ow smoothly from one question to the next. As a general rule,"
5955,unknown,"to speci ﬁc, impersonal to personal, and easy to dif ﬁcult. 2. Questions related to the same issue should be grouped together. For example, if a survey captures different forms of political participation, questions capturing voting, partaking in demonstrations, boycotting, and signing petitions should be grouped together. Ideally, the questions could also go from conventional political participati"
5956,unknown,"participation to unconventional political participation. The same applies to other sorts of common issues such as opinions about related subjects (e.g., satisfaction with the government ’s economic, social, and foreign policy, respectively). Con- sequently, it makes sense to divide the questionnaire in different sections, each of which should begin with a short introductory sentence. However, whil"
5957,unknown,"sense to cluster questions by theme, there should not be too many similar questions, with similar measurements either. In particular, the so-called consis- tency bias might come into play, if there are too many similar questions. Cogni- tively, some respondents wish to appear consistent in the way they answer the questions, in particular if these questions look alike. To mitigate the consistency b"
5958,unknown,"bias effects, it is useful to switch up the questions and do not have a row of 20 or 30 questions that are openly interrelated and use the same scale (Weisberg et al. 1996: 89 f.). 4.3 Number of Questions Researchers always have to draw a ﬁ ne line between the exhaustiveness and the parsimony of the questionnaire. They want to ask as many questions as are necessary to capture the dependent and all"
5959,unknown,"to capture the dependent and all necessary independent variables in the research project. Yet, they do not want to ask too many questions either, because the more questions that are included in an interview or a survey, the less likely people are to ﬁnish a face-to-face or phone interview or to return a completed paper or online questionnaire. There are no strict provisions for the length of a sur"
5960,unknown,"questionnaire. There are no strict provisions for the length of a survey, though. The general rule that guides the construction of a research could also guide the construc- tion of a questionnaire; a questionnaire should include as many questions as neces- sary and as few questions as possible. 4.4 Getting the Questions Right When writing surveys, researchers normally follow some simple rules. The"
5961,unknown,"wording needs to be clear, simple, and precise. In other words, questions need to be written so that respondents understand their meaning right away. In contrast, poorly 4.4 Getting the Questions Right 39 written questions lead to ambiguity and misunderstandings and can lead to untrust- worthy answers. In particular, vague questions, biased/value-laden questions, threat- ening questions, complex q"
5962,unknown,"should be avoided. 4.4.1 Vague Questions Vague questions are questions that do not clearly communicate to the respondent what the question is actually all about. For example, a question like “ Taken alto- gether, how happy are you with Chancellor Merkel? ”is unclear. It is imprecise because the respondent does not know what substantive area the questions is based on. Does the pollster want to know"
5963,unknown,"on. Does the pollster want to know whether the respondent is “ happy”with her rhetorical style, her appearance, her leadership style or her government ’s record, or all of the above? Also the word happy should be avoided because it is at least somewhat value laden. Hence, the researcher should re ﬁne her research question by specifying a policy area and a measurement scale and by using more neutra"
5964,unknown,"colloquial language. Hence a better question would be: Overall, how would you rate the performance of Chancellor Merkel in the domain of refugee policy during the refugee crisis in 2015 on a 0 –100 scale? 4.4.2 Biased or Value-Laden Questions Biased or value-laden questions are questions that predispose individuals to answer a question in a certain way. These questions are not formulated in a neut"
5965,unknown,"Rather, they use strong normative words. Consider, for example, the question: On a scale from 0 to 100, how evil do you think the German Christian Democratic Union (CDU) is? This question is clearly inappropriate for a survey as it bears judgement on the question subject. Therefore a better formulation of the same question would be: On scale from 1 to 100 how would you rate the performance of the "
5966,unknown,"Democratic Union in the past year? Ideally, the pollster could also add a policy area to this question. 4.4.3 Threatening Questions Threating questions might render respondents to surveys uneasy and/or make it hard for the respondent to answer to her best capacities. For instance, the question: “ Do you have enough knowledge about German politics to recall the political program of the four parties"
5967,unknown,"the four parties the Christian Democrats, the Social Democrats, the Green Party, and the Party of Democratic Socialism? ”might create several forms of uneasiness on the beholder. The person surveyed might question their capability to answer the ques- tion, they might assume that the pollster is judging them, and they might not know how to answer the question, because it is completely unclear what "
5968,unknown,"how to answer the question, because it is completely unclear what enough 40 4 Constructing a Survey knowledge means. Also, it might be better to reduce the question to one party, because citizens might know a lot about one party program and relatively few things about another program. A better question would be: On a scale from 0 to 100, how familiar are you with the political programs of the Soci"
5969,unknown,"familiar are you with the political programs of the Social Democratic Party for the General Election 2017. (0 means I am not familiar at all and 100 means I am an expert.) 4.4.4 Complex Questions Researchers should avoid complex questions that ask the polled about various issues at once. For example, a question like this: “ On a scale from 1 to 10, please rate, for each of the 12 categories listed"
5970,unknown,"each of the 12 categories listed below, your level of knowledge, con ﬁdence, and experience”should be avoided, as it confuses respondents and makes it impossible for the respondent to answer precisely. Rather than clustering many items at once, the researcher should ask one question per topic. 4.4.5 Negative Questions The usage of negative questions might induce bias or normative connotation into "
5971,unknown,question. A question such as “ On a scale from 0 to 100 how un ﬁt do you think American President Donald Trump is for the Presidency of the United States? ”is more value laden than the same question expressed in positive terms: “ On a scale from 0 to 100 how ﬁ t do you think American President Donald Trump is for Presidency of the United States? ”Using the negative form might also confuse people. 
5972,unknown,"people. Therefore, it is a rule in survey research to avoid the usage of negative wording. This includes the use of words such as “ not,”“ rarely,”“ never,”or words with negative pre ﬁxes“ in-,”“ im-,”“ un-.”Therefore, researchers should always ask their questions in a positive fashion. 4.4.6 Pointless Questions A pointless question is a question that does not allow the researcher or pollster to g"
5973,unknown,"gain any relevant information. For example, the form I-94 from the US Citizenship and Immigration Services that every foreigner, who enters the United States has to ﬁll out does not make any sense. The form asks any foreigner entering the United States: Have you ever been or are you now involved in espionage or sabotage or in terrorist activities or genocide, or between 1933 and 1945 were you invo"
5974,unknown,"way, in persecutions associated with Nazi Germany or its allies? Respondents can circle either yes or no. This question is futile in two ways. First, people involved in any of these illegal activities have no incentive to admit to it; admitting to it would automatically mean that their entry into the United States would be denied. Second, and possibly even more importantly, there remain very few i"
5975,unknown,"4.5 Social Desirability 41 theory, could have been involved in illegal activities during the Nazi era. And furthermore, virtually all of those very few individuals who are still alive are probably too old now to travel to the United States. 4.5 Social Desirability The“ social desirability paradigm ”refers to a phenomenon that we frequently encounter in social science research: survey respondents a"
5976,unknown,"encounter in social science research: survey respondents are inclined to give socially desirable responses, or responses that make them look good against the background of social norms or common values. The social desirability bias is most prevalent in theﬁ eld of sensitive self-evaluation questions. Respondents can be inclined to avoid disapproval, embarrassment, or legal consequences and therefo"
5977,unknown,"disapproval, embarrassment, or legal consequences and therefore opt for an incorrect answer in a survey. For example, respondents know that voting in elections is a socially desirable act. Hence, they might be tempted to af ﬁrm that they have voted in the last presidential or parliamentary election even if they did not. Vice versa, the consumption of hard drugs such as cocaine or ecstasy is a soci"
5978,unknown,"consumption of hard drugs such as cocaine or ecstasy is a socially undesirable act that is also punishable by law. Consequently, even if the survey is anonymously conducted, a respondent might not admit to consuming hard drugs, even if she does so regularly. The social desirability construct jeopardizes the validity of a survey as socially undesired responses are underestimated. In a simple survey"
5979,unknown,"undesired responses are underestimated. In a simple survey, it is very dif ﬁcult for a researcher to detect how much the social desirability bias impacts the given responses. One implicit way of detection would be to check if a respondent answers a number of similar questions in a socially desirable manner. For example, if a researcher asks if respondents have ever speeded, smoked cigarettes, smok"
5980,unknown,"juana, cheated with the taxes, or lied to a police of ﬁcer, and gets negative answers for allﬁ ve items, there is a possibility that the respondent chose social desirability over honesty. Yet, the researcher a priori cannot detect, whether this cheating with the answers happened and for what questions. The only thing she can do is to treat the answers of this particular questionnaire with care, wh"
5981,unknown,"answers of this particular questionnaire with care, when analyzing the questionnaire. Another, albeit also imperfect, way to detect socially desirable rather than correct answers is by follow-up questions. For example, a researcher could ﬁ rst ask the question. Did you vote in the last parliamentary election? A follow-up question could then ask the respondent for which party she voted for? This fo"
5982,unknown,"could include the do not know category. It is likely that respondents remember for which party they voted, in particularly, if the survey is conducted relatively shortly after the election. This implies that somebody, who indicated that she voted, but does not recall her party choice, might be a candidate for a socially desirable rather than an honest answer for the voting question (see Steenkamp "
5983,unknown,"honest answer for the voting question (see Steenkamp et al. 2010: 200 –202; Hoffmann 2014: 7 f.; Van de Vijver and He 2014: 7 for an extensive discussion of the social desirability paradigm). Not only can social desirability be inherent in attitudinal and behavioral questions, it can also come from outside in ﬂuences such as from the pollster itself. A famous example of the effect of socially desi"
5984,unknown,"42 4 Constructing a Survey Virginia gubernatorial race between the African-American Douglas Wilder and the Caucasian Marshall Coleman. Wilder, who had been ahead in the polls by a comfortable buffer of 4 –11%, ended up winning the election merely by the tiny margin of 0.6%. The common explanation for that mismatch between the projected vote margin in polls and the real vote margin was that a numbe"
5985,unknown,respondents interviewed by African-Americans declared their alleged preference for Wilder in order to appear tolerant in the eyes of the African-American inter- viewer (Krosnick 1999: 44 f.). 4.6 Open-Ended and Closed-Ended Questions In survey research we normally distinguish between two broad types of questions: open-ended and closed-ended questions. Questionnaires can be open ended and closed en
5986,unknown,"closed ended or can include a mixture of open- and closed-ended questions. The main difference between these types of questions is that open-ended questions allow respondents to come up with their own answers in their own words, while closed- ended questions require them to select an answer from a set of predetermined choices (see Table 4.1 for a juxtaposition between open- and closed-ended questi"
5987,unknown,"example of an open-ended question would be: “ Why did you join the German Christian Democratic Party? ”To answer such a question, the person surveyed must describe, in her own words, what factors enticed her to become a member of that speci ﬁc party and what her thought process was before joining. In contrast, an example of a closed-ended question would be: “ Did you partake in a demonstration in "
5988,unknown,"in the past 12 months? ”This question gives respondents two choices —they can either choose the af ﬁrmative answer or the negative answer. Of course, not all open- ended questions are qualitative in nature. For example, the question, “ how much money did you donate to the Christian Democratic Party ’s election campaign in 2012”requires a precise number. In fact, all closed- and open-ended question"
5989,unknown,"require numerical answers or answers that can relatively easily be converted into a number in order for them to be utilized in quantitative research (see Table 4.1). In fact, the usage of non-numerical open-ended questions and closed-ended questions frequently follows different logics. Open-ended questions are frequently Table 4.1 Open-ended versus closed-ended questions Open-ended questions Close"
5990,unknown,"No predetermined responses given Designed to obtain predetermined responses Respondent is able to answer in his or her own words Possible answers: yes/no; true/false, scales, values Useful in exploratory research and to generate hypotheses Useful in hypotheses testing Require skills on the part of the researcher in asking the right questions Easy to count, analyze, and interpret Answers can lack u"
5991,unknown,"to analyze There is the risk that the given question might not include all possible answers 4.6 Open-Ended and Closed-Ended Questions 43 Table 4.2Response choices for the question how satis ﬁed are you with the economic situation in your country? Operationalization 1 Scale from 0 to 100 (zero meaning not satis ﬁed at all, 100 meaning very satisﬁed) Operationalization 2 Scale from 0 to 10 (zero mea"
5992,unknown,"satisﬁed) Operationalization 3 5 value scale including the categories very satis ﬁed, satisﬁed, neutral, not satisﬁed, not satis ﬁed at all Operationalization 4 4 value scale including the categories very satis ﬁed, satisﬁed, not satis ﬁed, not satis ﬁed at all Operationalization 5 6 value scale including the categories very satis ﬁed, satisﬁed, neutral, not satisﬁed, not satis ﬁed at all, and do "
5993,unknown,"satisﬁed, not satis ﬁed at all, and do not know Operationalization 6 5 value scale including the categories very satis ﬁed, satisﬁed, not satis ﬁed, not satis ﬁed at all, I do not know used in more in-depth questionnaires and interviews aiming to generate high-quality data that can help researchers generate hypotheses and/or explain causal mechanisms. Since it can sometimes be tricky to coherently"
5994,unknown,"mechanisms. Since it can sometimes be tricky to coherently summarize qualitative interview data, qualitative researchers must frequently utilize sophisticated data analytical techniques including re ﬁned coding schemes and thought-through docu- ment analytical techniques (Seidman 2013; Yin 2015). However, since this is a book about quantitative methods and survey research, I do not discuss qualita"
5995,unknown,"interviewing and coding in detail here. Rather, I focus on quantitative research. Yet, working with a set of categories is not without caveats either; it can lead to biased answers and restrict the respondent in his or her answering possibilities. As a rule, the amount of response choices should not be too restricted to give respondents choices. At the same time, however, the amount of options sho"
5996,unknown,"choices. At the same time, however, the amount of options should not exceed the cognitive capacities of the average respondent either. With regard to a phone or face- to-face interview, there is some consensus that not more than seven response categories should be offered (Schumann 2012: 74). The choice of the appropriate number of response choices can be a tricky process. As a general rule, it ma"
5997,unknown,"As a general rule, it makes sense to be consistent with respect to the selection of response scales for similar questions to prevent confusing the respondent (Weisberg et al. 1996: 98). For example, if a researcher asks in a survey of citizens ’satisfaction with the country ’s economy, the country ’s government, and the country ’s police forces, it makes sense to use the same scale for all three q"
5998,unknown,"forces, it makes sense to use the same scale for all three questions. However, there are many options I can use. Consider the following question: How satis ﬁed are you with the economic situation in your country? A researcher could use a scale from 0 to 10 or a scale from 0 to 100, both of which would allow respondents to situate themselves. A researcher could also use a graded measure with or wit"
5999,unknown,"themselves. A researcher could also use a graded measure with or without a neutral category and/or a do not know option (for a list of options see Table 4.2). To a certain degree, the chosen operationalization depends on the purpose of the study and the research question but to a certain degree is also a question of taste. 44 4 Constructing a Survey 4.7 Types of Closed-Ended Survey Questions Resea"
6000,unknown,"Researchers can choose from a variety of questions including scales, dichotomous questions, and multiple-choice questions. In this section, this book presents the most important types of questions: 4.7.1 Scales Scales are ubiquitous in social sciences questionnaires. Nearly all opinion-based questions use some sort of scale. An example would be: “ Rate your satisfaction with your country’s police "
6001,unknown,"your country’s police forces from 0 (not satis ﬁed at all) to 10 (very satis ﬁed).”Many questions tapping into personal attributes use scales, as well [e.g., “ how would you rate your personal health? ”(0, poor; 1, rather poor; 2, average; 3, rather good; 4, very good)]. Finally, some behavioral questions use scales, as well. For example, you mightﬁ nd the question: “ How often do you generally wa"
6002,unknown,"mightﬁ nd the question: “ How often do you generally watch the news on TV? ” Response options could be not at all (coded 0), rather infrequently (coded 1), sometimes (coded 2), rather frequently (coded 3), and very frequently (coded 4). The most prominent scales are Likert and Guttman scales. Likert Scale A Likert Scale is the most frequently used ordinal variable in questionnaires (maybe the most"
6003,unknown,"questionnaires (maybe the most frequently used type of questions overall) (Kumar 1999: 129). Likert Scales use ﬁ xed choice response formats and are designed to measure attitudes or opinions (Bowling 1997; Burns and Grove 1997). Such a scale assumes that the strength/intensity of the experience is linear, i.e., on a continuum from strongly agree to strongly disagree, and makes the assumption that "
6004,unknown,"be measured. Respondents may be offered a choice of ﬁ ve to seven or even nine pre-coded responses with the neutral point being neither agree nor disagree (Kumar 1999: 132). Example 1: Going to War in Iraq Was a Mistake of the Bush Administration The respondent has ﬁ ve choices: strongly agree, somewhat agree, neither agree nor disagree, somewhat disagree, and strongly disagree. Example 2: How Oft"
6005,unknown,"Example 2: How Often Do You Discuss Politics with Your Peers? The respondent has ﬁ ve choices: very frequently, frequently, occasionally, rarely, and never. In the academic literature, there is a rather intense debate about the usage of the neutral category and the do not know option. This applies to Likert Scales and other survey questions as well. Including a neutral category makes it easier for"
6006,unknown,"people to choose/respond. It might be a safe option, in particular for individuals, who prefer not taking a choice. However, excluding the neutral option forces individuals to take a position, even if they are torn, or really stand in the middle for the precise question at hand (Sapsford 2006; Neuman and Robson 2014). A similar logic applies 4.7 Types of Closed-Ended Survey Questions 45 to the do "
6007,unknown,"to the do not know option (see Mondak and Davis 2001; Sturgis et al. 2008). The disadvantage of this option is that many respondents go with “ no opinion ”just because it is more comfortable. In most cases, respondents choose this option due to conﬂicting views and not due to a lack of knowledge. So, in a sense, they usually do have an opinion and lean at least slightly to one side or another. Tha"
6008,unknown,"inclusion of a “ no opinion ”option can reduce the number of respondents who give answers to more complex subjects. On the other hand, if this option is omitted, respondents can feel pressed to pick a side although they really are indifferent, for example, because they know very little about the matter. Pushing respondents to give arbitrary answers can therefore affect the validity and reliability"
6009,unknown,"For example, somebody might not be interested in economics and besides getting her monthly paycheck has no idea how the economy functions in her country. Such a person can only take a random guess for this question, if the “ do not know ”option is not included in the questionnaire. For sure, she could leave out the question, but this might feel challenging. Hence, the respondent might feel compell"
6010,unknown,"answer even if it is just a random guess. While there is no panacea for avoiding these problems, the researcher must be aware of the dif ﬁculty of ﬁ nding the right number and type of response categories and might think through these categories carefully. A possible way of dealing with this challenge is to generally offer “ no opinion ”options, but to omit them for items on well-known issues or qu"
6011,unknown,"on well-known issues or questions where individuals should have an opinion. For example, if you ask respondents about their self-reported health, there is no reason to assume that respondents do not know how they feel. In contrast, if you ask them knowledge questions (e.g., “ who is the president of the European Commission? ”), the do not know option is a viable choice, as it is highly possible th"
6012,unknown,"uninterested citizens do not know who the president of the European Commission is. Alternatively, the researcher can decide to omit the “ no opinion ”option and test the strength of an attitude through follow-up questions. This way, it might become evident whether the given response represents a real choice rather than a guess (Krosnick 1999: 43 f.; Weisberg et al. 1996: 89 f.). For example, a pol"
6013,unknown,"(Krosnick 1999: 43 f.; Weisberg et al. 1996: 89 f.). For example, a pollster could ﬁ rst ask an individual about her political knowledge using the categories low, rather low, middle, rather high, and very high. In a second step, she could ask “ hard”questions about the function of the political system or the positioning of political parties. These “hard”questions could verify whether the responden"
6014,unknown,"level accurately. The Semantic Differential Scale The use of the semantic differential scale allows for more options than the use of a Likert Scale, which is restricted to four or ﬁ ve categories. Rather than having each category labeled, the semantic scale uses two bipolar adjectives at each end of the question. Semantic differential scales normally range from 7 to 10 response choices (see Tables"
6015,unknown,Large Rating Scales These are scales that have a larger range than 0 –10. Such scales can often have a range from 0 to 100. Within this range the respondent is free 46 4 Constructing a Survey Table 4.3 Example of a semantic differential scale with seven response choices How satis ﬁed are you with the services received? 1 7 Not at all satis ﬁed Very satis ﬁed Table 4.4 Example of a semantic differe
6016,unknown,"Table 4.4 Example of a semantic differential scale with ten response choices Do you think immigrants make your country a better or a worse place to live in? 1 10 Better place Worse place to choose the number that most accurately re ﬂects her opinion. For example, researchers could ask respondents how satis ﬁed they are with the state of the economy in their country, on a scale from 0, not satis ﬁe"
6017,unknown,"economy in their country, on a scale from 0, not satis ﬁed at all, to 100 very satis ﬁed. Guttman Scale The Guttman scale represents another rather simple way of mea- suring ordinal variables. This scale is based on a set of items with which the respondents can agree or disagree. All items refer to the exact same variable. However, they vary in their level of “ intensity”which means that even peop"
6018,unknown,"with low agreement might still agree with the ﬁ rst items (questions with a low degree of intensity usually come ﬁ rst) while it takes high stakes for respondents to agree with the last ones. The respondent ’s score is the total number of items she agrees with —a high score implies a high degree of agreement with the initial questions. Since, all items of one variable are commonly listed in increa"
6019,unknown,"according to their intensity, this operationalization is based on the assumption that if a respondent agrees with any one of the asked items, she should have agreed with all previous items too. The following example clari ﬁes the idea of “ intensity”in response patterns. Example: Do You Agree or Disagree that Abortions Should Be Permitted? 1. When the life of the woman is in danger. 2. I n the cas"
6020,unknown,"2. I n the case of incest or rape. 3. When the fetus appears to be unhealthy. 4. When the father does not want to have a baby. 5. When the woman cannot afford to have a baby. 6. Whenever the woman wants. Due to the Guttman scale ’s basic assumption, a respondent with a score of 4 agrees with the ﬁ rst four items and disagrees with the last two items. The largest challenge when using a Guttman scal"
6021,unknown,"challenge when using a Guttman scale is to ﬁ nd a suitable set of items that (perfectly) match the required response patterns. In other words, there must be graduation between each response category and consensus that agreement to the fourth item is more dif ﬁcult and restricted than agreement to the third item. 4.7 Types of Closed-Ended Survey Questions 47 Table 4.5 Example of single-answer multi"
6022,unknown,Who is your favorite politician among the four politicians listed below? A Donald Trump B Emmanuel Macron C Angela Merkel D Theresa May Table 4.6 Example of a multiple-answer multiple-choice question Which medium do you use to get your political information from? (click all that apply) A Television news broadcasts B Radio C Internet D Newspaper E Discussion with friends F Other (please specify) G 
6023,unknown,"F Other (please specify) G I do not inform myself politically 4.7.2 Dichotomous Survey Questions The dichotomous survey question normally leaves respondents with two answering choices. These two choices can include personal characteristics such as male and female, or they can include questions about personal attributes such as whether somebody is married or not. Example: Did You Participate in a L"
6024,unknown,"Example: Did You Participate in a Lawful Demonstration in the Past 12 Months? (Yes/No) Please note that some questions that were asked dichotomously for decades have recently been asked in a more trichotomous fashion. The prime example is gender. For decades, survey respondents had two options to designate their gender man or woman, not considering that some individuals might not identify with eit"
6025,unknown,"dichotomous choices. Thus the politically correct way now is to include a third option such as neither nor. 4.7.3 Multiple-Choice Questions Multiple-choice questions are another form of a frequently used question type. They are easy to use, respond to, and analyze. The different categories the respondents can choose from must be mutually exclusive. Multiple-choice questions can be both single answ"
6026,unknown,"single answer (i.e., the respondent can only pick one option) and multiple answer (i.e., the respondent can pick several answers) (see Tables 4.5 and 4.6). One of the weaknesses of some type of multiple-choice questions is that the choice of responses is rather limited and sometimes not ﬁ nal. For example, in 48 4 Constructing a Survey Table 4.7 Example of a categorical survey question withﬁ ve ch"
6027,unknown,"Which category best describes your age? Younger than 18 19–34 35–50 51–64 65 and higher Table 4.8 Example of a categorical survey question with six choices In which bracket does your net income fall? Lower than $ 20,000 $20,000–$39,999 $40,000–$59,999 $60,000–$79,999 $80,000–$99,999 Over 100,000 Table 4.6, some body could receive her political information from weekly magazines, talk shows, or poli"
6028,unknown,"magazines, talk shows, or political parties. In the current example, we have not included these options for parsimony reasons. Yet, the option E (other) allows respondents to add other items, thus allowing for greater inclusivity. (But please note that if the respondents list too many options under the category “ other,”the analysis of the survey becomes more complicated). 4.7.4 Numerical Continuo"
6029,unknown,"Numerical continuous questions ask respondents a question that in principle allows the respondent to choose from an in ﬁnite number of response choices. Examples would be questions that ask: what is your age? What is your net income? 4.7.5 Categorical Survey Questions Frequently, researchers split continuous numerical questions into categories. These categories frequently correspond to established"
6030,unknown,"categories frequently correspond to established categories in the literature. For example, instead of asking what your age is, you could provide age brackets distinguishing young individuals (i.e., 18 and younger), rather young individuals (19–34), middle-aged individuals (35 –50), rather old individuals (51 –64), and old individuals (65 and higher) (see Table 4.7). The same might apply to income."
6031,unknown,"example, we could have income brackets for every $ 20,000 (see Table 4.8). Using brackets or categories might be particularly helpful for features, where respondents do not want to reveal the exact number. For example, some respondents might not want to reveal their exact age. The same might apply to income. If they are 4.7 Types of Closed-Ended Survey Questions 49 Table 4.9 Example of a rank orde"
6032,unknown,4.7 Types of Closed-Ended Survey Questions 49 Table 4.9 Example of a rank order question Consecutively rank the following ﬁ ve parties from most popular (coded 1) to least popular (coded 5) Christian Democratic Party Social Democratic Party Free Democratic Party Green Party Left Party Alternative for Germany Table 4.10 Example of a matrix table question How would you rate the following attributes 
6033,unknown,"Well below average Below average Average Above Average Well above average Honesty Competence Trustworthiness Charisma offered rather broad age or income brackets, they might be more willing to provide an answer than be probed for their exact age or income. 4.7.6 Rank-Order Questions Sometimes researchers might be interested in rankings. Rank-order questions allow respondents to rank persons, brand"
6034,unknown,"respondents to rank persons, brands, or products based on certain attributes such as the popularity of politicians or the vote choice for parties. In rank-order questions, the respondent must consecutively rank the choices from most favorite to least favorite (see Table 4.9). 4.7.7 Matrix Table Questions Matrix-level questions are questions in tabular format. They consist of multiple questions wit"
6035,unknown,"questions with the same response choices. The questions are normally connected to each other, and the response choices frequently follow a scale such as a Likert scale (see Table 4.10). 50 4 Constructing a Survey 4.8 Different Variables Regardless of the type of survey questions, there are four different ways to operationalize survey questions. The four types of variables are string variables, con"
6036,unknown,"continuous variables (interval variables), ordinal variables, and nominal variables. Astring variable is a variable that normally occupies the ﬁ rst column in a dataset. It is a non-numerical variable, which serves as the identi ﬁer. Examples are individuals in a study or countries. This variable is normally not part of any analysis. Acontinuous variable can have, in theory, an in ﬁnite number of "
6037,unknown,"(e.g., age, income). Such a question normally follows from a continuous numerical question. To highlight, personal income levels can in theory have any value between 0 and in ﬁnity. Aninterval variable is a speci ﬁc type of variable; it is a continuous variable with equal gaps between values. For example, counting the income in steps of thousand would be an interval variable: 1000, 2000, 3000, 400"
6038,unknown,"Anominal variable is categorized. There is no speci ﬁc order or value to the categorization (e.g., the order is arbitrary). Because there is no hierarchy in the organization of the data, this type of variable is the most dif ﬁcult to present in datasets (see discussion under Sect. 4.9.1). An example of a two-categorical nominal variable would be gender (i.e., men and women). Such a variable is als"
6039,unknown,"women). Such a variable is also called dichotomous or dummy variable. An example of a categorical variable with more than two categories would be religious af ﬁliation (e.g., Protestant, Catholic, Buddhist, Muslim, etc.) Anordinal variable consists of data that are categorized, and there is a clear order to the categories. Normally all scales are transformed into ordinal variables. For example, ed"
6040,unknown,"For example, educational experience is a typical variable to be grouped as an ordinal variable. For example, a coding in the following categories could make sense: no elementary school, elementary school graduate, high school graduate, college graduate, Master ’s degree, and Ph.D. An ordinal variable can also be a variable that categorizes a variable into set intervals. Such an interval question c"
6041,unknown,intervals. Such an interval question could be: How much time does it take you to get to work in the morning? Please tick the applicable category (see Table 4.11). What is important for such an ordinal coding of a continuous variable is that the intervals are comparable and that there is some type of linear progression. The most frequent types of ordinal variables are scales. Representative of such
6042,unknown,scales would be the answer to the question: Please indicate how satis ﬁed you are with Chancellor Merkel ’s foreign policy on a scale from 1 to 5 (see Table 4.12). Table 4.11 Ordinal coding of the variable time it takes somebody to go to work Less than 10 min 10-30 min 30-60 min More than 60 min Table 4.12Ordinal coding of the scaled variable satisfaction with Chancellor Merkel 1 not satisfied at 
6043,unknown,"1 not satisfied at all 2 3 4 5 very satisfied 4.9 Coding of Different Variables in a Dataset Except for string variables, question responses need to be transformed into numbers to be useful for data analytical purposes. Table 4.13 highlights some rules how this is normally done: 4.9 Coding of Different Variables in a Dataset 51 1. In the ﬁ rst column in Table 4.13, we have a string variable. This "
6044,unknown,"identiﬁes the participants of the study. In a real scienti ﬁc study, the researcher would render the names anonymous and write: student 1, student 2, ... 2. The second column is dichotomous variable (i.e., a variable with two possible response choices). Whenever researchers have a dichotomous variable, they create two categories labeling the ﬁ rst category 0 and the second category 1. (For any sta"
6045,unknown,"1. (For any statistical analysis, it does not matter which one of the two categories you code 0 and 1, but, of course, you need to remember your coding to interpret the data correctly later.) 3. Columns three and four are continuous variables representing the self-reported income and self-reported age the respondent has given in the questionnaire. 4. The fourth column captures a three-item ordinal"
6046,unknown,"4. The fourth column captures a three-item ordinal variable asking individuals about their satisfaction with their job. The choices respondents have are low satisfaction (coded 0), medium satisfaction (coded 1), and high satisfaction (coded 2). Normally, ordinal variables are consecutively labeled from 0 or 1 for the lowest category to how ever many categories there are. To code ordinal variables,"
6047,unknown,"important that categories are mutually exclusive (i.e., one value cannot be in two categories). It is also important that progression between the various categories is linear. 4.9.1 Coding of Nominal Variables For some analysis such as regression analysis (see Chaps. 8 and 9), non-dichotomous categorical variables (e.g., different religious af ﬁliations) cannot be expressed in any ordered form, be"
6048,unknown,"ordered form, because there is no natural progression in their values. For all statistical tests that assume progression between the categories, this causes a problem. To circumvent this problem for these tests, we use dummy variables to express such a variable. The rule for the creation of dummy variables is that I create one dummy variable less than I have categories. For example, if I have four"
6049,unknown,"Table 4.13Representing string, dichotomous, continuous, and ordinal variables in a dataset 52 4 Constructing a Survey Student Gender Income Age Job satisfaction John 1 12,954 43 2 Mary 0 33,456 67 1 James 1 98,534 54 0 Table 4.14 Representing a nominal variable in a dataset Dummy 1 Dummy 2 Dummy 3 Muslim 0 0 0 Christian 1 0 0 Buddhist 0 1 0 Hindu 0 0 1 three dumm y variables, with the ﬁ rst catego"
6050,unknown,"Hindu 0 0 1 three dumm y variables, with the ﬁ rst category serving what is called the reference category. In the example in Table 4.14, the Muslim religion is the reference category against which we compare the other religions. (Since this procedure is rather com- plex, we will discuss the creation of dummy variables in Chap. 5 again.) 4.10 Drafting a Questionnaire: General Information In real re"
6051,unknown,"In real research, the selection of an overarching question guiding the research process and the development of a questionnaire is theory driven. In other words, social science theory should inform researchers ’choices of survey questions, and a good survey should respond to a precise theoretically driven research question (Mark 1996: 15f). Yet, for the exercise in this book, where you are supposed"
6052,unknown,"1996: 15f). Yet, for the exercise in this book, where you are supposed to create your own questionnaire, you are not supposed to be an expert in a particular ﬁ eld. Rather, you can use your general interest, knowledge, and intuition to draft a questionnaire. Please also keep in mind that you can use your personal experience to come up with the topic and overarching research question of your sample"
6053,unknown,"the topic and overarching research question of your sample survey. To highlight, if a researcher has been affected by poverty in his childhood, he might intuitively have an idea about the consequences of child poverty. In her case, it is also likely that she has read about this phenomenon and liked a hypothesis of one of the authors she has read (e.g., that people who suffered from poverty in thei"
6054,unknown,"read (e.g., that people who suffered from poverty in their childhood are less likely to go to college). Once you have identi ﬁed a topic and determined the goals and objectives of your study, think about the questions you might want to include in your survey. Locating previously conducted surveys on similar topics might be an additional step enabling you to discover examples of different research "
6055,unknown,"the same topic. When you select the subject of your study, it is important to take the subject ’s practicability into account, as well. For one, a study on the political participation of university students can be relatively easily carried through by a university professor or a student. On the other hand, studies on human behavior in war situations for instance are very dif ﬁcult to be carried out"
6056,unknown,"instance are very dif ﬁcult to be carried out (Schnell et al. 2011: 3 f.). Also keep in mind that before thinking about the survey questions, you must have identi ﬁed dependent, independent, and control variables. You must have a concrete idea how these variables can be measured in your survey. For example, if a researcher wants to explain variation in a student ’s grades, a student ’s cumulative "
6057,unknown,"explain variation in a student ’s grades, a student ’s cumulative grade average is the dependent variable. Possible independent or explanatory variables are the numbers of hours a student studies per week, the level of her class attendance gauged in percent of all classes, her interest in her ﬁ eld of study gau ged from 0 not interested at all to 10 very interested, and her general life satisfacti"
6058,unknown,"all to 10 very interested, and her general life satisfaction again measured on a 0 –10 scale. In addition, she might add questions about the gender, place of residency (city, suburban area, or countryside), and the year of study (i.e., freshman, sophomore, junior, and senior or ﬁ rst, second, third, and fourth year). 4.10.1 Drafting a Questionnaire: A Step-by-Step Approach 4.10 Drafting a Question"
6059,unknown,"4.10 Drafting a Questionnaire: General Information 53 1. Think of an interesting social science topic of your choice (be creative), some- thing that is simple enough to ask fellow students or your peers. 2. Specify the dependent variable as a continuous variable. 1 3. Think of six to eight independent (explanatory) variables which might affect the dependent variable. You should include three types"
6060,unknown,"dependent variable. You should include three types of independent variables: continuous, ordinal, and nominal/dichotomous variables. Do not ask questions that are too sensitive, and do not include open-ended questions. 4. On the question sheet, clearly label your dependent variable and your indepen- dent variables. 5. On a separate sheet, add a short explanation where you justify your study topic."
6061,unknown,"Also add several sentences per independent variable where you formulate a hypothesis and very quickly justify your hypothesis. In justifying your hypothe- sis, you do not need to consult the academic literature. Rather you can use your intuition and knowledge of the work you have already read. 6. I f you encounter problems specifying and explaining a hypothesis, the survey question you have chosen"
6062,unknown,"question you have chosen might be suboptimal, and you may want to choose another research question. 7. Add a very short introduction to your survey, where you introduce your topic. The example below can serve as a basis when you construct your survey. 1For data analytical reasons, it is important to specify your dependent variable as a continuous variable, as the statistical techniques, you will l"
6063,unknown,variable is continuous. 54 4 Constructing a Survey Sample Questionnaire Dear participants This survey is about the money college students spend partying and possible factors that might in ﬂuence students ’partying expenses. Please answer the following questions which will be exclusively used for data analytical purposes in my political science research methods class. If you are not sure about an a
6064,unknown,"not sure about an answer, just give an estimate. All answers will be treated conﬁdentially. I thank you for contributing to our study. Dependent variable : • How much money do you spend partying per week? Independent variables : • What is your gender? Male Female (circle one) • What is your year of study? 123456 o r higher (circle one) • • On average, how many hours do you spend studying per week?"
6065,unknown,"On average, how many days do you go partying per week? 12345 o r more (circle one) • On a scale from 0 to 100, determine how much fun you can have without alcohol (0 meaning you can have a lot of fun, 100 you cannot have fun at all). • On a scale from 0 to 100, determine the quality of the of ﬁcially sanctioned free-time activities at your institution (0 meaning they are horrible, 100 meaning they"
6066,unknown,"100 meaning they are very good). • What percent of your tuition do you pay yourself? 4.11 Background Information About the Questionnaire Study PurposeThis research analyzes university students ’spending patterns, when they go out to party. This information might be important for university administrations, businesses, and parents. It might also serve as an indication of how serious they take their"
6067,unknown,"how serious they take their studies. Finally, it gives us some measurement on how much money students have at their disposal and how they spend it (or at least part of it). Hypotheses: H(1): Guys spend more money than girls. References 55 It is highly possible that guys like the bar scene more than girls do. For one, they go there to watch sports events such as soccer games. Additionally, they lik"
6068,unknown,"to hang out there with friends while having some pints. H(2): The more advanced students are in their university career, the less money and time they spend going out. More advanced classes are supposedly getting more dif ﬁcult than ﬁ rst or second year classes. This would imply that students must spend more time studying and would therefore have less time to go out. This, in turn, would entail tha"
6069,unknown,"that they are unlikely to spend money for partying purposes. H(3): The more time students spend studying, the less money they spend going out. The rationale for this hypothesis is that students that study long hours just do not have the time to go out a lot and hence are unlikely to spend a lot of money. H(4): The more students think they need alcohol to have fun, the more they will spend going ou"
6070,unknown,"spend going out. Alcohol is expensive, and the more students drink while going out, the more money they will spend at bars and nightclubs. H(5): The more students think that their university offers them good free time activities, the less money they will spend while going out. If students spend a lot of their free time participating in university sponsored sports or social clubs, they will not hav"
6071,unknown,"sports or social clubs, they will not have the time to go to bars or nightclubs frequently. As a result, they will not spend signi ﬁcant amounts of money while going out. H(6): People that pay their tuition in full or partly will not spend as much money at bars than people that have their tuition paid. The rationale for this hypothesis is straightforward: students that must pay a signiﬁcant amount"
6072,unknown,"and nightclubs. References Bowling, A. (1997). Research methods in health . Buckingham: Open University Press. Burns, N., & Grove, S. K. (1997). The practice of nursing research conduct, critique, & utilization . Philadelphia: W.B. Saunders. Hoffmann, A. (2014). Indirekte Befragungstechniken zur Kontrolle sozialer Erwünschtheit in Umfragen.Doctoral Thesis, Düsseldorf. Krosnick, J. A. (1999). Maxim"
6073,unknown,"Krosnick, J. A. (1999). Maximizing questionnaire quality . In J. P. Robinson, P. R. Shaver, & L. S. Wrightsman (Eds.),Measures of political attitudes: Volume 2 in measures of social psychologi- cal attitudes series . San Diego: Academic Press. Kumar, R. (1999). Research methodology: A step-by-step guide for beginner s. In E. D. De Leeuw, J. J. Hox, & D. A. Dillman (Eds.), International handbook of"
6074,unknown,"Lawrence Erlbaum Associates. Mark, R. (1996). Research made simple: A handbook for social workers . Thousand Oaks, CA: Sage. Mondak, J., & Davis, B. C. (2001). Asked and answered: Knowledge levels when we will not take “don’t know ”for an answer. Political Behaviour, 23 (3), 199 –224. Neuman, W. L., & Robson, K. (2014). Basics of social research . Toronto: Pearson Canada. Sapsford, R. (2006). Surv"
6075,unknown,"Sapsford, R. (2006). Survey research . Thousand Oaks, CA: Sage. Schnell, R., Hill, P. B., & Esser, E. (2011). Methoden der empirischen Sozialforschung(9th ed.). München: Oldenbourg. Schumann, S. (2012). Repräsentative Umfrage: Praxisorientierte Einführung in empirische Methoden und statistische Analyseverfahren (6th ed.). München: Oldenbourg. Seidman, I. (2013). Interviewing as qualitative researc"
6076,unknown,"the social sciences . New York: Teachers College Press. Steenkamp, J.-B., De Jong, M. G., & Baumgartner, H. (2010). Socially desirable response tendencies in survey research. Journal of Marketing Research, 47 (2), 199 –214. Sturgis, P., Allum, N., & Smith, P. (2008). An experiment on the measurement of political knowledge in surveys. Public Opinion Quarterly, 72 (1), 90 –102. Van de Vijver, F. J. "
6077,unknown,"Van de Vijver, F. J. R., & He, J. (2014). Report on social desirability, midpoint and extreme responding in TALIS 2013. OECD Education Working Papers, No. 107. Paris: OECD. Weisberg, H. F., Krosnick, J. A., & Bowen, B. D. (1996). An introduction to survey research, polling, and data analysis (3rd ed.). Thousand Oaks, CA: Sage. Yin, R. K. (2015). Qualitative research from start to ﬁ nish. New York:"
6078,unknown,"Further Reading Nuts and Bolts of Survey Research 56 4 Constructing a Survey Nardi, P. M. (2018). Doing survey research: A guide to quantitative methods . London: Routledge (Chap. 1; Chap. 4). Chapter 1 of this book provides a nice introduction why we do survey research, why it is important, and what important insights it can bring to the social sciences. Chapter 4 gives a nice introduction into q"
6079,unknown,"go into the construction of a survey. Constructing a Survey Krosnick, J. A. (2018). Questionnaire design. In The Palgrave handbook of survey research (pp. 439 –455). Basingstoke: Palgrave Macmillan. Short and comprehensive summary of the dominant literature into questionnaire design. Good as a ﬁ rst read of the topic. Saris, W. E., & Gallhofer, I. N. (2014). Design, evaluation, and analysis of que"
6080,unknown,"research. San Francisco: Wiley. A very comprehensive guide into the design of surveys. Among others, the book thoroughly discusses how concepts become questions, how we can come up with response categories for the questions we use, and how to structure questions. Applied Texts: Question Wording Lundmark, S., Gilljam, M., & Dahlberg, S. (2015). Measuring generalized trust: An examination of questio"
6081,unknown,"question wording and the number of scale points. Public Opinion Quarterly, 80(1), 26 –43. The authors show that the question wording of the general questions about trust in other individuals (i.e. generally speaking, would you say that most people can be trusted?) matters in getting accurate responses. Conducting a Survey 5 Abstract When the questionnaire is in its ﬁ nal form, the researcher needs"
6082,unknown,"the sample and what the population of her study is. This chapter ﬁ rst explains both terms and further distinguishes between random, representative, and biased samples. Second, it discusses several sampling techniques such as quota sampling and snowball sampling. Third, it introduces different types of surveys (e.g., face- to-face surveys, telephone surveys, and mail-in or Internet surveys) the au"
6083,unknown,"a survey can use to distribute it. As a practical component, students test their surveys in an empirical setting by soliciting answers from peers. While this procedure does not allow students to get representative or random samples, it nevertheless offers students the possibility to collect their own data, which they can analyze later. At the end of the unit, students are taught how to input their"
6084,unknown,"responses into an SPSS or Stata dataset. 5.1 Populat ion and Sample When the questionnaire is in its ﬁ nal form, the researcher needs to determine what the population of her study is and what type of sample she will take (see Fig. 5.1). Thepopulation is the entire group of subjects the researcher wants information on. Normally the researcher or polling ﬁ rm is not able to interview all units of th"
6085,unknown,"population because of the sheer size. To highlight, the United States has over 300 million inhabitants, Germany over 80 million, and France over 65 million. It is logistically and ﬁ nancially impossible to interview the whole population. There- fore, the pollster needs to select a sample of the population instead. To do so, she ﬁrst needs to de ﬁne a sampling frame . A sampling frame consists of a"
6086,unknown,"which the sample will be drawn. Ideally, the sample frame should be identical to the population or at least closely resemble it. In reality, population and sampling frame frequently differ (Weisberg et al. 1996: 39). For example, let us consider that the # Springer International Publishing AG 2019 D. Stockemer, Quantitative Methods for the Social Sciences , https://doi.org/10.1007/978-3-319-99118-"
6087,unknown,"57 Fig. 5.1Graphical display of a population and a sample ________________ ________________ Population Sample population are all inhabitants of Berlin. A reasonable sampling frame would include all households in the German capital, from which a random sample could be taken. Using this sample frame, a researcher could then send a questionnaire or survey to these randomly chosen households. However,"
6088,unknown,"these randomly chosen households. However, this sampling frame does not include homeless people; because they do not have a ﬁ xed address, they cannot receive the questionnaires. Consequently, this sample drawn from the popul ation will be slightly biased, as it will not include the thousands of people, who live on the streets in Berlin. 58 5 Conducting a Survey Asampl eis a subset of the populati"
6089,unknown,"Asampl eis a subset of the population the researcher actually examines to gather her data. The collected data on the sample aims at gaining information on the entire population (Bickman and Rog 1998: 102). For example, if the German government wants to know whether individuals favor the introduction of a highway usage fee in Germany, it could ask 1000 people whether or not they agree with this pro"
6090,unknown,"this survey, the population is the 82 million habitants of Germany, and the sample is the 1000 people, which the government asks. To make valid inference from a sample for a whole population, the sample should be either representative or random. 5.2 Representative, Random, and Biased Samples Representative SampleA representative sample is a sample in which the people in the sample have the same ch"
6091,unknown,"the sample have the same characteristics as the people in the population. For example, if a researcher knows that in the population she wishes to study 55% of people are men, 18% are African –Americans, 7% are homeless, and 23% earn more than 100,000 Euros, she should try to match these characteristics in the sample in order to represent the population. Random SampleIn many social settings, it is "
6092,unknown,"Random SampleIn many social settings, it is basically impossible for researchers to match the population characteristics in the sample. Rather than trying any matching technique, researchers can take a random sample. Randomization helps to offset the confounding effects of known and unknown factors by randomly choosing cases. For example, the lottery is a random draw of 6 numbers between 1 and 49."
6093,unknown,"1 and 49. Similarly large-scale international surveys (e.g., the European Social Survey) use randomization techniques to select participants (Nachmias and Nachmias 2008). Ideally, such randomization techniques give every individual in the population the same chance to be selected in the sample. 5.2 Representative, Random, and Biased Samples 59 Biased Sample A biased sample is a sample that is neit"
6094,unknown,"Biased Sample A biased sample is a sample that is neither representative nor random. Rather than being a snapshot of the population, a biased sample is a sample, whose answers do not re ﬂect the answers we would get had we the possibility to poll the whole population. There are different forms of biases survey responses can suffer from: Selection Bias We have selection bias if the sample is not re"
6095,unknown,"Selection Bias We have selection bias if the sample is not representative of the population it should represent. In other words, a sample is biased if some type of individuals such as middle-class men are overrepresented and other types such as unemployed women are underrepresented. The more this is the case, the more biased the sample becomes, and the potentially more biased the responses will be"
6096,unknown,"the early opinion polls that were conducted in the early twentieth century were biased. For example, the aforementioned Literary Digest poll, which was sent to around ten million people prior to the Presidential Elections 1916 to 1936, was a poll that suffered from serious selection bias. Despite the fact that it correctly predicted the presidential winners of 1916 to 1932 (but it failed to predic"
6097,unknown,"did not represent the American voting population accurately. To mail out its questionnaire,The Literary Digest used three sources, its own readership, registered automobile users, and registered telephone users. Yet, the readers of The Literary Digestwere middle- or upper-class individuals and so were automobile and tele- phone owners. For sure, in the 1910s, 1920s, and 1930s, the use of these sou"
6098,unknown,"probably a convenient way to reach millions of Americans. Nevertheless, the sampling frame failed to reach poor working-class Americans, as well as the unemployed. In particular, during the Great Depression in 1936, rather few Americans could afford the luxuries of reading a literary magazine or owning a car or a telephone. Hence, the unrepresentativeness of The Literary Digest ’s sample was proba"
6099,unknown,"was probably aggravated in 1936. To a large degree, this can explain why The Literary Digest survey predicted Alfred Landon to win the presidential election in 1936, whereas in reality Franklin D. Roosevelt won in a landslide amassing 61% of the popular vote. In fact, for The Literary Digest poll, the bias was aggravated by non-response bias (Squire 1988). Non-response BiasNon-response bias occurs"
6100,unknown,"Non-response BiasNon-response bias occurs, if certain individuals in your sample have a higher likelihood to respond than others and if the responses of those who do not respond would differ considerably from the responses of those who respond. The source of this type of bias is self-selection bias. For most surveys (except for the census in some countries), respondents normally have their entirel"
6101,unknown,"decide whether or not to participate in the survey. Naturally, some people are more likely to participate than others are. Most frequently, this self-selection bias stems for the topic of the survey. To highlight, individuals who are politically interested and knowledgeable might be more prone to answer a survey on conventional and unconventional political participation than individuals who could "
6102,unknown,"about politics. Yet, non-response bias could also stem from other sources. For example, it could stem from the time that somebody has at her disposal. In addition, persons with a busy professional and private life might simply forget to eitherﬁ ll out or return a survey. In contrast, individuals with more free time might be less likely to forget to ﬁ ll out the survey; they might also be more thor"
6103,unknown,"forget to ﬁ ll out the survey; they might also be more thorough in ﬁ lling it out. Finally, somebody’s likelihood to ﬁ ll out a survey might also be linked to technology (especially for online surveys). For example, not everybody has a smartphone or permanent access to the internet, some people differ in their email security settings, and some people might just not regularly check their emails. Th"
6104,unknown,"survey reaches some people of the initial sample, but probably not all of them (especially if it is a survey that was sent out by email). 60 5 Conducting a Survey Response Bias Response bias happens when respondents answer a question mis- leadingly or untruthfully. The most common form of response bias is the so-called social desirability bias; respondents might try to answer the questions less ac"
6105,unknown,"to their own convictions but more in an attempt to adhere to social norms (see also Sect. 4.5). For example, social or behavioral surveys (such as the European Social Survey or National Election Studies) frequently suffer from response bias for the simple question whether individuals voted or not. Voting is an act that is socially desirable; a good citizen is expected to vote in an election. When "
6106,unknown,"question whether or not they voted in the past national, regional, or any other election, citizens know this social convention. Some nonvoters might feel uneasy to admit they did not vote and indicate in the survey that they cast their ballot, even if they did not do so. In fact, over-reporting in election surveys is about 10 –15 percentage points in Western democracies (see Zeglovits and Kritzing"
6107,unknown,"percentage points in Western democracies (see Zeglovits and Kritzinger 2014). In contrast to the persistent over-reporting of electoral participation, the vote share for radical right-wing parties is frequently under-reported in surveys. Parties like the Front National in France or the Austrian Freedom Party attract voters and followers with their populist, anti-immigration, and anti-elite platfor"
6108,unknown,"Kaltwasser 2012). Voting for such a party might involve some negative stigmatiza- tion as these parties are shunned by the mainstream political elites, intellectuals, and the media. For these reasons, citizens might feel uneasy divulging their vote decision in a survey. In fact, the real percentage of citizens voting for a radical right-wing party is sometimes twice as high as the self-reported vo"
6109,unknown,"(see: Stockemer 2012). Response bias also frequently occurs for personality traits and for the description of certain behaviors. For example, when asked, few people are willing to admit that they are lazy or that they chew their ﬁ ngernails. The same applies to risky and illegal behaviors. Few individuals will openly admit to engage in drug consumption or will indicate in a survey that they have c"
6110,unknown,"5.2 Representative, Random, and Biased Samples 61 Biased samples are ubiquitous in the survey research landscape. Nearly any freely accessible survey you ﬁ nd in a magazine or on the Internet has a biased sample. Such a sample is biased because not all the people from the population see the sample, and of those who see it, not everybody will have the same likelihood to respond. In many freely acce"
6111,unknown,"freely accessible surveys, there is frequently also the likelihood to respond several times. A blatant example of a biased sample would be the National Gun Owner ’s Action Survey 2018 conducted by the in ﬂuential US gun lobbyist, the National Ri ﬂe Association (NRA). The survey consists of ten value-laden questions about gun rights. For example, in the survey, respondents are asked: should Congres"
6112,unknown,"states eliminate so-called gun free zones that leave innocent citizens defenseless against terrorists and violent criminals? The anti-gun media claims that most gun owners support mandatory, national gun registration? Do you agree that law-abiding citizens should be forced to submit to mandatory gun registration or else forfeit their guns and their freedom? In addition to these value-laden questio"
6113,unknown,"mainly restricted to NRA members, gun owners who cherish the second amendment of the American Constitution. Consequently, they will show high opposition toward gun control. Yet, their opinion will certainly not be representative of the American population. Yet, s urveys a re not only (ab)used by think tanks and non-governmental organizations to push their demands; in recent times, political partie"
6114,unknown,"organizations to push their demands; in recent times, political parties and candidates also use (online) surveys for political expediency or as a political stunt. For example, after taking of ﬁce in January 2017, President Trump ’s campaign sent out an online survey to his supporters entitled “ Mainstream Media Accountability Survey. ”In the introduction the survey directly addressed the American "
6115,unknown,introduction the survey directly addressed the American people —“ you are our last line of defense against the media ’s hit jobs. You are our greatest asset in helping our movement deliver the truth to the American people. ”The questionnaire then asked questions like:“has the mainstream media reported unfairly on our movement? ”“ Do you believe that the mainstream media does not do their due dilig
6116,unknown,"checking before publishing stories on the Trump administration? ”or “ Do you believe that political correctness has created biased news coverage on both illegal immigration and radical Islamic terrorism? ”From a survey perspective, Trump ’s survey is the anti-example of doing survey research. The survey pretends to address the American people, whereas in fact it is only sent to Trump ’s core suppo"
6117,unknown,"further uses biased and value-laden questions. The results of such a biased survey is a political stunt that the Trump campaign still uses for political purposes. In fact, with the proliferation of online surveys, with the continued discussion about fake news, and with the ease with which a survey can be created and distributed, there is the latent danger that organizations try to send out surveys"
6118,unknown,"the latent danger that organizations try to send out surveys less to get a “ valid” opinion from a population or clearly de ﬁned subgroup of that population, but rather as a stunt to further their cause. 62 5 Conducting a Survey 5.3 Sampling Error For sure, the examples described under biased surveys have high sampling errors, but even with the most sophisticated randomization techniques, we can n"
6119,unknown,"100% accurate representation of a population from a sample. Rather, there is always some statistical imprecision in the data. Having a completely random sample would imply that all individuals who are randomly chosen to participate in the survey actually do participate, something that will never happen. The sampling error depicts the degree to which the results derived from a sample differs from t"
6120,unknown,"the degree to which the results derived from a sample differs from the results derived from a population. For a random sample, there is a formula of how to calculate the sampling error (see Sect. 6.6). Basically, the sampling error depends on the number of observations (i.e., the more observations I have in the sample, the more precision there is in the data) and the variability of the data (i.e.,"
6121,unknown,"there is in the data) and the variability of the data (i.e., how much peoples ’opinions differ). For example, if I ask Germans to rate chancellor Merkel ’s popularity from 0 to 100, I will have a higher sampling error if I ask only 100 instead of 1000 individuals.1 Similarly, I will have a higher sampling error, if individuals ’opinions differ widely rather than being clustered around a speci ﬁc v"
6122,unknown,"differ widely rather than being clustered around a speci ﬁc value. To highlight, if Merkel’s popularity values differ considerably — that is, some individuals rate her at 0, others at 100 — there is more sampling error than when nearly everybody rates her around 50, because there is just more variation in the data. 5.4 Non-random Sampling Techniques Large-scale national surveys, measuring the popu"
6123,unknown,"Large-scale national surveys, measuring the popularity of politicians, citizens ’ support for a law, or citizens ’voting intentions, generally use random sampling techniques. Yet, not all research questions require a random sampling. Sometimes random sampling might not be possible or too expensive. The most common non-probabilistic sampling techniques are convenience sampling, purposive sam- pling"
6124,unknown,"pling, volunteer sampling, and snowball sampling, Convenience Sampling Convenience sampling is a type of non-probabilistic sam- pling technique where people are selected because they are readily available. The primary selection criterion relates to the ease of obtaining a sample. One of the most common examples of convenience sampling is using student volunteers as subjects for research (Battaglia"
6125,unknown,"for research (Battaglia 2008). In fact, college students are probably the most fre- quently used group in psychological research. For instance, many researchers (e.g., Praino et al. 2013) examining the in ﬂuence of physical attractiveness on the electoral success of candidates for political of ﬁce use college students from their local college or university to rank the physical attractiveness of th"
6126,unknown,"or university to rank the physical attractiveness of the their study subjects (i.e., 1Increasing the number of participants in the sample increases the precision of the data up to a certain number such as 1000 or 2000 participants. Beyond that number, the gains in increasing precision are limited. political candidates running for of ﬁce). These students are readily available and cheap to recruit. "
6127,unknown,"cheap to recruit. 5.4 Non-random Sampling Techniques 63 Purposive Sampling In purposive sampling subjects are selected because of some characteristics, which the researcher predetermines before the study. Purposive sampling can be very useful in situations where the researcher needs information for a speci ﬁc target group (e.g., blond women aged 30 –40). She can purposefully restrict her sample to"
6128,unknown,"restrict her sample to the required social group. A common form of purposive sampling is expert sampling. An expert sample is a sample of experts with known and demonstrable expertise in a given area of interest. For example, a researcher uses an expert sampling technique, if she sends out a survey to corruption specialists to ask them about their opinions about the level of corruption in a countr"
6129,unknown,"1990). In fact, most major international corruption indicators such as Transparency International, the World Bank anti-corruption indicator, or the electoral corruption indicators collected by the Electoral Integrity Project are all constructed from expert surveys. Volunteer Sampling Volunteer sampling is a sampling technique frequently used in psychology or marketing research. In this type of sam"
6130,unknown,"in psychology or marketing research. In this type of sampling, volunteers are actively searched for or invited to participate. Most of the internet surveys that ﬂood the web also use volunteer sampling. Participants in volunteer samples often have an interest in the topic, or they participate in the survey because they are attracted by the money or the non ﬁnancial compensation they receive for th"
6131,unknown,"participation (Black 1999). Sometimes pollsters also offer a high monetary prize for one or several lucky participants to increase participation. In our daily lives, volunteer surveys are ubiquitous, ranging from airline passenger feedback surveys to surveys about costumer habits and to personal hygiene questionnaires. Snowball Sampling Snowball sampling is typically employed with populations, whi"
6132,unknown,"which are dif ﬁcult to access. The snowball sampling technique is relatively straight- forward. In the ﬁ rst step, the researcher has to identify one or several individuals of the group she wants to study. She then asks the ﬁ rst respondents if they know others of the same group. By continuing this process, the researcher slowly expands her sample of respondents (Spreen 1992). For example, if a re"
6133,unknown,"sample of respondents (Spreen 1992). For example, if a researcher wants to survey homeless people in the district of Kreuzberg in Berlin, she is quite unlikely to ﬁ nd a list with all the people who live in the street. However, if the researcher identi ﬁes several homeless people, they are likely to know other individuals that live in the streets, who again might know others. Quota Sampling As imp"
6134,unknown,"Quota Sampling As implied in the word, quota sampling is a technique (which is frequently employed in online surveys), where sampling is done according to certain preestablished criteria. For example, many polls have an implicit quota. For exam- ple, customer satisfaction polls, membership polls, and readership polls all have an implicit quota. They are restricted to those that have purchased a pr"
6135,unknown,"for customer satisfaction surveys, the members of an organization or party for membership surveys, and the readers of a journal, magazine, or online site for readership polls. Yet, quota sampling can also be deliberately used to increase the representativeness of the sample. For example, let us assume that a researcher wants to know how Americans think about same-sex marriage. Let us further assum"
6136,unknown,"the researcher sets the sample size at 1000 people. Using an online questionnaire, she cannot get a random or fully representative sample, because still not everybody has continuous access to the Internet. Yet, what she can do is to make her sample representative of some characteristics such as gender and region. By setting up quotas, she can do this relatively easily. For example, as regions, she"
6137,unknown,"quotas, she can do this relatively easily. For example, as regions, she could identify the East, the Midwest, the West, and the South of the United States. For gender, she could split the sample so that she has 50% men and women. This gives her a quota of 125 men and 125 women for each region. Once, she re aches this quota, she closes the survey for this particular cohort (for the technical detail"
6138,unknown,"the survey for this particular cohort (for the technical details on how this works, see also Fluid Survey University 2017). Using such a technique allows researchers to build samples that more or less re ﬂect the population at least when it comes to certain characteristics. While it is cheaper than random sampling, quota sampling with the help of a survey company can still prove rather expensive. "
6139,unknown,"online quota sampling for a short questionnaire on Germans ’knowledge and assessment of the fall of the Berlin Wall in 1989, which I conducted in 2014, I paid$ 8 per strati ﬁed surve y. The strati ﬁcation criteria I used were ﬁ rst gender balance and second the requirement that half the participants must reside in the East of Germany and the other half in the West. 64 5 Conducting a Survey 5.5 Dif"
6140,unknown,"64 5 Conducting a Survey 5.5 Different Types of Surveys Questions about sampling and the means through which a survey is distributed often go hand in hand. Several sampling techniques lend themselves particularly well to one or another distribution medium. In survey research, we distinguish four different ways to conduct a survey: face-to-face surveys, telephone surveys, mail-in surveys, and onlin"
6141,unknown,"and online surveys. Face-to-Face Surveys Historically the most commonly employed survey method is the face-to-face survey. In essence, in a face-to-face interview or survey, the interviewer travels to the respondent ’s location, or the two meet somewhere else. The key feature is the personal interaction between the interviewer who asks questions from a questionnaire and the respondent who answers "
6142,unknown,"questions. The direct personal contact is the key difference from a telephone interview, and it comes with both opportunities and risks. One of the greatest advantages is that the interviewer can also examine the interviewee ’s nonverbal behavior and draw some conclusions from this. She can also immediately respond when problems arise during the task performance; for example, if the respondent doe"
6143,unknown,"does not understand the content of a question, the interviewer can explain the question in more detail. 5.5 Different Types of Surveys 65 Therefore, this type of survey is especially suitable when it comes to long surveys on more complex topics, topics where the interviewer must sit down with the respondent to explain certain items. Nevertheless, this type of survey also has its drawbacks: a great"
6144,unknown,"drawbacks: a great risk with this type of survey stems from the impact of the interviewer’s physical presence on the respondents ’answers. For example, slight differences about the interviewers ’ways of presenting an item can in ﬂuence the responses. In addition, there is the problem of social desirability bias. In particular, in the presence of an interviewer, respondents could feel more pressure"
6145,unknown,"norms than when answering questions. For example, in the presence of a pollster, individuals might be less likely to admit that they have voted for a radical right-wing party or that they support the death penalty. In a more anonymous survey, such as an online survey, the respondents might be more willing to admit socially frowned- upon behaviors or opinions. Face-to-face surveys are still employe"
6146,unknown,"national surveys such as the census in some countries. Some sampling techniques, such as snowball sampling, also work best with face-to-face surveys. Telephone SurveyIn many regards, the telephone interview resembles the face-to- face interview. Rather than personal, the interaction between interviewer and inter- viewee is via the phone. Trained interviewers can ask the same questions to different"
6147,unknown,"respondents in a uniform manner thus fostering precision and accuracy in soliciting responses. The main difference between a telephone interview and a personal interview is logistics. Because the interviewer does not have to travel to the interviewee’s residence or meet her in a public location, a larger bene ﬁt from this technique is cost. Large-scale telephone surveys signi ﬁcantly simplify the "
6148,unknown,"sion of the interviewers as most or all of them conduct the interviews from the same site. More so than face-to-face surveys, telephone surveys can also take advantage of recent technological advancements. For example, so-called computer-assisted tele- phone surveys (CATS) allow the interviewer to record the data directly into a computer. This has several advantages: (1) there is no need for any t"
6149,unknown,"transfer process from a transcription medium to a computer (which is a potential source of errors too). (2) The computer can check immediately whether given responses are invalid and change or skip questions depending on former answers. In particular, survey ﬁ rms use telephone interviews extensively, thus bene ﬁting from the accuracy and time ef ﬁciency of modern telephone surveys coupled with mo"
6150,unknown,computer technology (Weisberg et al. 1996: 112 –113; Carr and Worth 2001). Mail-in Survey Mail-in surveys are surveys that are sent to peoples ’mailboxes. The key difference between these self-administered questionnaires and the afore- mentioned methods is the complete absence of an interviewer. The respondent must cope with the questionnaire herself; she only sees the questions and does not hear 
6151,unknown,"them; assistance cannot be provided, and there is nobody who can clarify unclear questions or words. For this reason, researchers or survey ﬁ rms must devote great care when conceiving a mail-in survey. In particular, question wording, the sequence of questions, and the layout of the questionnaire must be easy to understand for respondents (De Leeuw et al. 2008: 239–241). Furthermore, reluctant “r"
6152,unknown,"cannot be persuaded by an interviewer to participate in the survey, which results in relatively low response rates. Yet, individuals can take the surveys at their leisure and can think about an answer as much time as they like. 66 5 Conducting a Survey A potential problem of mail-in surveys is the low response rate. Sometimes, only 5, 10, or 20% of the sample sends the questionnaire back, a featur"
6153,unknown,"render the results biased. To tackle the issue of low response rates, the researcher should consider little incentives for participation (e.g., the chance to win a prize or some compensation for participation), and she should send follow-up mailings to increase the participants ’willingness to participate. The upside of the absence of an interviewer is that the researcher does not need to worry ab"
6154,unknown,"interviewer is that the researcher does not need to worry about interviewer effects biasing the results. Another advantage of mail-in surveys is the possibility to target participants. Provided that the researcher has demographic information about each household in the population or sample she wants to study, mail-in surveys allow researchers to target the type of individuals she is most intereste"
6155,unknown,"researchers to target the type of individuals she is most interested in. For example, if a researcher wants to study the effect of disability on political participation, she could target only those individuals, who fall under the desired category, people with disabilities, provided she has the addresses of those individuals. Online Survey Online surveys are essentially a special form of mail in su"
6156,unknown,"Instead of sending a questionnaire by mail, researchers send online surveys by email or use a website such as SurveyMonkey to host their questionnaire. Online surveys have become more and more prominent over the past 20 years in research. The main advantage is costs. Thanks to survey sites such as SurveyMonkey or Fluid Survey, everybody can create a survey with little cost. This also implies that "
6157,unknown,"everybody can create a survey with little cost. This also implies that the use of online surveys is not restricted to research. To name a few, fashion or sports magazines, political parties, daily newspapers, as well as radio and television broadcasting stations all use online surveys. Most of the time, these surveys are open to the interested reader, and there are no restrictions for participatio"
6158,unknown,"and there are no restrictions for participation. Consequently, the answers to such surveys frequently do not cater to the principles of representativeness. Rather, these surveys are based on a quota or convenience sample. Sometimes this poses little problems, as many online surveys target a speci ﬁc population. For example, it frequently happens to researchers who publish in a scienti ﬁc journal w"
6159,unknown,frequently happens to researchers who publish in a scienti ﬁc journal with publishing houses such as Springer or Tyler and Francis that they receive a questionnaire asking them to rate their level of satisfaction with the publishing experience with the speci ﬁc publishing house. Despite the fact that the responses to these questionnaires are almost certainly unrepresentative of the whole populatio
6160,unknown,"certainly unrepresentative of the whole population of scientists, the feedback they receive might give these publishing houses an idea which authors ’services work and which do not work and what they can improve to make the publishing experience more rewarding for authors. Yet, online surveys can also be more problematic. They become particularly problematic if an online sample is drawn from a bia"
6161,unknown,"draw inferences beyond the target group (see Sect. 5.2). 5.7 Pre-tests 67 5.6 Which Type of Survey Should Researchers Use? While none of the aforementioned survey types is a priori superior to the others, the type of survey researchers should use depends on several considerations. First, and most importantly, it depends on the purpose of the research and the researcher ’s priorities. For instance,"
6162,unknown,"priorities. For instance, many surveys, including surveys about voting intentions or the popularity of politicians, nearly by de ﬁnition require some national random telephone and face-to-face survey or online samples. Second, surveys of a small subset of the population, who are dif ﬁcult to reach by phone or mail, such as the homeless, similarly require face-to-face interactions. For such a surve"
6163,unknown,"will most likely be a nonrandom snowball or convenience sample. Third, for other surveys more relevant for marketing ﬁ rms and companies, an online convenience sample might suf ﬁce to draw some “ valid”inferences about customer satisfaction with a service or a product. There are some more general guidelines. For one, online surveys can reach a large number of individuals at basically no cost, in p"
6164,unknown,"number of individuals at basically no cost, in particular if there are no quotas involved and if the polling ﬁ rm has the relevant email addresses or access to a hosting website. One the other hand, face-to-face and telephone interviews can target the respondents more thoroughly. If the response rate is a particularly important consideration, then personal face-to-face surveys or telephone surveys"
6165,unknown,"a good choice. The drawback to these types of surveys is the cost; these personal surveys are the most expensive type of survey (with telephone surveys having somewhat lower costs than face-to-face surveys). For research questions where the representativeness or randomness of the sample is less of an issue, or for surveys that target a speci ﬁc constituency, mail-in or online surveys could be a ra"
6166,unknown,"effective means. In particular, the latter are more and more frequently used, because through quota sampling techniques, these surveys can also generate rather represen- tative samples, at least according to some characteristics. However, as the example of the Trump survey illustrates, online surveys can also be used for political expediency. That is why the reader, before believing any of these s"
6167,unknown,"in particular, in a nonscienti ﬁc context, should inform herself about the sampling techniques, and she should look at question wording and the layout of the survey. 5.7 Pre-tests 5.7.1 What Is a Pre-test? Questionnaire design is complex; hardly any expert can design a perfect question- naire by just sitting at her desk (Campanelli 2008: 176). Therefore, before beginning the real polling, a resear"
6168,unknown,"the real polling, a researcher should test her questions in an authentic setting to see if the survey as a whole and individual questions make sense and are easily understood by the respondents. To do so, she could conduct the survey with a small subset of the original sample or population aiming to minimize problems before the actual data collection begins (Krosnick and Presser 2010: 266 f.; Kros"
6169,unknown,"preliminary research or pre-tests make particular sense in ﬁ ve cases. 68 5 Conducting a Survey First, for some questions researchers need to decide upon several possible measurements. For example, for questions using a Likert scale, a pre-test could show if most individuals choose the middle category or the do not know option. If this is the case, researchers might want to consider eliminating th"
6170,unknown,"this is the case, researchers might want to consider eliminating these options. In addition, for a question about somebody ’s income, a pre-test could indicate whether respondents are more comfortable answering a question with set income brackets or if they are willing to reveal their real income. Second, questions need to be culturally and situationally appropriate, easy to understand, and they m"
6171,unknown,"understand, and they must carry the inherent concept ’s meaning. To highlight, if a researcher conducts a survey with members of a populist radical right-wing party, these members might feel alienated or insulted if she uses the word radical right or populist right. Rather, they de ﬁne themselves as an alternative that incorporates common sense. Normally, a researcher engaging in this type of rese"
6172,unknown,"already know this, but in case she does not, a pre-test can alert her to such speciﬁcities allowing her to use situationally appropriate wording. Third, a pre-test can help a researcher discover if responses to a speci ﬁc item vary or not. In case there is no variance in the responses to a speci ﬁc question at all, or very little variance, the researcher could think about omitting the issue to ass"
6173,unknown,"theﬁ nal questionnaire is solely comprised of discriminative items (items with variance) (Kumar 1999: 132). For example, if a researcher asks the question whether the United States should leave NATO and everybody responds with no, the researcher might consider dropping this question, as there is no variation in answers. Fourth, a pre-test is particularly helpful if the survey contains open-ended q"
6174,unknown,"questions and if a coding scheme for these open-ended questions is developed alongside the survey. Regardless of the level of sophistication of the survey, there is always the possibility that unexpected and therefore unclassi ﬁable responses that will be encountered during the survey arise. Conducting a pre-test can reduce this risk. Fifth, and more practically, a pre-test is especially recommend"
6175,unknown,"this risk. Fifth, and more practically, a pre-test is especially recommendable if a group of interviewers (with little experience) run the interviews. In this case, the pre-test can be part of the interviewers ’training. After the pre-test, the interviewers not only share their experiences and discuss which questions were too vague or ambiguous but also share their experience asking the questions "
6176,unknown,"but also share their experience asking the questions (Behnke et al. 2006: 258 f.). After t he p re-test, several questions might be changed depending on the respondents’reactions (e.g., low response rates) to the initial questionnaire. If signiﬁcant changes are made in the aftermath of the pre-test, the revised questions should also be retested. This subsequent pre-test allows the researcher to ch"
6177,unknown,"new questions, or the new question wordings, are clearer or if the changes have caused new problems (for more information on pre-testing, see Guyette 1983, pp. 54 –55). References 69 5.7.2 How to Conduct a Pre-test? The creation of a questionnaire/survey is a reiterative process. In a ﬁ rst step, the researcher should check the questions several times to see if there are any uncertain or vague que"
6178,unknown,"vague questions, if the question ﬂ ow is good, and if the layout is clear and appealing. To this end, it also makes sense for the researcher to read the questions aloud so that she can reveal differences between written and spoken language. In a next step, she might run the survey with a friend or colleague. This preliminary testing might already allow the researcher to identify (some) ambiguous o"
6179,unknown,"already allow the researcher to identify (some) ambiguous or sensitive questions or other problematic aspects in the questionnaire. Then, after this preliminary check, the researcher should conduct a trial or pre-test to verify if the topic of the questionnaire and every single question are well understood by the survey respondents. To conduct such a pre-test, researchers normally choose a handful"
6180,unknown,"such a pre-test, researchers normally choose a handful of individuals who are similar to those that will actually take the survey. When conducting her trial, the researcher must also decide whether the interviewer (if he does not run the pre-test himself) informs the respondent about the purpose of the survey beforehand. An informed respondent could be more aware of the interviewing process and an"
6181,unknown,"respondent could be more aware of the interviewing process and any kinds of issues that arise during the pre-test. However, the ﬂ ipside is that the respondent may take the survey less seriously. The so-called respondent debrie ﬁng session offers a middle way addressing the described obstacles. In a ﬁ rst step, the pre-test is conducted without informing the respondent beforehand. Then, shortly af"
6182,unknown,"pre-test, the interviewer asks the respondent about obstacles and challenges the respondent encountered in the course of the pre-test (Campanelli 2008: 180). References Battaglia, M. (2008). Convenience sampling. In P. J. Lavrakas (Ed.), Encyclopedia of survey research methods . Thousand Oaks, CA: Sage. Behnke, J., Baur, N., & Behnke, N. (2006). Empirische Methoden der Politikwissenschaft . Paderb"
6183,unknown,"Paderborn: Schöningh. Bickman, L., & Rog, D. J. (1998). Handbook of applied social research methods . Thousand Oaks, CA: Sage. Black, T. R. (1999). Doing quantitative research in the social sciences: An integrated approach to research design, measurement, and statistics . Thousand Oaks, CA: Sage. Campanelli, P. (2008). Testing survey questions . In E. D. De Leeuw, J. J. Hox, & D. A. Dillman (Eds.)"
6184,unknown,"(Eds.),International handbook of survey methodology . New York: Lawrence Erlbaum Associates. Carr, E. C., & Worth, A. (2001). The use of the telephone interview for research. NT research, 6 (1), 511–524. De Leeuw, E. D., Hox, J. J., & Dillman, D. A. (2008). Mixed-mode surveys: When and why. In International handbook of survey methodology (pp. 299 –316). New York: Routledge. Fluid Surveys Universit"
6185,unknown,"Fluid Surveys University. (2017). Quota sampling effectively – How to get a representative sample for your online surveys . Retrieved December 1, 2017, from http://ﬂuidsurveys.com/university/ using-quotas-effectively-get-representative-sample-online-surveys/ Guyette, S . ( 1983).Community based research: A handbook for native Americans . Los Angeles: American Indian Studies Center. 70 5 Conducting"
6186,unknown,"American Indian Studies Center. 70 5 Conducting a Survey Krosnick, J. A. (1999). Maximizing questionnaire quality . In J. P. Robinson, P. R. Shaver, & L. S. Wrightsman (Eds.),Measures of political attitudes: Volume 2 in measures of social psychologi- cal attitudes series . San Diego: Academic Press. Krosnick, J. A., & Presser, S. (2010). Question and questionnaire design . In P. V. Marsden & J. D."
6187,unknown,"Wright (Eds.), Handbook of survey research . Bingley: Emerald. Kumar, R. (1999). Research methodology: A step-by-step guide for beginner s. In E. D. De Leeuw, J. J. Hox, & D. A. Dillman (Eds.), International handbook of survey methodology . New York: Lawrence Erlbaum Associates. Mudde, C., & Kaltwasser, C. R. (Eds.). (2012). Populism in Europe and the Americas: Threat or corrective for democracy ."
6188,unknown,"corrective for democracy . Cambridge: Cambridge University Press. Nachmias, C. F., & Nachmias, D. (2008). Research methods in the social sciences (7th ed.). New York: Worth. Patton, M. Q. (1990). Qualitative evaluation and research methods (2nd ed.). Newbury Park, CA: Sage. Praino, R., Stockemer, D., & Moscardelli, V. G. (2013). The lingering effect of scandals in congressional elections: Incumben"
6189,unknown,"1045–1061. Spreen, M. (1992). Rare populations, hidden populations and link-tracing designs: What and why? Bulletin Methodologie Sociologique, 36 (1), 34 –58. Squire, P. (1988). Why the 1936 literary digest poll failed. Public Opinion Quarterly, 52 (1), 125–133. Stockemer, D. (2012). The Swiss radical right: Who are the (new) voters of Swiss Peoples ’Party? Representation, 48 (2), 197 –208. Weisbe"
6190,unknown,"Weisberg, H. F., Krosnick, J. A., & Bowen, B. D. (1996). An introduction to survey research, polling, and data analysis (3rd ed.). Thousand Oaks, CA: Sage. Zeglovits, E., & Kritzinger, S. (2014). New attempts to reduce overreporting of voter turnout and their effects. International Journal of Public Opinion Research, 26 (2), 224 –234. Further Reading Constructing and Conducting a Survey Kelley, K."
6191,unknown,"survey research. International Journal for Quality in Health Care ,15 (3), 261 –266. The short article provides a hands-on step-by-step approach into data collection, data analysis, and ﬁreporting. For each step of the survey process, it identi es best practices and pitfalls to be avoided so that the survey becomes valid and credible. Krosnick, J. A., Presser, S., Fealing, K. H., Ruggles, S., & Va"
6192,unknown,"Krosnick, J. A., Presser, S., Fealing, K. H., Ruggles, S., & Vannette, D. L. (2015). The future of survey research: Challenges and opportunities . The National Science Foundation Advisory Committee for the social, behavioral and economic sciences subcommittee on advancing sbe survey research. Available online at: Survey_Research.pdf. Comprehensive reports on the best practices, challenges, innovat"
6193,unknown,"http://www.nsf.gov/sbe/AC_Materials/The_Future_of_ and new data-collection strategies in survey research. Rea, L. M., & Parker, R. A. (2014). Designing and conducting survey research: A comprehensive guide. San Francisco: Wiley. A very comprehensive book into survey research consisting of ﬁthree parts: (1) developing and administering a questionnaire, (2) ensuring scienti c accuracy, and (3) prese"
6194,unknown,"and (3) presenting and analyzing survey results. References 71 Internet Survey Alessi, E. J., & Martin, J. I. (2010). Conducting an internet-based survey: Bene ﬁts, pitfalls, and lessons learned. Social Work Research ,34 (2), 122 –128. This text provides a very hands-on introduction into the conduct of an Internet survey with a special focus on recruitment strategies and response rate. De Bruijne,"
6195,unknown,"De Bruijne, M., & Wijnant, A. (2014). Improving response rates and questionnaire design for mobile web surveys. Public Opinion Quarterly , 78(4), 951 –962. This research note provides some practical lessons how the response rate and data quality can be improved for Internet surveys especially constructed for smartphones. Univariate Statistics 6 Abstract Thisﬁ rst practical chapter is split into tw"
6196,unknown,"Abstract Thisﬁ rst practical chapter is split into two parts. In the ﬁ rst part, I succinctly present the two statistical software packages, SPSS and Stata, which are probably the most used statistical programs in the social sciences. I also explain how to input data into SPSS and Stata datasets. Second, I cover the two univariate statistical categories, frequency tables and descriptive statistics"
6197,unknown,"statistical categories, frequency tables and descriptive statistics, as well as some graphical representations of a variable (i.e., boxplot, pie chart, and histogram). For each test, the mathematical foundations as well as the practical implementa- tion in SPSS and Stata will be introduced based on the sample survey presented at the end of Chap. 4 . 6.1 SPSS and Stata SPSS and Stata are probably t"
6198,unknown,"SPSS and Stata are probably the most frequently used software packages in introductory statistics’classes. Both programs are complete, integrated statistics packages that allow for data analysis, data management, and graphics. They are available in most university libraries and are rather simple to naviga te. For each test or graphic, the book will illustrate the logic behind the test, its math em"
6199,unknown,"illustrate the logic behind the test, its math ematical foundations, as well as illustrate how to conduct the respective analysis in both SPSS and Stata. Depending on which software package you use, you only need to read either the SPSS or the Stata example. 6.2 Putting Data into an SPSS Spreadsheet To create a data ﬁ le in SPSS, open SPSS then click on new dataset . This opens up a spreadsheet si"
6200,unknown,"spreadsheet similar to an Excel document. There are two parts to any SPSS dataset, one part labeled Variable View and one part labeled Data View. Data View is used to enter data. Variable View is used to set up the data —names, variable labels, value # Springer International Publishing AG 2019 D. Stockemer, Quantitative Methods for the Social Sciences , https://doi.org/10.1007/978-3-319-99118-4_6 "
6201,unknown,"https://doi.org/10.1007/978-3-319-99118-4_6 73 Fig. 6.1Display of the sample data in Variable View Fig. 6.2Display of the sample data in Data View labels, etc. To change from Data View to Variable View , click on the icons in the lower left corner of the dataset. 74 6 Univariate Statistics Theﬁ rst step of creating a dataset consists normally of labeling the data. We generally do this labeling in "
6202,unknown,"generally do this labeling in the Variable View . In the Variable View , each row represents one variable. Each column represents a case such as a person or a country (see Fig. 6.1). Theﬁ rst variable we normally enter into a dataset is a string variable, an identi ﬁer of the cases for which we have collected data for. To enter your survey data, go to the Variable View and write in “ respondent”in"
6203,unknown,"data, go to the Variable View and write in “ respondent”in the ﬁ rst row of the ﬁ rst column labeled Name . If you enter your own data, label these data appropriately. Because thisﬁ rst variable is normally a string variable (i.e., a non-numeric variable), choose the option String in the second column. The other variables are normally numeric variables —the actual data. Include their variable name"
6204,unknown,"numeric variables —the actual data. Include their variable names in the subsequent rows, and make sure that these variables are labeled as Numeric (i.e., check the second column). When you label the data, make sure that you do not leave any space between letters, as SPSS will not allow you to leave any space. The copied graph above shows the Variable View of the data from our sample questionnaire "
6205,unknown,"questionnaire from Sect. 4.11. The ﬁ rst variable, labeled student, is the identi ﬁer variable. The second variable is the dependent variable measuring the amount of money students spend per week while going out. The remaining variables are the independent variables. Figure 6.2 shows theData View of our sample questionnaire data. The variable in theﬁ rst column is the identi ﬁer. The second column"
6206,unknown,"theﬁ rst column is the identi ﬁer. The second column is the dependent variable, the amount of money students spend going out. Variables 3 –8 are the independent variables. 6.3 Putting Data into a Stata Spreadsheet 75 6.3 Putting Data into a Stata Spreadsheet To create a data ﬁ le, you have to click on the icon displaying a spreadsheet and a pencil or type “ edit”into the command line. Thi s opens "
6207,unknown,"pencil or type “ edit”into the command line. Thi s opens up a spreadsheet similar to an Excel document. You can then input the data by hand into this data editor. To do so, you have to ﬁ rst label the data. The ﬁ rst variable we normally enter into a dataset is a string variable, an identi ﬁer of the cases for which we have collected data. In our case, this is a string variable, which we label stu"
6208,unknown,"In our case, this is a string variable, which we label student. Add the name of the other variables in the subsequent ﬁ elds of the ﬁ rst row. When you label the data, make sure that you do not leave any space between letters, as Stata does not allow you to leave any space. After labeling the variables, you can then input the data. Alternatively, you can enter the data in an Excel spreadsheet and "
6209,unknown,"Alternatively, you can enter the data in an Excel spreadsheet and copy it to Stata. When you paste the data into the Stata data editor, Stata asks you whether you want to treat the ﬁ rst row as variable names or data. You click variable names; otherwise, the data will not be labeled. In Stata we have two different screens, the main screen we use to do our data analysis (see Fig. 6.3) and the data "
6210,unknown,"analysis (see Fig. 6.3) and the data editor or the screen in which we can see the data (see Fig. 6.4). On the main screen, there is some information about Stata and an indication of the dataset you use. On the right side of the screen (i.e., the right upper corner), there is a listing of all the variables. On the bottom of the screen is the command editor, which you will use to do your analyses. F"
6211,unknown,"Figure 6.4 shows the data edito r featuring our sample questionnaire data. The variable in the ﬁ rst column is the identi ﬁer. The second column is the dependent Fig. 6.3 The main Stata screen Fig. 6.4 Display of the sample data in the Stata data editor variable, the amount of time students spend per week when partying. Variables 3 –8 are the independent variables. 76 6 Univariate Statistics (Plea"
6212,unknown,"(Please note that when you create your Stata ﬁ le, some of the original variable names might be too long, and Stata might not accept them. If this is the case, you have to shorten the original name.) 6.4 Frequency Tables A frequency table is a rather simple univariate statistic that is particularly useful for categorical variables such as ordinal variables that do not have too many categories. For"
6213,unknown,"For each category or value, such a table indicates the number of times or percentage of times each value occurs. A frequency table normally has four columns. The ﬁ rst column lists the available categories. The second column displays the raw frequency or the number of times a single value occurs. The third column shows the percentage of observations that fall into each category —the basic formula "
6214,unknown,"of observations that fall into each category —the basic formula to calculate this percentage is the number of observations in the category divided by the total number of observations. The ﬁ nal column, labeled cumulative percentage, displays the percentage of individuals up to a certain category or point. Table 6.1 displays a freque ncy table, of the variable times partying . The ﬁ rst column disp"
6215,unknown,"column displays the available options (i.e., the six categories ranging from zero to ﬁve times and more). The second column shows the raw frequencies (i.e., how many individuals normally party, zero times, once, twice, three times, four times or ﬁ ve times, and more per week). The third column displays the raw frequency in percentages. The ﬁ nal column displays the cumulative percentage. For examp"
6216,unknown,"Table 6.1 highlights that 60% of the polled, on average, party two times or fewer per week. Table 6.1Frequency table of the variable times partying 6.4 Frequency Tables 77 On average, how many times per week do Frequency Percentageyou party Cumulative percentage Zero times 3 7.5 7.5 Once 8 20 27.5 Twice 13 32.5 60 Three times 10 25 85 Four times 5 12.5 97.5 Five or more times 1 2.5 100 Total 40 10"
6217,unknown,Five or more times 1 2.5 100 Total 40 100 Fig. 6.5Doing a frequency table in SPSS ( ﬁrst step) Fig. 6.6Doing a frequency table in SPSS (second step) 6.4.1 Constructing a Frequency Table in SPSS Step 1: Go to Analyze —Descriptive Statistics —Frequencies (see Fig. 6.5). Step 2: Highlight the variable —times_partying—and then click on the arrow. The variable appears in the right rectangle. Then click
6218,unknown,"variable appears in the right rectangle. Then click okay (see Fig. 6.6). The SPSS output has ﬁ ve columns (see Table 6.2). Columns one, two, three, and ﬁ ve mimic Table 4.6. The ﬁ rst variable is the identi ﬁer and displays the existing Table 6.2SPSS Frequency table output Times_Partying Frequency Percent Valid percent Cumulative percent Valid 0.0 3 7.5 7.5 7.5 1.0 8 20.0 20.0 27.5 2.0 13 32.5 32."
6219,unknown,"3.0 10 25.0 25.0 85.0 4.0 5 12.5 12.5 97.5 5.0 1 2.5 2.5 100.0 Total 40 100.0 100.0 Fig. 6.7 Doing a frequency table in Stata 0 0 0 0 0 0 categories. Colum ns two, three, and ﬁ ve display the raw frequency, the corresponding percentage for each category, and the cumulative percentage, respec- tively. The fourth column, labeled valid percent, displays the percentage of each cell by taking into cons"
6220,unknown,"by taking into consideration missing values. Missing values are answers to questions left blank by the respondent (i.e., a respondent did not answer the question). In our example, all survey respondents answered the question on how many times they normally go party. Therefore, there are no missing values, and the values in column four match the values in column three. 78 6 Univariate Statistics 6."
6221,unknown,"78 6 Univariate Statistics 6.4.2 Constructing a Frequency Table in Stata Step 1: Write in the Stata Command ﬁ eld“ tab Times_Partying, ”and press enter. (Alternatively, you can also write tab and then click on the variable Times_Partying in the upper right corner of the display.) (See Fig. 6.7 .) The Stata output in Table 6.3 has four columns: (1) variable name (this lists all the possible categor"
6222,unknown,"possible categories), (2) raw frequency (lists the occurrence of each category), (3) Percent per category (lists the percentage of values that fall into any category), and (4) cumulative percentage (the cumulative percentage of values that fall into the listed category or a lower category). 6.5 The Measures of Central Tendency: Mean, Median, Mode, and Range 79 Table 6.3 Stata frequency table outpu"
6223,unknown,"6.5 The Measures of Central Tendency: Mean, Median, Mode, and Range This part shortly introduces the most widely used measures of central tendency or univariate statistics: mean, median, mode, and range. Mean The mean is the value we commonly call the average. To calculate the mean, sum up all observations, and then divide this sum by the number of subjects or observations. In statistical language"
6224,unknown,"the sample is denoted by n . The mean in mathematical language: x x x x Pxi x– ¼ 1 þ 2 þ 3 þ... þ n n ¼ i¼1 n n The mean can be strongly in ﬂuenced by outliers (i.e., observations that fall far from the rest of the data). To highlight, we take a subsample from our sample dataset. For example, the last ﬁ ve values of the variable money spent partying are 60, 60, 90, 70, and 200. If we calculate the"
6225,unknown,"90, 70, and 200. If we calculate the mean, we will get (60 + 60 + 90 + 70 + 200)/5 ¼ 96 We can clearly see that the mean is strongly in ﬂuenced by the outlier 200. While theﬁ rst 4 students in our subsample spend between 60 and 90 $ /week partying, the last student spends 200 dollars, which is much different from the other values. Without the outlier 200, the mean would be only 70. Median The medi"
6226,unknown,"Median The median, or “ midpoint,”is the middle number of a distribution. It is less sensitive to outliers and therefore a more “ resistant”measure of central tendency than the mean. To calculate the median by hand, just line all values up in order and ﬁ nd the middle one (or average of the two middles when n , the number of observations, is even.). In our example, we would line up the values: 60,"
6227,unknown,"In our example, we would line up the values: 60, 60, 70 , 90, and 200, and the median would be 70. If we were to calculate the median without the outlier 200, we would again line up the values: 60, 60, 70, and 90. We would then calculate the mean between the two middle values 60 and 70, which is 65. Mode The mode is the value that occurs most often in the sample. In cases where there are several v"
6228,unknown,"several values that occur most often, the mode can consist of these several values. Taking our ﬁ ve values again (i.e., 60, 60, 70, 90, 200), the value that appears most often is 60, which is the mode of this subsample. Range The range is a measur e that provides us with some indication how widely spread our data are. It is calculated by subtracting the highest from the lowest value. In the ﬁ ve- "
6229,unknown,"value subsample used above, the range would be 140 (i.e., 200 ⎻ 60). 6.6 Displaying Data Graphically: Pie Charts, Boxplots, and Histograms 6.6.1 Pie Charts A pie chart , sometimes also called circle chart, is one way to display a frequency table graphically. The graphic consists of a circle that is divided into slices. Each slice represents one of the variable ’s categories. The size of each slice"
6230,unknown,"slice represents one of the variable ’s categories. The size of each slice is proportional to the frequency of the value. Pie charts can be strong graphical representation of categorical data with few categories. However, the more categories we include into a pie chart the harder it is to succinctly compare categories. It is also rather dif ﬁcult to compare data across different pie charts. Figure"
6231,unknown,"compare data across different pie charts. Figure 6.8 displays the pie chart of the variable Times_Partying. We can see that the interpretation is already dif ﬁcult, as it is already hard to guess from the graph the frequency of each slice. If we take another variable with more categories such as our dependent variable money spent partying (see Fig. 6.9 ), we see that the categories are basically i"
6232,unknown,"other. Hence, a pie chart is not a good option to display this variable. 80 6 Univariate Statistics 6.6 Displaying Data Graphically: Pie Charts, Boxplots, and Histograms 81 01 23 45 Fig. 6.8 Pie chart of the variable times partying 30 35 40 50 60 70 75 80 90 100 110 120 130 Fig. 6.9 Pie chart of the variable money spent partying 82 6 Univariate Statistics Fig. 6.10 Doing a pie chart in SPSS ( ﬁrst"
6233,unknown,Fig. 6.11 Doing a pie chart in SPSS (second step) 6.6.2 Doing a Pie Chart in SPSS Step 1: Go to Graphs —Legacy Dialogue – Pie (see Fig. 6.10 ). Step 2: Click Summaries of groups of cases (see Fig. 6.11 ). Step 3: Highlight the variable —Times_Partying—and click on the arrow next to Deﬁne Slices to include the variable in the ﬁ eld. Then click okay (see Fig. 6.12 ). 6.6 Displaying Data Graphically:
6234,unknown,"Fig. 6.12 Doing a pie chart in SPSS (third step) The SPSS output in Fig. 6.13 displays the pie chart; in the right-hand upper corner, it denotes the variable name and the existing categories including the corresponding colors in the chart. 6.6.3 Doing a Pie Chart in Stata Step 1: Write in the Stata Command ﬁ eld: graph pie, over(Times_Partying) (see Fig. 6.14). Figures 6.8 and 6.9 display the pie "
6235,unknown,partying and money spent partying. Fig. 6.13SPSS pie chart output of the variable times partying Fig. 6.14Doing a pie chart in Stata 6.7 Boxplots A boxplot is a very convenient way of displaying variables. This type of graph allows us to display three measures of central tendency in one graph. The median or the midpoint of each dataset is indicated by the black centerline. The blue/gray shaded box
6236,unknown,"shaded box, which is also known as the interquartile range (IQR) includes the mid-50% of the data. The two outer lines denote the range of the data. If values extend up to 1.5 times the interquartile range from the upper or lowe r boundary of the mid-50%, they are plotted individually as asterisks. These individually plotted values are the outliers. Figure 6.15 displays the boxplot of the variable"
6237,unknown,"Figure 6.15 displays the boxplot of the variable average study time. From the graph, we see that the median study time of those students, who participated in the survey, is approximately 10 h. We also learn that the mid-50% of the data generally study between 7 and 11 h/week. The range of the data is 14. (The maximum value denoted by the upper line is 15; the minimum value denoted by the lower lin"
6238,unknown,84 6 Univariate Statistics week). We see that the median amount of money students spend partying is Figure 6.16 displays the boxplot of the variable —money spent partying (per 6.7 Boxplots 85 0 5 10 15 Study_Time Fig. 6.15 Stata boxplot of the variable study time (per week) 0 50 100 150 200 Money_Spent_Partying Fig. 6.16 Stata boxplot of the variable money spent partying (per week) Fig. 6.17 Doing
6239,unknown,"Fig. 6.17 Doing a boxplot in SPSS ( ﬁrst step) approximately 75 /week. The mid-50% of students spend between 60 and 90 dollars $ approximately. At the extremes, students spend 120 dollars at the upper end for partying and 30 dollars at the lower end. The boxplot also indicates that there is one outlying student in the data. By spending 200 dollars, she does not ﬁ t the pattern of other students. 8"
6240,unknown,"other students. 86 6 Univariate Statistics 6.7.1 Doing a Boxplot in SPSS Step 1 : Go to Graphs —Chart Builder; a dialogue box will open; if this is the case, press okay. You will be directed to the Chart Builder, which you see below (see Fig. 6.17). Step 2: Go to the item —Choose from—click on Boxplot. Then in the rectangle to the right, three different types of boxplots will appear. Drag the ﬁ rs"
6241,unknown,"right, three different types of boxplots will appear. Drag the ﬁ rst boxplot image to the open ﬁ eld above. After that, click on your variable of interest —in our case, study time—and drag it to the y -axis. Finally, click okay (see Figs. 6.18 and 6.19). Figure 6.19 displays the boxplot of the variable study time. The median is at 10 h/ week. The range is 14 h (i.e., the minimum in the sample is 1"
6242,unknown,"15 h), and the interquartile range or the mid-50% of the data goes from 7 to 11. 6.7.2 Doing a Boxplot in Stata Step 1: Write in the Stata Command ﬁ eld: graph box Study_Time (see Fig. 6.20 ). Figures 6.15 and 6.16 display two Stata boxplot outputs featuring the variables study time per week and money spent partying per week. 6.8 Histograms 87 Fig. 6.18 Doing a boxplot in SPSS (second step) 6.8 Hi"
6243,unknown,Histograms are one of the most widely used graphs in statistics to graphically display continuous variables. These graphs display the frequency distribution of a given variable. Histograms are very important for statistics in that they tell us if the data is normally distributed. In statistical inference —which means using a sample to gener- alize about a population —normally distributed data is a
6244,unknown,"alize about a population —normally distributed data is a prerequisite for many statistical tests (see below), which we use to generalize from a sample toward a population. Figure 6.21 shows two normal distributions (i.e., the blue line and the red line). In their ideal shape, these distributions have the following features: (1) the mode, mean, 88 6 Univariate Statistics Fig. 6.19 SPSS boxplot of t"
6245,unknown,"the variable study time (per week) 16 14 12 10 8 6 4 Study_Time Fig. 6.20Doing a boxplot in Stata Fig. 6.21 Shape of a normal distribution Data The Normal (Bell) Curve Frequency and median are the same value in the center of the distribution. (2) The distribution is symmetrical, that is, it has the same shape on each side of the distrib ution. 6.8.1 Doing a Histogram in SPSS Step 1: Go to Gra phs—"
6246,unknown,"Step 1: Go to Gra phs—Chart Builde r—a dialogue b ox opens; when this is the case, press okay. You will be direc ted to the Chart Builde r (this is the same procedu re as c onstructing a box plot). Step 2: Go to the item“ Choose from,”and click on Histog ram. Then, in the rectangle to the right , four diff erent types of histogram s will ap pear. Dra g the ﬁ rst type of Fig. 6.22Doing a histogram "
6247,unknown,"histogram that appears to the open ﬁ eld above. After that, click on your variable of interest —in our case times spent partying —and drag it to the x -axis. Finally, click okay (see Table 7.3 and Fig. 6.22 ). 6.8 Histograms 89 The histogram in Fig. 6.23 displays the distribution of the variable money spent partying. We see that the mode is approximately 80 $ /month. Pertaining to the normality as"
6248,unknown,"normality assumption, we see that the data is very roughly normally distributed. There are fewer observations on the extremes and more observations in the center. However, to be perfectly normally distributed, the bar at 100 should be higher; there should also not be any outlier at 200. However, for analytical purposes, we would Mean = 76.50 Std. Dev. = 30.15345 N = 4020.0 15.0 10.0 5.0 0.0 Money_"
6249,unknown,N = 4020.0 15.0 10.0 5.0 0.0 Money_Spent_Partying Frequency .00 50.00 100.00 150.00 200.00 250.00 Fig. 6.23SPSS Histogram of the variable money spent partying per week Fig. 6.24Doing a histogram in Stata say that this graph is close enough to a normal distribution to make “ correct” inferences from samples to populations. 6.8.2 Doing a Histogram in Stata Step 1: Write in the Stata Command ﬁ eld: h
6250,unknown,"Step 1: Write in the Stata Command ﬁ eld: hist Money_Spent_Partying (see Fig. 6.24 ). The Stata output of the variable money spent partying (see Fig. 6.25 ) uses somewhat larger bars than the SPSS output and is therefore a little less “ precise”than the SPSS output. 90 6 Univariate Statistics 6.9 Deviation, Variance, Standard Deviation, Standard Error, Sampling Error ... 91 0 .005 .01 .015 .02 Den"
6251,unknown,"0 .005 .01 .015 .02 Density 0 50 100 150 200 Money_Spent_Partying Fig. 6.25Stata Histogram of the variable money spent partying (per week) 6.9 Deviation, Variance, Standard Deviation, Standard Error, Sampling Error, and Confidence Interval On the following pages, I will illustrate how you can calculate the sampling error and the conﬁdence interval, two univariate statistics that are of high value "
6252,unknown,"research. In order to calculate the sampling error and con ﬁdence interval, we have to follow several intermediate steps. We have to calculate the deviation, sample variance, standard deviation, and standard error. Deviation Every sample has a sample mean, and for each observation there is a deviation from that mean. The deviation is positive when the observation falls above the mean and negative "
6253,unknown,"negative when the observation falls below the mean. The magnitude of the value reports how different (in the relevant numerical scale) an observation is from the mean. Formula deviation : Difference between the observation and the mean, Y i –Ŷ Example: Assume we have the following three numbers: 1, 2, and 6 For these numbers the deviations are: 1– 3 ¼ – 2 2– 3 ¼ – 1 6– 3 ¼ 3 (By de ﬁnition, the su"
6254,unknown,"(By de ﬁnition, the sum of these deviations is 0) 92 6 Univariate Statistics Sample Variance can be negative, and squaring gets rid of the negative sign. The variance is the approximate average of the squared deviations. In other words, the variance measures the approximate average of the squared distance between observations and the mean. For this measure, we use squares because the deviations Fo"
6255,unknown,"Formula Sample Variance S¼ PðÞ ⎻2 xi x 2 n⎻ 1 Standard Deviation The standard deviation is a measure of volatility that measures the amount of variability or volatility around the mean. The standard deviation is large if there is high volatility in the data and low if the data is closely clustered around the mean. In other words, the smaller the standard deviation, the less “ error”we have in our "
6256,unknown,"and the more secure we can be in knowing that our sample mean closely matches our population mean. Formula Standard Deviation S¼ ------------------------ PN i¼1 ( xi ⎻ –x )2 N⎻ 1 vuuut The standard deviation is also important for standardizing variables. If the data are normally distributed (i.e., they follow a bell-shaped curve), the data have the following properties. Sixty-eight percent of the "
6257,unknown,"following properties. Sixty-eight percent of the cases fall within one standard deviation, 95% of the cases fall between two standard deviations, and 99.7% cases fall between three standard deviations (see Figs. 6.26 ). Fig. 6.26Standard deviation in a normal distribution"
6258,unknown,"Standard Error The standard error allows researchers to measure how close the mean of the sample is to the population mean. Formula of the Standard Error The stand ard error is important, because it allows researchers to calculate the conﬁdence interval. The con ﬁdence interval, in turn, allows researchers to make inferences from the sample mean toward the population mean. It allows researchers to"
6259,unknown,"to calculate the population mean based on the sample mean. In other words, it g ives us a range in which the real mean falls. (In reality, this method only works if we have a random sample and a normally distributed variable.) Formula of the Con ﬁdence Interval The con ﬁdence interval applied: 6.9 Deviation, Variance, Standard Deviation, Standard Error, Sampling Error ... 93 Surveys generally use "
6260,unknown,"Surveys generally use the con ﬁdence interval to depict the accuracy of their predictions. For example, a 2006 opinion poll of 1000 randomly selected Americans aged 18–24 conducted by the Roper Public Affairs Division and National Geographic ﬁnds that: •Sixty-three percent of young adults ages 18 –24 cannot ﬁ nd Iraq on a map of the Middle East. 94 6 Univariate Statistics •Eighty-eight percent of "
6261,unknown,"•Eighty-eight percent of young adults ages 18 –24 cannot ﬁ nd Afghanistan on a map of Asia. At the end of the survey, we ﬁ nd the stipulation that the results of this survey are ± ±accurate at the 95% con ﬁdence level 3% points (margin of error 3%). This means that we are 95% con ﬁdent that the true population statistic, i.e., the truepercentage of American youths who cannot ﬁ nd Iraq on a map is "
6262,unknown,"between 60 and 66. In other words, the “ real”mean in the population is anywhere between± 3% points from the mean. This error range is normally called the margin of error or the sampling error . In the Iraqi example, we say we have a sampling error of ± 3% points (mean ± 3% points) (see Fig. 6.27 ). Calculating the Con dence Interval (by Hand) tenﬁ rst values of the variable study time per week of"
6263,unknown,tenﬁ rst values of the variable study time per week of our sample dataset (see Appendix 1). ﬁ To give you an idea on how to construct the con ﬁdence interval by hand using the Step 1: Calculating the mean ðÞ7þ 8 þ 12 þ 3 þ 11 þ 14 þ 11 þ 10 þ 9 þ 8 =10¼ 9 :3 Fig. 6.27 Graphical depiction of the con ﬁdence interval
6264,unknown,"----- 6.9 Deviation, Variance, Standa rd Deviation, Standard Error, Sampling Error ... 95 Step 2: Calculating the variance Þ ¼ ðÞ7⎻ 9 :3 2 þ ðÞ8⎻ 9 :3 2 þ ðÞ12⎻ 9 :3 2 þ ðÞ3⎻ 9 :3 2 þ ðÞ11⎻ 9 :3 2 þ ð14⎻ 9 :3 2 þ ðÞ11⎻ 9 :3 2 þ ðÞ10⎻ 9 :3 2 þ ðÞ9⎻ 9 :3 2 þ ðÞ8⎻ 9 :3 =9 9:34 Step 3: Calculating the standard deviation ---------p 9:34 ¼3 :06 Step 4: Calculating the standard error 9:34----- 10 p ⎧⎫ ¼2"
6265,unknown,"10 p ⎧⎫ ¼2 :95 Step 5: Calculating the con ﬁdence interval 9:3± 1 :96× 9:34 10 p ⎧⎫ ¼15 :09;3 :51 Assuming that this sample is random and normally distributed, we would ﬁ nd that the real average study time of students lies between 3.5 and 15.1 h. This con ﬁdence interval is large because we have few observations and relatively widespread data. 6.9.1 Calculating the Confidence Interval in SPSS Wit"
6266,unknown,"With the help of SPSS, we can calculate the standard deviation and the standard error. We cannot directly calculate the con ﬁdence interval but instead use the SPSS Descriptive Statistics to calculate it by hand (i.e., we have to do the last step by hand). Step 1: Go to Analyze Descriptive Statistics Descriptives (see Fig. 6.28 ).— — Step 2: Once the following window appears, drag the variable stu"
6267,unknown,"Step 2: Once the following window appears, drag the variable study time to the right. Then, click on options. The menu, which you will see to the right, will open. On this menu, you can choose what statistics SPSS will display. Add the option 96 6 Univariate Statistics Fig. 6.28Calculating the con ﬁdence interval in SPSS ( ﬁrst step) S.E. mean. (The options for Mean, Std. Deviation, Minimum, and M"
6268,unknown,"checked automatically). Click continue and okay (see Fig. 6.29). You will receive the following SPSS output (see Table 6.4 ) (please note that this output is based on data for the variable study time from the whole dataset and not only theﬁ rst ten observations). The output displays the number of observations ( N), the minimum and maximum value, and the mean, accompanied by its standard error and "
6269,unknown,"and the standard deviation. If we want to calculate the con ﬁdence interval, we have to do it by hand by using the formula introduced above. The con ﬁdence interval for the variable study time is: Calculating the upper limit: 9.38 + 1.96 0.489 × Calculating the lower limit: 9.38 – 1.96 × 0.489 Assuming that the questionnaire data (see appendix) was drawn from a random sample of students that is no"
6270,unknown,"sample of students that is normally distributed, we could conclude with 95% certainty that the real mean in students ’study time would lie between 8.42 and 10.34 h (see Table 6.4 ). 6.9.2 Calculating the Confidence Interval in Stata Step 1: Write in the Stata Command ﬁ eld: tabstat Study_Time, stats(mean sd semean min max n ) (see Fig. 6.30). (mean¼ mean; sd ¼ standard deviation; semean ¼ standard"
6271,unknown,"min¼ minimum; max ¼ maximum; max ¼ maximum; n ¼ number of observations) 6.9 Deviation, Variance, Standard Deviation, Standard Error, Sampling Error ... 97 Fig. 6.29Calculating the con ﬁdence interval in SPSS (second step) The stat output provides ﬁ ve statistics: (1) the number of observation the calculations are based on, (2) the sample mean, (3) the standard deviation, (4) the minimum sample val"
6272,unknown,"minimum sample value, and (5) the maximum sample value. The con ﬁdence interval is not explicitly listed. If we want to calculate it, we can do so by hand (see also Table 6.5 ): Table 6.4Descriptive statistics of the variable study time (SPSS output) Descriptive statistics N Minimum Maximum Mean Statistic Statistic Statistic Statistic Statistic 98 6 Univariate Statistics Std. deviation Std. error "
6273,unknown,Std. deviation Std. error Study_Time 40 3 15 9.38 0.489 3.094 Valid N(listwise) 40 Fig. 6.30 Calculating the con ﬁdence interval in Stata Table 6.5 Descriptive statistics of the variable study time (Stata output) × Calculating the upper limit: 9.38 + 1.96 × 0.489 Calculating the lower limit: 9.38 – 1.96 0.489 Assuming that the questionnaire data (see appendix) would be drawn from a random sample o
6274,unknown,"sample of students that is normally distributed, we could conclude with 95% certainty that the real mean in students ’study time would lie between 8.42 and 10.34 h. Further Reading SPSS Introductory Books Cronk, B. C. (2017). How to use SPSS ®: A step-by-step guide to analysis and interpretation . London: Routledge. Hands-on introduction into the statistical package SPSS designed for beginners. Sh"
6275,unknown,"beginners. Shows users how to enter data and conduct some rather simple statistical tests. Green, S. B., & Salkind, N. J. (2016). Using SPSS for Windows and Macintosh, Books a la Carte . Upper Saddle River: Pearson. An introduction into SPSS speci ﬁcally designed for students of the social and political sciences. Guides users through basic SPSS techniques and statistics. Further Reading 99 Stata I"
6276,unknown,"Further Reading 99 Stata Introductory Books Mehmetoglu, M., & Jakobsen, T. G. (2016). Applied statistics using Stata: A guide for the social sciences. London: Sage. A good applied textbook into regression analysis with plenty of applied examples in Stata. Pollock III, P. H. (2014). A Stata ®companion to political analysis . Thousand Oaks: CQ Press. Provides a step-by-step introduction into Stata. "
6277,unknown,"Provides a step-by-step introduction into Stata. It includes plenty of supplementary material such as a sample dataset, more than 50 exercises and customized screenshots. Univariate and Descriptive Statistics Park, H. M. (2008). Univariate analysis and normality test using SAS, Stata, and SPSS . Technical working paper. The University Information Technology Services (UITS) Center for Statistical a"
6278,unknown,"and Mathematical Computing, Indiana University. https://scholarworks.iu.edu/dspace/handle/ 2022/19742. A concise introduction into descriptive statistics and graphical representations of data including a discussion of their underlying statistical assumptions. Bivariate Statistics with Categorical Variables 7 Abstract In this part, we will discuss three types of bivariate statistics:ﬁ rst, an indep"
6279,unknown,"samplest -test meas ures if two groups of a continuous varia ble are different from one a nother; second, an f -test or ANOVA measures if severa l groups of one continuous variable are different from one another; third, a chi-square test gauges whether there are diff erences in a freque ncy table (i.e., two-by- two table or two- by-three table). Wherev er possible we use money spent party ing per "
6280,unknown,"dependent varia ble. For the indepe ndent variables, we empl oy an appropr iate explanatory variable from our samp le survey. 7.1 Independent Sample t -Test An independen t samp lest -test asses ses whet her the means of two groups are statisticallydiff erent from each other . To proper ly con duct such a t -test, the foll ow- ing condition s shoul d be met : (1) The d ependent variable shoul d be"
6281,unknown,"ing condition s shoul d be met : (1) The d ependent variable shoul d be conti nuous. (2) The independe nt variable shoul d consi st of mutuall y exclus ive groups (i.e., be categorical). (3) All observ ations shoul d be indepe ndent, which means that there shoul d not be any link age between observ ations (i.e., there shoul d be no direct in ﬂuence from one value withi n one group over other value"
6282,unknown,"one value withi n one group over other values in this same group). (4) There shoul d not be many signi ﬁcant outl iers (this appli es the more the smaller the sample is). (5) The d ependent variable shoul d be more or less norm ally distrib uted. (6) The v ariances betw een groups shoul d be simil ar. # Springer International Publishing AG 2019 D. Stockemer, Quantitative Methods for the Social Sci"
6283,unknown,"https://doi.org/10.1007/978-3-319-99118-4_7 101 Fig. 7.1The logic of a t -test control group mean treatment group mean For example, for our data we might be interested whether guys spend more money than girls while partying, and therefore our dependent variable would be money spent partying (per week) and our independent variable gender. We have relative independence of observations as we cannot a"
6284,unknown,"relative independence of observations as we cannot assume that the money one individual in the sample spends partying directly hinges upon the money another individual in the sample spends partying. From Figs. 6.23 and 6.25 , we also know that the variable money spent partying per week is approximately normally distributed. As a preliminary test, we must check if the variance between the two distr"
6285,unknown,"distributions is equal, but a SPSS or Stata test can later help us detect that. 102 7 Bivariate Statistics with Categorical Variables Having veriﬁed that our data ﬁ t the conditions for a t -test, we can now get into the mechanics of conducting such a test. Intuitively, we could ﬁ rst compare the means for the two groups. In other words, we should look at how far the two means are apart from each "
6286,unknown,"from each other. Second, we ought to look at the variability of the data. Pertaining to the variability, we can follow a simple rule; the less there is variability, the less there is overlap in the data, and the more the two groups are distinct. Therefore, to determine whether there is a difference between two groups, two conditions must be met: (1) the two group means must differ quite considerab"
6287,unknown,"be met: (1) the two group means must differ quite considerably, and (2) the spread of the two distributions must be relatively low. More precisely, we have to judge the difference between the two means relative to the spread or variability of their scores (see Fig. 7.1). The t -test does just this. Figure 7.2 graphically illustrates that it is not enough that two group means are different from one"
6288,unknown,"different from one another. Rather, it is also important how close the values of the two groups cluster around a mean. In the last of the three graphs, we can see that the two groups are distinct (i.e., there is basically no data overlap between the two groups). In the middle graph, we can be rather sure that these two groups are similar (i.e., more than 80% of the data points are indistinguishabl"
6289,unknown,"(i.e., more than 80% of the data points are indistinguishable; they could belong to either of the two groups). Looking at the ﬁ rst graph, we see that most of the observations clearly belong to one of the two groups but that there is also some overlap. In this case, we would not be sure that the two groups are different. 7.1 Independent Sample t -Test 103 Fig. 7.2 The question of variability in a "
6290,unknown,"variability high variability low variability Fig. 7.3Formula/logic of a t - test signal difference between group means variability of groups noise t-value = = = XT SE(XT - XC) XC- __ __ Statistical Analysis of the t -Test The difference between the means is the signal, and the bottom part of the formula is the noise, or a measure of variability; the smaller there are differences in the signal and "
6291,unknown,"and the larger the variability, the harder it is to see the group differences. The logic of at -test can be summarized as follows (see Fig. 7.3): The top part of the formula is easy to compute — justﬁ nd the difference between the means. The bottom is a bit more complex; it is called the standard error of the difference. To compute it, we have to take the variance for each group and divide it by t"
6292,unknown,by the number of people in that group. We add these two values and then take their square root. The speci ﬁc formula is as follows: SE ( -XT – -XC ) = ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ VarT nT þ VarC nC r Theﬁ nal f ormula for the t -test is the following: t= X-T – X-Cﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ VarT nT þ VarC q nC Thet -value will be positive if the ﬁ rst mean is larger than the second one and negative if it 
6293,unknown,"negative if it is smaller. However, for our analysis this does not matter. What matters more is the size of the t -value. Intuitively, we can say that the larger the t -value the higher the chance that two groups are statistically different. A high T -value is triggered by a considerable difference between the two group means and low variability of the d ata around the two group means. To statisti"
6294,unknown,"variability of the d ata around the two group means. To statistically determine whether the t -value is large enough to conclude that the two groups are statistically different, we need to use a test of signi ﬁcance. A test of signi ﬁcance sets the amount of error, called the alpha level, which we allow our statistical calculation to have. In most social research, the “ rule of thumb ”is to set th"
6295,unknown,"most social research, the “ rule of thumb ”is to set the alpha level at 0.05. This means that we allow 5% error. In other words, we want to be 95% certain that a given relationship exists. This implies that, if we were to take 100 samples from the same population, we could get a signi ﬁcantT -value in 95 out of 100 cases. As you can see from the formula, doing a t -test by hand can be rather compl"
6296,unknown,"Therefore, we have SPSS or Stata to do the work for us. 7.1.1 Doing an Independent Samples t -Test in SPSS 104 7 Bivariate Statistics with Categorical Variables Step 1: Pre-test — Create a histogram to detect whether the dependent variable — money spent partying — is normally distributed (see Sect. 6.8 ). Despite the outlier (200$ /month), the data is approximately normally distributed, and we can"
6297,unknown,"proceed with the independent samples t -test (see Fig. 7.4 ). Step 2: Go to Analyze — Compare Means — Independent Samples T -Test (see Fig. 7.5 ). Step 2: Put your continuous variable as test variable and your dichotomous variable as grouping variable. In the example that follows, we use our dependent vari- able— money spent partying from our sample dataset — as the test variable. As the grouping "
6298,unknown,"grouping variable, we use the only dichotomous variable in our dataset — gender. After dragging over gender to the grouping ﬁ eld, click on De ﬁne Groups and label the grouping variable 1 and 0. Click okay (see Fig. 7.6 ). Step 3: Verifying the equal variance assumption — before we conduct and interpret thet -test, we have to verify whether the assumption of equal variance is met. Columns 1 and 2 "
6299,unknown,"Columns 1 and 2 in Table 7.1 display the Levene test for equal variances, which measures whether the variances or spread of the data is similar between the two groups (in our case between guys and girls). If the f -value is not signi ﬁcant (i.e., the signiﬁcance level in the second column is larger than 0.05), we do not violate the assumption of equal variances. In this case, it does not matter wh"
6300,unknown,"interpret the upper or the lower row of the output table. However, in our case the signiﬁcance value in the second column is below 0.05 ( p = 0.018). This implies that the assumption of equal variances is violated. Yet, this is not dramatic for 7.1 Independent Sample t -Test 105 Mean = 76.50 Std. Dev. = 30.153 N = 40 20 15 10 5 0 Money_Spent_Partying Frequency .00 50.00 100.00 150.00 200.00 250.00"
6301,unknown,".00 50.00 100.00 150.00 200.00 250.00 Fig. 7.4 Histogram of the variable money spent partying Fig. 7.5 Independent samples t -test in SPSS (second step) interpreting the output, as SPSS offers us an adapted t -test, which relaxes the assumption of equal variances. This implies that, in order to interpret the t -test, we have to use the second row (i.e., the row labeled equal variances not assumed)"
6302,unknown,"our case, it is the outlier that skews the variance, in particular in the girls ’group. 106 7 Bivariate Statistics with Categorical Variables Fig. 7.6 Independent samples t -test in SPSS (third step) Table 7.1 SPSS output of an independent samples t -test The significance level determines the alpha level. In our case, the alpha level is superior to 0.05. Hence, we would conclude that the two group"
6303,unknown,"two groups are not different enough to conclude with 95% certainty that there is a difference 7.1.2 Interpreting an Independent Samples t -Test SPSS Output Having tested the data for normality and equal variances, we can now interpret the t - test. The t -test output provided by SPSS has two components (see Table 7.1): one summary table and one independent samples t -test table. The summary table "
6304,unknown,"n = the mean amount of money that girls and guys spent partying. We ﬁ nd that girls (who were coded 1) spend slightly more money when they go out and party compared to guys (which we coded 0). Yet, the difference is rather moderate. On average, girls merely spend 6 dollars more per week than guys. If we further look at the standard deviation, we see that it is rather large, especially for group 1 "
6305,unknown,"the standard deviation, we see that it is rather large, especially for group 1 featuring girls. Yet, this large standard deviation is expected and at least partially trig gered by the outlier. Based on these observations, we can take the educated guess that there is, in fact, no signi ﬁcant difference between the two groups. In order to con ﬁrm or disprove this conjecture, we have to look at the s"
6306,unknown,"disprove this conjecture, we have to look at the second output in Table 7.1,i particular the ﬁ fth column of the second table (which is the most important ﬁ eld to interpret a t -test). It displays the signi ﬁcance or alpha level of the independent samplest -test. Assuming that we take the 0.05 benchmark, we cannot reject the null hypothesis with 95% certainty. Hence, we can conclude that there is"
6307,unknown,"cally signi ﬁcant difference between the two groups. 7.1.3 Reading an SPSS Independent Samples t -Test Output Column by Column 7.1 Independent Samplet -Test 107 Column 3 displays the actualt -value. (Large t -values normally trigger a difference in the two groups, whereas small t -values indicate that the two groups are similar.) Column 4displays what is called degrees of freedom ( df) in statisti"
6308,unknown,"similar.) Column 4displays what is called degrees of freedom ( df) in statistical language. The degrees of freedom are important for the determination of the signi ﬁcance level in the statistical calculation. For interpretation purposes, they are less important. In short, the df are the number of observations that are free to vary. In our case, we have a sample of 40 and we have 2 groups, girls an"
6309,unknown,"have a sample of 40 and we have 2 groups, girls and guys. In order to conduct a t - test, we must have at least one girl and one guy in our sample, these two parameters are ﬁ xed. The remaining 38 people can then be either guys or girls. This means that we have 2 ﬁ xed parameters and 38 free- ﬂying parameters or df . Column 5 displays the signi ﬁcance or alpha level. The signi ﬁcance or alpha leve"
6310,unknown,"the most important sample statistic in our interpretation of the t -test; it gives us a level certainty about our relationship. We normally use the 95% certainty level in our interpretation of statistics. Hence, we allow 5% error (i.e., a signi ﬁcance level of 0.05). In our example, the signi ﬁcance level is 0.103, which is higher than 0.05. Therefore, we cannot reject the null hypothesis and henc"
6311,unknown,"spend more than guys. Column 6 displays the difference in means between the two groups (i.e., in our example this is the difference in the average amount spent partying between girls and guys, which is 5.86). The difference in means is also the numerator of the t - test formula. Column 7 d isplays t he denominator of the t -test, which is the standard error of the difference of the two groups. If "
6312,unknown,"difference of the two groups. If we divide the value in column 6 by the value in column 7, we get the t -statistic (i.e., 5.86/9.27 0.632). 108 7 Bivariate Statistics with Categorical Variables 0 .005 .01 .015 .02 Density 0 50 100 150 200 Money_Spent_Partying Fig. 7.7Stata histogram of the variable money spent partying Fig. 7.8Levene test of equal variances Column 8This nal split column gives the "
6313,unknown,"Column 8This nal split column gives the con dence interval of the differenceﬁ ﬁ between the two groups. Assuming that this sample was randomly taken, we could be con ﬁdent that the real difference between girls and guys lies between –0.321 and 3.554. Again, these two values con ﬁrm that we cannot reject the null hypothesis, because the value 0 is part of the con ﬁdence interval. 7.1.4 Doing an Ind"
6314,unknown,"7.1.4 Doing an Independent Samples t -Test in Stata Step 1: Pre-test— Histogram to detect whether the dependent variable — money spent partying per week — is normally distributed. Despite the outlier (200 $ /month), the data is approximately normally distributed, and we can proceed with the independent samples t -test (Fig. 7.7). Step 2 : P re-test— Checking for equal variances — write into the St"
6315,unknown,"ﬁeld: robvar Money_Spent_Partying, by(Gender) (see Fig. 7.8)— this command will conduct a Levene test of equal variances; if this test turns out to be signi ﬁ- cant, then the null hypothesis of equal variances must be rejected (to interpret the = 7.1 Independent Sample t -Test 109 Table 7.2 Stata Levene test of equal variances Fig. 7.9 Doing a t -test in Stata Levene test, use the test labeled WO)"
6316,unknown,"Levene test, use the test labeled WO). This is the case in our example (see Table 7.2). The signi ﬁcance level (PR > F 0.018) is below the bar of 0.05. Step 3: Doing a t -test in in Stata — type into the Stata Command ﬁ eld:“ ttest Money_Spent_Partying, by(Gender) unequal ”(see Fig. 7.9). (Note: if the Levene test for equal variances does not come out signi ﬁcant, you do not need to add equal at t"
6317,unknown,"equal at the end of the command.) 7.1.5 Interpreting an Independent Samplest -Test Stata Output Having tested the data for normality and equal variances, we can now interpret the t - test. The t -test output provid ed by Stata has six columns (see Table 7.3 ): Column 1 labels the two groups (in our case group 0 = guys and group 1 = girls). Column 2 gives the number of observations. In our case, we"
6318,unknown,"Column 2 gives the number of observations. In our case, we have 19 guys and 21 girls. Column 3 displays the mean spending value for the two groups. We ﬁ nd that girls spend slightly more money when they go out and party compared to guys. Yet, the difference is rather moderate. On average, girls merely spend roughly 6 dollars more per week than guys. Columns 4 and 5 show the standard error and stan"
6319,unknown,"Columns 4 and 5 show the standard error and standard deviation, respectively. If we look at both measures, we see that they are rather large, especially for group 1 featuring girls. Yet, this large standard deviation is expected and at least 110 7 Bivariate Statistics with Categorical Variables Table 7.3 Stata independent samples t -test output The significance level determines the alpha level. In"
6320,unknown,"alpha level is superior to .05. Hence, we would conclude that the two groups are not different enough to conclude with 95 percent certainty that there is a difference partially triggered by the outlier. (Based on these two observations — the two means are relatively close each other and the standard deviation/standard errors are comparatively large — we can take the educated guess that there is no"
6321,unknown,"cant difference in the spending patterns of guys and girls when they party.) Column 6 presents the 95% con ﬁdence interval. It highlights that if these data were randomly drawn from a sample of college students, the real mean would fall between 65.88 dollars per week and 80.96 dollars per week for guys (allowing a certainty level of 95%). For girls, the corresponding con ﬁdence interval would be b"
6322,unknown,"between 61.45 dollars per week and 97.12 dollars per week. Because there is some large overlap between the two con ﬁdence intervals, we can already con- clude that the two groups are not statistically different from zero. In order to statistically determine via the appropriate test statistic whether the two groups are different, we have to look at the signi ﬁcance level associated with the t - = 7"
6323,unknown,"= 7.2F -Test or One-Way ANOVA 111 test (see arrow below). The signi ﬁcance level is 0.53, which is above the 0.05 benchmark. Consequently, we cannot reject the null hypothesis with 95% certainty and can conclude that that there is no statistically signi ﬁcant difference between the two groups. 7.1.6 Reporting the Results of an Independent Samples t -Test In Sect. 4.12 we hypothesized that guys lik"
6324,unknown,"In Sect. 4.12 we hypothesized that guys like the bar scene more than girls do and are therefore going to spend more money when they go out and party. The independent samplest -test discon ﬁrms this hypothesis. On average, it is actually girls who spend slightly more than guys do. However, the difference in average spending (73 dollars for guys and 79 dollars for girls) is not statistically differe"
6325,unknown,"for guys and 79 dollars for girls) is not statistically different from zero ( p 0.53). Hence, we cannot reject the null hypothesis and can conclude that the spending pattern for partying is similar for the two genders. 7.2F -Test or One-Wa y ANOVA T-tests wor k great with dummy variables, but sometimes we have categorical variables with more than two categories. In cases where we have a continuous"
6326,unknown,"variable paired with an ordinal or nominal variable with more than two categories, we use what is called an f -test or one-way ANOVA. The logic behind an f -test is similar to the logic for a t -test. To highlight, if we compare the two graphs in Fig. 7.10, we would probably conclude that the three groups in the second graph are different, while the three groups in the ﬁ rst graph are rather simil"
6327,unknown,"are different, while the three groups in the ﬁ rst graph are rather similar (i.e., in the ﬁrst graph, there is a lot of overlap, whereas in the second graph, there is no overlap, which entails that each value can only be attributed to one distribution). Fig. 7.10 What makes groups different?"
6328,unknown,"112 7 Bivariate Statistics with Categorical Variables Table 7.4Within and between variation (as in other occassions, make sure to put the table heading and then the table) While the logic of an f -test re ﬂects the logic of a t -test, the calculation of several group means and several measures of variability around the group means becomes more complex in an f -test. To reduce this complexity, an A"
6329,unknown,"more complex in an f -test. To reduce this complexity, an ANOVA test uses a simple method to determine whether there is a difference between several groups. It splits the total variance into two groups: between variance and within variance. The between variance measures the variation between groups, whereas the within vari- ance measures the variation within groups. Whenever the between variation "
6330,unknown,"considerably larger than the within variation, we can say that there are differences within groups. The following example highlights this logic (see Table 7.4 ). Let us assume that Table 7.4 depicts two hypothetical samples, which measure the approval ratings of Chancellor Merkel based on social class. In the survey, an approval score of 0 means that respondents are not at all satis ﬁed with her p"
6331,unknown,"mance as chancellor. In contrast, 100 signi ﬁes that individuals are very satis ﬁed with her performance as chancellor. The ﬁ rst sample consists of young people (i.e., 18–25) and the second sample of old people (65 and older). Both samples are split into three categories — high, medium, and low. High stands for higher or upper classes, med stands for medium or middle classes, and low stands for t"
6332,unknown,"working classes. We can see that the mean satisfaction ratings for Chancellor Merkel per social strata do not differ between the two samples; that is, the higher classes, on average, rate her at 52, the middle classes at 42, and the lower classes at 32. However, what differs tremendously between the two samples is the variability of the data. In the ﬁ rst sample, the values are very closely cluste"
6333,unknown,"throughout each of the three categories. We can see that there is much more variability between groups than between observations within one group. Hence, we would conclude that the groups are different. In contrast, in sample 2, there is large within-group variation. That is, the values within each group differ much more than the corresponding values between groups. Therefore, we would predict for"
6334,unknown,"sample 2 that the three groups are probably not that different, despite the fact that their means are different. Following this logic, the formula for an ANOVA analysis orf -test isbetween-group variance/within-group variance . Since it is too dif ﬁcult to c alculate t he between- and within-group variance by hand, we let statistical computer programs do it for us. Fig. 7.11Recoding the variable t"
6335,unknown,"computer programs do it for us. Fig. 7.11Recoding the variable times partying 1 ( ﬁrst step) A standardf -test or ANOVA analysis illustrates if there are differences between groups or group means, but it does not show which speci ﬁc group means are different from one another. Yet, in most cases researchers want to know not only that there are some differences but also between which groups the diff"
6336,unknown,So-called multiple comparison tests — basicallyt -tests between the different groups— compare all means against one another and help us detect where the differences lie. 7.2.1 Doing an f -Test in SPSS For ourf -test we use the variable money spent partying per week as the dependent variable and the categorical variable times partying per week as the factor or grouping variable. Given that we only 
6337,unknown,"grouping variable. Given that we only have 40 observations and given that there should be at least several observations per category to yield valid test results, we will reduce the six categories to three. In more detail, we cluster together no partying and partying once, part ying twice and three times, and partying four times and ﬁ ve times and more together. We can create this new variable by h"
6338,unknown,SPSS do it for us. We will label this variable times partying 1. 7.2F -Test or One-Way ANOVA 113 Step 1: Creating the variable times partying 1 — go to Transform — Recode into Different Variable (see Fig. 7.11). Step 2: Drag the variable Times_Partying into the middle ﬁ eld— name the Output Variable Times_Partying_1 — click on Change — click on Old and New Values (see Fig. 7.12 ). Step 3: Include 
6339,unknown,Step 3: Include in the Range ﬁ eld the value range that will be clustered together — add the new value in the ﬁ eld labeled New Value — click Add— do this for the all three ranges — once your dialog ﬁ eld looks like the dialog ﬁ eld below click Continue. You will be redirected to the initial screen; click okay and then the new variable will be added to the SPSS dataset (see Fig. 7.13 ). Fig. 7.12 
6340,unknown,Fig. 7.12 Recoding the variable Times Partying 1 (second step) Fig. 7.13 Recoding the variable Times Partying 1 (third step) Step 4 : For doing the actual f -test go to Analyze Compare Means One-Way— — — ANOVA (see Fig. 7.14 ). and your ordinal variable (times spent partying 1) as Factor. Click okay (see Fig. 7.15 ). 114 7 Bivariate Statistics with Categorical Variables Step 5:Put your continuous 
6341,unknown,"7.2F -Test or One-Way ANOVA 115 Fig. 7.14 Doing an f -test in SPSS (ﬁrst step) Fig. 7.15 Doing an f -test in SPSS (second step) 7.2.2 Interpreting an SPSS ANOVA Output The SPSS ANOVA output contains ﬁ ve columns (see Table 7.5). Similar to a t -test, the most important column is the signi ﬁcance level (i.e., Sig). It tells us whether there is a difference between at least two out of the however ma"
6342,unknown,"is a difference between at least two out of the however many groups we have. In our example, the signi ﬁcance level is 0.000, which means that we can tell with nearly 100% certainty that at least two groups differ in the money they spent partying per week. Yet, the SPSS ANOVA output does not tell us which groups are different; the only thing it tells us is that at least two groups are different fr"
6343,unknown,"In more detail, the different columns are interpreted as follows: Column 1 displays the sum of squares or the squared deviations for the different variance components (i.e., between-group variance, within-group variance, and total variance). 116 7 Bivariate Statistics with Categorical Variables Table 7.5 SPSS ANOVA output Column 2 displays the degrees of freedom, which allows us to calculate the w"
6344,unknown,"and between variance. The formula for these degrees of freedom is number of groups ( k)– 1 for the between-group estimator and the number of observations (N)– k for within group. Column 3 shows the between and within variance or sum of squares. According to thef -test formula, we need to divide the between variance by the within variance (F6242.50/620.95 10.053).= = Column 4displays the f -value. "
6345,unknown,"Column 4displays the f -value. The larger the f -value, the more likely it is that at least two groups are statistically different from one another. Column 5 gives us an alpha level or level of certainty indicating a probability level that there is a difference between at least two groups. In our case, the signi ﬁcance level is 0.000 indicating that we can be nearly be 100% certain that at least t"
6346,unknown,"groups differ in, how much money the spent partying per week. 7.2.3 Doing a Post hoc or Multiple Comparison Test in SPSS The violation of the equal variance assumption (i.e., the distributions around the group means are different for the various groups in the sample) is rather unproblem- atic for interpreting a one-way ANOVA analysis (Park 2009). Yet, having an equal or unequal variability around "
6347,unknown,or unequal variability around the group means is important when doing multiple pairwise comparison tests between means. This assumption needs to be tested before we can do the test: Step 1: Testing the equal variance assumption — go to the One-Way ANOVA Screen (see step 5 in Sect. 7.2.1)— click on Options — a new screen with options opens — click on Homogeneity of Variance Test (see Fig. 7.16). St
6348,unknown,(see Fig. 7.17). The equality of means test provides a signi ﬁcant result (Sig = 0.024) indicating that the variation around the three group means in our sample is not equal. We have to take this inequality of variance in the three distributions into consideration when conducting the multiple comparison test (see Table 7.6). Fig. 7.16 Doing a post hoc multiple comparison test in SPSS ( ﬁrst step) 
6349,unknown,Fig. 7.17Doing a post hoc multiple comparison test in SPSS (second step) Step 3: Conducting the multiple comparison test go to the One-Way ANOVA— 7.2F -Test or One-Way ANOVA 117 Command window (see Step 2) — click on the button Post Hoc and choose any of the four options under the label Equal Variances Not Assumed. Please note if your equality of variances test did not yield a statistically signi 
6350,unknown,your equality of variances test did not yield a statistically signi ﬁcant result (i.e. the sig value is not smaller than 0.05) you should choose any of the options under the label Equal Variances Assumed) (see Fig. 7.18). 118 7 Bivariate Statistics with Categorical Variables Table 7.6 Robust test of equality of means Fig. 7.18 Doing a post hoc multiple comparison test in SPSS (third step) The post
6351,unknown,"The post hoc multiple comparison test (see Table 7.7) allows us to decipher which groups are actually statistically different from one another. From the SPSS output, we see that individuals who either do not go out not at all or go out once per week (group 0) spend less than individuals who go out four times or more (group 2). The average mean difference is 43 dollars per week, this difference is "
6352,unknown,"different from zero ( p = 0.032). We also see from the SPSS output that individuals who go out and party two or three times (group 1) spend signi ﬁcantly less than individuals who party four or more times (group 2). In absolute terms, they are predicted to spend 39.5 dollars less per week. This difference is statistically different from zero ( p = 0.041). In contrast, there is no statistical diffe"
6353,unknown,"Table 7.7SPSS output of a post hoc multiple comparison test patterns between groups 0 and 1. In absolute terms, there is a mere 3.5 dollars difference between the two groups. This difference is not statistically different from zero ( p = 0.955). 7.2.4 Doing an f -Test in Stata For ourf -test we use money spent partying per week as the dependent variable, and as the independent variable, we use the"
6354,unknown,"as the independent variable, we use the categorical variable times partying per week. Given that we only have 40 observations and given that there should be at least several observat ions per category to yield valid test results, we reduce the six categories to three. In more detail, we cluster together no partying and partying once, partying twice and three times, and partying four times and ﬁ ve"
6355,unknown,"We can create this new variable by hand, or we can also have Stata do it for us. We will label this variable times partying 1. Preliminary Procedure: Creating the variable times partying 1 Write in the Stata Command editor: 7.2F -Test or One-Way ANOVA 119 (1) generat e Times_Partying_1 0 = (this creates the variable and assigns the value of 0 to all observations) (see Fig. 7.19 ) (2) replace Times"
6356,unknown,Fig. 7.19 ) (2) replace Times_Partying_1 1 if(Times_Partying 2) and Times_Partying 3 = ≥ ≤ (this assigns the value of 1 to all individuals in groups 2 and 3 for the variable times partying) (see Fig. 7.20 ). (3) replace Times_Partying_1 2 if(Times_Partying 4) (see Fig.= ≥ 7.21) (this assigns the value of 2 to all individuals in groups 4 and 5). 120 7 Bivariate Statistics with Categorical Variables
6357,unknown,"Fig. 7.19 Generating the variable Times_Partying 1 ( ﬁrst step) Fig. 7.20 Generating the variable Times_Partying 1 (second step) Fig. 7.21 Generating the variable Times_Partying 1 (third step) Fig. 7.22 Doing an ANOVA analysis in Stata Main analysis: conducting the ANOVA Write in the Stata Command editor: oneway Money_Spent_Partying Times_Partying_1, tabulate (see Fig. 7.22). 7.2.5 Interpreting an"
6358,unknown,"Times_Partying_1, tabulate (see Fig. 7.22). 7.2.5 Interpreting an f -Test in Stata In the ﬁ rst part of the table, Stata provides summary statistics of the three group means (Table 7.8 ). The output reports that individuals who go out partying once a week or less spend on average 64 dollars per week. Those who party two to three times per week merely spend 3.5 dollars more than the ﬁ rst group. In"
6359,unknown,"individuals in the third group of students, who party four or more times, spend approximately 107 dollars per week partying. We also see that the standard deviations are quite large, especially for the third group — students that party four or more times per week. These descriptive statistics also give us a hint that there is probably a statistically signi ﬁcant difference between the third group "
6360,unknown,"other groups. The f -test (Prob > F = 0.0003) further reveals that there are in fact differences between groups. However, the f -test does not allow us to detect where the differences lie. In order to gauge this, we have to conduct a multiple comparison test. However, before doing so we have to gauge whether the variances of the three Table 7.8Stata ANOVA output this table is misplaced here put be"
6361,unknown,"Table 7.8Stata ANOVA output this table is misplaced here put below the text in 7.2.5 groups are equal or if they are dissimilar. The Bartlett ’s test for equal variance, which is listed below the ANOVA output, gives us this information. If the test statistic provides a statistically signi ﬁcant value, then we have to reject the null hypothesis of equal variances and accept the alternative hypothes"
6362,unknown,"equal variances and accept the alternative hypothesis that the variances between groups are unequal. In our case, the Bartlett ’s test displays a statistically signi ﬁcant value (prob > chi2 = 0.001). Hence, we have to proceed with a post-strati ﬁcation test with unequal variance. 7.2.6 Doing a Post hoc or Multiple Comparison Test with Unequal Variance in Stata Since the Bartlett ’s test indicates"
6363,unknown,"Since the Bartlett ’s test indicates some violation of the equal variance assumption (i.e., the distributions around the group means are different for the various groups in the sample), we have to conduct a multiple comparison test in which the unequal variance assumption is relaxed. A Stata module to compute pairwise multipl e comparisons with unequal variances exists, but is not included by defa"
6364,unknown,"be downloaded. To do the test we have follow a multistep procedure: 7.2F -Test or One-Way ANOVA 121 Step 1: Write in the Command ﬁ eld: search pwmc (see Fig. 7.23). This brings us to a Stata page with several links — click on the ﬁ rst link and click on “(Click here to install) ”to download the program (Once, downloaded, the program is installed and does not need to be downloaded again) (see Fig. "
6365,unknown,"Step 2: Write into the Command ﬁ eld: pwmc Money_Spent_Partying, over (Times_Partying_1) (Fig. 7.25 ). 122 7 Bivariate Statistics with Categorical Variables Fig. 7.23Downloading a post hoc or multiple comparison test with unequal variance Fig. 7.24Download page for the post hoc multiple comparison test Fig. 7.25Doing a post hoc or multiple comparison test with unequal variance in Stata Stata actua"
6366,unknown,"Stata actually provides test results of three different multiple comparison tests, all using some slightly different algebra (see Table 7.9). Regardless of the test, what we can see is that, substantively, the results of the three tests do not differ. These tests are also slightly more dif ﬁcult to interpret because they do not display any signi ﬁ- cance level. Rather, we have to interpret the con"
6367,unknown,"cance level. Rather, we have to interpret the con ﬁdence interval to determine whether two means are statistically different from zero. We ﬁ nd that there is no statistically signiﬁcant difference between groups 0 and 1. For example, if we look at theﬁ rst of the three tests, we ﬁ nd that the con ﬁdence interval includes positive and negative values; it ranges from – 16.90 to 23.90. Hence, we cann"
6368,unknown,"negative values; it ranges from – 16.90 to 23.90. Hence, we cannot accept the alternative hypothesis that groups 0 and 1 are different from one another. In contrast, we can conclude that groups 0 and 2, as well as 1 and 2, are statistically different from one another, respectively, because both con ﬁdence intervals include only positive values (i.e., the con ﬁdence interval for the difference betw"
6369,unknown,"0 and 2 ranges from 2.38 to 83.62 and the con ﬁdence interval for the difference between groups 1 and 2 ranges from 2.54 to 76.46). Please also note that in case the Bartlett ’s test of equal variance (see Table 6.3) does not display any signi ﬁcance, a multiple comparison test assuming equal variance can be used. Stata has many options; the most prominent ones are probably the algorithms by Schef"
6370,unknown,"the algorithms by Scheffe and Sidak. For presentation purposes, let us assume that the Bartlett test was not signi ﬁcant in Table 6.3 . In such a case, we would type into the Stata Command ﬁ eld: = 7.2F -Test or One-Way ANOVA 123 Table 7.9 Stata output of post hoc multiple comparison test Fig. 7.26 Multiple comparison test according to Sidak oneway Money_ Spent_Partying Times_Partying_1, sidak (se"
6371,unknown,"To determine whether there is a signi ﬁcant difference between groups, we would interpret the two-by-two table (see the second part of Table 7.10). The table highlights that the mean difference between group 0 and group 1 is 3.5 dollars, a value that is not statistically different from 0 (Sig = 0.978). In contrast the difference in spending between group 0 and group 2, which is 43 dollars, is stat"
6372,unknown,"signiﬁcant (sig 0.001). The same applies to the difference in spending (39.5 dollars) between groups 1 and 2 (sig = 0.001). 124 7 Bivariate Statistics with Categorical Variables Table 7.10 Multiple comparison test with equal variance in Stata 7.2.7 Reporting the Results of an f -Test In Sect. 4.12, we hypothesized that individuals who party more frequently will spend more money for their weekly pa"
6373,unknown,"more money for their weekly partying habits than individuals that party less fre- quently. Creating three groups of party goers — (1) students, who party once or less, on average, (2) students who party between two and three times, and (3) students who party more than four times — weﬁ nd some support for our hypothesis; that is, a generalf -test conﬁrms (Sig= 0.0003) that there are differences bet"
6374,unknown,"generalf -test conﬁrms (Sig= 0.0003) that there are differences between groups. Yet, thef -test cannot tell us between which groups the differences lie. In order to ﬁ nd this out, we must compute a post hoc multiple comparison test. We do so assuming unequal variances between the three distributions, because a Bartlett ’s test of equal variance (sig= 0.001) reveals that the null hypothesis (get ri"
6375,unknown,"variance (sig= 0.001) reveals that the null hypothesis (get rid of the plural, I cannot delete the word hypotheses) hypotheses of equal variances must be rejected. Our results indicate that the mean spending average statistically signi ﬁcantly differs between groups 0 and 2 [i.e., the average difference in party spending is 43 dollars (sig= 0.032)]. The same applies to the difference between group"
6376,unknown,"(sig= 0.032)]. The same applies to the difference between groups 1 and 2 [i.e., the average difference in party spending is 39.5 dollars (Sig = 0.041)]. In contrast, there is no difference in spending between those who party once or less and those who party two or three times (sig 0.955). 7.3 Cross-tabulation Table and Chi-Square Test 125 7.3 Cross-tabulation Table and Chi-Square Test 7.3.1 Cross-"
6377,unknown,"7.3 Cross-tabulation Table and Chi-Square Test 7.3.1 Cross-tabulation Table So far, we have discussed bivariate tests that work with a categorical variable as the independent variable and a continuous variable as the dependent variable (i.e., an independent samples t -test if the independent variable is binary and the dependent variable continuous and an ANOVA or f -test if the independent variabl"
6378,unknown,"than two categories). What happens if both the independent and dependent variables are binary? In this case, we can present the data in a crosstab. Table 7.11 provides an example of a two-by-two table. The table presents the results of a lab experiment with mice. A researcher has 105 mice with a severe illness; she treats 50 mice with a new drug and does not treat 55 mice at all. She wants to know"
6379,unknown,"wants to know whether this new drug can cure animals. To do so she creates four categories: (1) treated and dead, (2) treated and alive, (3) not treated and dead, and (4) not treated and alive. Based on this two-by-two table, we can ask the question: How many of the dead are either treated or not treated? To answer this question, we have to use the column as unit of analysis and calculate the perc"
6380,unknown,"as unit of analysis and calculate the percentage of dead, which are treated and the percentage of dead, which are not treated. To do so, we have to convert the column raw numbers into percentages. To calculate these percentages for the ﬁ rstﬁ eld, we take the number in the ﬁ eld— treated/dead— and divide it by the column total (36/66= 55.55%). We do analogously for the other ﬁ elds (see Table 7.12"
6381,unknown,"Interpreting Table 6.7, we can ﬁ nd, for example, that roughly 56% of the dead mice have been treated, but only 35.9% of those alive have undergone treatment. Instead of asking the question how many of the dead mice have been treated, we might change the question and ask how many of the dead have been treated or alive? To get at this information, we ﬁ rst calculate the percentage of dead mice with"
6382,unknown,"treatment and of alive mice with treatment. We do analogously for the dead that are not treated and the alive that are not treated. Doing so, we ﬁ nd that of all treated, 72% are dead and only 28% are alive. In contrast, the not treated mice have a higher chance of survival. Only 55.55% have died and 44.45% have survived (see Table 7.13). Table 7.11Two-by-two table measuring the relationship betwe"
6383,unknown,of animals Dead Alive Treated 36 14 Not treated 30 25 Total 66 39 126 7 Bivariate Statistics with Categorical Variables Table 7.12 Two-by-two table focusing on the interpretation of the columns Dead Alive Treated 36 14 Percent 55.55 35.90 Not treated 30 25 Percent 44.45 64.10 Total 66 39 Percent 100 100 Table 7.13 Two-by-two table focusing on the interpretation of the rows Dead Alive Total Treated
6384,unknown,Percent 72.00 28.00 100 Not treated 30 25 55 Total 55.55 44.45 100 Table 7.14 Calculating the expected values Dead Alive Total Treated 36 (31.4) 14 (18.6) 50 Not treated 30 (34.6) 25 (20.4) 55 Total 66 39 105 7.3.2 Chi-Square Test Having interpreted the crosstabs a researcher might ask whether there is a relation- ship between drug treatment and the survival of mice. To answer this research questi
6385,unknown,"question, she might postulate the following hypothesis: H0: The survival of the animals is independent of drug treatment. Ha: The survival of the animals is higher with drug treatment. In order to determine if there is a relationship between drug treatment and the survival of mice, we use what is called a chi-square test. To apply this test, we compare the actual value in each ﬁ eld with a random "
6386,unknown,"compare the actual value in each ﬁ eld with a random distribution of values between the four ﬁ elds. In other words, we compare the real value in each ﬁ eld with an expected value, calculated so that the chance that a mouse falls in each of the four ﬁelds is the same. To calculate this expected value, we use the following formula: Row Total× Column Total Table Total Table 7.14 displays the observe"
6387,unknown,"Table Total Table 7.14 displays the observed and the expected values for the four possible categories: (1) treated and dead, (2) treated and alive, (3) not-treated and dead, and (4) not-treated and alive. The logic behind a chi-square test is that the larger the gap between one or several observed and expected values, the higher the chance that there actually is a pattern or relationship in the da"
6388,unknown,"inﬂuence on survival). The formula for a chi-square test is: X2 = ðÞObserved– Expected 2 Expected In more detail the four steps to calculate a chi-square value are the following: 7.3 Cross-tabulation Table and Chi-Square Test 127 (1) For each observed value in the table, subtract the corresponding expected value (O–E). (2) Square the difference (O–E)2. (3) Divide the squares obtained for each cell"
6389,unknown,"that cell (O–E)2/E. (4) Sum all the values for ( O–E)2/E. This is the chi-square statistic. Our treated animal example gives us a chi-square value of 3.418. This value is not high enough to reject the null hypothesis of a no effect between treatment and survival. Please note that in earlier times, researchers compared their chi-square test value with a critical value, a value that marked the 5% al"
6390,unknown,"test value with a critical value, a value that marked the 5% alpha level (i.e., a 95% degree of certainty). If their test value fell below this critical value, then the researcher could conclude that there is no difference between the groups, and if it was above, then the researcher could conclude that there is a pattern in the data. In modern times, statistical outputs display the signi ﬁcance le"
6391,unknown,"modern times, statistical outputs display the signi ﬁcance level associated with a chi-square value right away, thus allowing the researcher to determine right away if there is a relationship between the two categorical variables. A chi-square test works with the following limitations: – No expected cell count can be less than 5. – Larger samples are more likely to trigger statistically signi ﬁcan"
6392,unknown,"– The test only identi ﬁesthat a difference exists, not necessarily where it exists. (If we want to decipher where the differences are, we have look at the data and detect in what cells are the largest differences between observed and expected values. These differences are the drivers of a high chi-square value.) 7.3.3 Doing a Chi-Square Test in SPSS The dependent variable of our study, which we u"
6393,unknown,"The dependent variable of our study, which we used for the previous tests (i.e., t -test andf -test)— money spent partying — is continuous, and we cannot use it for our chi- square test. We have to use two categorical variables and make sure that there are Fig. 7.27Doing crosstabs in SPSS ( ﬁrst step) leastﬁ ve observations in each cell. The two categorical variables we choose are gender and times"
6394,unknown,"gender and times partying per week. Since, we have only 40 observations, we further contract the variable times partying and create 2 categories: (1) partying a lot (three times or more a week) and partying moderately (less than 3 times a week). We name this new variable times partying 2. To create this new variable, see Sect. 7.1.2. 128 7 Bivariate Statistics with Categorical Variables Step 1: Go"
6395,unknown,"Step 1: Go to Analyze — Descriptive Statistics — Crosstabs (see Fig. 7.27). Step 2: Put Gender in the Row(s) ﬁ eld and Times Partying 2 in the Columns ﬁ eld. Click on statistics (see Fig. 7.28). Step 3: In the Crosstabs: Statistics ﬁ eld, check the box Chi-square — click continue— you will be redirected to the previous box — click okay (see Fig. 7.29). 7.3.4 Interpreting an SPSS Chi-Square Test An"
6396,unknown,"An SPSS chi-square test output consists of two parts: a cross-tabulation table and the actual chi-square test (see Table 7.15). The crosstab indicates that the sample consists of 21 guys and 19 girls. Within the 2 genders, partying habits are quite equally distributed; 9 guys party 2 times or less per week, and 12 guys party 3 times or more. For girls, the numbers are rather similar — ten girls pa"
6397,unknown,"week, on average, and nine girls three times or more. We already see from the chi-square table that there is hardly any difference between guys and girls. The Pearson chi-square test (i.e., ﬁ rst row of the chi-square test table) con ﬁrms this observation. The chi-square value is 0.382 and the corresponding signi ﬁcance level is 0.536, which is far above the benchmark of 0.05. Based on these test-"
6398,unknown,"is 0.536, which is far above the benchmark of 0.05. Based on these test-statistics, we cannot reject the null hypothesis; this implies that we can conclude that the partying habits of guys and girls (in terms of the times per week both genders party) do not differ. 7.3 Cross-tabulation Table and Chi-Square Test 129 Fig. 7.28 Doing crosstabs in SPSS (second step) Fig. 7.29 Doing crosstabs in SPSS ("
6399,unknown,"Table 7.15SPSS chi-square test output Fig. 7.30Doing a chi-square test in Stata 7.3.5 Doing a Chi-Square Test in Stata To do a chi-square test, we cannot use the dependent variable of our study — money spent partying — because it is continuous. For a chi-square test to function, we have to use two categorical variables and make sure that there are at least ﬁ ve observations in each cell. The two c"
6400,unknown,"in each cell. The two categorical variables we choose are gender and times partying per week. Since we have only 40 observations, we further collapse the variable times partying and creat e 2 categories: (1) partying a lot (3 times or more a week) and (2) partying moderately (less than 3 times a week). We name this new variable times partying 2. To create this new variable, see Sect. 8.1.3. 130 7 "
6401,unknown,130 7 Bivariate Statistics with Categorical Variables Step 1: Write into the Stata Command ﬁ eld: tab Gender Times_Partying_2 (see Fig. 7.30) (the order in which we write our two categorical variables does not matter). Reference 131 Table 7.16 Stata chi-square output The Stata test output in Table 7.16 consists of a cross-tabulation table and the actual chi-square test below. The crosstab indicate
6402,unknown,"actual chi-square test below. The crosstab indicates that the sample consists of 21 guys and 19 girls. Within the 2 genders, partying habits are quite equally distributed; 9 guys party 2 times or less per week, and 12 guys party 3 times or more. For girls, the numbers are rather similar: ten girls party twice or less per week, on average, and nine girls three times or more. We already see from the"
6403,unknown,"table that there is hardly any difference between guys and girls. The Pearson chi-square test result below the cross-table con ﬁrms this observation. The chi-square value is 0.382, and the corresponding signi ﬁcance level is 0.536, which is far above the benchmark of 0.05. Based on these test statistics, we can conclude that the partying habits of guys and girls (in terms of the times per week bot"
6404,unknown,"party) do not differ. 7.3.6 Reporting a Chi-Square Test Result Using a chi-square test, we have tried to detect if there is a relationship between gender and the times per week students party. We ﬁ nd that in absolute terms roughly about half the girls and guys, respectively, either party two times or less or three times or more. The chi-square test con ﬁrms that there is no statistically signi ﬁc"
6405,unknown,"difference in the number of times either of two genders goes out to party (i.e., the chi-square value is 0.38 and the corresponding signi ﬁcance level is 0.534). Reference Park, H. (2009). Comparing group means: T-tests and one-way ANOVA using Stata, SAS, R, and SPSS. Working paper, The University Information Technology Services (UITS), Center for Statistical and Mathematical Computing, Indiana Un"
6406,unknown,"Statistical and Mathematical Computing, Indiana University. Further Reading Statistics Textbooks chosen these books because they are accessible and approachable and they do not use math Basically every introductory to statistics book covers bivariate statistics between categorical and continuous variables. The books I list here are just a short selection of possible textbooks. I have excessively. "
6407,unknown,"excessively. . London: Routledge (chapter 11). Provides a concise introduction into different types Brians, C. L. (2016). Empirical political analysis: Pearson new international edition coursesmart etextbook of means testing. shows through several examples how statistics can be used as a tool in making informed, Macﬁe, B. P., & Nufrio, P. M. (2017). Applied statistics for public policy . New York:"
6408,unknown,"Macﬁe, B. P., & Nufrio, P. M. (2017). Applied statistics for public policy . New York: Routledge. This practical text provides students with the statistical tools needed to analyze data. It also intelligent policy decisions (part 2). 132 7 Bivariate Statistics with Categorical Variables Walsh, A., & Ollenburger, J. C. (2001). Essential statistics for the social and behavioral sciences: A conceptua"
6409,unknown,"conceptual approach . Upper Saddle River: Prentice Hall (chapters 7 –11). These chapters explain in rather simple forms the logic behind different types of statistical tests between categorical variables and provide real life examples. Presenting Results in Publications Morgan, S., Reichert, T., & Harrison, T. R. (2016). From numbers to words: Reporting statistical results for the social sciences "
6410,unknown,"results for the social sciences . London: Routledge. This book complements introductory to statistics books. It shows scholars how they can present their test results in either visual or text form in an article or scholarly book Bivariate Relationships Featuring Two Continuous Variables 8 Abstract In this chapte r, we discuss bivariate relations hips between two conti nuous variables. In researc h"
6411,unknown,"variables. In researc h, these are the relat ionships that occur the most often . We can expres s bivariate relationships between conti nuous variables in three ways: (1) through a graphi cal repres entation in the form of a scatterplot, (2) throu gh a correlation analysis, and (3) throug h a bivar iate regression analysis. We d escribe and explai n each met hod and show how to implem ent it in SP"
6412,unknown,"8.1 What Is a Bivariate Rel ationship Betwe en Two Continuous Variables? A bivar iate relations hip involving two continuous variab les can be displayed graphically and throu gh a correl ation or regres sion analys is. Such a relat ionship can exist if there is ageneral tenden cy for these two variables to be related, even if it is not a c ompletely determin ed rule. In statistical terms, we say t"
6413,unknown,"is not a c ompletely determin ed rule. In statistical terms, we say that these two variables“ vary toget her”; this means that values of the v ariablex (the indepe ndent variable) tend to occur more often with some values of the variabley (the dependen t variable) than wi th other values of the varia bley . 8.1.1 Positive and Negative Relationships When we describe relationships between variables,"
6414,unknown,"positive and negative relat ionships. Positive relationshipHigh, or a bove average, values ofx tend to occur with high, or above average, values ofy . Also, low values ofx tend to occur with low values of y. # Springer International Publishing AG 2019 D. Stockemer, Quantitative Methods for the Social Sciences , https://doi.org/10.1007/978-3-319-99118-4_8 133 Examples: 134 8 Bivariate Relationships"
6415,unknown,"Examples: 134 8 Bivariate Relationships Featuring Two Continuous Variables • Income and education • National wealth and degree of democracy • Height and weight Negative relationshipHigh, or above average, values of x tend to occur with low ,o r belowaverage, values of y . Also, low values of x tend to occur with high values of y. Examples: • State social spending and income inequality • Exposure t"
6416,unknown,• Exposure to Fox News and support for Democrats • Smoking and life expectancy 8.2 Scatterplots A scatterplot graphically describes a quantitative relationship between two continu- ous variables: Each dot (point) is one individual observation ’s value on x and y . The values of the independent variable ( X) appear in sequence on the horizontal or x -axis. The values of the dependent variable ( Y) 
6417,unknown,"The values of the dependent variable ( Y) appear on the vertical or y -axis. For a positive association, the points tend to move diagonally from lower left to upper right. For a negative association, the points tend to move from upper left to lower right. For NO association, points are scattered with no discernable diagonal line. 8.2.1 Positive Relationships Displayed in a Scatterplot Figure 8.1 d"
6418,unknown,"Figure 8.1 displays a positive association, or a positive relationship, between countries’per capita GDP and the amount of energy they consume. We see that even if the data do not exactly follow a line, there is nevertheless a tendency that countries with higher GDP per capita values are associated with more energy usage. In other words, higher values of the x -axis (the independent variable) corr"
6419,unknown,"higher values on the y -axis (the dependent variable). 8.2.2 Negative Relationships Displayed in a Scatterplot Figure 8.2 displays a negative relationship between per capita GDP, and the share agriculture makes up of a country ’s GDP; that is, our results indicate that the richer a country becomes, the more agriculture loses its importance for the economy. In statistical terms, we ﬁ nd that low va"
6420,unknown,"statistical terms, we ﬁ nd that low values of the x -axis correspond to high values on they -axis and high values on the x -axis correspond to low values on the y -axis. 8.2 Scatterplots 135 Fig. 8.1 Bivariate relationship between the GDP per capita and energy spending Fig. 8.2 Bivariate relationship between national wealth and agriculture 8.2.3 No Relationship Displayed in a Scatterplot Figure 8."
6421,unknown,"Figure 8.3 displays an instance in which the independent and dependent variable are unrelated to one another. In more detail, the graph highlights that the af ﬂuence of a country is unrelated to its size. In other words, there is no discernable direction to the points in the scatterplot. Fig. 8.3Relationship between a country ’s GDP per capita and its size 8.3 Drawing the Line in a Scatterplot The"
6422,unknown,"The line we draw in a scatterplot is called the ordinary least square line (OLS line). In theory, we can draw a multitude of lines, but in practice we want to ﬁ nd the best ﬁtting line to the data. The best ﬁ tting line is the line where the summed up distance of the points from below the line is equal to the summed up distance of the points from above the line. Figure 8.4 clearly shows a line tha"
6423,unknown,"from above the line. Figure 8.4 clearly shows a line that does not ﬁ t the data properly. The distance of all the points toward the line is much larger for the points below the line in comparison to the points above the line. In contrast, the distance of the points toward the line is the same for the points above the line as for the points below the line in Fig. 8.4. In contrast, the line in Fig. "
6424,unknown,"line in Fig. 8.4. In contrast, the line in Fig. 8.5 is the best ﬁ tting line (i.e. the sum of the distance of all the points from the line is zero). 8.4 Doing Scatterplots in SPSS For our scatterplot, we will use money spent partying as the dependent variable. For the independent variable, we will use quality of extra-curricular activities, hypothesizing that students who enjoy the university-spon"
6425,unknown,"hypothesizing that students who enjoy the university-sponsored activities will spend less money partying. Rather than going out and party, they will be in sports and social or political u niversity clubs and partake in their activities. A scatterplot can help us con ﬁrm or discon ﬁrm this hypothesis. 136 8 Bivariate Relationships Featuring Two Continuous Variables Step 1: Go to Graphs —Legacy Dial"
6426,unknown,"Fig. 8.4An example of a line that ﬁ ts the data poorly Fig. 8.5An example of the best ﬁ tted line Step 2: Then you see the box below click on Simple Scatter and De ne (see 8.4 Doing Scatterplots in SPSS 137 —ﬁ Fig. 8.7). Step 3:Put the dependent variable (i.e., money spent partying) on the y -axis and the independent variable (quality of extra-curricular activities) on the x -axis. Click okay (see"
6427,unknown,"Step 4: After the completion of the steps explained in step 3, the scatterplot will show up. However, it would be nice to add a line, which can give us a more robust estimate of the relationship between the two variables. In order to include the line, double-click on the scatterplot in the output window. The chart builder below will appear. Then, just click on the line icon on the second row (the "
6428,unknown,"the scatterplot with the line will appear (see Fig. 8.9). 138 8 Bivariate Relationships Featuring Two Continuous Variables Fig. 8.6 Doing a scatterplot in SPSS ( ﬁrst step) Fig. 8.7 Doing a scatterplot in SPSS (second step) As expected, Fig. 8.9 displays a negative relationship between the quality of extra- curricular activities and the money students spent partying. The graph displays that studen"
6429,unknown,"students who think that the extra-curricular activities offered by the university are poor do in fact spend more money per week partying. In contrast, students who like the sports and social and political clubs at their university are less likely to spend a lot of money partying. Because we can see that the line is relatively steep, we can already detect that the relationship is relatively strong;"
6430,unknown,"already detect that the relationship is relatively strong; a bivariate regression analysis (see below) will give us some information about the strength of this relationship. 8.5 Doing Scatterplots in Stata 139 Fig. 8.8Doing a scatterplot in SPSS (third step) 8.5 Doing Scatterplots in Stata For our scatterplot, we will use money spent partying as the dependent variable. For the independent variable"
6431,unknown,"For the independent variable, we will use quality of extra-curricular activities, hypothesizing that students who enjoy the university-sponsored free time activities will spend less money partying. Rather than going out and party, they will be involved in sports or social and political university clubs and partake in their activities. A scatterplot can he lp us con ﬁrm or discon ﬁrm this hypothesi"
6432,unknown,140 8 Bivariate Relationships Featuring Two Continuous Variables y=1.15E2-0.84*x R2 Linear = 0.361 200.00 150.00 100.00 50.00 .00 Quality_Extra_Curricular_Activities Money_Spent_Partying .00 20.00 40.00 60.00 80.00 100.00 Fig. 8.9 SPSS scatterplot between the quality of extra-curricular activities and money spent partying per week Step 1: Typ e in the command edit or: graph t woway ( scatter M one
6433,unknown,"graph t woway ( scatter M oney_Spent_Partying Q uality_Extra_Curricular_Activ) (lﬁt M oney_Spent_Partying Q uality_Extra_Curricular_Activ)1 (see Fig. 8.10). Figure 8.11 displays a negati ve slope, that is, the graph displays that students who think that the extra-curricular activities offered by the university are poor do in fact spend more money party ing. In contr ast, stude nts who like the spo"
6434,unknown,"social, and political clubs at their university are less likely to spend a lot of money partying. Because we can see that the line is relatively steep , we can alrea dy detect that the relationship is relatively strong; a bivariate regression analysis (see below) wi ll give us some infor mation about the strength of this relations hip. Fig. 8.10 Doing a scatterplot in Stata 1In the dataset, you mi"
6435,unknown,"1In the dataset, you might want to write Atktiv because writing out activities makes the word too long to ﬁ t into the data ﬁ eld in Stata. 8.5 Doing Scatterplots in Stata 141 0 50 100 150 200 0 20 40 60 80 100 Quality_Extra_Curricular_Activities Money_Spent_Partying Fitted values Fig. 8.11 Stata scatterplot between the quality of extra-curricular activities and the money spent partying Step 2: St"
6436,unknown,"partying Step 2: Stata allows us to include the con ﬁdence interval around the ﬁ tted line: graph twoway (scatter Money_Spent_Partying Quality_Extra_Curricular_Activ) (lﬁtci Money_Spent_Partying Quality_Extra_Curricular_Activ) (see Fig.8.12). Assuming that our sample was randomly picked from a population of univer- sity students, the con ﬁdence interval in Fig. 8.13 depicts the range around the ﬁt"
6437,unknown,"ﬁtted line in which the real relationship falls. In other words, the population line displaying the relationship between the independent and dependent variable should be anywhere in the gray shaded area. Fig. 8.12 Doing a scatterplot with con ﬁdence interval in Stata 142 8 Bivariate Relationships Featuring Two Continuous Variables 0 50 100 150 200 0 20 40 60 80 100 Quality_Extra_Curricular_Activit"
6438,unknown,"0 20 40 60 80 100 Quality_Extra_Curricular_Activities Money_Spent_Partying 95% CI Fitted values Fig. 8.13 Stata scatterplot between the quality of extra-curricular activities and the money spent partying with the con ﬁdence interval 8.6 Correlation Analysis A correlation analysis is closely linked to the analysis of scatterplots. In order to do a correlation analysis, the relationship between inde"
6439,unknown,"correlation analysis, the relationship between independent and dependent variable must be linear. In other words, we must be able to use a line to express the relationship between the independent variable x and the dependent variable y .T o interpret a scatterplot, two things are important: (1) the direction of the line (i.e., a relationship can only exist if the ﬁ tted line is either positive or "
6440,unknown,"relationship can only exist if the ﬁ tted line is either positive or negative) and (2) the closeness of the points toward the line (i.e., the closer the points are clustered around the line, the stronger the correlation is). In fact, in a correlation analysis, it is solely the second point, the closeness of the points to the line that helps us determine the strength of the relationship. To highlig"
6441,unknown,"strength of the relationship. To highlight, if we move from graph 1 to graph 4 in Fig. 8.14, we can see that the points get closer to the line for each of the fourth graphs. Hence, the correlation between the two variables becomes stronger. In statistical terms, the correlation coef ﬁcient, also called Pearson correlation coefﬁcient, is denoted by r . It expresses both the strength and direction o"
6442,unknown,"relationship between two variables in a single number. If the dots line up to exactly one line, we have a perfect correlation or a correlation coef ﬁcient of 1. In contrast, the more all over the place the dots are, the more the correlation coef ﬁcient approaches 0 (see Fig. 8.3). In terms of direction, a correlation coef ﬁcient with a positive sign depicts a positive correlation, whereas a correl"
6443,unknown,"negative sign depicts a negative correlation. Fig. 8.14 Assessing the strength of relationships in correlation analysis Formula for r : r¼ 1 n– 1 Xn i¼1 xi – -x Sx ⎧⎫ yi – -y Sy ⎧⎫ Properties of r : 8.6 Correlation Analysis 143 – – 1< r < 1. – r> 0 means a positive relationship —the stronger the relationship, the closer r is to 1. – r< 0 means a negative relationship —the stronger the relationship"
6444,unknown,"to– 1. – r0 means no relationship.¼ Benchmarks for establishing the level of correlation: (–) 0.3 < r < ( –) 0.45 weak correlation¼ (–) 0.45 < r < ( –) 0.6 medium strong correlation¼ r< ( –) 0.6 strong correlation¼ R, like the mean and the standard deviation, is sensitive to outliers. For example, Fig. 8.15 displays nearly identical scatterplots, with the sole difference being that we add an outli"
6445,unknown,"add an outlier to graph 2. Adding this outlier decreases the correlation coef ﬁcient by 0.2 points. Given that the line is drawn so that the sum of the points below the line and the sum of the points above the line equal 0, an outlier pushes the line in one direction (in our case downward), thus increasing the distance of each point toward the line, which, in turn, decreases the strength of the co"
6446,unknown,"Fig. 8.15R with and without an outlier There are two important caveats with correlation analysis. First, since a correla- tion analysis de ﬁnes strength at how close the points in a scatterplot are toward a line, it does not provide us with any indication of the strength in impact, in the substantial sense, of a relationship of an independent on a dependent variable. Second, a correlation analysis"
6447,unknown,"Second, a correlation analysis depicts only whether two variables are related and how closely they follow a positive or a negative direction. It doe s not give us any indication which variable is the cause and which is the effect. 8.6.1 Doing a Correlation Analysis in SPSS 144 8 Bivariate Relationships Featuring Two Continuous Variables Step 1:Go to Analyze —Correlate—Bivariate (see Fig. 8.16). Fi"
6448,unknown,Fig. 8.16Doing a correlation in SPSS ( ﬁrst step) 8.6 Correlation Analysis 145 Fig. 8.17Doing a correlation in SPSS (second step) Step 2:Choose the continuous variables you want to correlate and click okay (see Fig. 8.17). It is important to note the correlation analysis only functions with continuous variables. We have ﬁ ve continuous variables in our data and will include them all in the correla
6449,unknown,"in the correlation matrix. These ﬁ ve variables are (1) money spent partying, (2) qual- ity of extra-curricular activities, (3) the amount of tuition students pay themselves, (4) whether or not students can have fun without alcohol when they go out, and (5) their study time per week. 8.6.2 Interpreting an SPSS Correlation Output The correlation output in Table 8.1 displays the bivariate correlatio"
6450,unknown,"ﬁve continuous variables in our dataset. To determine whether two variables are correlated, we ﬁ rst look at the signi ﬁcance or alpha level for each correlation. If we ﬁnd that sig > 0.05, we can conclude that the two variables are not correlated. In this case, we do not interpret the correlation coef ﬁcient, which should be small anyways. If weﬁ nd a signi ﬁcantp -value (sig, ≤ 0.05), we can go "
6451,unknown,"If weﬁ nd a signi ﬁcantp -value (sig, ≤ 0.05), we can go on and interpret the strength of the correlation using the benchmarks provided in Sect. 8.6. To highlight, let us 146 8 Bivariate Relationships Featuring Two Continuous Variables Table 8.1 SPSS correlation output when they party. look at the correlation between the amount of tuition students pay and the amount of time students generally dedi"
6452,unknown,"time students generally dedicate to their studies per week; we ﬁ nd that there is no correlation between these two variables. The signi ﬁcance or alpha level (denoted p in statistical language) is 0.228, which is much higher than the benchmark of 0.05. Hence, we would conclude from there that there is no relationship between these two variables. In other words, we would stop our interpretatio n he"
6453,unknown,"variables. In other words, we would stop our interpretatio n here, and we would not interpret the correlation coef ﬁcient, because it is not statistically signi ﬁcant. In contrast, if we look at the relationship between the variable money spent partying and the variable quality of extra-curricular activities, we ﬁ nd that the signi ﬁcance level is 0.000. This means that we can be nearly 100% sure "
6454,unknown,"level is 0.000. This means that we can be nearly 100% sure that there is a correlation between the two variables. Having established that a correlation exists, we can now look at the sign and the magn itude of the coef ﬁcient. We ﬁ nd that the sign is negative, indicating that the more money students spend while going out, the less they participate in extra-curricular activities in their universit"
6455,unknown,"they participate in extra-curricular activities in their university. Knowing that there is a negative correlation, we can now interpret the magnitude of the correlation coefﬁcient, which is – 0.600, indicating that we have medium strong to strong negative correlation. In addition to the correlation between the quality of extra- curricular a ctivities and money spent partying, there is one more sta"
6456,unknown,"curricular a ctivities and money spent partying, there is one more statistically signi ﬁ- cant and substantively strong correlation, namely, between students ’study time per week and whether or not they can have fun partying without alcohol. This correlation is signi ﬁcant at the 0.000 level and substantively strong (i.e., r ¼ – 0.777). The negative sign further highlights that it is negative indi"
6457,unknown,"negative sign further highlights that it is negative indicating that high values for one variable trigger low values for the other variables. In t his case, this implies that students that study a lot also think that they can have a lot of fun without alcohol 8.6 Correlation Analysis 147 8.6.3 Doing a Correlation Analysis in Stata It is important to note that correlation analyses only function wit"
6458,unknown,"variables. We have ﬁ ve continuous variables in our data and include them in the Stata correlation matrix: (1) money spent partying, (2) quality of extra-curricular activities, (3) the amount of tuition students pay themselves, (4) whether or not students can have fun without alcohol when they go out, and (5) their study time per week. Step 1: Write in the command editor pwcorr Money_Spent_Partyin"
6459,unknown,"Step 1: Write in the command editor pwcorr Money_Spent_Partying Study_Time Fun_Without_Alcohol Quality_Extra_Curricular_Activ Amount_Tuition_Student_Pays, sig (see Fig. 8.18) The correlation output in Table 8.2 displays the bivariate correlations between the ﬁve continuous variables in our dataset. For each bivariate correlation, Stata provides the Pearson correlation coef ﬁcient and the signi ﬁca"
6460,unknown,"provides the Pearson correlation coef ﬁcient and the signi ﬁcance level ( p ). For example, 0.1711 is the correlation coef ﬁcient between study time and money spent partying. The second number (0.29) is the corresponding signi ﬁcance level. To determine whether two variables are correlated, we ﬁ rst look at the signi ﬁcance or alpha level for each correlation. If we ﬁ nd that sig > 0.05, we can co"
6461,unknown,"two variables are not correlated. In this case, we do not interpret the correlation coefﬁcient, which should be small anyways. In cases where we ﬁ nd a signi ﬁcantp - value (sig< 0.05), we can go on and interpret the strength of the correlation using the benchmarks provided in Sect. 8.6. To highlight, let us look at the correlation between Fig. 8.18 Doing a correlation in Stata Table 8.2 Stata cor"
6462,unknown,"the amount of tuition students pay and the amount of time students generally dedicate to their studies per week; we ﬁ nd that there is no correlation between these two variables. The signi ﬁcance or alpha level (denoted p in statistical lan- guage) is 0.228, which is much higher than the benchmark of 0.05. Hence, we would conclude from there that there is no correlation between these two variables"
6463,unknown,"words, we would stop our interpretation here, and we would not interpre t the correlation coef ﬁcient, because it is not statistically different from zero. In contrast, if we look at the relationship between the variable money spent partying and the variable quality of extra-curricular activities, we ﬁ nd that the signi ﬁcance level is 0.000. This means that we can be nearly 100% sure that there i"
6464,unknown,"between the two variables. Having established that a correlation exists, we can now look at the sign and the magnitude of the coef ﬁcient. We ﬁ nd that the sign is negative, indicating that the more students spend money while going out, the less they participate in extra-curricular activities in their university. Knowing that there is a negative correlation, we can now interpret the magnitude of t"
6465,unknown,"coefﬁcient, which is – 0.601, indicating that we have a medium strong to strong negative correlation. In addition to the correlation between the quality of extra- curricular activities and money sp ent partying, there is one more statistically signi ﬁ- cant and substantively strong correlation, namely, between students ’study time per week and whether or not they can have fun partying without alco"
6466,unknown,"week and whether or not they can have fun partying without alcohol. This correlation is signi ﬁcant at the 0.000 level and substantively strong (i.e., r ¼ – 0.762). The negative sign further highlights that it is a negative relationship, indicating that high values for one variable trigger low values for the other variables. In this ca se, this implies that students that study a lot also think tha"
6467,unknown,"alcohol when they party. 148 8 Bivariate Relationships Featuring Two Continuous Variables 8.7 Bivariate Regression Analysis In correlation analyses, we look at the direction of the line (positive or negative) and at how closely the points of the scatterplot follow that line. This allows us to detect the degree to which two variables covary, but it does not allow us to determine how strongly an ind"
6468,unknown,"strongly an independent variable in ﬂuences a dependent variable. In regression analysis, we are interested in the magnitude of the in ﬂuence of independent on dependent variable, as measured by the steepness of the slope. To determine the inﬂuence of an independent variable on a dependent variable, two things are important: (1) the steeper the slope, the more strongly the independent variable imp"
6469,unknown,"impacts the dependent variable. (2) The closer the points are to the line, the more certain we can be that this relationship actually exists. 8.7.1 Gauging the Steepness of a Regression Line To explain the notion that a steeper slope indicates a stronger relationship, let us compare the two graphs in Fig. 8.19. Both graphs depict a perfect relationship, meaning that all the points are on a straigh"
6470,unknown,"meaning that all the points are on a straight line. The correlation for both would be 1. However, we can see that the ﬁ rst line is much steeper than the second line. 8.7 Bivariate Regression Analysis 149 Fig. 8.19 Two regression lines featuring a strong and weak relationship, respectively In other wor ds, the y value grows stronger with higher x values. In contrast, the second line only moves sli"
6471,unknown,"second line only moves slightly upward. Consequently, the regression coef ﬁcient is higher in the ﬁ rst compared to the second graph. To determine the relationship between x and y , we use a point slope: y¼ a þ bx yis the dependent variable. xis the independent variable. bis the slope of the line. ais the intercept ( ywhen x 0).¼ The formula for the regression line: b¼ r Sy Sx a¼ -y– b -x When we "
6472,unknown,"Sx a¼ -y– b -x When we draw the regression line, we could in theory draw an in ﬁnite number of lines. The line that explains the data best is the line that has the smallest sum of squared errors. In statistical terms, this line is called the least square line (OLS line). Figure 8.20 displays the least square line between the independent variable average GDP per capita per country and the dependent"
6473,unknown,"average GDP per capita per country and the dependent variable energy usage in kilograms of oil per person. The equation denoting this relationship is as follows: energy usage ¼ 318 + 0.25 average country GDP per capita. This means 318 kg is the amount of energy that an average citizen uses when GDP is at 0. The equation further predicts that for every 1 unit (dollar) increase on they -axis,y value"
6474,unknown,"they -axis,y values increase by 0.25 kg. So in this example, this means that for each extra dollar the average citizen in a country becomes richer, she uses 0.25 kg more of energy (oil) per year. To render the interpretation more tangible, the equation would 150 8 Bivariate Relationships Featuring Two Continuous Variables Energy Usage in Kilograms 15000 10000 5000 0 0 10000 20000 30000 GDP per Cap"
6475,unknown,"GDP per Capita or Country’s Average Income Energy Usage = 318 + .25 . GDP per Capita Fig. 8.20 The equation between per capita GDP and energy usage predict that a resident in a country, where the average citizens earn 10,000 dollars, is predicted to consume 2818 kg of energy (318 + 0.25 10000 2818). ¼× 8.7.2 Gauging the Error Term The metric we use to determine the magnitude of a relationship betw"
6476,unknown,"The metric we use to determine the magnitude of a relationship between an indepen- dent and a depend ent variable is the steepness of the slope. As a rule, we can say that the steeper the slope, the more certain we can be that a relationship exists. However, the steepness of the slope is not the only criterion we use to determine whether or not an independent variable relates to a dependent variab"
6477,unknown,"an independent variable relates to a dependent variable. Rather, we also have to look how close the data points are to the line. The closer the points are to the line, the less error there is in the data. Figure 8.21 displays two identical lines measuring the relationship between age in months and height in centimeters for babies and toddlers. What we can see is that the relationship is equally st"
6478,unknown,"What we can see is that the relationship is equally strong for the two lines. However, the data ﬁ ts the ﬁ rst line much better than the second line. There is more “ noise”in the data in the second line, thus rendering the estimation of each of the points or observations less exact in the second line compared to the ﬁ rst line. Figure 8.22 graphically e xplains what we mean by error term or residu"
6479,unknown,"error term or residual in a regression analysis measures the distance from each data point to the regression line. The farther any single observation or data point is away from the line, the less this observation ﬁ ts the general relationship. The larger the average distance is from the line, the less well does the average data point ﬁ ts the linear prediction. In other words, the greater the dist"
6480,unknown,"8.7 Bivariate Regression Analysis 151 Fig. 8.21 Better and worse ﬁ tting data Fig. 8.22 The error term in a regression analysis point and the line, the less we can be assured that the relationship portrayed by the regression line actually exists. 8.8 Doing a Bivariate Regression Analysis in SPSS To conduct the bivariate regression analysis, we take the same variables we used for the scatterplots, "
6481,unknown,"the scatterplots, that is, our dependent variable money spent partying and the independent variable quality of extra-curricular activities 152 8 Bivariate Relationships Featuring Two Continuous Variables Step 1: Go to —Analyze—Regression—Linear (see Fig. 8.23). Step 2: Choose the dependent variable —choose the independent variable —click okay (Fig. 8.24). Fig. 8.23 Doing a regression analysis in S"
6482,unknown,"8.9 Interpreting an SPSS (Bivariate) Regression Output 153 Fig. 8.24 Doing a regression analysis in SPSS (second step) 8.9 Interpreting an SPSS (Bivariate) Regression Output The SPSS regression output consists of three tables: (1) a model summary table, (2) an ANOVA output, and (3) a coef ﬁcients’table. We interpret each table separately 8.9.1 The Model Summary Table The model summary table (see T"
6483,unknown,"The model summary table (see Table 8.3) indicates how well the model ﬁ ts the data. It consists of four parameters: (1) the Pearson correlation coef ﬁcient ( r), (2) the R - squared value, (3) the adjusted R -squared value, and (4) the standard error of the estimate. Table 8.3 SPSS model summary table"
6484,unknown,"154 8 Bivariate Relationships Featuring Two Continuous Variables Ris the correlation coef ﬁcient between the real data points and the values predicted by the model. For the example at hand, the correlation coef ﬁcient is high indicating that real values and predicated values correlate at the level of 0.600. R-squaredis the most important parameter in the model summary statistic ’s table and is a m"
6485,unknown,"and is a measure of model ﬁ t (i.e., it is the squared correlation between the model ’s predicted values and the real values). It explains how much of the variance in the dependent variable the independent variable(s) in the model explain. In theory, the R-squared values can range from 0 to 1. An R -squared value of 0 means that the independent variable(s) do not explain any of the variance of the"
6486,unknown,"independent variable(s) do not explain any of the variance of the dependent variable, and a value of 1 signi ﬁes that the independent variable(s) explain all the variance in the dependent variable. In our example, the R -squared value of 0.361 implies that the independent variable —the quality of extra-curricular activities —explains 36.1% of the variance in the dependent variable, the money stude"
6487,unknown,"the variance in the dependent variable, the money students spent partying per week. The adjusted R -squaredis a second statistic of model ﬁ t. It helps us compare different models. In real research, it might be helpful to compare models with a different number of independent variables to determine which of the alternative models is superior in a statistical sense. To highlight, the R -squared will"
6488,unknown,"increase or remain constant if I add variables. Yet, a new variable might not add anything substantial to the model. Rather, some of the increase in R -squared could be simply due to coincidental variation in a speci ﬁc sample. Therefore, the adjusted R - squared will be smaller than the R -squared since it controls for some of the idiosyn- cratic variance in the original estimate. As such, the ad"
6489,unknown,"cratic variance in the original estimate. As such, the adjusted R -squared is a measure of model ﬁ t adjusted for the number of independent variables in the model. It helps us compare different models; the best ﬁ tting model is always the model with the highest adjusted R -squared (not the model with the highest R -squared).2 In our sample, the adjusted R -squared is 0.344 (We do not interpret thi"
6490,unknown,"bivariate regression analysis). The standard error of the estimate is the standard deviation of the error term and the square root of the mean square residual (or error). (Normally, we do not interpret this estimator when we conduct regression models.) In our sample, the standard error of the estimate is 24.43. 8.9.2 The Regression ANOVA Table Thef -test in a regression model works like an f -test"
6491,unknown,"Thef -test in a regression model works like an f -test or ANOVA analysis (see Table 8.4). It is an indication of the signi ﬁcance of the model (does the regression equationﬁ t the observed data adequately?). If the f -ratio is signi ﬁcant, the regression equation has predictive power, which means that we have at least one statistically signiﬁcant variable in the model. In contrast, if the f -test "
6492,unknown,"signiﬁcant variable in the model. In contrast, if the f -test is not signi ﬁcant, then none of the variables in the model is statistically signi ﬁcant, and the model has no predictive power. In our sample, the f -value is 21.43, and the corresponding signi ﬁ- cance level is 0.000. Hence, we can already conclude that our independent 2Please note that we can only compare adjusted R -squared values o"
6493,unknown,"models have the same number of observations. 8.9 Interpreting an SPSS (Bivariate) Regression Output 155 Table 8.4 SPSS ANOVA table of the regression output variable—the quality of extra-curricular activities —inﬂuences our dependent vari- able, the money students spend per week partying. 8.9.3 The Regression Coefficient Table The coef ﬁcients’table (see Table 8.5) is the most important part of a r"
6494,unknown,"output. It tells us how the independent variable relates to the dependent variable. A coefﬁcients’table has the following statistics: Unstandardized Regression Weights The ﬁ rst column portrays the unstandardized regression weights, which give us an indication of how the independent variable (s) relates to the dependent variable. In our example, the unstandardized regression weight or coef ﬁcient "
6495,unknown,"weight or coef ﬁcient depicting the in ﬂuence of extra-curricular activities on students ’ party spending is – 0.839. The constant is 114.87. In other words, the – 0.839 is the slope coefﬁcient; it indicates the change in the DV associated with a 1-unit change in the IV. In our example, this implies that for each point on the 0 –100 scale, students like the extra-curricular activities more, their "
6496,unknown,"like the extra-curricular activities more, their party spending decreases by 0.839 dollars per week. The 114.87 is the predicted spending pattern if the independent Table 8.5 The SPSS regression coef ﬁcients’."
6497,unknown,"variable has the value 0 (i.e., if students think that the extra-curricular activities at their university are very bad). We can also express this relationship in a regression equation: y ¼ 114.87 – 0.839 x. The Standard Error This statistic gives us an indication of how much variation there is around the predicted coef ﬁcient. As a rule, we can say that the smaller the standard error in relation "
6498,unknown,"standard error in relation to the regression weight is, the more certainty we can have in the interpretation o f our relationship. In our case, the standard error behind the relationship between extra-curricular activities and money spent partying is 0.181. Standardized Regression Coef ﬁcients (Beta) In multiple regression models, it is impossible to compare the magnitude of the coef ﬁcients of th"
6499,unknown,"variables. To highlight, the variable gender is a dummy variable coded 0 for guys and 1 for girls. In contrast, the variable fun without alcohol is a variable coded on a 100-point scale (i.e., 0 –100). Unstandardized weights are standardized coef ﬁcients that allow us to compare the effect size of several independent variables. To do so, SPSS converts the unstandardized coef ﬁcients toz -scores (i"
6500,unknown,"SPSS converts the unstandardized coef ﬁcients toz -scores (i.e., weights with mean of zero and standard deviation of 1.0). The size of the standardized regression weights gives us some indication of the importance of the variable in the regression equation. In a multiple regression analysis, it allows us to determine the relative strength of each independent variable in comparison to other variabl"
6501,unknown,"able. The standardized beta is – 0.600 in our bivariate model. T-valueThe t -test compares the unstandardized regression against a predicted value of zero (i.e., no contribution to regression equation). It does so by dividing the unstandardized regression coef ﬁcient (i.e., our measure of effect size) by the stan- dard error (i.e., our measure of vari ability in the data). As a rule, we can say th"
6502,unknown,"higher the T -value, the higher the chance that we have a statistically signi ﬁcant relationship (see signi ﬁcance value). In our example, the T -value is – 4.629 ( –0.839/ 0.181). Signiﬁcance Value (Alpha Level) The signi ﬁcance level (sig) indicates the proba- bility that a speci ﬁc independent variable impacts the dependent variable. In our example, the signi ﬁcance level is 0.000, which implie"
6503,unknown,"example, the signi ﬁcance level is 0.000, which implies that we can be nearly 100% sure that the quality of extra-curricular activities in ﬂuences students ’spending patterns while partying. 8.10 Doing a (Bivariate) Regression Analysis in Stata To conduct the regression analysis, we will use the same variables that we used for the scatterplots, that is, our dependent variable is money spent partyi"
6504,unknown,independent variable is the quality of extra-curricular activities (see Table 7.11). 156 8 Bivariate Relationships Featuring Two Continuous Variables 8.10 Doing a (Bivariate) Regression Analysis in Stata 157 Fig. 8.25Doing a bivariate regression analysis in Stata Step 1: Write into the command edito r: reg Money_Spent_Partying Quality_Extra_Curricular_Activ (see Fig.8.25). 8.10.1 Interpreting a St
6505,unknown,"8.10.1 Interpreting a Stata (Bivariate) Regression Output On the following pages, we explain how to interpret a Stata bivariate regression output (see Table 8.6): Number of obs indicates how many observ ations the model is based upon Thef -testprovides the f -test value for the regression analysis. It works like an f -test or ANOVA analysis and gives an indication of the signi ﬁcance of the model "
6506,unknown,"measures whether the regression equation ﬁ ts the observed data adequately). If the f - ratio is signiﬁcant, the regression equation has predictive power, which means that we have at least one statistically signi ﬁcant variable in the model. In contrast, if the f -test is not signi ﬁcant, then none of the variables in the model is statistically signi ﬁcant, and the model has no predictive power. I"
6507,unknown,"and the model has no predictive power. In our sample, the f -value is 21.43 and the corresponding signi ﬁcance level is 0.000 (Prob > F ). Hence, we can already con- clude that our independent variable —the quality of extra-curricular activities — inﬂuences our dependent variable, the money students spend per week partying. (The Table you see to the left of the summary statistics (i.e., Number of "
6508,unknown,"Prob>F ,R -squared, AdjR -squared, Root MSE) is the summary statistics of the f -test (see section on t -test or ANOVA for more details).) Table 8.6The Stata bivariate regression"
6509,unknown,"158 8 Bivariate Relationships Featuring Two Continuous Variables R-squaredis an important parameter when we interpret a regression output. It is a measure of model ﬁ t (i.e., it is the squared correlation between the model ’s predicted values and the real values). It explains how much of the variation in the dependent variable is explained by the independent variables in the model. In theory, the "
6510,unknown,"squared values can range from 0 to 1. An R -squared value of 0 means that the independent variable(s) do not explain anything in the variance of the dependent variable. AnR -squared value of 1 signi ﬁes that the independent variable(s) explain all the variance in the dependent variable. In our example, the R -squared value of 0.361 implies that the independent variable —the quality of extra-curric"
6511,unknown,"implies that the independent variable —the quality of extra-curricular activities — explains 36.1% of the variance in the dependent variable, the money students spent partying per week. The adjusted R -squaredis a second statistic of model ﬁ t that helps us compare different models. In real research, it might be helpful to compare models with a different number of independent variables to determin"
6512,unknown,"different number of independent variables to determine which of the alternative models is superior in a statistical sense. To highlight, the R -squared will always increase or remain constant if I add variables even though a new variable might not add anything substantial to the model. Rather, some of the increase in the R -squared could be simply due to coincidental variation in a speci ﬁc sample"
6513,unknown,"could be simply due to coincidental variation in a speci ﬁc sample. Therefore, the adjustedR -squared will be smaller than the R -squared since it controls for some of the idiosyncratic variance in the original estimate. Therefore, the adjusted R is a measure of model ﬁ t adjusted for the number of independent variables in the model. It helps us to compare different models; the best ﬁ tting model "
6514,unknown,"It helps us to compare different models; the best ﬁ tting model is always the model with the highest adjusted R -squared (not the model with the highest R -squared).3 In our sample, the adjusted R -squared is 0.344. (We do not interpret this estimator in bivariate regression analysis.) The root MSE (root mean squared error) is the standard deviation of the error term and the square root of the mea"
6515,unknown,"and the square root of the mean square residual (or error). (Normally, we do not interpret this estimator when we conduct regression models.) In our sample, the standard error of the estimate is 24.43. Coef. (coef ﬁcient)The ﬁ rst column portrays the unstandardized regression weights, which give us an indication of how the independent variable(s) relate to the dependent variable. In our example, t"
6516,unknown,"the dependent variable. In our example, the unstandardized regression weight or coefﬁcient depicting the in ﬂuence of extra-curricular activities on students ’party spending is —0.839. The constant is 114.87. In other words, the – 0.839 is the slope coefﬁcient; it indicates the change in the dependent variable associated with a 1-unit change in the independent variable. In our example, this implie"
6517,unknown,"change in the independent variable. In our example, this implies that for each additional point on the 0 –100 scale that students like the extra-curricular activities, their party spending decreases by 0.839 dollars per week. The constant (cons) coefﬁcient of 114.87 is the predicted spending pattern if the independent variable 3Ibid. has the value 0 (i.e., this implies that if students do not like"
6518,unknown,"3Ibid. has the value 0 (i.e., this implies that if students do not like the extra-curricular activities at their school at all, they are predicted to spend 115 dollars per week partying). 8.10 Doing a (Bivariate) Regression Analysis in Stata 159 We can also express this relationship in a regression equati on:y ¼114.87 – 0.839 x The standard error gives us an indication of how much variation there "
6519,unknown,"the predicted coef ﬁcient. As a rule, we can say that the smaller the standard error relative to the regression weight, the more certainty we can have in the interpretation of our relationship between independent and dependent variable. In our case, the standard error behind the relationship between extra-curricular activities and money spent partying is 0.181. Standardized regression coef ﬁcients"
6520,unknown,"Standardized regression coef ﬁcients (beta) In multiple regression models, it is impossible to compare the magnitude of the coef ﬁcients of the independent variables. To highlight, the variable gender is a dummy variable coded 0 for guys and 1 for girls. In contrast, the variable fun without alcohol is a variable coded on a 100-point scale (i.e., 0 –100). Standardized weights are standardized coef"
6521,unknown,"allow us to compare the effect size of several independent variables. To do so, Stata converts the unstandardized coef ﬁcients to z-scores (i.e., weights with mean of zero and standard deviation of 1). The size of the standardized regression weights gives us some indication of the importance of the variable in the regression equation. In a multiple regression analysis, it allows us to determine th"
6522,unknown,"multiple regression analysis, it allows us to determine the relative strength of each independent variable in comparison to other variables on the dependent variable. The standardized beta is not one of the default options in the Stata package. However, it can be easily calculated. To do so, add the option beta at the end of the regression analysis. Put in the command ﬁ eld: reg Money_Spent_Partyi"
6523,unknown,"reg Money_Spent_Partying Quality_Extra_Curricular_Activ, beta (see Fig. 8.26) In our bivariate example, the standardized beta coef ﬁcient is – 0.600 (see Table 8.7). T-valueThe t -test compares the unstandardized regression weight against a predicted value of zero (i.e., no contribution to regression equation). It does so by dividing the unstandardized regression coef ﬁcient (i.e., our measure of "
6524,unknown,"dividing the unstandardized regression coef ﬁcient (i.e., our measure of effect size) by the standard error (i.e., our measure of variability in the data). As a rule, we can say that the higher the t -value, the higher the chance that we have a statistically signiﬁcant relationship (see signi ﬁcance value). In our example, the t -value is– 4.629 (–0.839/0.181). Fig. 8.26 Doing a bivariate regressi"
6525,unknown,(–0.839/0.181). Fig. 8.26 Doing a bivariate regression analysis with standardized beta coef ﬁcients 160 8 Bivariate Relationships Featuring Two Continuous Variables Table 8.7 The Stata bivariate regression table with standardized coefﬁcients P> t is the signi ﬁcance value or alpha level The signi ﬁcance level determines the probability with which we can determine that a speci ﬁc independent variab
6526,unknown,"impacts the dependent variable. In our example, the signi ﬁcance level is 0.000, which implies that we can be nearly 100% sure that the quality of extra-curricular activities in ﬂuences students ’spending patterns while partying. (95% conf. interval) Assuming that our sample was randomly picked from a population of university students, the con ﬁdence interval depicts the interval in which the real"
6527,unknown,"which the real relationship between the perceived quality of extra-curricular activities and students spending patterns lies. Our coef ﬁcient for the quality of extra-curricular activities has a con ﬁdence interval that ranges from – 1.21 to – 0.472. This means that the relationship in the population could be anywhere in this range. Similarly, the conﬁdence interval around the constant implies tha"
6528,unknown,"conﬁdence interval around the constant implies that somebody who rates the quality of extra-curricular activities at 0 is expected to spend between 96.36 dollars and 133.38 for partying in a week. 8.10.2 Reporting and Interpreting the Results of a Bivariate Regression Model When we report the results of bivariate regression model, we report the coef ﬁcient, standard error, and signi ﬁcance level, "
6529,unknown,"standard error, and signi ﬁcance level, as well as the R -squared and the number of observations in the model. In an article or report, we would report the results as follows: Table 8.8 reports the results of a bivariate regression model measuring the inﬂuence of the quality of extra-curricular activities on students ’weekly spending patterns when they party based on a survey conducted with 40 und"
6530,unknown,"students at a Canadian university. The model portrays a negative and statistically signiﬁcant relationship between the two variables. In substantive terms, the model predicts that for every point students ’evaluation of the extra-curricular activities at their university increases, they spent 84 cents less per week partying. This in ﬂuence is substantial. For example, somebody, who thinks that the"
6531,unknown,"is substantial. For example, somebody, who thinks that the extra-curricular activities at her university are poor (and rates them at 20), is predicted to spend approximately 98 dollars for party activities. In contrast, somebody, who likes the extra-curricular Further Reading 161 Table 8.8 Reporting a bivariate regression outcome Coefﬁcient Std. error Sig Quality of extra-curricular activities –0."
6532,unknown,"Constant 114.87 9.14 0.000 R-squared 0.34 N 40 activities (and rates them at 80), is only expected to 48 dollars for her weekly partying. The R -squared of the model further highlights that the independent vari- able, the quality of extra-curricular activities, explains 34% of the variance in the dependent variable partying spending per week. Further Reading Gravetter, F. J., & Forzano, L. A. B. ("
6533,unknown,"Cengage Learning (chapter 12). Nice introduction into correlational research; covers the data and methods for correlational analysis, applications of the correlational strategy, and strength and weakness of the correlational research strategy. Montgomery, D. C., Peck, E. A., & Vining, G. G. (2012). Introduction to linear regression analysis (Vol. 821). San Francisco: John Wiley & Sons (chapters 1 "
6534,unknown,"(Vol. 821). San Francisco: John Wiley & Sons (chapters 1 and 2). Chapters 1 and 2 provide a hands on and practical introduction into linear regression modelling, ﬁ rst in the bivariate realm and then in the multivariate realm. Ott, R. L., & Longnecker, M. T. (2015). An introduction to statistical methods and data analysis . Toronto, ON: Nelson Education (chapter 11). Provides a good introduction i"
6535,unknown,"Toronto, ON: Nelson Education (chapter 11). Provides a good introduction into correlation and regression analysis clearly highlighting the differences between the two techniques. Roberts, L. W., Wilkinson, L., Peter, T., & Edgerton, J. (2015). Understanding social statistics. Don Mills: Oxford University Press (chapters 10 –14). These chapters offer students the basic tools to examine the form and"
6536,unknown,"Multivariate Regression Analysis 9 Abstract Thisﬁ nal chapter provides an introduction into multivariate regression modeling. We will cover the logic behind multiple regression modeling and explain the interpretation of a multivariate regression model. We will further cover the assumptions this type of model is based upon. Finally, and using our data, we will provide concrete examples on how to in"
6537,unknown,"9.1 The Logic Behind Multivariate Regression Analysis Bivariate regression analysis is very rarely used in real applied research, because an outcome is hardly ever just dependent on one predictor. Rather, multiple factors normally explain the dependent variable (see Fig. 9.1). To highlight, if we want to explain a student ’s grade in an exam, several factors might come into play. A student’s grade"
6538,unknown,student’s grade might depend on how much the respective student studied for the exam; it might depend on her health and even on her general mood. Multiple regres- sion modeling allows us to absolutely and comparatively gauge the in ﬂuence of all of these factors on the dependent variable. Multiple regression analysis is an extension of bivariate regression analysis. It allows us to test the in ﬂue
6539,unknown,"allows us to test the in ﬂuence of multiple independent (predictor) variables on a dependent variable. Just like in the case of two variables, the goal of this method is to create an equation or a “ model”that explains the impact of/relationship between these variables. Let us assume that we want to explain the dependent variable “ Y”and we have several independent variables X 1... Xp. Then, the m"
6540,unknown,"need to calculate is: # Springer International Publishing AG 2019 D. Stockemer, Quantitative Methods for the Social Sciences , https://doi.org/10.1007/978-3-319-99118-4_9 163 164 9 Multivariate Regression Analysis Fig. 9.1 Predictors of a student’s grade Y0 ¼A þ B 1X1 þB 2X2 þ... þB pXp Y'is the predicted score of the dependent (criterion) variable ( dependent variable ). Ais a constant which give"
6541,unknown,"Ais a constant which gives the value of Y 0 when allX s are zero ( Y-intercept or constant). Xsare all the independent variable values ( values of the independent variables ). B1–Bp are regression weights. They are the contribution of each independent variable to the predicted value of the dependent variable (i.e., each represents the change in Y 0 resulting from a unit change in a speci ﬁc predic"
6542,unknown,"all other predictors are held at constant values ). ExampleSuppose we want to study the predictors of a student ’s grade in a math exam (see Figure 8.21). We ask a random sample of students at a German high school about their grade in their last math exam, the time they spent studying for this exam, their general mood, and whether they were in good perceived health when taking the exam. Let us fur"
6543,unknown,taking the exam. Let us further assume that we run a multiple regression model and receive the following equation (for now we ignore the question of whether the variables are statistically signi ﬁcant or not). We receive the following equation: Y0 ¼10 :5þ 3 :1X1 þ1 :5X2 þ0 :5X3 þe We would interpret the model as follows: 10.5(on a scale from 0 to 100) is the hypothetical grade a student is expecte
6544,unknown,"if she does not study at all, her general health is at its worst (she would rank her health by the value 0), and her general mood is also at the lowest value (she would also rank this at 0). 3.1is the slope coef cient for the variable study time. This implies that for every 9.2 The Functional Forms of Independent Variables to Include in a Multivariate ... 165 ﬁ hour a student studies, her grade is"
6545,unknown,"ﬁ hour a student studies, her grade is expected to increase by 3.1 points. 1.5. is the slope coef ﬁcient for somebody ’s general health, indicating that per each point somebody ’s perceived health increases, her math grade is predicted to improve by 1.5 points. 0.5is the slope coef ﬁcient for somebody ’s general mood. In other words, for each point somebody ’s general mood increases, her test perf"
6546,unknown,"increase by 0.5 points. As we can see from the example, the multivariate regression model is an extension of the bivariate model. It has the same parameters and the interpretation is analogous. To do a multiple regression analysis in SPSS (or Stata), follow the same steps as you would follow for a bivariate regression. Just add more variables as independent variables. 9.2 The Functional Forms of I"
6547,unknown,"9.2 The Functional Forms of Independent Variables to Include in a Multivariate Regression Model In this introduction to survey research and quantitative methods, we only cover continuous dependent variables (noncontinuous dependent variables will be the subject of more advanced statistical courses). Yet, we still have to determine in what type of functional form we would include our independent va"
6548,unknown,"that the relationship between a continuous independent and a continuous dependent variable is linear (the relationship follows a line), we will include the independent variables in its linear form. If a scatterplot would highlight that the relationship between independent and dependent variable is not linear (e.g., it follows a curve), we would need to transform the variable. However, this is also"
6549,unknown,"advanced class. We would also not change binary or dummy variables. The only variables we must be careful with, for the purpose of this introductory textbook, are categorical variables. If we have a categorical nominal variable (i.e., different religious af ﬁliations), we create N – 1 dummy variables, with one of the categories serving as a reference category (see Table 4.14). If we have ordinal v"
6550,unknown,"serving as a reference category (see Table 4.14). If we have ordinal variables, we could also test whether the relationship is in fact ordered. For example, we could test via a multiple comparison test whether the relationship between times partying per week and money spent partying per week is in principle ordinal. By including the variable— times partying per week — in its linear ordinal form, w"
6551,unknown,"variable— times partying per week — in its linear ordinal form, we also assume that the relationship between partying less than one time per week and one time per week is the same as between four and ﬁ ve times per week. However, in the ANOVA or f-test, we ﬁ nd that this is not true (see Table 6.2). Rather, we ﬁ nd that students that party three times or less regardless of whether they party, on a"
6552,unknown,"party three times or less regardless of whether they party, on average, less than once, once, twice, or three times per week, spend approximately the same amount of money when they party; only students that party four or more times spend signi ﬁ- cantly more. Because of this dichotomy in the relationship, it would make sense to create a dummy variable between the two categories. (In the dataset, w"
6553,unknown,"variable money spent partying 3.) 9.3 Interpretation Help for a Multivariate Regression Model When you want to interpret a multivariate regression model, we can follow the same logic as for a bivariate regression model. The four guiding steps can help: 166 9 Multivariate Regression Analysis 1. Look at what variables are signi ﬁcant. 2. I nterpret the substantive value of signi ﬁcant variable. 3. C"
6554,unknown,"3. Compare the relative strength of the signi ﬁcant variables. 4. I nterpret the model ﬁ t. 9.4 Doing a Multiple Regression Model in SPSS In our sample survey, we have included seven possible predictor variables, and we want to determine the relative and absolute in ﬂuence of these seven predictor variables on the dependent variable, money spent partying per week. Because we know from the ANOVA an"
6555,unknown,"know from the ANOVA analysis (see Sect. 7.2.) that the relationship between the ordinal variable times partying and money spent partying is not linear, but rather only matters for individuals who party four times per week or more, we create a binary variable, coded 0 for partying three times or less per week and 1 for partying four times or more. We add this recoded independent variable together w"
6556,unknown,"remaining six independent variables into the model (see Sect. 9.7) and label it Times_Partying_3. The dependent variable is money spent partying. 9.5 Interpreting a Multiple Regression Model in SPSS Following the four steps outlined under Sect. 9.3., we can proceed as follows (see Table 9.1): 1. I f we look at the signi ﬁcance level, we ﬁ nd that two variables are statistically signiﬁcant (i.e., q"
6557,unknown,"signiﬁcant (i.e., quality of extra-curricular activities and times partying 3). For all other variables, the signi ﬁcance level is higher than 0.05. Hence, we would conclude that these indicators do not in ﬂuence the amount of money students spent per week partying. 2. Theﬁ rst signi ﬁcant variable, the quality of extra-curricular activities, has the expected negative sign indicating that the more"
6558,unknown,"expected negative sign indicating that the more the students enjoy their extra- curricular activities at their institution, the less money they spent weekly partying. This observation also con ﬁrms our initial hypothesis. Holding everything else constant, the model predicts that per every point a student enjoys her extra- curricular activities more, she spends 62 cents less per week partying. 9.5 "
6559,unknown,"Table 9.1 Multiple regression output in SPSS For example, this implies that somebody who thinks that the extra-curricular acti- vities are very bad at her university (i.e., she rates the quality of extra-curricular activities at 0) spends 62 dollars more per week studying than somebody who thinks that the extra-curricular activities are excellent (i.e., she rates the quality of extra-curricular ac"
6560,unknown,"The second signi ﬁcant variable, times partying 3, also has the expected positive sign. The regression coef ﬁcient of 24.81 indicates that people that party four or more times are expected to spend nearly 25 dollars more on their partying habits than students that party three times or less. 168 9 Multivariate Regression Analysis 3. If we compare the two statistically signi ﬁcant variables, we ﬁ nd"
6561,unknown,"3. If we compare the two statistically signi ﬁcant variables, we ﬁ nd that the standard- ized beta coef ﬁcient is higher for the variable quality of extra-curricular activities (i.e., the standardized beta coef ﬁcient is – 0.421) than for the variable times partying 3 (0.387). This higher standard beta coef ﬁcient illustrates that the vari- able quality of extra-curricular activities has more expl"
6562,unknown,"model than the variable times partying 3. 4. The model ﬁ ts the data quite well; the seven independent variables explain 57% of the variance in the dependent variable, the amount of money students spent partying. (The R -squared is 0.568.) 9.6 Doing a Multiple Regression Model in Stata In our survey, we have included seven possible predictor variables, and we want to determine the relative and abs"
6563,unknown,"determine the relative and absolute in ﬂuence of these seven predictor variables on the dependent variable. Because we know from the ANOVA analysis (see Table 6.2) that the relationship between the ordinal variable times partying and money spent partying is not linear but rather only becomes different for individuals who party four times or more, we create a binary variable, coded 0 for partying t"
6564,unknown,"per week and 1 for partying four times or more. We add this recoded independent variable together with the remaining six independent variables into the model (see Sect. 9.8). The dependent variable is money spent partying. 9.7 Interpreting a Multiple Regression Model in Stata Following the four steps outlined under 10.3., we can proceed as follows (see Tables 9.2 and 9.3): Table 9.2Multiple regres"
6565,unknown,"9.7 Interpreting a Multiple Regression Model in Stata 169 Table 9.3 Multiple regression output in Stata with standardized coef ﬁcients 1. I f we look at the signi ﬁcance level, we ﬁ nd that two variables are statistically signiﬁcant (i.e., quality of extra-curricular activities and times partying 3). For all other variables, the signi ﬁcance level is higher than 0.05. Hence, we would conclude that"
6566,unknown,"spent per week partying. 2. Theﬁ rst signi ﬁcant variable, the quality of extra-curricular activities, has the expected negative sign indicating that the more students enjoy their extra- curricular activities at their institution, the less money they spent weekly partying. This observation also con ﬁrms our initial hypothesis. Holding everything else constant, the model predicts that per every poi"
6567,unknown,"constant, the model predicts that per every point a student enjoys her extra- curricular activities more, she spends 62 cents less per week partying. For example, this implies that somebody who thinks that the extra-curricular activities are very bad at her university (i.e., she rates the quality of extra-curricular activities at 0) spends 62 dollars more per week studying than somebody who thinks"
6568,unknown,"thinks that the extra-curricular activities are excellent (i.e., she rates the quality of extra-curricular activities at 100). The second signi ﬁcant variable, times partying 2, also has the expected positive sign. The regression coef ﬁcient of 24.81 indicates that people that party four or more times are expected to spend nearly 25 dollars more on their weekly partying habits than students that p"
6569,unknown,"habits than students that party three times or less. 3. I f we compare the two statistically signi ﬁcant variables, we ﬁ nd that the standardized beta coef ﬁcient is higher for the variable quality of extra-curricular activities (i.e., the standardized beta coef ﬁcient is – 0.421) than for the variable times partying 3 (0.387). This higher standard beta coef ﬁcient illustrates that the variable qu"
6570,unknown,"variable quality of extra-curricular activities has more explanatory power in the model than the variable times partying 3. 170 9 Multivariate Regression Analysis 4. The modelﬁ ts the data quite well; the seven independent variables explain nearly 57% of the variance in the dependent variable, the amount of money students spent partying. (The R -squared is 0.573.) 9.8 Reporting the Results of a Mu"
6571,unknown,"9.8 Reporting the Results of a Multiple Regression Analysis In the multiple regression analysis (see Table 9.2), we evaluated the in ﬂuence of seven independent variables (the quality of extra-curricular activities, students ’study time per week, the year students are in, gender, whether they party two times or less or three times or more per week, the degree to which they think that they can have"
6572,unknown,"fun without alcohol, and the amount of tuition the students pay) on the dependent variable, the weekly amount of money students spent partying. We ﬁ nd that two of the seven variables are statistically signi ﬁcant and show the expected effect; that is, the more students think that the extra-curriculars at their university are good, the less money they spent partying per week. The same applies to s"
6573,unknown,"money they spent partying per week. The same applies to students that party few times; they too spend less money going out. In substantive terms, the model predicts that per every point students increase their ranking of the extra-curricular activities at their school, they will spend 59 cents less partying per week. The coef ﬁcient for the dummy variable, partying two times or less or three times"
6574,unknown,"dummy variable, partying two times or less or three times or more per week, indicates that students that party three or more times are predicted to spend 26 dollars more on their partying habits than students that party less. Using the 95% bench- mark, none of the other variables is statistically signi ﬁcant. Consequently, we cannot interpret the other coef ﬁcients because they are not different f"
6575,unknown,"interpret the other coef ﬁcients because they are not different from zero. In terms of modelﬁ t, the data ﬁ ts the model fairly well: the seven independent variables explain 57% of the variance in the dependent variable. 9.9 Finding the Best Model In real research the inclusion of variables into a regression model should be theoreti- cally driven; that is, theory should tell us which independent v"
6576,unknown,"cally driven; that is, theory should tell us which independent variables we should include in a model to explain and predict a dependent variable. However, we might also be interested in ﬁ nding the best model. There are two ways to proceed, and there is some disagreement among statisticians: One way is to only include statistically signiﬁcant variables into the model. Another way is to use the ad"
6577,unknown,"signiﬁcant variables into the model. Another way is to use the adjusted R -squared as a benchmark. To recall, the adjusted R -squared is a measure of model ﬁ t that allows us to compare different models. For every additional predictor I include in the model, the adjustedR -squared increases only if the new term improves the model beyond pure chance. (Please note that a poor predictor can decrease "
6578,unknown,"chance. (Please note that a poor predictor can decrease the adjusted R -squared, but it can never decrease the R -squared.) Using the adjusted R -squared as a benchmark to ﬁnd the best model, we should proceed as follows: (1) start with the complete model, which includes all the predictors, (2) remove the non-statistically signi ﬁcant predictor with the lowest standardized coef ﬁcient, and (3) con"
6579,unknown,Table 9.4Finding the best model Model 1 Model 2 Model 3 Model 4 Model 5 Quality of extra-curricular activities –0.421 –0.415 –0.416 0.421 –0.442 Gender 0.056 0.062 0.051 Study time per week 0.141 0.200 0.159 0.140 Year of study 0.065 0.051 Times partying (two times or less/ three times or more 0.387 0.401 0.421 0.418 0.418 Fun without alcohol –0.047 Amount of tuition student pays 0.179 0.218 0.201
6580,unknown,Constant 75.47 70.22 76.36 77.53 92.66 R-squared 0.5731 0.5725 0.5707 56.87 0.5502 AdjustedR -squared 0.4797 0.4948 0.5075 51.94 0.5127 adjustedR -squared does no longer increases. Table 9.4 highlights this procedure. We start with the full model. The full model has an adjusted R -squared of 0.4797. We take out the variable with the lowed standardized beta coef ﬁcient (fun without alcohol). After 
6581,unknown,"After taking out this variable, we see that the adjusted R -squared increases to 0.4948 (see Model 2). This indicates that the variable fun without alcohol does not add anything substantial to the model and should be removed. In a next step, we remove the variable, year of study . Removing this variable leads to another increase in the adjustedR -squared (i.e., the new adjusted R -squared is 0.507"
6582,unknown,"adjustedR -squared (i.e., the new adjusted R -squared is 0.5075), indicating again that this variable does not add anything substantively to the model and should be removed (see Model 3). Next, we remove the variable gender and see another increase in the adjustedR -squared to 0.5194. If we now remove the variable with the lowest adjusted R-squared, the study time per week, we ﬁ nd that the adjust"
6583,unknown,"R-squared, the study time per week, we ﬁ nd that the adjusted R -squared decreases to 0.5127 (see Model 5), which is lower than the adjusted R -squared from Model 4, which is 0.5114. Based on these calculations, we can conclude that Model 4 has the best model ﬁ t. 9.10 Assumpti ons of the Classical Linear Regression Model or Ordinary Least Square Regression Model (OLS) The classical linear regress"
6584,unknown,The classical linear regression model (OLS) is the simplest type of regression model. OLS only works with a continuous dependent variable. It has ten underlying 9.10 Assumptions of the Classical Linear Regression Model or Ordinary Least ... 171 assumptions: 1. Linearity in the parameters : Linearity in the parameters implies that the relationship between a continuous independent variable and a dep
6585,unknown,"able must roughly follow a line. Relationships that do not follow a line (e.g., they might follow a quadratic function or a logarithmic function) must be included into the model using the correct functional forms (more advanced textbooks in regression analysis will capture these cases). 172 9 Multivariate Regression Analysis 2. Xis ﬁ xed: This rule implies that one observation can only have one x "
6586,unknown,"oney value. 3. Mean of disturbance is zero : This follows the rule to draw the ordinary least square line. We draw the best ﬁ tting line, which implies that the summed up distance of the points below the line is the same as the summed up distance above the line. 4. Homoscedasticity:The homoscedasticity assumption implies that the variance around the regression line is similar for all the predictor"
6587,unknown,"around the regression line is similar for all the predictor variables around the regression line ( X) (see Fig. 9.2). To highlight, in the ﬁ rst graph, the points are distributed rather equally around a hypothetical line. In the second graph, the points are closer to the hypothetical line at the bottom of the graph in com- parison to the top of the graph. In our example, the ﬁ rst graph would be a"
6588,unknown,"example of homoscedasticity and the second graph an example of data suffering from heteroscedasticity. At this stage in your learning, it is important that you have heard about heteroscedasticity, but details of the problem will be covered in more advanced textbooks and classes. 5. No autocorrelation: There are basically two forms of autocorrelation: (1) con- temporaneous correlation, where the de"
6589,unknown,"temporaneous correlation, where the dependent variable from one observations affects the dependent variable of another observation in the same dataset (e.g., Mexican growth rates might not be independent because growth rates in the United States might affect growth rates in Mexico), and (2) autocorrelation in pooled time series datasets. That is, past values of the dependent variable inﬂuence futu"
6590,unknown,"inﬂuence future values of the dependent (e.g., the US growth rate in 2017 might affect the US growth rate in 2018). This second type of autocorrelation is not really pertinent for cross-sectional analysis but becomes relevant for panel analysis. 6. No endogeneity:Endogeneity is one of the fundamental problems in regression analysis. Regression analysis is based on the assumption that the independe"
6591,unknown,"variable impacts the dependent variable but not vice versa. In many real-world political science scenarios, this assumption is problematic. For example, there is debate in the literature whether high women ’s representation in instances of power inﬂuences/decreases corruption or whether low levels of corruption foster the election of women (see Esarey and Schwindt-Bayer 2017). There are stati- sti"
6592,unknown,"stical remedies such as instrumental regression techniques, which can model a feedback loop, that is, more advanced techniques can measure whether two variables in ﬂuence themselves mutually. These techniques will also be covered in more advanced books and classes. 7. No omitted variables: We have an omitted variable problem if we do not include a variable in our regression model that theory tells"
6593,unknown,"include. Omitting a relevant or important variable from a model can have four negative consequences: (1) If the omitted variable is correlated with the included variables, then the parameters estimated in the model are biased, meaning that their expected values do not match their true values. (2) The error variance of the estimated parameters is biased. (3) The con ﬁdence intervals of included var"
6594,unknown,"ables and more general the hypothesis testing procedures are unreliable, and (4) the R -squared of the estimated model is unreliable. 9.10 Assumptions of the Classical Linear Regression Model or Ordinary Least ... 173 100 80 60 40 20 0 100 80 60 40 20 0 0 02 0 4 0 6 0 Homoscedasticity Heteroscedasticity 80 100 20 40 60 80 100 (Graph image published under the CC-BY-SA-3.0 license (http://creativeco"
6595,unknown,"(http://creativecommons.org/licenses/by-sa/3.0/), via Wikimedia Commons) (Graph image published under the CC-BY-SA-3.0 license (http://creativecommons.org/licenses/by-sa/3.0/), via Wikimedia Commons) Fig. 9.2 Homoscedasticity and heteroscedasticity 8. More cases than parameters ( N> k ):Technically, a regression analysis only runs if we have more cases than parameters. In more general terms, the r"
6596,unknown,"sion estimates become more reliable the more cases we have. 9. No constant “ variables”:For an indepe ndent variable to explain variation in a dependent variable, there must be variation in the independent variable. If there is no variation, then there is no reason to include the independent variable in a 174 9 Multivariate Regression Analysis regression model. The same applies to the dependent va"
6597,unknown,"variable is constant or near constant, and does not vary with independent variables, then there is no reason to conduct any analysis in the ﬁ rst place. 10. No perfect collinearity among regressors : This rule means that the independent variables included in a regression should represent different concepts. To high- light, the more two variables are correlated, the more they will take explanatory "
6598,unknown,"power from each other (if they are perfectly collinear, a regression program such as Stata or SPSS cannot distinguish these variables from one another). This becomes problematic because relevant variables might become nonsigni ﬁcant in a regression model, if they are too highly correlated with other relevant variables. More advanced books and classes will also tackle the problem of perfect colline"
6599,unknown,"perfect collinearity and multicollinearity. For the purposes of an introductory course, it is enough if you have heard about multicollinearity. Reference Esarey, J., & Schwindt-Bayer, L. A. (2017). Women ’s representation, accountability and corruption in democracies. British Journal of Political Science ,1 –32. Further Reading Since basically all books listed under bivariate correlation and regre"
6600,unknown,"Since basically all books listed under bivariate correlation and regression analysis also cover multiple regression analysis, the books I present here go beyond the scope of this textbook here. These books could be interesting further reads, in particular to students, who want to learn more what is covered here. Heeringa, S. G., West, B. T., & Berglund, P. A. (2017). Applied survey data analysis ."
6601,unknown,"Chapman and Hall/CRC. An overview of different approaches to analyze complex sample survey data. In addition to multiple linear regression analysis the topics covered include different types of maximum likelihood estimations such as logit, probit, and ordinal regression analysis, as well as survival or event history analysis. Lewis-Beck, C., & Lewis-Beck, M. (2015). Applied regression: An introduc"
6602,unknown,"Lewis-Beck, C., & Lewis-Beck, M. (2015). Applied regression: An introduction (Vol. 22). Thousand Oaks: Sage A comprehensive introduction into different types of regression techniques. Pesaran, M. H. (2015). Time series and panel data econometrics . Oxford: Oxford University Press. Comprehensive introduction into different forms of time series models and panel data estimations. Wooldridge, J. M. (2"
6603,unknown,"Wooldridge, J. M. (2015).Introductory econometrics: A modern approach . Mason, OH: Nelson Edu- cation. Comprehensive book about various regression techniques; it is, however, mathematically relatively advanced. tinued) Appendix 1: The Data of the Sample Questionnaire MSP ST Gender Year TP FWA QECA ATSP Student 1 50 7 0 2 2 60 90 30 Student 2 35 8 1 3 1 70 40 50 Student 3 120 12 1 3 4 30 20 60 Stud"
6604,unknown,Student 4 80 3 0 4 4 50 50 100 Student 5 100 11 0 1 1 30 10 0 Student 6 120 14 1 5 4 20 20 0 Student 7 90 11 0 4 2 50 50 0 Student 8 80 10 1 4 3 40 50 10 Student 9 70 9 0 3 3 30 50 60 Student 10 80 8 1 2 3 40 40 100 Student 11 60 12 1 4 2 60 40 0 Student 12 50 14 1 2 1 30 70 10 Student 13 100 13 0 3 4 0 30 0 Student 14 90 15 0 3 0 0 20 0 Student 15 60 7 0 3 3 60 50 0 Student 16 40 6 1 4 0 70 90 0 
6605,unknown,Student 17 60 5 1 4 2 80 50 60 Student 18 90 8 0 5 2 90 30 70 Student 19 130 12 1 4 5 10 20 50 Student 20 70 11 0 1 1 20 60 30 Student 21 80 13 1 3 4 10 70 50 Student 22 50 6 0 4 4 60 40 10 Student 23 110 5 1 4 4 70 30 100 Student 24 60 8 1 2 3 50 60 100 Student 25 70 10 1 4 2 60 40 80 Student 26 60 10 1 4 2 40 70 10 Student 27 50 11 0 3 2 30 50 0 Student 28 75 4 0 4 3 80 70 0 Student 29 80 7 0 6 
6606,unknown,"Student 29 80 7 0 6 4 90 30 0 Student 30 30 12 1 3 0 20 80 40 Student 31 70 7 0 4 3 70 30 10 Student 32 70 14 1 2 1 0 50 100 (con # Springer International Publishing AG 2019 D. Stockemer, Quantitative Methods for the Social Sciences , https://doi.org/10.1007/978-3-319-99118-4 175 176 Appendix 1: The Data of the Sample Questionnaire MSP ST Gender Year TP FWA QECA ATSP Student 33 70 3 0 4 3 60 40 50"
6607,unknown,"Student 34 60 11 1 4 2 50 50 70 Student 35 70 9 0 2 1 60 20 40 Student 36 60 11 0 3 1 20 60 60 Student 37 60 11 1 4 1 30 40 20 Student 38 90 8 1 1 2 50 10 50 Student 39 70 9 0 3 3 50 90 30 Student 40 200 10 1 4 5 40 20 100 MSPMoney spent partying, ST Study time, Gender Gender, Year Year, TS Times spent parting per week,FWA Fun without alcohol, QECA Quality of extra curricula activities, ATSP Amoun"
6608,unknown,"tuition the student pays Appendix 2: Possible Group Assignments That Go with This Course As an optional component, this book is built around a practical assignment. The assignment consists of a semester-long group project, which gives students the opportunity to practically apply their quantitative research skills. In more detail, at the beginning of the term, students are assigned to a study/ wor"
6609,unknown,"the beginning of the term, students are assigned to a study/ working group that consists of four individuals. Over the course of the semester, each group is expec ted to draft an original questionnaire, solicit 40 respondents of their survey (i.e. 10 per student) and perform a set of exercises with their data (i.e. some exercises on descriptive statistics, means testing/ correlation and regression"
6610,unknown,"descriptive statistics, means testing/ correlation and regression analysis). Assignment 1: Go together in groups of four to ﬁ ve people and design your own questionnaire. It should include continuous, dummy and categorical variables (after Chap. 4). Assignment 2: Each group member should collect ten surveys based on a convenience sample. Because of time constraints there is no need to conduct a pr"
6611,unknown,"Because of time constraints there is no need to conduct a pre-test of the survey. Assignment 3: Conduct some descriptive statistics with some of your variables. Also construct a Pie Chart, Boxplot and Histogram. Assignment 4: Your assignment will consist of a number of data exercises 1. Graph your dependent variable as a histogram 2. Graph your dependent variable and one continuous independent var"
6612,unknown,"boxplot 3. Display some descriptive statistics 4. Conduct an independent samples t -test. Use the dependent variable of your study; as grouping variable use your dichotomous variable (or one of your dichotomous variables). 5. Conduct a one way anova test. Use the dependent variable of your study; As factor use one of your ordinal variables # Springer International Publishing AG 2019 D. Stockemer, "
6613,unknown,"https://doi.org/10.1007/978-3-319-99118-4 177 178 Appendix 2: Possible Group Assignments That Go with This Course 6. Run a correlation matrix with your dependent variable and two other continuous variables. 7. Run a multivariate regression analysis with all your independent variables and your dependent variable. Index A Adjusted R-squared, 153, 154, 158, 170, 171 ANOVA, 111 –113, 115 –118, 120 –12"
6614,unknown,"125, 153 –155, 157, 165, 166, 168, 177 Autocorrelation, 172 B Boxplot, 80, 84 –86, 88, 177 C Causation reversed causation, 18, 32 Chi-square test, 125 –128, 130, 131 Collinearity, 174 Comparative Study of Electoral Systems (CSES), 28, 29 Concepts, 9, 10, 12 –14, 16 –19, 23, 68, 174 Conﬁdence interval, 91 –97, 108, 110, 122, 141, 142, 160, 172 Correlation, 2, 15, 133, 142 –148, 153, 154, 158, 172, "
6615,unknown,"158, 172, 177, 178 Cumulative, 7, 10, 53, 76 –78 D Deviation, 91–97, 107, 109, 115, 120, 143, 154, 156, 158, 159 E Empirical, 1, 2, 5 –20, 25, 31, 32 Endogeneity, 32, 172 Error term, 150, 151, 154, 158 European Social Survey (ESS), 20, 27, 28, 30, 33, 59, 60 179 F Falsiﬁability, 6 F-test, 111 –122, 124, 125, 127, 154, 157, 165 H Heteroscedasticity, 172, 173 Histogram, 80, 82, 83, 87 –91, 104, 105,"
6616,unknown,"177 Homoscedasticity, 172, 173 Hypothesis alternative/research hypothesis, 18, 121, 122 null hypothesis, 18, 107, 108, 111, 128 I Independent samples t-test, 101 –113, 125, 177 M Means, 1, 2, 6, 26, 29, 40, 46, 64, 67, 79, 80, 87, 91–94, 96 –98, 101 –104, 107, 109, 110, 112, 114, 116, 118, 120 –124, 133, 143, 146, 148 –150, 154, 156–160, 172, 174, 177 Measure of central tendency, 79, 80, 84 Measur"
6617,unknown,"Measurements, 6, 8, 14, 15, 19, 20, 30, 38, 39, 54, 68 Median, 79, 80, 84, 86, 88 Modelﬁ t, 153, 154, 158, 166, 168, 170, 171 N Normal distribution, 87, 88, 90, 92 Normative, 5, 8, 12, 39, 40 # Springer International Publishing AG 2019 D. Stockemer, Quantitative Methods for the Social Sciences, https://doi.org/10.1007/978-3-319-99118-4 180 Index O Operationalization, 1, 13, 14, 19, 43, 46 Ordinary"
6618,unknown,"Ordinary least squares (OLS), 171 –173 P Parsimony, 12, 38, 48 Pie charts, 80 –84, 177 Population, 12, 23, 25 –27, 29, 30, 32 –34, 57–59, 61–64, 66, 67, 87, 90, 92 –94, 104, 141, 160 Pre-test, 30, 68, 69, 104, 108, 177 Q Question closed-ended question, 42, 43 open-ended question, 42, 43, 53, 68 R Range, 13, 26, 33, 45, 79, 80, 84, 86, 93, 94, 113, 122, 141, 154, 158, 160 Rational choice, 11 Regres"
6619,unknown,"Rational choice, 11 Regression bivariate regression, 138, 140, 148 –150, 152–161, 163, 165, 166 multiple regression, 1, 156, 159, 163 –170 regression coef ﬁcient, 149, 154 –156, 159, 167, 169 standardized regression coef ﬁcient, 156, 159 Research qualitative research, 8 –10, 27 quantitative research, 2, 8 –10, 18 –20, 42, 43, 165, 177 Residual, 150, 154, 158 R-squared, 153, 154, 158, 160, 161, 168"
6620,unknown,"S Sample biased sample, 58 –61, 66 non-random sampling convenience sampling, 62, 67 purposive sampling, 62, 63 quota sampling, 63 snowball sampling, 62, 63 volunteer sampling, 62, 63 random sample, 23, 28, 58 –62, 93, 96, 98, 164 representative sample, 20, 23, 27, 58 –61, 64, 67 Sampling, 19, 26, 30, 57, 59, 62 –65, 67, 91 –97 Sampling error, 62, 91 –97 Scales Guttman scale, 44, 46 Likert scale, 4"
6621,unknown,"Likert scale, 44, 45, 49, 68 Scatterplot, 134 –144, 148, 152, 156, 165 Social desirability social desirability bias, 41, 42, 60, 65 Standard deviation, 91 –97, 107, 109, 120, 143, 154, 156, 158, 159 Standard error standard error of the estimate, 153, 154, 158 Statistical signiﬁcance, 107 Statistics bivariate statistics, 101 –131 descriptive statitistics, 1, 77, 95, 98, 120, 128, 177 univariate sta"
6622,unknown,"Survey cohort survey, 33 cross sectional survey, 29 –32 face-to-face survey, 64, 65, 67 longitudinal survey, 30, 32, 33 mail in survey, 65, 66 online survey, 60, 61, 63 –67 panel survey, 33, 34 telephone survey, 64, 65, 67 trend survey, 32 T Theory, 9 –12, 16 –19, 31, 40, 50, 52, 136, 149, 154, 158, 170, 172 Transmissibility, 7 V Validity construct validity, 41 content validity, 14, 15, 19 Variabl"
6623,unknown,"Variable continuous variable, 50 –53, 87, 104, 111, 114, 125, 133 –161, 177 control variable, 16, 19, 20, 53 dependent variable, 16 –20, 31, 32, 53, 74, 75, 80, 101, 104, 108, 113, 114, 119, 125, 127, 130, 133 –137, 139, 141, 142, 144, 148 –150, 152, 154 –161, 163–166, 168, 170 –173, 177, 178 dichotomous variable, 50 –53, 104, 177 dummy variable, 50, 51, 111, 156, 159, 165, 170, 177 independent va"
6624,unknown,"34, 38, 53, 74 –76, 102, 119, 125, 133, 134, 136, 137, 139, 142, 148–150, 152, 154 –161, 163, 165, 166, 168, 170 –174, 177, 178 interval variable, 50 nominal variable, 50 –53, 111, 165 omitted variable, 172 Index 181 ordinal variable, 44, 46, 50 –53, 76, 111, 114, 165, 166, 168, 177 string variable, 50 –52, 74, 75 W World V alue Survey ( WVS), 15, 27 –29, 33 Multivariate Regression Trees: A New Te"
6625,unknown,"Relationships Author(s): Glenn De'ath Source: Ecology , Apr., 2002, Vol. 83, No. 4 (Apr., 2002), pp. 1105-1117 Published by: Wiley on behalf of the Ecological Society of America Stable URL: https://www.jstor.org/stable/3071917 REFERENCES Linked references are available on JSTOR for this article: https://www.jstor.org/stable/3071917?seq=1&cid=pdf- reference#references_tab_contents You may need to l"
6626,unknown,"reference#references_tab_contents You may need to log in to JSTOR to access the linked references. JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JST"
6627,unknown,"Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at https://about.jstor.org/terms Ecological Society of America and Wiley are collaborating with JSTOR to digitize, preserve and extend access to Ecology This content downloaded from              88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https://about.jstor.org/te"
6628,unknown,"Ecology, 83(4), 2002, pp. 1105-1117 ? 2002 by the Ecological Society of America MULTIVARIATE REGRESSION TREES: A NEW TECHNIQUE FOR MODELING SPECIES-ENVIRONMENT RELATIONSHIPS GLENN DE'ATH' Cooperative Research Center for the Great Barrier Reef World Heritage Area, James Cook University, Townsville, Queensland 4811, Australia Abstract. Multivariate regression trees (MRT) are a new statistical techni"
6629,unknown,"Abstract. Multivariate regression trees (MRT) are a new statistical technique that can be used to explore, describe, and predict relationships between multispecies data and en- vironmental characteristics. MRT forms clusters of sites by repeated splitting of the data, with each split defined by a simple rule based on environmental values. The splits are chosen to minimize the dissimilarity of site"
6630,unknown,"chosen to minimize the dissimilarity of sites within clusters. The measure of species dis- similarity can be selected by the user, and hence MRT can be used to relate any aspect of species composition to environmental data. The clusters and their dependence on the en- vironmental data are represented graphically by a tree. Each cluster also represents a species assemblage, and its environmental va"
6631,unknown,"assemblage, and its environmental values define its associated habitat. MRT can be used to analyze complex ecological data that may include imbalance, missing values, nonlinear relationships between variables, and high-order interactions. They can also predict species composition at sites for which only environmental data are available. MRT is compared with redundancy analysis and canonical corres"
6632,unknown,"a field data set. Key words: canonical correspondence analysis; CART; classification tree; cluster analysis; cross- validation; ecological distance; gradient analysis; multivariate regression tree; ordination; prediction; redundancy analysis; regression tree. INTRODUCTION The importance of being able to effectively model relationships between species and environment has long been recognized, and m"
6633,unknown,"applied to this task (see Franklin [1995] and Guisan and Zimmermann [2000] for reviews). Recently, the focus of such modeling has shifted towards prediction, with less emphasis on description and explanation. This shift towards a predictive ecology is profound, in both philosophy and method, and is not simply a case of using well-known methods for a new purpose. Driven by issues such as global cli"
6634,unknown,"by issues such as global climate change, accurate pre- diction is now often seen as the principal objective of species-environment analyses (Franklin 1998, Iverson and Prasad 1998, Vayssieres et al. 2000). Predictive accuracy is also routinely used as a method of statistical model selection (Stone 1974, Breiman et al. 1984, Rip- ley 1996, Burnham and Anderson 1998), and can re- place the widely us"
6635,unknown,"place the widely used practice of repeated hypothesis tests (e.g., stepwise selection); an approach that fre- quently leads to the inclusion of spurious explanatory variables (Draper 1995). The recognition of deficien- cies in the use of statistical hypothesis tests as a method for addressing ecological hypotheses (Burnham and Anderson 1998, Johnson 1999) has also led to increased emphasis on pred"
6636,unknown,"emphasis on prediction. Manuscript received 12 January 2001; revised 14 May 2001; accepted 14 May 2001; final version received 11 July 2001. I Present address: Australian Institute of Marine Science, PMB No. 3, Townsville Mail Centre, Queensland 4810, Aus- tralia. E-mail: g.death@aims.gov.au Species-environment relationships can be modeled using either individual species or assemblages. For in- di"
6637,unknown,"dividual species analyses, species are independently related to the environmental variables, whereas for community analyses, species are jointly related to the environment using a single model. Thus community analyses are more constrained. Popular methods used for individual species analysis include generalized lin- ear models (GLM; McCullagh and Nelder 1983), gen- eralized additive models (GAM; H"
6638,unknown,"1990), and classification and regression trees (CART; Breiman et al. 1984). Comparisons of GLM and GAM vs. CART (Franklin 1998, Vayssieres et al. 2000) found that the latter gave better predictions. This is not sur- prising as trees are well suited for analysis of complex ecological data that may include lack of balance, miss- ing values, nonlinear relationships between variables, and high-order i"
6639,unknown,"and high-order interactions (Breiman et al. 1984, Rip- ley 1996, De'ath and Fabricius 2000). Ordination meth- ods are widely used for community analysis (Jongman et al. 1995, ter Braak and Prentice 1988, Legendre and Legendre 1998), and typically assume that abundances of individual species vary in a linear or unimodal man- ner along ecological gradients. The gradients are ex- plained by linear co"
6640,unknown,"plained by linear combinations of environmental var- iables, the latter being used either directly or indirectly in the analysis. When used directly, the species and environmental data jointly determine the gradients. When used indirectly, the structure of the species data is first determined, and is then related to the environ- mental data. Clustering methods are also used for com- 1105 This cont"
6641,unknown,"mental data. Clustering methods are also used for com- 1105 This content downloaded from              88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https://about.jstor.org/terms 1106 GLENN DE'ATH Ecology, Vol. 83, No. 4 munity analysis, and with clusters are often used to define community or assemblage types (Anderberg 1973, Legendre and Legendre 1998). The cluster"
6642,unknown,"subsequently be related to environmental values. How- ever, methods that directly determine clusters in terms of environment are less well developed (Gordon 1996). Notable exceptions are various forms of contiguous clustering that impose temporal or spatial ordering on the clusters (Lefkovitch 1978, 1980, Legendre and Le- gendre 1998). This paper presents a new method for modeling spe- cies-enviro"
6643,unknown,"cies-environment data; namely, the multivariate re- gression tree (MRT). MRT is a natural extension of univariate regression trees, with the univariate response of the latter being replaced by a multivariate response. MRT analyzes community data, but makes no assump- tions about the form of relationships between species and their environment. MRT can be used for explo- ration, description, and pre"
6644,unknown,"ration, description, and prediction, and trees are usually selected on the basis of their predictive accuracy. MRT is a form of multivariate regression in that the response is explained, and can be predicted, by the explanatory variables. However, it is also a method of constrained clustering, because it determines clusters that are sim- ilar in a chosen measure of species dissimilarity, with each"
6645,unknown,"each cluster defined by a set of environmental values. As with unconstrained clustering, each cluster can de- fine an assemblage type, but additionally the environ- mental values define an associated habitat type. Following this introduction, a review of the principal concepts of univariate regression trees is presented, because they also form the basis for MRT. The sums of squares MRT is then out"
6646,unknown,"ample follows, which includes additional techniques that help interpret the tree analysis. More general forms of MRT are then discussed, and two types of MRT are defined; one generalizes the sums of squares MRT, and the other is distance based, analogous to distance-based redundancy analysis (db-RDA; Legendre and Anderson 1999, McArdle and Anderson 2000). MRT is then com- pared to redundancy analy"
6647,unknown,"canonical correspondence analysis . (CCA; ter Braak 1986) using simulated data and a previously published set of field data. Throughout this paper, response variables are spe- cies, data cases are sites, and explanatory variables are environmental characteristics. Hence, for ease of read- ing, i will often simply refer to species, sites, and en- vironment. Note, however, that applications of MRT a"
6648,unknown,"are not limited to modeling species in terms of their environment. The choice of response and explanatory variables is unrestricted, other than the response being numeric. UNIVARIATE REGRESSION TREES Univariate regression trees (URT) explain the vari- ation of a single numeric response variable using ex- planatory variables that may be numeric and/or cate- Herbs < 8.5 I Herbs ? 8.5 4.5 (2.8) Light"
6649,unknown,"4.5 (2.8) Light ? 6 Light < 6 L L ~~~~~~~~~8.5 2.9 C (8) (20) 1.1 4.7 A (10) B (10) FIG. 1. An example of a univariate regression tree. The response variable is the abundance (0-9 scale) of a species of hunting spider, Trochosa terricola, and the explanatory variables are six environmental characteristics (water, sand, twigs, moss, herbs, and light; 0-9 scale). These data are from Van der Art and "
6650,unknown,"Van der Art and Smeenk-Enserink (1975). The mean values of T. terricola and the number of cases (in parentheses) are displayed at each node; e.g., the overall mean is 4.5 (n = 28). The histograms show the distributions of T. terricola values at the nodes. The three-leaf tree is formed by two splits; the first based on herbs (<8.5 and -8.5) and then, for low levels of herbs, a second split based on"
6651,unknown,"The tree explains 93% of the variance of T. terricola, with virtually no overlap of values at the three leaves (A, B, and C). The depth of the tree following each split is proportional to the variance explained by the split. gorical (Breiman et al. 1984, Clark and Pregibon 1992, Ripley 1996, De'ath and Fabricius 2000). They do this by growing a tree structure that partitions the data set into mutu"
6652,unknown,"into mutually exclusive groups, each of which has sim- ilar values of the response variable. Starting with all the data represented by a single node at the top of the tree (Fig. 1), the tree is grown by repeated binary split- ting of the data. Each split is defined by a simple rule, usually based on a single explanatory variable, and forms two nodes. Splits are (generally) chosen to max- imize the"
6653,unknown,"imize the homogeneity (minimize the impurity) of the resulting two nodes. The terminal nodes (i.e., unsplit nodes) represent the groups of data formed by the tree, and are also called the leaves of the tree. Foi a re- gression tree, impurity of a node is typically defined as the total sum of squares of the response variable values about the node mean, and each split minimizes the total sums of squ"
6654,unknown,"the total sums of squares within the two nodes formed by the split. Equivalently, this maximizes the between- nodes sums of squares. The splitting procedure is continued until an over- large tree is grown, that is then pruned back to the desired size. The size may depend on the objective of the analysis; be it exploration, description, or predic- tion. For example, tree size can be selected choosi"
6655,unknown,"the most accurate predictor from a collection of trees This content downloaded from              88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https://about.jstor.org/terms April 2002 MULTIVARIATE REGRESSION TREES 1107 of varying size. Cross-validation is most often used to select tree size, with the chosen tree having the smallest (or close to) predicted mean squa"
6656,unknown,"al. 1984). This tree can be thought of as the ""best predictive tree"" in the sense that, on average, it should give the most accurate predictions. Trees are summarized by their size (the number of leaves) and overall fit. For a sums of squares regression tree, variation of the response is expressed as variance and overall fit is simply the fraction of variance not explained by the tree. More genera"
6657,unknown,"relative error (RE); the total impurity of the leaves divided by the impurity of the root node (the undivided data). RE gives an over-optimistic estimate of how ac- curately a tree will predict for new data, and predictive accuracy is better estimated from the cross-validated relative error (CVRE). CVRE varies from zero for a perfect predictor to close to one for a poor predictor. EXTENDING TO MUL"
6658,unknown,"EXTENDING TO MULTIVARIATE REGRESSION TREES A simple way to extend URT to MRT is to replace the univariate response by a multivariate response, and to redefine the impurity of a node by summing the univariate impurity measure over the multivariate re- sponse. Thus, to extend the sum of squares URT, im- purity can be defined as the sum of squares about the multivariate mean (see Appendix). Geometric"
6659,unknown,"is simply the sum of squared Euclidean distances of sites about the node centroid. Each split minimizes the sums of squared distances (SSD) of sites from the cen- troids of the nodes to which they belong. Equivalently, this maximizes the SSD between the node centroids. It can also be shown that this minimizes the SSD be- tween all pairs of sites within the nodes, and maximizes the SSD between all "
6660,unknown,"the SSD between all pairs of sites in different nodes (Digby and Gower 1981, Gower and Hand 1996). This equivalence only holds for sums of squares. Pruning and selecting tree size by cross-validation carry over from the univariate tree provided the prediction error for a new observation is defined analogously (see Ap- pendix). Finally, each leaf of the tree can be charac- terized by the multivaria"
6661,unknown,"terized by the multivariate mean of its sites, the number of sites at the leaf, and the environmental values that define it. I will refer to this tree as the sums of squares MRT (SS-MRT). Although the growing and pruning of univariate trees carry over to MRT, new challenges arise. First, because the multivariate response introduces extra complexity, additional ways to interpret the results of an M"
6662,unknown,"ysis are needed. In An introductory example of multi- variate regression trees below, three methods to assist that interpretation are outlined, namely (1) identifica- tion of species that most strongly determine the splits of the tree, (2) tree biplots to represent group means and species and site information, and (3) identification of species that best characterize the groups. Second, because MRT"
6663,unknown,"Second, because MRT is a form of constrained clus- tering, it is useful to compare the MRT solution to unconstrained clustering using a metric equivalent to the MRT impurity and the same numbers of clusters. If the unconstrained cluster analysis accounts for sub- stantially more of the species variation than an MRT analysis, this could indicate that unobserved factors, additional to the explanator"
6664,unknown,"additional to the explanatory variables of the tree anal- ysis, are responsible for the difference in explained species variation. However, if the two forms of analysis explain similar amounts of species variation, it is likely that important environmental variables (or their sur- rogates) have been identified. In such cases, the groups from the two forms of analysis will often coincide sub- stant"
6665,unknown,"stantially. In addition, clustering may be weak, but if there are strong relationships between species and en- vironment, MRT analyses can detect distinct groups not detectable by unconstrained clustering. Third, MRT can be extended to include impurity measures equivalent to the various measures of species dissimilarity currently used in ordination and clustering techniques. This issue is dealt wi"
6666,unknown,"techniques. This issue is dealt with in Beyond sums of squares multivariate regression trees. AN INTRODUCTORY EXAMPLE OF A MULTIVARIATE REGRESSION TREE In this example, a SS-MRT is used to relate abun- dances of 12 species of hunting spiders to six envi- ronmental characteristics (water, sand, twigs, moss, herbs, and light). These data, first presented by Van der Art and Smeenk-Enserink (1975), we"
6667,unknown,"by ter Braak (1986), using various forms of corre- spondence analysis. Canonical correspondence analy- sis (CCA) suggested a curvilinear distribution of sites in two dimensions. The first axis was interpreted as a moisture gradient and the second axis as a contrast between sites high in twigs, and sites high in herbs and moss. These data have also been analyzed using prin- cipal curves (De'ath 199"
6668,unknown,"one-dimensional solution, with largely unimodal dis- tributions of species along the estimated gradient. In this analysis, species and then sites were stan- dardized to the same mean. Site standardization con- verts abundances to relative abundances, and is widely used in gradient analyses methods because it increases the strength of the relationship between species dis- similarity and ecological "
6669,unknown,"similarity and ecological distance for moderate or long gradients (Faith et al. 1987). The tree analysis was also run on data standardized to give chi-squared distances between sites. The tree groups from this analysis were identical in number and composition to those of the tree analysis of the mean standardized data, and are not reported. The species and site standardizations were chosen to perm"
6670,unknown,"chosen to permit subsequent comparisons with both redundancy analysis (RDA) and CCA. The MRT analysis gave a four-leaf tree with the splits based only on twigs and water (Fig. 2). The tree ex- plained 78.8% of the standardized species variance. Species composition varied strongly across the four groups, with sites in groups A (twigs <8; water <2.5) This content downloaded from              88.97.1"
6671,unknown,"             88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https://about.jstor.org/terms 1108 GLENN DE'ATH Ecology, Vol. 83, No. 4 * arct.Iute * pard.lugu Twigs<8 j Twigs>8 0 zora.spin * pard~nigr * pard.pull 0 aulo~albi n LI * troc. terrmmu * alop.cune (28) O pard.mont * alop.acce * alop.fabr * arct.pen Water < 2.5 Water > 2.5 (20) Water < 5.5 Water 2 5.5 D (8) (1"
6672,unknown,"Water < 2.5 Water > 2.5 (20) Water < 5.5 Water 2 5.5 D (8) (14) A (6) B (6) C (8) FIG. 2. Multivariate regression tree for the hunting spider data. The data were species standardized to the same mean and site standardized to the same mean; Euclidean distance was used for splitting. Barplots show the multivariate species mean at each node, and the numbers of sites are shown in parentheses. Indicato"
6673,unknown,"stars. The cyclical shadings (black, gray, and white) indicate the various species and run from left to right across the barplots; for example, alop.fabr and arct.peri are indicator species for group A. The species abbreviations are as follows: alop.acce = Alopecosa accentuate; alop.cune = Alopecosa cuneata; alop.fabr = Alopecosa fabrilis; arct.lute = Arctosa lutetiana; arct.peri = Arctosa perita;"
6674,unknown,"pard.nigr = Pardosa nigriceps; pard.pull = Pardosa pullata; troc.terr = Trochosa terricola; zora.spin = Zora spinimana. 1.2 - 1.0 2 0.8 - 0.4- 0.2 - 1 2 3 4 5 6 7 8 9 10 Size of tree FIG. 3. Selection of the multivariate regression tree for the hunting spider data. The relative error (open circles) de- creases with tree size, whereas the cross-validated relative error (filled circles) decreases to"
6675,unknown,"error (filled circles) decreases to a minimum for a tree size of four, and then increases before flattening to a typical pla- teau. The vertical bars indicate one standard error for the cross-validated relative error, and the dashed line indicates one standard error above the minimum cross-validated rela- tive error and suggests a tree size of four leaves. and D (twigs 28) having only low levels o"
6676,unknown,"in common. The four associated habitats (A-D) are simply defined by high levels of twigs (D) or by mod- erate to low levels of twigs with three increasing levels of water (A, B, and C, respectively). The size of the tree was selected by cross-validation, with the four-leaf tree clearly identified as having the smallest estimated predictive error (Fig. 3). More typically, the cross-val- idations ge"
6677,unknown,"idations generate a series of trees that predict only mar- ginally worse than the best predictive tree. From this series, the smallest tree within one standard error of the best is often selected (the 1-SE rule; Breiman et al. 1984). Examining the splits Inspection of the barplots at each node, and at the four leaves, shows how individual species contribute to each split, and to the composition of"
6678,unknown,"groups. For example, Pardosa lugubris strongly de- termines the first split and is the dominant species of group D (Fig. 2). It also has a minimal presence in the left hand branch of the tree, and thus does not con- tribute to subsequent splits that form groups A, B, and C. In this example, the barplots effectively illustrate the species distributions throughout the tree, however, numerical summar"
6679,unknown,"numerical summaries of tree characteristics are also This content downloaded from              88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https://about.jstor.org/terms April 2002 MiULTIVARIATE REGRESSION TREES 1109 TABLE 1. Tabulation of species variance for the tree analysis of the hunting spider data. Species variance (%) explained by tree splits and whole tre"
6680,unknown,Species Twigs < 8 Water < 2.5 Water < 5.5 Tree total Species total Arctosa perita 1.90 15.54 0.00 17.45 20.85 Alopecosa fabrilis 2.33 6.72 0.79 9.83 13.44 Pardosa monticola 1.75 1.34 5.06 8.16 10.82 Alopecosa accentuate 1.93 0.70 2.15 4.78 5.77 Arctosa lutetiana 0.47 0.71 1.78 2.96 4.37 Aulonia albimana 0.35 0.90 0.70 1.96 3.28 Pardosa pullata 0.47 0.99 0.48 1.94 2.53 Pardosa nigriceps 0.20 0.88 0
6681,unknown,"Pardosa nigriceps 0.20 0.88 0.41 1.49 2.23 Zora spinimana 0.80 0.64 0.64 2.08 4.04 Alopecosa cuneata 0.25 0.84 0.02 1.11 2.89 Pardosa lugubris 23.22 0.02 0.04 23.28 24.41 Trochosa terricola 3.29 0.40 0.05 3.74 5.38 Total species variance 36.97 29.69 12.12 78.78 100.00 Notes: The total species variance is partitioned by species, the whole tree, and the three splits of the tree. The first split"
6682,unknown,"is dominated by Pardosa lugubris (23 percentage points of 37% of the total species variance explained by that split), the second by Arctosa perita (15 percentage points of 30%), and the third by Pardosa monticola (5 percentage points of 12%). useful, particularly when the number of species is large. For example, the contributions of individual species at each split and how well each species is exp"
6683,unknown,"the tree can be quantified, by tabulating the explained variance at each split for each species (Table 1). The variance of Pardosa lugubris comprises 24.4% of the total species variance, of which 23.3% is explained by the tree, with 23.2% explained by the first split (Table 1). Arctosa perita and Alopecosa fabrilis largely de- termine the second split (water <2.5), and Pardosa monticola dominates "
6684,unknown,"monticola dominates the third split (water <5.5). De- spite these four species accounting for 69.5% of total species variance, the remaining eight species are also well separated by the four tree groups, with the least well explained, Alopecosa cuneata, having 38.4% (1.1 1 of 2.89%) of its variance explained. Tree biplots For URT, each group is characterized by the mean response, and comparisons o"
6685,unknown,"ward. However, for MRT, the mean response is mul- tivariate, and this makes comparisons of groups more difficult. One way to examine the structure of multi- variate data is to plot them in a low-dimensional space using a principal components biplot (Gabriel 1971, ter Braak 1994, Gower and Hand 1996: Fig. 4). The dis- tance biplot (ter Braak 1994) gives the best least squares representation of the "
6686,unknown,"sistent with the MRT maximizing the sums of squares between groups; i.e., the biplot and MRT are using the same metric. For these data, the species have unimodal distributions, and thus each species is located by its weighted mean of the group means (also weighting on group size). Thus, species are located close to groups with relatively high abundance of that species. Species values of individual"
6687,unknown,"values of individual sites can be projected onto the plot and identified by group (Fig. 4). This shows the ho- mogeneity of sites within groups and can also reveal outlier sites. These points represent the observed spe- cies values and are referred to as supplementary points. Observed values were originally used in constrained (canonical) biplots (ter Braak 1986), however fitted values from the un"
6688,unknown,"values from the underlying model are now more typical (Oksanen 1987, ter Braak 1994, Palmer 1993). For tree biplots, the fitted values are the group means and the observed values are species values of individual sites; hence, both are shown in the tree biplot. Finally, the interset correlations can be calculated for each axis (ter Braak 1986) using the scores of the group means and the scores of t"
6689,unknown,"These correlations estimate the strength of the species- environment relationship. Axes with high interset cor- relations (typically >0.8), and that also account for a substantial proportion of the species variance explained by the environmental variables (i.e., the between-group variance), should be retained in the biplot representa- tion. Identifying indicator species Species that characterize e"
6690,unknown,tion. Identifying indicator species Species that characterize each group can be identified using an indicator species index based on relative abun- dance and relative frequency of occurrence (Dufr~ne and Legendre 1997). The index is defined as the product of relative abundance and relative frequency of occur- rence of the species within a group. If there are no occurrences of the species within a 
6691,unknown,"takes the value of zero, increasing to 100 if the species occurs at all sites within the group and does not occur at any other site. The index can be calculated for each species-group combination, and species with high in- dex values for a group are indicator species for that group (Fig. 2). The index distinguishes between ubiq- uitous species that dominate many groups in absolute abundance, and s"
6692,unknown,"abundance, and species that occur consistently within single groups but have low abundance. For the four MRT groups, the index values for the 12 species vary from 100 for Arctosa perita (which occurs in all sites of group A and no other sites),. down to 29 for Alo- This content downloaded from              88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https://about"
6693,unknown,"1110 GLENN DE'ATH Ecology, Vol. 83, No. 4 B C arct.lute B B C pard.pull aulo.albi C pard.nigr B pard.mont alop.cune . alop.acce B or g ~~+ zoa.spin troc.terr alop.fabr D pard.lugu N A Al5 A; Dim 2 28.1%[0.96] arctperi A Dim 1 57.9% [0.99] FIG. 4. Principal-components biplot of the four group means from the tree analysis of Fig. 2. The large letters A-D"
6694,unknown,"represent the multivariate group means and correspond to groups A-D in Fig. 2. The individual sites denoted by small letters A-D suggest a good fit at all sites. Each species label is located at its weighted mean from the four group means. The first two dimensions (Dim 1 and Dim 2) account for 57.9% and 29.1% of the between-groups sums of squares, respectively, with interset correlations of 0.99 a"
6695,unknown,"pecosa cuneata (which has moderate abundance and presence across three groups). Nine of the 12 species had high indicator values for single group (range: 57- 100). The remaining three species had moderate values for two or three groups (27-48). For each of the three species, the values were very similar across groups, thereby suggesting they were indicators for composite groups. Comparing tree gro"
6696,unknown,"groups. Comparing tree groups with unconstrained clusters Unconstrained cluster solutions, comprising two to four clusters, were derived using complete linkage hi- erarchical clustering (Euclidean metric). For each so- lution, the clusters were refined using k-means clus- tering (Hartigan and Wong 1979) to minimize the with- in-cluster sums of squares. This procedure gives com- pact clusters, and "
6697,unknown,"pact clusters, and thus provides a stringent comparison with the constrained clusters defined by the MRT. The tree and the unconstrained clusters had almost identical membership for two to four clusters and explained the species variance equally well. Together with the large proportion of species variance explained by the tree, this supports the conclusion that the two of the six environmental var"
6698,unknown,"environmental variables that formed the tree (twigs and water) adequately account for the species variance. BEYOND SUMS OF SQUARES MULTIVARIATE REGRESSION TREES Regression trees are modular in the sense that the impurity measure, splitting criteria and prediction error are all independent of the growing and pruning pro- cesses (Breiman et al. 1984). This presents opportu- nities to develop MRT usi"
6699,unknown,"nities to develop MRT using impurity measures, ad- ditional to SS, that are useful to ecologists, e.g., robust measures and measures that correspond to popular mea- sures of species dissimilarity. To do this, two strategies are adopted, and these lead to additive MRT (A-MRT) and distance-based MRT (db-MRT). Additive multivariate regression trees Following the method of extending SS-URT to SS- MRT,"
6700,unknown,"MRT, i.e., adding the univariate measure of impurity over the multivariate response, measures other than sums of squares about the mean can be used, e.g., sums of absolute deviations about the median. These im- purity measures are examples of additive distances (Gower and Hand 1996), with each variable of the mul- tivariate response contributing to the impurity inde- pendently of the others. One u"
6701,unknown,"is the potential to make them robust to outliers. For URT, the two most widely used impurity measures are sums of squares about the mean, and sums of absolute deviations about the median (LAD; Breiman et al. 1984), with LAD being more robust than SS to outliers. Though the median is an extremely robust measure of location, the sum of absolute deviations about medians is sensitive to outliers, and "
6702,unknown,"of LAD trees. Sums of squares about trimmed means, This content downloaded from              88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https://about.jstor.org/terms April 2002 MULTIVARIATE REGRESSION TREES 1111 where extreme values are simply dropped from each group, are even more robust, but are less widely used (SPSS Inc. 1997). Distance-based multivariate re"
6703,unknown,"Multivariate trees can be formulated to work directly from a dissimilarity matrix. Treating the dissimilarities as distances, the clusters can be formed by splitting the data on environmental values that minimize the inter- site SSD (sums of squared distances) within clusters. SS-MRT is based on Euclidean distance, a measure of dissimilarity often used for the analysis of species- environment rela"
6704,unknown,"environment relationships. However, other measures are often preferred. For example, most forms of gra- dient analysis depend, either explicitly or implicitly, on a strong linear relationship between some measure of species dissimilarity and ecological distance. Anal- yses based on Euclidean distance often fail for mod- erate to long gradients, because compared to alterna- tives such as site stand"
6705,unknown,"tives such as site standardized Bray-Curtis and ex- tended dissimilarity, it is only weakly correlated with ecological distance (Faith et al. 1987, De'ath 1999b). The relative merits of various measures have been widely discussed (e.g., Faith et al. 1987, Legendre and Legendre 1998) and I will not add to that debate. Rath- er, I am assuming that a particular measure has been chosen, and that if th"
6706,unknown,"chosen, and that if the resulting matrix of site dissim- ilarities is treated as distances, then these distances adequately represent the structure in the data. Distance- based MRT (db-MRT) can be defined in terms of with- in-group intersite SSD, independent- of the particular measure of dissimilarity chosen for the analysis. The impurity of the node is defined as the sum of within- group intersit"
6707,unknown,"group intersite SSD, the splitting criterion maximizes the reduction of impurity at a split, and the prediction error is defined as the SSD of the prediction site from all other sites, minus the within-group SSD of other sites (see the Appendix). If the dissimilarities used in a db-MRT are Euclidean distances, then SS-MRT and db-MRT are exactly equivalent, because as noted ear- lier, minimizing sq"
6708,unknown,"lier, minimizing squared Euclidean distances of sites about node centroids is identical to minimizing within- node intersite squared Euclidean distances. Distance- based MRT extends SS-MRT, just as distance-based redundancy analysis (db-RDA: Legendre and Anderson 1999, McArdle and Anderson 2000) extends RDA. In both cases, the distance-based methods allow any mea- sure of dissimilarity to be used,"
6709,unknown,"tance. Comparison of additive and distance-based trees Despite the coincidence of SS-MRT and Euclidean db-MRT, A-MRT and db-MRT are quite different. For example, the impurity of A-MRT can be defined in many ways and focuses on the typical characteristic of the node, e.g., the multivariate mean. Conversely, the impurity of db-MRT is always defined as the within- group intersite SSD (see Appendix) a"
6710,unknown,"on individual sites with all sites contributing equally to the impurity of a node. The following two examples illustrate these differences. First, if the impurity of an A-MRT is defined as sums of squares about trimmed means, sites with extreme values may have no influence on any splits of the tree. This cannot occur for db-MRT because every site will always influence at least one split (the first"
6711,unknown,"split (the first). Second, because db-MRT depends sole- ly on the dissimilarities and the environmental data, the species observations themselves are not needed. Thus db-MRT, but not A-MRT, can be used in pairwise comparison studies, for which only the dissimilarities (or similarities) between subjects are available (Thur- stone 1927). There are, however, links between A-MRT and db- MRT. First, th"
6712,unknown,"MRT. First, there are several measures of species dis- similarity that are equivalent to Euclidean distances of suitably scaled species data; e.g., chi-squared and chord dissimilarity. For such measures, db-MRT can be de- termined by simply scaling the data appropriately and then using SS-MRT. This is not possible for species dissimilarities based on sums of absolute deviations; e.g., Bray-Curtis "
6713,unknown,"e.g., Bray-Curtis dissimilarity. Secondly, for many spe- cies dissimilarities, a principal coordinates analysis will generate site coordinates such that the Euclidean distances between the sites are exactly proportional to the dissimilarities (i.e., they are Euclidean embeddable [Gower and Hand 1996]). A SS-MRT analysis using these coordinates as the response data will give the identical tree as t"
6714,unknown,"identical tree as the db-MRT of the original dissimi- larities. For these two situations, SS-MRT analysis can be used to determine db-MRT more efficiently; partic- ularly for large dissimilarity matrices. COMPARISON OF MULTIVARIATE REGRESSION TREES WITH REDUNDANCY ANALYSIS AND CANONICAL CORRESPONDENCE ANALYSIS Redundancy analysis (RDA; Rao 1964) and canon- ical correspondence analysis (CCA; ter Br"
6715,unknown,"are widely used methods of direct gradient analysis (DGA). Similar to RDA and CCA, MRT seeks to max- imize the species variation explained by the environ- mental variables. Unlike RDA and CCA, MRT is not a method of DGA, in the sense of locating sites along ecological gradients. However, MRT does locate sites (albeit grouped) in an ecological space defined by the environmental variables. For RDA a"
6716,unknown,"lution lies in the linear space defined by the environ- mental values, whereas for MRT the solution space is nonlinear and includes interactions between the envi- ronmental variables. For RDA and CCA, interactions have to be individually added to the environmental var- iables, making such effects more difficult to detect, particularly so when the number of environmental var- iables is large. Howev"
6717,unknown,"iables is large. However, MRT automatically detects interactions because each split partitions the data into independent subsets, each of which is analyzed inde- pendently. This content downloaded from              88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https://about.jstor.org/terms 1112 GLENN DE'ATH Ecology, Vol. 83, No. 4 0 0 0 0 00 0~~~~~~~ 0) 00 0 C,) 00"
6718,unknown,"0) 00 0 C,) 00 g A L A~~~~~~~~~~~0) 0 10~~~~~~~~~~~~~~~~~ ~ Ecological gradient FIG. 5. Simulated species abundances along a one-dimensional ecological gradient. Sites and species modes are distributed evenly along the gradient. Comparisons using simulated data Abundance data were simulated for 100 sites and four species each with a Gaussian response curve driv- en by a single ecological gradient "
6719,unknown,"and species optima were uniformly distributed along the gradient. The span (species tolerance) and maxima of the response curves were fixed for all species, and Poisson noise was added to the abundances. The gra- dient length was eight standard deviations of the re- sponse curves. These data closely satisfy the conditions for which correspondence analysis and CCA are ef- fective estimators of ecol"
6720,unknown,"fective estimators of ecological gradients (ter Braak 1986). The data were analyzed using SS-MRT, RDA, and CCA. For each method, the data were: (1) untrans- formed, (2) square-root transformed, and (3) square- root transformed and site standardized to the same mean. CCA implicitly site standardizes the species and thus (3) was not used in the CCA analyses. The trans- formation and standardization "
6721,unknown,"arch effect present in principal components biplots of the species data by linearizing the relationship between species dissimilarity (in this case Euclidean) and eco- logical distance. Linear and quadratic terms in site lo- cations on the ecological gradient were used as ex- planatory variables for RDA and CCA. For MRT, only the linear term was used because the (noncentered) quadratic has the sam"
6722,unknown,"quadratic has the same rank order of values as the linear term and hence it does not contribute additional splits. For RDA and CCA, models were fitted with both linear and quadratic terms, and with only the linear term. For MRT, the variance explained by the constrained and unconstrained models (unconstrained clustering) differed little and was >91%, irrespective of transfor- mations and standardi"
6723,unknown,"mations and standardization (Table 2). Individual spe- cies analyses of the simulated data using univariate regression trees gave marginally higher mean CVRE values compared to MRT for all three analyses. RDA differed greatly between constrained and unconstrained solutions for other than the transformed and standard- ized data, whereas CCA showed good agreement be- tween the two due to the implici"
6724,unknown,"site standardization. The variance explained by the first two axes of ordination plots, the species-environment correlations and permutation tests (P < 0.05; ter Braak 1992) all indicated a two-dimensional solution was re- quired for all RDA and CCA analyses. Strong arch effects were present in all RDA and CCA biplots (not shown). Differences between the transformed and stan- dardized RDA analysis"
6725,unknown,"dardized RDA analysis and the transformed CCA anal- ysis were minimal (Table 2). The cross-validated relative error (CVRE) of all methods decreased (i.e., predictions were more accu- rate) as the species-environment was linearized by the transformation and site standardization (Table 2). The differences were small for MRT, which predicted com- paratively well irrespective of transformation and sta"
6726,unknown,"dardization. The MRT, using only linear terms in site locations, predicted far better than the linear RDA and CCA models. The differences in prediction for MRT (linear), RDA (quadratic), and CCA (quadratic) were small when the data were transformed and standard- ized. These results show the necessity of satisfying model assumptions for RDA and CCA. They also demonstrate two clear advantages of MRT"
6727,unknown,"two clear advantages of MRT; namely (1) the absence This content downloaded from              88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https://about.jstor.org/terms April 2002 MULTIVARIATE REGRESSION TREES 1113 TABLE 2. Comparison of analyses of simulated data using three methods: sums of squares multivariate regression tree (MRT), redundancy analysis (RDA), a"
6728,unknown,Square-root Square-root + site Analysis Parameters Untransformed transformation standardization MRT Variance explained (%) 91.1 (94.2) 93.8 (94.7) 93.9 (94.2) Size of tree 10 12 10 CVRE (L) 0.144 0.099 0.097 CVRE(URT) 0.169 0.104 0.107 RDA Variance explained (%) 55.5 (86.5) 74.4 (95.0) 90.3 (96.2) CVRE (Q) 0.475 0.276 0.098 CVRE (L) 0.732 0.609 0.242 CCA Variance explained (%) 88.3 (93.7) 91.3 (96
6729,unknown,"CVRE (Q) 0.124 0.107 CVRE (L) 0.308 0.237 Notes: The response variables are the four species abundances, and the explanatory variable(s) are linear and quadratic terms in site locations on the ecological gradient. For MRT only the linear term was used, and for RDA and CCA linear and quadratic terms, and then only the linear term, were used. For each method, the data were (1) untransformed, (2) squ"
6730,unknown,"root transformed, and (3) square-root transformed and site standardized to the same mean (not applicable to CCA). For each method, the percentage of variance explained by the constrained and equivalent unconstrained analyses (in parentheses; clustering, principal components, and correspondence analysis, respectively) is shown. The cross-validated relative error (CVRE: L = linear, Q = quadratic, an"
6731,unknown,"decreases (predictions are more accurate) with transformation and site standardization of the data. of model assumptions that results in a greater robust- ness and (2) the invariance of trees to monotonic trans- formations of explanatory variables. These analyses also raise the issue of inclusion of polynomial effects or detrending of CCA analyses for moderate to long gradients (ter Braak 1986, Pa"
6732,unknown,"environmental effects are included, gradients may be accurately estimated, but species composition may be both poorly explained and predicted (Table 2). It also follows that the proportion of explained species vari- ance is not a good indicator of how well gradients have been estimated. For example, in the analyses of our untransformed simulated data, the RDA and CCA mod- els with only linear expl"
6733,unknown,"els with only linear explanatory variables exactly re- cover the site locations, despite having relatively lower explained species variance and species-environment correlations. This will always be the case when the latent variables) representing the gradients) are an exact linear combination of the environmental data. Comparisons using the hunting spider data As outlined in An introductory exampl"
6734,unknown,"variate regression trees, the SS-MRT analysis of the standardized spider data explained 78.8% of scaled spe- cies variance with a four-leaf tree based on two envi- ronmental variables (twigs and water). For the same data, RDA explained 73.6% using all six environmental variables, with 61.7%, 28.9%, and 6.1% explained by the first three axes, respectively (Fig. 6). The species- environment correlat"
6735,unknown,"environment correlations were 0.99, 0.93, and 0.61, suggesting a two-dimensional solution. Omitting the nonsignificant variables (herbs and moss; permutation tests; P > 0.05), the percentage variance explained drops to 71.2%. Using only twigs and water, the per- centage variance explained drops to 61.5%, 12.1% less than the full RDA model and 17.3% less than the four- leaf tree. In addition to MRT"
6736,unknown,"species variance than RDA (RE = 0.21 for MRT vs. 0.26 for RDA), it was also a marginally more accurate predictor (CVRE = 0.44 [SE = 0.07] for MRT vs. 0.47 [0.04] for the two-dimensional RDA model based on all six environmental variables). Individual species analyses using univariate regression trees were simi- larly accurate (CVRE = 0.45 [SE = 0.09]) to MRT. It could be argued that RDA explains le"
6737,unknown,"variance due the unimodal distributions of the species of spider, and that CCA is a more appropriate form of analysis. However, the species and site standardizations of the RDA analysis largely removes that nonlinearity, in an analogous manner to the species and site stan- dardizations implicit in CCA. Also, comparison of the configurations of site locations (gradients) from the RDA analysis and f"
6738,unknown,"transformed species data showed the two configura- tions to be almost identical (two-dimensional Procrus- tes analysis [Gower 1975]: Sibson's stress = 0.018). The variances explained and species-environment cor- relations from the two analyses were also very similar, although CCA performs a weighted analysis of the spe- cies data, and hence statistics from the CCA and RDA analyses are only approxi"
6739,unknown,"analyses are only approximately equivalent. DIscusSION Multivariate regression trees (MRT) are a powerful method for the community analysis of species-environ- ment data; one that can be used for exploration, de- scription, and prediction. MRT can be viewed and used in many ways. First, they are a regression technique that both explains and predicts species abundances from en- vironmental variable"
6740,unknown,"vironmental variables. The regression is constrained in that all species are explained (and predicted) by the same environmental values. This is similar to redundancy analysis, which is a form of constrained multivariate linear regression. MRT can also be seen as a form of This content downloaded from              88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https"
6741,unknown,"1114 GLENN DE'ATH Ecology, Vol. 83, No. 4 C herbs arcd lute C \ard~pul * C c C\ ; pard. nigr C B C C~C B B \aulo~albic C \ zora.spin water B *m alop. cune \admn troc. terr * reft B\ moss alop.acce=~ A B / D alop.fabr ~~~~~~~~~~~~pard.Iugu twigs sand D A A D D Am Dim 2 28.9% [0.93] D D arct.peri A Dim 1 61.7% [0.99] FIG. 6. Redundancy analysis of hunting spider data. The data were species standardi"
6742,unknown,"standardized to the same mean. The six explanatory variables (twigs, water, herbs, reflected light, moss, and sand) explained 73.6% of the species variance compared to 78.8% of the variance explained by the multivariate tree analysis (Figs. 2 and 4). The first two dimensions (Dim 1 and Dim 2) account for 61.7% and 28.9% of the explained species variance, respectively,"
6743,unknown,"and the interset correlations are 0.99 and 0.93. The individual sites are denoted by small letters A-D corresponding to the tree groups, and each species label is located at its weighted mean from all sites. As will generally be the case, the plot is very similar to the tree biplot. constrained cluster analysis that, dependent on trans- formations, standardizations, and choice of splitting cri- te"
6744,unknown,"terion, can relate different aspects of species composi- tion to environmental data. In this form, the clusters defined by MRT define species assemblages and asso- ciated environment types in a simple manner not avail- able in other techniques. MRT can present a compre- hensive view of species-environment relationships by (1) displaying the annotated tree; (2) tabulating variation at the splits of"
6745,unknown,"at the splits of the tree; (3) identifying indicator species to characterize groups; (4) displaying the group means, species, and sites in a low-dimensional space; and (5) comparing the hierarchy of tree groupings with the equivalent unconstrained clusters. Two forms of MRT have been outlined, namely, ad- ditive MRT (A-MRT) and distance-based MRT (db- MRT). A-MRT works directly from the species-en"
6746,unknown,"ronment data matrix, with the tree clusters defined by a chosen measure suitable to the problem at hand, e.g., a robust measure if outliers are problematic. Although this work has focused on sums of squares MRT, other mea- sures can be used as a basis. For example, sums of absolute deviations about medians give a robust MRT. Also, if the species-environment data matrix is not avail- able, or if a "
6747,unknown,"able, or if a particular measure of dissimilarity is pre- ferred, then db-MRT can be constructed directly from the dissimilarity matrix. These two forms of MRT, ad- ditive and distance based, are not exhaustive. For ex- ample, nonadditive impurity measures that take into ac- count relationships between species (e.g., Mahalanobis distance) could be used. Established unconstrained clus- tering crite"
6748,unknown,"tering criteria based on the trace and determinant of the within-groups covariance matrix (Krzanowski and Mar- riot 1995) could also be used as measures of impurity. This flexibility in the choice of method for forming clus- ters is a significant strength of MRT. Though inherently a clustering technique suited to identifying assemblage types, MRT is an alternative to, but also complements, constra"
6749,unknown,"such as redundancy analysis (RDA) and canonical cor- respondence analysis (CCA). MRT differs from these techniques in that (1) MRT is a divisive technique, RDA and CCA model continuous structure; (2) MRT empha- sizes local structure and interactions between environ- mental effects, RDA and CCA determine global struc- ture; and (3) MRT does not assume particular relation- This content downloaded fr"
6750,unknown,"             88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https://about.jstor.org/terms April 2002 MULTIVARIATE REGRESSION TREES 1115 ships between species abundances and environmental characteristics, RDA and CCA respectively assume lin- ear and unimodal distributions of species along gradi- ents. These differences make MRT and RDA/CCA ideal complementary techniq"
6751,unknown,"used to determine interactions between environmental variables, a task that is difficult for RDA and CCA, and the interactions could then be included in an RDA or CCA analysis. MRT has been compared with RDA and CCA analysis in examples that should favor the latter techniques; namely strong gradients with a continuous distribution of sites. Despite this, MRT outperformed or matched these technique"
6752,unknown,matched these techniques in both explaining and pre- dicting species composition. The advantage of MRT in- creases with the strength of interactions and the nonlin- earity of relationships between species composition and the environmental variables. I have found this to be the case over a broad range of ecological data sets and sim- ulations. MRT and RDA/CCA present different descrip- tions of the
6753,unknown,"tions of the relationships between species abundances and their environment; the former based on discrete en- vironmental types, and the latter on gradient(s) linearly related to the environmental values. Which of these de- scriptions is most useful is open to debate, but clearly MRT is an interesting alternative to RDA and CCA. As a form of community analysis, MRT compares favorably with the equi"
6754,unknown,"favorably with the equivalent individual species anal- ysis based on univariate regression trees (URT). For both the data simulation and field data examples, the accuracy of MRT was at least as good as that of URT. Given MRT is a single tree, and is thus easy to interpret compared to the multiple trees of the URT analysis, this suggests the potential to both accurately predict, and also to generat"
6755,unknown,"and also to generate a simple descriptive model, using a single analysis. Most univariate tree concepts and practices carry over to MRT, e.g., tree selection based on cross-vali- dation, and methods for determining splits of the tree. MRT also inherits characteristics of univariate regres- sion trees in that (1) mixtures of unlimited numbers of numeric and categorical variables can be used as ex- "
6756,unknown,"planatory variables, (2) they are invariant to monotonic transformations of explanatory variables, (3) interac- tions between explanatory variables are automatically detected, (4) they are easy to construct, and the re- sulting groups are often simple to interpret, (5) they are robust to the addition of pure noise response and/ or explanatory variables, and to the collinearity of-ex- planatory var"
6757,unknown,"planatory variables, and (6) they handle missing values in explanatory variables with minimal loss of infor- mation. STATISTICAL ANALYSES All data analyses used the S-Plus statistical software (Statistical Sciences, a division of Mathsoft, Seattle, Washington, USA). The library of tree routines (RPART; Recursive PARTitioning) developed by T. Therneau (unpublished manuscript) was extended by inclus"
6758,unknown,inclusion of additional C routines to fit multivariate regression trees. The index for determining indicator species was calculated as in Dufrene and Legendre (1997). Code to simulate data sets was written in S- Plus. The hierarchical and k-means clustering used S- Plus routines. A library of S-Plus functions for tree analyses (in- cluding multivariate regression trees) is available in ESA's Elect
6759,unknown,"ESA's Electronic Data Archive; see Supplementary Materials. ACKNOWLEDGMENTS Thanks are due to Dave Roberts and two anonymous ref- erees whose constructive comments substantially improved this paper. Comments from Pierre Legendre, Steve Delean, and Katharina Fabricius are also appreciated. I would also like to thank John Birks, Cajo ter Braak, Allan Stewart-Oaten, Lee Belbin, and Mike Palmer for th"
6760,unknown,"Lee Belbin, and Mike Palmer for their creative works and encouragement over recent years. Finally but not least, thanks are due to Terry Therneau for the development of the RPART library. This work was funded by the Cooperative Research Centre for Great Barrier Reef World Heritage Area. REFERENCES Anderberg, M. R. 1973. Cluster analysis for applications. Academic Press, London, UK. Breiman, L., J."
6761,unknown,"Academic Press, London, UK. Breiman, L., J. H. Friedman, R. A, Olshen, and C. G. Stone. 1984. Classification and regression trees. Wadsworth In- ternational Group, Belmont, California, USA. Burnham, K. P., and D. R. Anderson. 1998. Model selection and inference: a practical information theoretic approach. Springer Verlag, New York, New York, USA. Clark, L. A., and D. Pregibon. 1992. Tree-based mod"
6762,unknown,"Pages 377-420 in J. M. Chambers and T. J. Hastie, editors. Statistical models in S. Wadsworth and Brooks. Pacific Grove, California, USA. De'ath, G. 1999a. Principal curves: a new technique for in- direct and direct gradient analysis. Ecology 80:2237-2253. De'ath, G. 1999b. Extended dissimilarity: a method of robust estimation of ecological distances from high beta diversity data. Plant Ecology 14"
6763,unknown,"De'ath, G., and K. E. Fabricius. 2000. Classification and re- gression trees: a powerful yet simple technique for the analysis of complex ecological data. Ecology 81:3178-3192. Digby, P. G. N., and J. C. Gower. 1981. Ordination between and within groups applied to soil classification. Pages 53- 75 in D. F Merriam, editor. Down to earth statistics: so- lutions looking for geological problems. Syrac"
6764,unknown,"sity Geology Contributions. Syracuse, New York, USA. Draper, D. 1995. Assessment and propagation of model un- certainty (with discussion). Journal of the Royal Society Series B 57:45-97. Dufrene, M., and P. Legendre. 1997. Species assemblages and indicator species: the need for a flexible asymmetrical approach. Ecological Monographs 67:345-366. Faith, D. P., P. R. Minchin, and L. Belbin. 1987. Com"
6765,unknown,"tional dissimilarity as a robust measure of ecological dis- tance. Vegetatio 69:57-68. Franklin, J. 1995. Predictive vegetation mapping: geographic modeling of biospatial patterns in relation to environmental gradients. Progress in Physical Geography 19:474-499. Franklin, J. 1998. Predicting the distribution of shrub species in southern California from climate and terrain-derived var- iables. Jour"
6766,unknown,"iables. Journal of Vegetation Science 9:733-748. Gabriel, K. R. 1971. The biplot graphical display of matrices with application to principal component analysis. Biome- trika 58:453-467. Gordon, A. D. 1996. A survey of constrained classification. Computational Statistical Data Analysis 21:17-29. This content downloaded from              88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              Al"
6767,unknown,"All use subject to https://about.jstor.org/terms 1116 GLENN DE'ATH Ecology, Vol. 83, No. 4 Gower, J. C. 1975. Generalized procrustes analysis. Psy- chometrika 40:33-51. Gower, J. C., and D. J. Hand. 1996. Biplots. Chapman and Hall, London, UK. Guisan, A., and N. E. Zimmerman. 2000. Predictive habitat distribution models in ecology. Ecological Modeling 135: 147-186. Hartigan, J. A., and M. A. Wong."
6768,unknown,"A K-means clustering algorithm. Applied Statistics 28: 100-108. Hastie, T. J., and R. J. Tibshirani. 1990. Generalized additive models. Chapman and Hall, London, UK. Iverson, L. R., and A. M. Prasad. 1998. Predicting abundance of 80 tree species following climate change in the eastern United States. Ecological Monographs 68:465-485. Johnson, D. H. 1999. The insignificance of hypothesis test- ing. "
6769,unknown,"ing. Journal of Wildlife Management, 63(3):763-772. Jongman, R. H. G., C. F J. ter Braak, and 0. F R. Tongeren. 1995. Data analysis in community and landscape ecology. Second edition. Cambridge University Press, Cambridge, UK. Krzanowski, W. J., and F H. C. Marriot. 1995. Multivariate analysis: part 2. Classification, covariance structures and repeated measurements. Arnold, London, UK. Lefkovitch,"
6770,unknown,"Lefkovitch, L. P. 1978. Cluster generation and grouping using mathematical programming. Mathematical BioScience 41: 91-110. Lefkovitch, L. P. 1980. Conditional clustering. Biometrics 36:43-58. Legendre, P., and M. J. Anderson. 1999. Distance-based re- dundancy analysis: testing multispecies responses in mul- tifactorial ecological experiments. Ecological Monographs 69: 1-24. Legendre, P., and L. L"
6771,unknown,"69: 1-24. Legendre, P., and L. Legendre. 1998. Numerical ecology. Second English edition. Elsevier, Amsterdam, The Neth- erlands. McArdle, B. H., and M. J. Anderson. 2000. Fitting multi- variate models to community data: a comment on distance- based redundancy analysis. Ecology 82:290-297. McCullagh, P., and J. A. Nelder. 1983. Generalized linear models. Chapman and Hall, London, UK. Oksanen, J. 1"
6772,unknown,"Oksanen, J. 1987. Problems of joint display of species and site scores in correspondence analysis. Vegetatio 72:51-57. Palmer, M. W. 1993. Putting things in even better order: the advantages of canonical correspondence analysis. Ecology 74:2215-2230. Rao, C. D. 1964. The use and interpretation of principal components analysis in applied research. Sankhya A 26: 329-358. Ripley, B. D. 1996. Pattern "
6773,unknown,"Cambridge University Press, Cambridge, UK. SPSS Inc. 1997. SYSTAT Version 7.0 for Windows. SPSS, Prentice Hall, New Jersey, USA. Stone, M. 1974. Cross-validatory choice and assessment of statistical predictions (with discussions). Biometrika 64: 29-35. ter Braak, C. J. F 1986. Canonical correspondence analysis: a new eigenvector technique for multivariate direct gradient analysis. Ecology 67:1167-"
6774,unknown,"analysis. Ecology 67:1167-1 179. ter Braak, C. J. F 1992. Permutation versus bootstrap sig- nificance tests in multiple regression and ANOVA. Pages 79-86 in K. H. Rickel, G. Rothe, and W. Sendler, editors. Bootstrapping and related techniques. Springer Verlag, Berlin, Germany. ter Braak, C. J. F 1994. Canonical community ordination. Part 1: basic theory and linear methods. Ecoscience 1(2): 127-140"
6775,unknown,"127-140. ter Braak, C. J. F, and I. C. Prentice. 1988. A theory of gradient analysis. Advances in Ecological Research 18: 271-317. Thurstone, L. L. 1927. The method of paired comparisons for social values. Journal of Abnormal Psychology 31:384-400. Van der Aart, P. J., and N. Smeenk-Enserink. 1975. Corre- lations between distributions of hunting spiders (Lycosidae, Ctenidae) and environmental char"
6776,unknown,"Netherlands Journal of Zoology 25:1-45. Vayssieres, M. P., R. E. Plant, and B. H. Allen-Diaz. 2000. Classification trees: an alternative non-parametric approach for predicting species distributions. Journal of Vegetation Science 11:679-694. APPENDIX Growing multivariate regression trees In order to grow and prune any tree, and to select the tree size by cross-validation, it is necessary to define "
6777,unknown,"purity of a node, (2) the rule for splitting nodes, and (3) the prediction error for a new observation (Breiman et al. 1984). The overall objective is to minimize the total impurity of the groups, and hence the splitting rule (generally) maximizes the reduction of impurity at each split. The prediction error for new observations is needed to determine the best predic- tive tree by cross-validation"
6778,unknown,"tive tree by cross-validation. The impurity and prediction error for A-MRT using sums of squares and least absolute deviations, and for db-MRT using any dissimilarity measures, are shown in Table 3. Tree biplots The tree biplots are simply a distance biplot of the fitted and observed species values from the MRT analysis. The information needed to generate the plot can be calculated as follows. Sup"
6779,unknown,"as follows. Suppose Y is the n x p matrix of species ob- servations (columns are species and rows are sites), and Y is the n x p matrix of fitted values (i.e., the group means). Let Y = U d VT be the singular value decomposition of Y. The display points for the group means (the fitted val- ues) are YV = U d, where we need to plot only one point from each group. The display points for sites (observ"
6780,unknown,"species values) are Y V. The species can be represented TABLE Al. Impurity measures and prediction error for mul- tivariate regression trees. Tree description Impurity Prediction error Multivariate sums of E (xl - (x - )2 squared deviations Ij about the mean (SS- MRT) Multivariate sums of ab- E Ix, - x,| E Ix* - XJ solute deviations about Ij the median (LAD-MRT) Distance-based (db-MRT) E 2 - i z>k"
6781,unknown,"the median (LAD-MRT) Distance-based (db-MRT) E 2 - i z>kfk ln >kk n Notes: Notation: xl) denotes the species data for site i and species j, x* denotes a new observation, x and x denote mean and median, respectively, d2 and d*2 denote the squared dis- similarities between sites i and k, and between a new obser- vation and site i, respectively, and n denotes the number of cases in the prediction err"
6782,unknown,"cases in the prediction error group. by either vectors from the plot origin given by U, or by locating them at their weighted averages from the group means given by S-' yT Y V where S is the column sums of Y. The tree biplot is equivalent to the RDA biplot in the fol- This content downloaded from              88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https://ab"
6783,unknown,"All use subject to https://about.jstor.org/terms April 2002 MULTIVARIATE REGRESSION TREES 1117 lowing way. First, run the MRT analysis. Then run an RDA using the same species data as the multivariate response, and for explanatory variables use indicator variables, one for each cluster of the tree, where the variable takes the value 1 if a site is in a cluster and 0 otherwise. The fitted values of "
6784,unknown,"RDA are the same as those of the tree, and the RDA biplot will be identical to the tree biplot, provided both fitted and observed values are plotted. SUPPLEMENTARY MATERIAL A library of S-Plus functions for tree analyses (including multivariate regression trees) is available in ESA's Electronic Data Archive: Ecological Archives E083-017-S1. This content downloaded from              88.97.199.152 o"
6785,unknown,"             88.97.199.152 on Tue, 22 Apr 2025 10:12:22 UTC              All use subject to https://about.jstor.org/terms MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition Nicolas Menet1,2∗ menetn@ethz.ch Michael Hersche1,2 her@zurich.ibm.com Geethan Karunaratne1 kar@zurich.ibm.com Luca Benini2 lbenini@iis.ee.ethz.com Abu Sebastian1 ase@zurich.ibm.com"
6786,unknown,"ase@zurich.ibm.com Abbas Rahimi1 abr@zurich.ibm.com 1IBM Research – Zurich, 2ETH Zurich Abstract With the advent of deep learning, progressively larger neural networks have been designed to solve complex tasks. We take advantage of these capacity-rich models to lower the cost of inference by exploiting computation in superposition . To reduce the computational burden per input, we propose Multiple"
6787,unknown,"Output Neural Networks (MIMONets) capable of handling many inputs at once. MIMONets augment various deep neural network architectures with variable bind- ing mechanisms to represent an arbitrary number of inputs in a compositional data structure via fixed-width distributed representations. Accordingly, MIMONets adapt nonlinear neural transformations to process the data structure holistically, lead"
6788,unknown,"leading to a speedup nearly proportional to the number of superposed input items in the data structure. After processing in superposition, an unbinding mechanism recovers each transformed input of interest. MIMONets also provide a dynamic trade-off between accuracy and throughput by an instantaneous on-demand switch- ing between a set of accuracy-throughput operating points, yet within a single se"
6789,unknown,"set of fixed parameters. We apply the concept of MIMONets to both CNN and Transformer architectures resulting in MIMOConv and MIMOFormer, respectively. Empirical evaluations show that MIMOConv achieves ≈2 – 4×speedup at an accuracy delta within [+0.68,−3.18]% compared to WideResNet CNNs on CI- FAR10 and CIFAR100. Similarly, MIMOFormer can handle 2–4 inputs at once while maintaining a high average "
6790,unknown,"while maintaining a high average accuracy within a [−1.07,−3.43]% delta on the long range arena benchmark. Finally, we provide mathematical bounds on the in- terference between superposition channels in MIMOFormer. Our code is available at https://github.com/IBM/multiple-input-multiple-output-nets . 1 Introduction Driven by the successes of deep learning in image and natural language processing ta"
6791,unknown,"large neural network models have been developed to reach state-of-the-art performance [1–4]. These large models, however, increase computational complexity in terms of operation count for every event of input processing. One viable option to reduce the computational cost of processing per input is to create a compositional data structure where a variable number of input items (i.e., values) can be"
6792,unknown,"bound to corresponding protection keys, creating key-value pairs that can coexist and be processed ∗Research conducted at IBM Research – Zurich. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2312.02829v1 [cs.LG] 5 Dec 2023 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Figure 1: MIMONets simultaneously pass multiple inputs through a nonline"
6793,unknown,"Figure 1: MIMONets simultaneously pass multiple inputs through a nonlinear function, e.g., a deep convolutional network (on top) or a Transformer (on bottom). Input samples are bound with high-dimensional keys to project the samples into quasi-orthogonal subspaces. The results of the individual samples are retrieved at the end of the network by unbinding with corresponding keys. concurrently. This"
6794,unknown,"concurrently. This variable-sized data structure can be represented by fixed-width distributed rep- resentations in vector-symbolic architectures (VSAs) [5–7]. In VSAs, the composition of different items in the data structure is based on functional compositionality (i.e., key-value binding), which yields a dimensionality-preserving distributed representation, rather than concatenative composi- tio"
6795,unknown,"one-time application of a function, whereby all input items are jointly transformed, leading to holistic transformation or computation in superposition [8–10]. This concept of computation in superposition can reduce the effective number of operations per input by a factor of the number of input items in the data structure, because the function is applied to the data structure holistically without "
6796,unknown,"the constituent items for individual transformations. However, processing the VSA data structure via computation in superposition has so far been limited to linear maps [8–10]. Motivated by these observations, we make the following contributions: (1) We introduce a principled and transparent approach to Multiple-Input-Multiple-Output Neural Networks (MIMONets) based on VSA, enabling computation in"
6797,unknown,"Networks (MIMONets) based on VSA, enabling computation in superposition for highly nonlinear transformations in neural networks (Section 2). The MIMONets concept can be applied to various architectures, embracing the rich capacity provided by increasingly large models. The resulting network can handle many inputs at once, thus reducing the computational cost per input. We describe and overcome the"
6798,unknown,"and overcome the challenges of computation in superposition, which originates from nonlinear interference of inputs in separate superposition channels. (2) We propose MIMOConv, a realization of MIMONets for deep convolutional neural network (CNN) architectures (Section 3). We provide several strategies for mitigating interference between different superposition channels, including a novel locality"
6799,unknown,"different superposition channels, including a novel locality-preserving binding operation (PWHRR) and isometry-inducing regularization. Empirical evaluations on CIFAR10 and CIFAR100 show that a MIMOConv built on a WideResNet-28-10 [11] can process concurrently two inputs in superposition (≈2×speedup) even with a slightly higher accuracy ( 0.11–0.68% gain), and four inputs ( ≈4× speedup) with a mar"
6800,unknown,"speedup) with a marginal drop (1.24–3.18%) (see Section 5.1). (3) We further extend the concept of MIMONets to Transformer architectures, where the calculation of attention scores poses additional challenges for the computation in superposition paradigm. To this end, we propose MIMOFormer, which relies on a 2D grid binding scheme for computing attention in superposition (Section 4). We derive prob"
6801,unknown,"channel interference and show that our method converges to noise-free attention in the limit of high dimension. Our method succeeds empirically (≥96.52% accuracy) at synthetic sequence modelling tasks [12], while previous work [13] fails ( ≤20.04%). We also provide evaluations on the long range arena (LRA) dataset [14] in Section 5.2 using a MIMOFormer that is based on the Performer architecture ["
6802,unknown,"architecture [15]. Compared to the Performer, MIMOFormer maintains a high average accuracy with a marginal drop of 1.07% and 3.43% when handling two and four inputs at once, respectively. (4) MIMONets allow a dynamic trade-off at inference time between accuracy and speed, i.e., they offer an instantaneous on-demand switching between accuracy-throughput operating points using a 2 37th Conference on"
6803,unknown,2 37th Conference on Neural Information Processing Systems (NeurIPS 2023) single set of fixed parameters. Experimental results in Section 5.1 show that a dynamic MIMOConv can seamlessly operate at various modes ( ≈1 – 4×speedup) while maintaining a high accuracy compared to the best static models (≤1.82% drop). 2 MIMONets enabling Computation in Superposition The central idea behind MIMONets (see 
6804,unknown,"The central idea behind MIMONets (see Figure 1) is to simultaneously pass multiple inputs as a superposition through a nonlinear function fθ parameterized by neural network weights θ. We isolate the individual inputs into separate protected channels by binding them with protection keys resulting in a key-value data structure [5–7]. Concept. Assuming two inputs ( x(1) and x(2)), which can be generi"
6805,unknown,"Concept. Assuming two inputs ( x(1) and x(2)), which can be generic embeddings either from images or natural language, we define a unique high-dimensional key (a(1) and a(2)) for each protected channel, drawn randomly at initialization. Owing to the Blessing of Dimensionality, randomly drawn high-dimensional vectors are quasi-orthogonal with high probability (see Appendix A). Consequently, binding"
6806,unknown,"binding (⊙) the inputs with these keys yields quasi-orthogonal key-value pairs ( x(1) ⊙a(1) and x(2) ⊙a(2)), which enables one to superpose the pairs with low interference: s= a(1) ⊙x(1) + a(2) ⊙x(2). (1) As discussed in Appendix A, sadmits a noisy retrieval of x(1) and x(2) through unbinding: ˆx(1) = a(1) ⃝∗s= a(1) ⃝∗a(1) ⊙x(1) + a(1) ⃝∗a(2) ⊙x(2) = x(1) + noise (2) To accelerate computing, inspi"
6807,unknown,"To accelerate computing, inspired by the above-mentioned noisy retrieval, we pass the superposition sthrough a nonlinear function fθ with parameters θ, such as a neural network, before retrieval. The quasi-orthogonality of the bound inputs allows processing each in a separate protected subspace—all with a single function call. To be able to recover the first processed sample fθ(x(1)) from fθ(s), w"
6808,unknown,"aim to find an unbinding key ˜a(1) for which ˜a(1) ⃝∗fθ(s) ≈˜a(1) ⃝∗fθ ( a(1) ⊙x(1) ) + ˜a(1) ⃝∗fθ ( a(2) ⊙x(2) ) (3) ≈fθ ( x(1) ) + ˜a(1) ⃝∗fθ ( a(2) ⊙x(2) ) . (4) The first approximation holds exact for linear fθ. As discussed in Section 3, a nonlinear fθ can still be encouraged to allow such an approximation through appropriate weight regularization techniques and well-suited activation functio"
6809,unknown,"estimation (Eq. (4)) can be achieved. Consequently, matching binding and unbinding keys (a(i) and ˜a(i)) that confirm the approximation (Eq. (3) and (4)) set up a protected channel through the nonlinear function fθ(s). Appendix A lists the design choices of the adopted VSA, which define the operations of key-value binding and unbinding, for all MIMONet variants presented in this work. In the case "
6810,unknown,"of image embeddings, we use circular convolution [6] for binding and Matrix Binding of Additive Terms (MBAT) [16] for unbinding. In the case of sequence tokens, we bind and unbind using the Hadamard product [17]. Binding and unbinding keys are always data-independent, i.e., they depend only on the index of the protected channel. See [18] for alternative binding and unbinding options. Dynamic Infer"
6811,unknown,"Dynamic Inference. Setting up N protected channels through a neural network fθ gives almost a speedup of N×due to most computations taking place in superposition. However, as is explored empirically, increasing N adds inter-channel noise leading to a decrease in predictive accuracy. If a fixed trade-off is unsatisfactory, one can build a dynamic model capable of running a superposition of one up t"
6812,unknown,"one up to N different inputs. By inserting the same input into multiple channels and averaging the output, one effectively forms an in-network ensemble, similar to [19,20]. Using all protected channels for different inputs leads to a fast but less accurate model, whereas using all protected channels for the same input yields a slower but accurate ensemble model. By partitioning the superposition c"
6813,unknown,"channels on demand, arbitrary configurations in between may be reached. Note that our method can instantaneously adapt to the current computational demand, without loading different model weights from the main memory. To perform across slow and fast configurations, the model should randomly switch between them during training. See Appendix A for a more detailed explanation. 3 37th Conference on Ne"
6814,unknown,3 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Figure 2: MIMOConv configured with N=3 channels. The input images are passed individually through the first convolutional layer before binding each feature value with a channel-specific high-dimensional key. The key-value pairs are superposed yielding a dimensionality-preserving composition and passed through the rest of the
6815,unknown,"composition and passed through the rest of the CNN layers. The output is unbound with corresponding keys, and the unbound representations are classified separately with a shared fully-connected layer. 3 MIMOConv This section presents how the MIMONets concept, introduced in Section 2, can be applied to construct a multiple-input-multiple-output CNN (MIMOConv) capable of simultaneously processing mu"
6816,unknown,"a multiple-input-multiple-output CNN (MIMOConv) capable of simultaneously processing multiple images in superposition. The MIMOConv architecture is shown in Figure 2. Multiple input samples (N) are passed through the first convolutional layer, bound with a unique high-dimensional key based on Holographic Reduced Representations (HRR) [6], and superposed by element-wise addition. After passing the "
6817,unknown,"passing the superposed tensors through the network’s main CNN layers, we obtain a combined feature vector with information on all inputs. By unbinding with separately learned keys based on MBAT [16], which amounts to a matrix multiplication, we extract the individual processed information, which is then passed through a final fully-connected layer to produce logits for classification. In the follo"
6818,unknown,"we introduce our three main contributions that lead to a highly accurate MIMOConv. Augmenting CNNs with locality-preserving variable bindings.We embrace a principled and transparent binding mechanism from HRR. Accordingly, binding is performed using circular con- volution with a binding key of dimension D, drawn from an i.i.d. Gaussian distribution with zero mean and 1/D variance. Instead of convo"
6819,unknown,"mean and 1/D variance. Instead of convolving the flattened image tensor, we repeatedly apply circular convolution between the binding key and each pixel volume spanning across the feature maps (D×1×1). This binding operation, which we call position-wise HRR (PWHRR), is translation equivariant and maintains locality, an essential property for subsequent layers with limited receptive fields. More co"
6820,unknown,"(a(k) ⊙x(k)):,w,h = a(k) ∗x(k) :,w,h (5) (˜a(k) ⃝∗h):,w,h = ˜a(k) ·h:,w,h, (6) with image tensors x(k) ∈RD×W×H, hidden representation h∈RD′×W×H, binding key a(k) ∈RD and unbinding key ˜a(k) ∈RD′×D′ . Here, D,D′,W, and H denote the hidden dimension at binding, the hidden dimension at unbinding (generally differs from D), image width, and image height, respectively. ∗is the circular convolution, ·th"
6821,unknown,"channel. Unbinding is applied after the global (average) pooling to reduce computational costs. The binding keys can be either learned or fixed during training (see ablation study in Appendix E). Embracing high dimensional embeddings. According to the Blessing of Dimensionality (see Appendix A), random vectors quickly become quasi-orthogonal as their dimension increases. To reduce interference bet"
6822,unknown,"reduce interference between protected channels, we increase the number of feature maps by adopting Wide Residual Networks [11], the most commonly used CNN architecture to achieve state-of-the-art accuracy on CIFAR100 [21]. The input tensors are passed individually through the first convolutional layer before being superposed in a suitably high dimensional space. We set the number of feature maps a"
6823,unknown,"after the first convolutional layer to D=64. This is 4×more than the standard Wide-ResNet-28 [11], which results in improved training stability at a marginally higher compute cost (see Section 5.1). 4 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Encouraging isometric layers. We aim to preserve the quasi-orthogonality of our protected channels as the superposition is pass"
6824,unknown,"are used and each subfunction gθ of type (strided) spatial convolution or activation function is made approximately inner-product preserving, i.e., ⟨gθ(x),gθ(y)⟩≈⟨ x,y⟩. (7) Inspired by [22], we deploy a regularization to the CNN weights and use a parametric ReLU [23] activation function, a learnable affine combination between identity and ReLU. Those adjustments lead to a near-isometric network. "
6825,unknown,"L(W) =γ 2 Conv(W,W) −δCo 2 F where δCo :,:,j,l=ICo×Co ·1j,l=⌊k 2 ⌋, (8) L(WT) =γ 2 Conv(WT,WT) −δCi 2 F where δCi :,:,j,l=ICi×Ci ·1j,l=⌊k 2 ⌋, (9) with γas hyperparameter. We use L(W) if Ci >Co, else L(WT). See Appendix B for more details. Figure 3: MIMOFormer layer applying compu- tation in superposition to single-head FA VOR+S attention and to the MLP. This example passes four channels ("
6826,unknown,"channels (N·M=2·2=4) to the FA VOR+S attention (see Eq. (19)). The individual outputs are retrieved by unbinding after the MLP. The skip connection superposes the individual inputs for alignment, us- ing the same protection keys as in unbinding. 4 MIMOFormer This section presents MIMOFormer, which ap- plies the principles of computation in superposi- tion to dot-product self-attention [24]. Figure"
6827,unknown,"shows a MIMOFormer layer with four protected channels, consisting of a single-head2 attention block, a concatenation, a linear layer, an MLP, and a skip connection. Merely superposing protected attention keys 3 and queries does not yield the desired result. As discussed in Appendix F, with scalar atten- tion scores between pairs of tokens, vanilla dot- product attention irreversibly combines atten"
6828,unknown,"scores of separate protected channels, effectively blurring the attention weights. By building in- stead on linear Transformers [15] [25], attention scores are not collapsed to scalars, thus enabling computation in superposition. Despite being compatible with other linear trans- formers (such as DPFP [25]), for concreteness we discuss changes to the Performer’s FA VOR+ attention block [15]. Enabli"
6829,unknown,"superposition, we label the block as FA VOR+S. Given attention keys (kj)L j=1, queries (qi)L i=1, and values (vj)L j=1, FA VOR+ estimates dot- product attention at sequence index ithrough oi = L∑ j=1 vj exp ( ⟨kj,qi⟩/ √ D ) ∑L l=1 exp ( ⟨kl,qi⟩/ √ D )≈ 1 Bi [L∑ j=1 vjϕ(kj)T ]    A ×ϕ(qi), (10) where ϕ: RD →RR + approximates the softmax kernelexp(⟨kj,qi⟩/ √ D) as an explicit inner product. Sinc"
6830,unknown,"Since computing ϕhas a computational complexity of O(DR), the construction of A∈RD×R takes O(LDR). Equally, multiplying A×ϕ(qi) ∀itakes O(LDR). Thus, FA VOR+ breaks the quadratic dependence on sequence length Lof attention. The denominator Bi is discussed in Appendix C. 2The application to multi-head attention is straightforward; empirical results are shown in Section 5.2. 3Protection keys are den"
6831,unknown,"5 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Our extension, FA VOR+S, computes attention in superposition, yielding a square-root speedup in the number of protected channels. It encodes them in an M ×N grid, distributing the computational burden among the setup of the value-key matrix Aand its product with ϕ(qi). Importantly, in the limit of high dimensional projection"
6832,unknown,"limit of high dimensional projections Dour mechanism converges to exact self-attention, completely separating the protected channels from one another. In the following, we assume the token values vj, keys kj, and queries qj to be in protected subspaces: v(m,n) j := v(m,n) j ⊙a(m,n), k (m,n) j := k (m,n) j ⊙a(m,n), q (m,n) j := q(m,n) j ⊙a(m,n), (11) where a(m,n) are i.i.d. bipolar vectors of Radem"
6833,unknown,"where a(m,n) are i.i.d. bipolar vectors of Rademachers [17] and (m,n) denotes a channel. MIMOFormer benefits from the low time complexity (O(D)) of the Hadamard product, especially since binding and unbinding are performed in every MIMOFormer layer. Our derivations rely on two estimates: ϕ(k)Tϕ(q) P ≈exp ( ⟨k,q⟩/ √ D ) ⟨ N∑ w=1 k(u,w) j , M∑ t=1 q(t,n) i ⟩ H ≈⟨k(u,n) j , q(u,n) i ⟩    intended"
6834,unknown,"√ D ) ⟨ N∑ w=1 k(u,w) j , M∑ t=1 q(t,n) i ⟩ H ≈⟨k(u,n) j , q(u,n) i ⟩    intended signal (12) The approximation P, which improves with increasing R= dim(ϕ(qi)), is due to FA VOR+ and is quantified in [15]. On the other hand, the approximation H follows from: Inter-channel distortion. The probability that inter-channel attention distorts the intended signal of the dot-product by a factor outsid"
6835,unknown,"exponentially w.r.t.Dα2 cos2(∡(k (u,n) j ,q(u,n) i ))/(NM −1)2. See Appendix D for the full theorem. 4.1 FA VOR+S: Computing self-attention in superposition We first discuss separately how to use a one-dimensional grid to carry out either the multiplication (×) or the construction of Ain superposition. Finally, the integration into a 2D grid will be shown. Placing multiple queries in superposition"
6836,unknown,"Placing multiple queries in superposition. We set up channels 1,...,M by simultaneously generating a superposition in the construction of As and of the queries to be applied. To avoid inter-channel attention we superpose value-key tensor products, i.e., we do not construct tensor products between superposed values and superposed keys: Si = [L∑ j=1 M∑ u=1 v(u) j ϕ(k(u) j )T ]    As ×ϕ( M∑ t=1 q"
6837,unknown,"i ) = L∑ j=1 M∑ u=1 v(u) j ( ϕ(k(u) j )T ϕ( M∑ t=1 q(t) i ) ) (13) P ≈ L∑ j=1 M∑ u=1 v(u) j exp ( ⟨k(u) j , M∑ t=1 q(t) i ⟩/ √ D )H ≈ L∑ j=1 M∑ u=1 v(u) j exp ( ⟨k(u) j , q(u) i ⟩/ √ D ) (14) = M∑ u=1 L∑ j=1 v(u) j exp ( ⟨k(u) j , q(u) i ⟩/ √ D )    unnormalized oi of channel u . (15) We obtain a superposition of bound output values; hence, the cost of computingA×ϕ(qi) for all i is amortized a"
6838,unknown,"is amortized across channels. However, the construction complexity of As is increased M-fold to O(LDR·M), hence the complexity per protected channel remains at O(LDR). Constructing value-key tensor products in superposition.Next, we demonstrate a value-key tensor product (As) shared across all channels, but with a mere O(LD(R+ N)) setup complexity. In O(LND) we compute ∑N q=1 v(q) j and ∑N w=1 k(w"
6839,unknown,"j for all j. These are then (re)used to build As. S(n) i = [L∑ j=1 (N∑ q=1 v(q) j ) ϕ( N∑ w=1 k(w) j )T ]    As ×ϕ(q(n) i ) = L∑ j=1 N∑ q=1 v(q) j ( ϕ( N∑ w=1 k(w) j )Tϕ(q(n) i ) ) (16) 6 37th Conference on Neural Information Processing Systems (NeurIPS 2023) P ≈ L∑ j=1 N∑ q=1 v(q) j exp ( ⟨ N∑ w=1 k(w) j , q(n) i ⟩/ √ D )H ≈ L∑ j=1 N∑ q=1 v(q) j exp ( ⟨k(n) j , q(n) i ⟩/ √ D ) (17) = L∑ j=1 v"
6840,unknown,"j=1 v(n) j exp ( ⟨k(n) j , q(n) i ⟩/ √ D )    unnormalized oi of channel n + ∑ q̸=n L∑ j=1 v(q) j exp ( ⟨k(n) j , q(n) i ⟩/ √ D ) .    noise in separate protected subspace (18) The output contains the nth channel together with noise. The operation As ×ϕ(q(n) i ), which takes O(LDR), must be repeated N times to produce outputs for all N channels, causing a bottleneck. Simultaneous superposi"
6841,unknown,"Simultaneous superposition of queries and value-key tensor products using a 2D grid.Finally, we combine the two previously described approaches to encode the superposition channels in a 2D grid of size N ×M. We multiply a constant matrix (As) with features derived from a superposition of queries to get the superposition vector S(n) i S(n) i = [L∑ j=1 M∑ u=1 (N∑ q=1 v(u,q) j ) ϕ( N∑ w=1 k(u,w) j )T"
6842,unknown,"]    construct As in O(LMD(R+N)) × ϕ( M∑ t=1 q(t,n) i ).    construct ∀i,nin O(LND(R+M)) (19) Computing the multiplication ×∀i,n takes O(LNDR). If we set M=N, we can evaluate S(n) i ∀i,n using only O(LNDR + LN2D) instead of the usual O(LDR·N2). Thus, one may achieve a speedup of O(min( √ N2,R)) compared to FA VOR+. SinceRis normally in the hundreds [15], we can assume improvements of O( √ "
6843,unknown,"√ N2) for reasonably large N2 = M ·N. Eq. (19) simplifies to: S(n) i = ∑ j,u,q v(u,q) j ( ϕ( ∑ w k(u,w) j )Tϕ( ∑ t q(t,n) i ) )P ≈ ∑ j,u,q v(u,q) j exp (⟨∑ w k(u,w) j , ∑ t q(t,n) i ⟩ √ D ) (20) H ≈ L∑ j=1 M∑ u=1 N∑ q=1 v(u,q) j exp ( ⟨k(u,n) j , q(u,n) i ⟩/ √ D ) (21) = M∑ u=1 L∑ j=1 v(u,n) j exp (⟨k(u,n) j , q(u,n) i ⟩√ D )    unnormalized oi of channel (u,n) + ∑ q̸=n M∑ u=1 L∑ j=1 v(u,q) j "
6844,unknown,"L∑ j=1 v(u,q) j exp (⟨k(u,n) j , q(u,n) i ⟩√ D ) .    noise in separate protected subspace (22) 4.2 Integrating FA VOR+S into MIMOFormer As is apparent in Eq. (19), the query superposition is along a different axis ( M) than the key and value superpositions (N). The output of attention, however, is only superposed along a single axis (M). To be able to set up superpositions along both axes (M "
6845,unknown,"channels (i.e., keys, queries, and values) in separation, i.e., not superposed, at the interface between MIMOFormer layers. We present two variants of MIMOFormer with different speedups. The first computes in superposition exclusively during the attention mechanism. The individual tokens of the channel (n,m) are directly retrieved from S(n) i by unbinding with the key ˜a(n,m) = a(n,m), and the rem"
6846,unknown,"and the remaining computational steps within FA VOR+S are performed separately. The second (and faster) MIMOFormer instance additionally performs the concatenation, the linear layer, as well as the MLP in superposition (shown in Figure 3). Unlike in the first variant, the skip connection around the attention block must account for the introduced superposition. To allow a potential embedding dimens"
6847,unknown,"potential embedding dimension mismatch, we instantiate two different sets of randomly drawn bipolar keys: one for the skip connection and post-MLP unbinding, and one for FA VOR+S binding. All keys are frozen during training; it is up to the trainable weights in the linear layer after concatenation to find the relationship between the binding and unbinding keys. The function ϕin the self-attention "
6848,unknown,"The function ϕin the self-attention block consists of an R=256 dimensional projection and a ReLU activation [15]. Appendix C provides theoretical justification for using ReLU in ϕand its benefits for MIMOFormer. Empirically, ReLU shows better numerical stability than unbiased softmax FA VOR+. 7 37th Conference on Neural Information Processing Systems (NeurIPS 2023) 5 Empirical Results This section"
6849,unknown,"This section evaluates the proposed MIMONets on various model architectures and benchmarks. Appendix E and Appendix F describe the experimental setup for MIMOConv and MIMONets, respectively. All experiments are repeated five times with a different random seed. We report the mean and standard deviations of accuracy to account for variability in training. 5.1 MIMOConv CIFAR10 and CIFAR100.Our main b"
6850,unknown,"5.1 MIMOConv CIFAR10 and CIFAR100.Our main baseline, which we adapt to MIMOConv, is a reproduced WideResNet-28-10 [11], i.e., a 28-layer CNN with the number of feature maps of each convolutional layer enlarged by 10×. Moreover, as a stronger baseline, we include the isometry regularization in the training of WideResNet-28-10, and call the resulting network WideIsoNet-28-10. Both baselines demand 5"
6851,unknown,"demand 5.251 GMACs (Giga multiply-accumulate operations) per sample. MIMOConv’s inference complexity per sample is 5.335 GMACs for N=1, 2.667 GMACs for N=2, and 1.334 GMACs for N=4; hence, we get a ≈N×speedup despite not accelerating the first and last layer. See Appendix E. Table 1 shows the accuracy of the static MIMOConv, which is exclusively trained to support either 1, 2, or 4 channels. The M"
6852,unknown,"2, or 4 channels. The MIMOConv with one channel (N=1) outperforms both baselines, which may be attributed to regularizing effects of the key-value binding. MIMOConv withN=2 channels still outperforms WideResNet-28-10, while reducing the inference complexity by 2×. The complexity can be further reduced by increasing the number of superpositions to N=4 at a slight accuracy drop of ≤3.18%, compared t"
6853,unknown,"of ≤3.18%, compared to WideResNet-28-10. Next, we evaluate the dynamic partitioning of the superposition channels to select a speed-accuracy operating point instantaneously, which is a main feature of our approach and sets it apart from other static approaches that opt for a fixed performance point like model downsizing, quantization aware training, and pruning (see Appendix A for a discussion). W"
6854,unknown,"training, and pruning (see Appendix A for a discussion). We set up a model with four channels, but evaluate its performance in different configurations: a fast (4 inputs/pass), a normal (2 inputs/pass), and a slow mode (1 input/pass). The fast mode maps each input to one channel; the medium mode distributes two inputs over pairs of channels; and the slow mode uses all channels for the same input. "
6855,unknown,"The models are trained on 80% of the batches in fast mode and on 20% of the batches in slow mode. Appendix E provides more details on the trade-off between fast and slow mode training. As Table 1 shows, a single dynamic model can seamlessly switch between operation points while maintaining a high accuracy compared to the static models (≤1.82% drop). Table 1: Average accuracy (%) of WideResNet-28-1"
6856,unknown,"Table 1: Average accuracy (%) of WideResNet-28-10 variants and our MIMOConv. Static models are trained to process N inputs in one pass, speeding up inference by N×. Dynamic models are trained with a variable number of inputs (N=1–4), and can process a variable number of inputs per pass. We report the average accuracy ±the standard deviation over five runs with different seeds. CIFAR10 CIFAR100 # i"
6857,unknown,# inputs/pass 1 2 4 1 2 4 WideResNet-28-10 96.82±0.06 n.a. n.a. 81.62±0.07 n.a. n.a. WideIsoNet-28-10 97.31±0.11 n.a. n.a. 82.38±0.20 n.a. n.a. MIMOConv static (N=1) 97.49±0.08 n.a. n.a. 83.19±0.17 n.a. n.a. MIMOConv static (N=2) n.a. 96.93±0.13 n.a. n.a. 82.30±0.19 n.a. MIMOConv static (N=4) n.a. n.a. 95.58±0.23 n.a. n.a. 78.44±0.30 MIMOConv dynamic (N=1–4) 97.13±0.11 96.41±0.14 95.43±0.07 82.52±
6858,unknown,MIMOConv dynamic (N=1–4) 97.13±0.11 96.41±0.14 95.43±0.07 82.52±0.09 80.48±0.08 78.19±0.10 The detailed ablation study in Appendix E makes the following findings: (1) isometry regularization improves accuracy for any number of channels; (2) training MIMOConv for more epochs closes the performance gap to the single-input baseline; (3) an appropriate number of feature maps (32 or 64) in the first la
6859,unknown,"(whereas unbinding keys are never frozen). 8 37th Conference on Neural Information Processing Systems (NeurIPS 2023) MNIST and SVHN. Figure 4 compares MIMO- Conv with DataMux [13] on the MNIST dataset. Even with a trivial downsizing for fair compar- ison from a 28-layer very-wide ( 10×) ResNet to a 10-layer narrow (1×) network, MIMOConv scales much better to high superposition chan- nels (N) than "
6860,unknown,"nels (N) than DataMUX does. Indeed, our model shows an accuracy of 80.4% against their 52.9% in case of N=16 superposition channels (high- est number of channels reported by DataMUX for vision tasks), despite being computationally cheaper (0.47 MMAC/s vs. 0.65 MMAC/s). Also, DataMux’s binding overhead results in a mere 1.35×reduction in MACs compared to our10.9× as N goes from 1 to 16. Ergo, our m"
6861,unknown,"better in accuracy and throughput as N increases. 1 2 4 8 16 Number of superposition channels (N) 60 70 80 90 100Accuracy (%) MIMOConv-10-1 DataMUX: CNN+Nonlinear (8x) Figure 4: Classification accuracy (%) on MNIST for downsized model. Finally, we tested MIMOConv on the SVHN dataset. Despite limited hyperparameter tuning, MIMOConv achieves a high accuracy of 97.17% (N=1), and can maintain its perf"
6862,unknown,"larger superpositions (97.05% and 96.84% for N=2 and N=4, respectively). 5.2 MIMOFormer LRA. We evaluate MIMOFormer on five tasks from LRA [14], and compare against the vanilla Transformer [24] and the Performer [15] using FA VOR+ attention with ReLU projection. Moreover, we also consider wide Transformer variants [26], consisting of only one layer but as many heads as their deep counterparts. Tas"
6863,unknown,"their deep counterparts. Task-specific architectures and training hyperparameters are kept the same for the Performer and the MIMOFormer (see Appendix F). Table 2: Test accuracy (%) on the long range arena (LRA). MIMOFormer uses an equal number of query superpositions (M) and value-key superpositions (N), i.e., N=M. Computation in superposi- tion is performed either in attention only (att.) or in "
6864,unknown,"tion is performed either in attention only (att.) or in both attention and MLP (att.+MLP). Lis the number of layers, H the number of heads, and ∗indicates curriculum learning. ListOps Text Retrieval Image Pathfinder Avg. Deep models L=6, H=8 L=6, H=8 L=4, H=4 L=3, H=4 L=4, H=8 Transformer [24] 36.37 64 .27 57 .46 42 .44 71 .40 53 .39 Performer [15] 18.01 65 .40 53 .82 42 .77 77 .05 51 .41 Performe"
6865,unknown,"Performer (reproduced) 38.94±0.23 65.70±0.31 81.58±0.18 40.14±0.86 73.82±0.78 60.04±0.47 MIMOFormer (N=2, att.) 38.08±0.21 65.00±0.28 79.37±0.81 38.21±0.63 72.36±0.54 58.61±0.49 MIMOFormer (N=2, att.+MLP)37.65±0.33 64.39±0.22 76.02±0.27 33.85±0.55 67.98±0.47 55.98±0.37 MIMOFormer (N=4, att.) 37.22±0.33 64.59±0.14 60.99±9.06 28.16±0.08 55.50±4.95 49.29±2.91 MIMOFormer (N=4, att.)∗ 37.64±0.73 64.46±"
6866,unknown,"MIMOFormer (N=4, att.)∗ 37.64±0.73 64.46±0.15 74.38±0.82 30.52±0.77 67.10±0.45 54.82±0.58 MIMOFormer (N=4, att.+MLP)17.74±0.63 60.71±5.14 72.20±0.28 24.01±0.47 50.33±0.16 45.00±1.34 Wide models L=1, H=48 L=1, H=48 L=1, H=16 L=1, H=12 L=1, H=32 Performer (reproduced) 39.40±0.51 65.73±0.32 83.67±0.25 41.67±0.44 74.11±0.33 60.93±0.37 MIMOFormer (N=2, att.) 38.90±0.53 65.39±0.18 81.27±0.28 40.25±0.21 "
6867,unknown,"MIMOFormer (N=2, att.) 38.90±0.53 65.39±0.18 81.27±0.28 40.25±0.21 73.51±0.23 59.86±0.29 MIMOFormer (N=2, att.+MLP)37.59±0.17 64.64±0.25 78.30±0.32 36.69±0.76 68.22±0.18 57.09±0.34 MIMOFormer (N=4, att.) 37.71±0.24 64.22±0.14 74.99±0.36 35.43±0.60 69.52±0.40 56.37±0.35 MIMOFormer (N=4, att.)∗ 37.68±0.36 64.56±0.25 76.37±0.50 35.53±0.48 73.37±0.22 57.50±0.36 MIMOFormer (N=4, att.+MLP)18.52±0.98 63."
6868,unknown,"MIMOFormer (N=4, att.+MLP)18.52±0.98 63.53±0.12 74.30±0.26 26.54±0.28 56.33±0.17 47.84±0.36 Owing to an improved training setup, our replicated deep and wide Performer baselines substantially outperform the results reported in [14] (see Table 2). Moreover, MIMOFormer enables accurate computation in superposition for both deep and wide attention models. The performance drop is less pronounced in wi"
6869,unknown,"pronounced in wide models (only 1.07% drop compared to Performer with N=2, att.), which may be attributed to the larger number of heads, increasing the effective dimension (Dtot = H·Dhead). When computing both attention and the MLP in superposition (att.+MLP), we observe better scaling (in N) for wide models. Also, MIMOFormer reduces the gap to the baseline as the number of epochs increases (see A"
6870,unknown,"9 37th Conference on Neural Information Processing Systems (NeurIPS 2023) To stabilize training in the case of N=4, we implemented a curriculum training procedure where the number of superpositions is reduced to N’=N/2 during a warmup phase (1/6th of the training steps), improving the average accuracy of MIMOFormer in both wide and deep models. Comparing against a reproduced DataMUX [13], MIMOForm"
6871,unknown,"Comparing against a reproduced DataMUX [13], MIMOFormer (att.) outperforms it on ListOps (38.08% vs. 30.54% accuracy) when using models of similar size and N=2, see Appendix F. Table 3: Accuracy (%) on synthetic sequence modelling. Architecture Attention Associative recall [12] Induction head [12] Transformer Softmax 98.48±1.87 100±0.0 Performer FA VOR+ 96.32±6.26 31.58±33.67 MIMOFormer (N=2, att."
6872,unknown,"MIMOFormer (N=2, att.) DPFP [25]93.64±12.66 98.56±0.86 DataMUX (N=2) [13] Softmax 20.04±1.72 6.06±2.24 Synthetic sequence modeling. Table 3 reports the accuracy on two synthetic sequence modeling tasks, which Trans- former alternatives such as S4 [27] have difficulties solving [12]. On these more nuanced NLP tasks, the accuracy of Data- MUX [13] drops to 20.04% and 6.06% for N=2 despite significan"
6873,unknown,"for N=2 despite significant efforts in training, while MIMOFormer, at a score of 96.52% and 99.40% respectively, succeeds. We attribute this difference in performance to attention score blurring in DataMux, discussed in Appendix F. Contrastingly, our method converges to exact attention without blurring. It is versatile and can be adjusted to other linear Transformers such as DPFP [25], achieving a"
6874,unknown,"DPFP [25], achieving a score of 93.64% and 98.56%. 6 Related Work So far, superposition principles have been applied in order to store the weights of multiple models in a single neural network [28–30], to circumvent catastrophic forgetting in continual learning [31, 32], and to render symbolic reasoning tractable [33]. To address privacy concerns when running remote inference, single inputs were b"
6875,unknown,"inference, single inputs were bound with random channels to implement pseudo-encryption [34]. Recently in [35], HRR was used to define an unconventional version of self-attention, whose attention scores are processed to a diagonal matrix. The value vectors are scaled according to their importance in the sequence instead of being combined in a weighted sum. In contrast to us, none of these works su"
6876,unknown,"superpose multiple inputs into a data structure to speed up computation. In [19, 20], an ensemble of CNN models was fit into one network. However, by only broadcasting a single input over the channels and by averaging all the outputs, this approach collapses to a single-input-single-output (SISO) network. On the contrary, we explore using protected channels for different inputs at inference, resul"
6877,unknown,"There has also been a line of work to accelerate Transformers using inputs in superposition [13] [36]. DataMux [13] claims to retain high performance for language understanding tasks, even when using up to 40 inputs in superposition. However, none of the reported tasks require attention layers at all [37]. In Section 5.2 we show failure of their method when actual attention is required (see also A"
6878,unknown,"Appendix F). MUX-PLMs [36] improves on DataMux with contextual binding and replaces token prefixes with unbinding keys, but does not address the blurry attention mechanism. In contrast to DataMUX and MUX-PLMs, our work approximates true attention and our theoretical derivations show convergence to actual dot-product attention as the dimension of attention projections increases, giving us an even s"
6879,unknown,"giving us an even stronger case for applicability to large language models. 7 Conclusion We present MIMONets that simultaneously process multiple inputs by performing computation in superposition. MIMONets bind arbitrary inputs with high-dimensional keys, which projects them to orthogonal subspaces that, together with near-isometric subfunctions, guarantee low interference through all nonlinear la"
6880,unknown,"through all nonlinear layers. Unbinding with (learned) keys can safely retrieve information on individual channels. We provide two MIMONets instances, MIMOConv and MIMOFormer, that show the effectiveness of computation in superposition through two dominant operations in neural network architectures: convolution and attention. Further investigations could explore the MIMO- capability of architectur"
6881,unknown,"input modalities. MIMONets could be suitable candidates to accelerate dynamically and on-demand the inference of foundation models [38]. 10 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Acknowledgement This work is supported by the Swiss National Science foundation (SNF), grant 200800. We thank Dario Bolli for conducting initial experiments and Aleksandar Terzi´c for help"
6882,unknown,"References [1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” arXiv preprint arXiv:1810.04805, 2018. [2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models are few-shot learners,” Advances in Neural Information Processing Sy"
6883,unknown,"Information Processing Systems (NeurIPS), vol. 33, pp. 1877–1901, 2020. [3] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer: Hierarchical vision transformer using shifted windows,” in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2021, pp. 10 012–10 022. [4] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell, "
6884,unknown,"[4] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A convnet for the 2020s,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 11 976–11 986. [5] R. W. Gayler, “Vector symbolic architectures answer Jackendoff’s challenges for cognitive neuroscience,” in Joint International Conference on Cognitive Science (ICCS/ASCS), 2003."
6885,unknown,"[6] T. A. Plate, “Holographic reduced representations,” IEEE Transactions on Neural Networks, vol. 6, no. 3, pp. 623–641, 1995. [7] P. Kanerva, “Hyperdimensional computing: An introduction to computing in distributed rep- resentation with high-dimensional random vectors,” Cognitive Computation, vol. 1, no. 2, pp. 139–159, 2009. [8] J. Neumann, “Learning holistic transformation of HRR from examples"
6886,unknown,"[8] J. Neumann, “Learning holistic transformation of HRR from examples,” in International Conference on Knowledge-Based Intelligent Engineering Systems and Allied Technologies (KES), 2000, pp. 557–560. [9] ——, “Learning the systematic transformation of holographic reduced representations,” Cogni- tive Systems Research, vol. 3, no. 2, pp. 227–235, 2002. [10] D. Kleyko, M. Davies, E. P. Frady, P. Ka"
6887,unknown,"[10] D. Kleyko, M. Davies, E. P. Frady, P. Kanerva, S. J. Kent, B. A. Olshausen, E. Osipov, J. M. Rabaey, D. A. Rachkovskij, A. Rahimi, and F. T. Sommer, “Vector symbolic architectures as a computing framework for emerging hardware,”Proceedings of the IEEE, vol. 110, no. 10, pp. 1538–1571, 2022. [11] S. Zagoruyko and N. Komodakis, “Wide residual networks,” in Proceedings of the British Machine Vis"
6888,unknown,"Machine Vision Conference (BMVC). BMV A Press, September 2016, pp. 87.1–87.12. [12] D. Y . Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re, “Hungry hungry hippos: Towards language modeling with state space models,” inThe Eleventh International Conference on Learning Representations (ICLR), 2022. [13] V . Murahari, C. Jimenez, R. Yang, and K. Narasimhan, “DataMUX: Data multiplexing for ne"
6889,unknown,"neural networks,” Advances in Neural Information Processing Systems (NeurIPS), vol. 35, pp. 17 515–17 527, 2022. [14] Y . Tay, M. Dehghani, S. Abnar, Y . Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler, “Long range arena: A benchmark for efficient transformers,” in International Conference on Learning Representations (ICLR), 2021. [15] K. M. Choromanski, V . Likhosherstov, D. Do"
6890,unknown,"[15] K. M. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser et al., “Rethinking attention with performers,” in International Conference on Learning Representations (ICLR), 2020. [16] S. I. Gallant and T. W. Okaywe, “Representing objects, relations, and sequences,” Neural Computation, vol. 25, no. 8, pp. 2038–2078, 2013. [17] R."
6891,unknown,"[17] R. W. Gayler, “Multiplicative binding, representation operators & analogy,” in Advances in Analogy Research: Integration of Theory and Data from the Cognitive, Computational, and Neural Sciences, 1998. 11 37th Conference on Neural Information Processing Systems (NeurIPS 2023) [18] D. Kleyko, D. Rachkovskij, E. Osipov, and A. Rahimi, “A survey on hyperdimensional comput- ing aka vector symboli"
6892,unknown,"ing aka vector symbolic architectures, part I: Models and data transformations,”ACM Computing Surveys, vol. 55, no. 6, 2022. [19] M. Havasi, R. Jenatton, S. Fort, J. Z. Liu, J. Snoek, B. Lakshminarayanan, A. M. Dai, and D. Tran, “Training independent subnetworks for robust prediction,” inInternational Conference on Learning Representations (ICLR), 2021. [20] A. Ramé, R. Sun, and M. Cord, “Mixmo: M"
6893,unknown,"[20] A. Ramé, R. Sun, and M. Cord, “Mixmo: Mixing multiple inputs for multiple outputs via deep subnetworks,” in Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR), 2021, pp. 823–833. [21] A. Krizhevsky, “Learning multiple layers of features from tiny images,” University of Toronto, 2009. [22] H. Qi, C. You, X. Wang, Y . Ma, and J. Malik, “Deep isometric learning for vi"
6894,unknown,"in International Conference on Machine Learning (ICML). PMLR, 2020, pp. 7824–7835. [23] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification,” in Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015, pp. 1026–1034. [24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. "
6895,unknown,"[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”Advances in Neural Information Processing Systems (NeurIPS), vol. 30, 2017. [25] I. Schlag, K. Irie, and J. Schmidhuber, “Linear transformers are secretly fast weight pro- grammers,” in International Conference on Machine Learning (ICML) . PMLR, 2021, pp. 9355–9366"
6896,unknown,"[26] J. R. Brown, Y . Zhao, I. Shumailov, and R. D. Mullins, “Wide attention is the way forward for transformers?” in NeurIPS ’22 Workshop on All Things Attention: Bridging Different Perspectives on Attention, 2022. [27] A. Gu, K. Goel, and C. Re, “Efficiently modeling long sequences with structured state spaces,” in International Conference on Learning Representations (ICLR), 2022. [28] B. Cheung"
6897,unknown,"[28] B. Cheung, A. Terekhov, Y . Chen, P. Agrawal, and B. Olshausen, “Superposition of many models into one,” Advances in Neural Information Processing Systems (NeurIPS), vol. 32, 2019. [29] M. Hersche, P. Rupp, L. Benini, and A. Rahimi, “Compressing subject-specific brain-computer interface models into one model by superposition in hyperdimensional space,” in 2020 Design, Automation & Test in Eur"
6898,unknown,"Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2020, pp. 246–251. [30] M. Zeman, E. Osipov, and Z. Bosni´c, “Compressed superposition of neural networks for deep learning in edge computing,” in 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 2021, pp. 1–8. [31] M. Wortsman, V . Ramanujan, R. Liu, A. Kembhavi, M. Rastegari, J. Yosinski, and A. Farhadi, “Supe"
6899,unknown,"“Supermasks in superposition,” Advances in Neural Information Processing Systems (NeurIPS), vol. 33, pp. 15 173–15 184, 2020. [32] M. Zeman, J. F. Pucer, I. Kononenko, and Z. Bosni ´c, “Superformer: Continual learning superposition method for text classification,” Neural Networks, vol. 161, pp. 418–436, 2023. [33] M. Hersche, M. Zeqiri, L. Benini, A. Sebastian, and A. Rahimi, “A neuro-vector-symbo"
6900,unknown,"architecture for solving Raven’s progressive matrices,” Nature Machine Intelligence, vol. 5, no. 4, pp. 363–375, 2023. [34] M. M. Alam, E. Raff, T. Oates, and J. Holt, “Deploying convolutional networks on untrusted platforms using 2D holographic reduced representations,” in International Conference on Machine Learning (ICML). PMLR, 2022, pp. 367–393. [35] ——, “Recasting self-attention with hologra"
6901,unknown,"[35] ——, “Recasting self-attention with holographic reduced representations,” in Proceedings of 8TH SIGKDD International Workshop on Mining and Learning from Time Series – Deep Forecasting: Models, Interpretability, and Applications (MiLeTS 2022), 2022. [36] Y . Su, V . Murahari, K. Narasimhan, and K. Li, “PruMUX: Augmenting data multiplexing with model compression,” arXiv preprint arXiv:2305.1470"
6902,unknown,"12 37th Conference on Neural Information Processing Systems (NeurIPS 2023) [37] M. Hassid, H. Peng, D. Rotem, J. Kasai, I. Montero, N. A. Smith, and R. Schwartz, “How much does attention actually attend? Questioning the importance of attention in pretrained transformers,” in Findings of the Association for Computational Linguistics: EMNLP 2022 , 2022, pp. 1403–1416. [38] R. Bommasani, D. A. Hudson"
6903,unknown,"[38] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al., “On the opportunities and risks of foundation models,” arXiv preprint arXiv:2108.07258, 2021. 13 Appendix MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition A MIMONets Details 2 A.1 VSA representations and operations i"
6904,unknown,A.1 VSA representations and operations in MIMONets . . . . . . . . . . . . . . . . . 2 A.2 Illustration of the Blessing of Dimensionality . . . . . . . . . . . . . . . . . . . . 2 A.3 Noisy retrieval of values from a key-value superposition . . . . . . . . . . . . . . 2 A.4 Illustration of dynamic inference . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 A.5 Alternative throughput-increasi
6905,unknown,B MIMOConv Details 6 B.1 Inner-product preserving activation functions . . . . . . . . . . . . . . . . . . . . 6 B.2 Details on isometric convolutional layers . . . . . . . . . . . . . . . . . . . . . . . 6 B.3 Binding key regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 C MIMOFormer Details 7 C.1 FA VOR+ in the Performer . . . . . . . . . . . . . . . . . . . . . . . . 
6906,unknown,C.1 FA VOR+ in the Performer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 C.2 ReLU activation in FA VOR+S . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 C.3 Attention normalization in FA VOR+S . . . . . . . . . . . . . . . . . . . . . . . . 10 D Theoretical Basis for Noise Mitigation in FA VOR+S 11 E Experimental Setup and Ablation Study on MIMOConv 15 E.1 Experimental 
6907,unknown,E.1 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 E.2 Computational complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 E.3 The effectiveness of position-wise binding (PWHRR) and isometry regularization . 16 E.4 Dynamic inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 E.5 Ablation study on CIFAR10/100
6908,unknown,E.5 Ablation study on CIFAR10/100 . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 F Experimental Setup and Evaluations on MIMOFormer 22 F.1 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 F.2 Number of training steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F.3 Computational complexity . . . . . . . . . . . . . . . . . . . 
6909,unknown,F.3 Computational complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F.4 The importance of faithful attention scores . . . . . . . . . . . . . . . . . . . . . . 25 G Supporting Theorems 26 H Limitations 28 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2312.02829v1 [cs.LG] 5 Dec 2023 37th Conference on Neural Information Processing Systems (Ne
6910,unknown,"A MIMONets Details A.1 VSA representations and operations in MIMONets There are numerous available options for binding and unbinding depending on the VSA models being used [1]. Table A1 summarizes the VSA representations and operations used in MIMOConv and MIMOFormer. MIMOConv relies on holographic reduced representation (HRR) [2] for binding, and matrix binding of additive terms (MBAT) [3] for un"
6911,unknown,"matrix binding of additive terms (MBAT) [3] for unbinding. The binding and unbinding keys are real-valued and trainable. At initialization, each element in the D-dimensional key vector is drawn from an i.i.d. Gaussian distribution with zero mean and 1/Dvariance. Optionally, the binding keys can be frozen during training while maintaining a high accuracy (see Appendix E.5). MIMOConv’s binding relie"
6912,unknown,"binding relies on our proposed position-wise HRR (PWHRR) binding, which maintains the image’s local structure. The unbinding is implemented with MBAT, which computes the vector-matrix multiplication between the CNN’s Do-dimensional output feature vector and an unbinding matrix ˜A(i) ∈RDo×Do. The output dimension is Do=640 in WideResNet-28-10. The MBAT unbinding provides a higher degree of freedom "
6913,unknown,"provides a higher degree of freedom by having D2 o trainable parameters, whereas HRR’s unbinding would generate a circulant matrix with Do trainable parameters. However, it requires only 409,600 parameters per superposition channel, which is negligible compared to the remaining layers in MIMOConv, which have 36.54 M trainable parameters. Due to deep neural networks being highly nonlinear, we thus "
6914,unknown,"MIMOFormer uses the multiply-add-permute (MAP) [4] model, which uses bipolar keys and the element wise multiplication (Hadamard product) for binding and unbinding. The bipolar keys are drawn from a Rademacher distribution and are frozen during training and inference. Table A1: Summary of VSA representations and operations used in MIMOConv and MIMOFormer. Binding Unbinding VSA framework Key represe"
6915,unknown,"Binding Unbinding VSA framework Key representation Operation Keys Operation Keys MIMOConv HRR/MBAT real-valued PWHRR trainable/frozen MBAT trainable MIMOFormer MAP bipolar Hadamard product frozen Hadamard product frozen A.2 Illustration of the Blessing of Dimensionality VSA builds upon the mathematical concept of the Blessing of Dimensionality. According to it, random vectors are (quasi-)orthogona"
6916,unknown,"random vectors are (quasi-)orthogonal with high probability. Let us illustrate one version of it. Suppose independent random bipolar vectors x,y ∈{−1,+1}D with i.i.d. Rademacher distributed components as used in MIMOFormer. It holds by Hoeffding’s inequality (Appendix G) P(|cos ∡(x,y)|≥ α) = P(|⟨x,y⟩|≥ αD) ≤2e−Dα2/2 ∀α≥0 (1) Similar bounds exist for Gaussian random vectors, which are used to gener"
6917,unknown,"Similar bounds exist for Gaussian random vectors, which are used to generate keys for MIMOConv. Let us set some cutoff for interference events (IE), namely, we consider two vectors to interfere with each other if the angle between them is less than 70◦(corresponding to α= cos(70◦)), already 20◦ off from exact orthogonality. The bound demonstrates that the probability for two vectors (with i.i.d."
6918,unknown,"Rademacher distributed components) to interfere (IE) is less than 0.785 for vectors in 16 dimensions. As such, high levels of interference could still occur frequently. In contrast, for 64 dimensions, the probability that two vectors interfere (IE) is already known to be less than 0.0474. Consequently, we are much more certain that interference occurs with low probability. A.3 Noisy retrieval of v"
6919,unknown,Consider a superposition of N bound values x(i) s= a(1) ⊙x(1) + a(2) ⊙x(2) + ... + a(N) ⊙x(N) (2) where the binding keys a(i) are (for instance) independent bipolar vectors of i.i.d. Rademacher entries. Unbinding with a(k) produces the signal of interest x(k) together with a noise vector orthogonal to it: a(k) ⃝∗s= a(k) ⃝∗a(1) ⊙x(1) + a(k) ⃝∗a(2) ⊙x(2) + ... + a(k) ⃝∗a(N) ⊙x(N) (3) 2 37th Conferen
6920,unknown,"2 37th Conference on Neural Information Processing Systems (NeurIPS 2023) = x(k) + noise. (4) The noise vector stems from the approximate unbinding ( a(k) ⃝∗ a(k) ⊙x(k) ≈x(k)) as well as randomized value vectors (a(k) ⃝∗a(j) ⊙x(j)). Importantly, noisebecomes orthogonal to x(1), hence distinguishable, with a growing embedding dimension according to the Blessing of Dimensionality. The effect of nois"
6921,unknown,"The effect of noise is mitigated after comparing against a dictionary of known outputs, based on a notion of inner product. Such a comparison naturally, but not exclusively, arises in classification tasks. Concretely, comparing against a(k) ⊙Ω for an Ω aligned with x(k) returns ⟨a(k) ⃝∗s,Ω⟩= ⟨s,a(k) ⊙Ω⟩ (5) = N∑ i=1 ⟨a(i) ⊙x(i),a(k) ⊙Ω⟩ (6) ≈⟨a(k) ⊙x(k),a(k) ⊙Ω⟩ (7) = ⟨x(k),Ω⟩ (8) where we still a"
6922,unknown,"where we still assume bipolar binding and where the approximation relies on the Blessing of Dimensionality producing orthogonal vectors. For the VSA framework MAP, which uses bipolar keys of i.i.d. Rademacher entries, we provide a more precise formulation: Theorem 1 (Dictionary Cleanup Noise). Let Ω ∈RD be an element of a dictionary and consider the superposition of bound values x(i) ∈RD s= a(1) ⊙"
6923,unknown,"superposition of bound values x(i) ∈RD s= a(1) ⊙x(1) + a(2) ⊙x(2) + ... + a(N) ⊙x(N) (9) where the binding keys a(i) ∈{−1,1}D are independent bipolar vectors of i.i.d. Rademacher entries. It then holds P { ⣨ s,a(k) ⊙Ω ⟩ ̸∈[1 −α,1 + α] · ⣨ x(k),Ω ⟩} ≤2 exp ( − α2⏐⏐⟨x(k),Ω⟩ ⏐⏐2 2 ∑ i̸=k x(i) ⊙Ω 2 2 ) (10) with the exponent, given x(i) are all of roughly equal norm, according to Theorem 6 in Appe"
6924,unknown,"typically scaling as −Dα2 cos2(∡(x(k),Ω)) / 2(N −1) (11) Thus, for D≫N comparison against elements from a dictionary allows faithful retrieval. Proof. By the following equivalence relation ⣨ s,a(k) ⊙Ω ⟩ ̸∈[1 −α,1 + α] · ⣨ x(k),Ω ⟩ ⇐⇒ ⏐⏐⏐⏐⏐⏐ ∑ i̸=k ⣨ a(i) ⊙x(i),a(k) ⊙Ω ⟩ ⏐⏐⏐⏐⏐⏐ >α ⏐⏐⏐ ⣨ x(k),Ω ⟩⏐⏐⏐ (12) it suffices to derive tail bounds on P    ⏐⏐⏐⏐⏐⏐ ∑ i̸=k ⣨ a(i) ⊙x(i),a(k) ⊙Ω ⟩ ⏐⏐⏐⏐⏐⏐ >α ⏐⏐⏐ "
6925,unknown,"⣨ x(k),Ω ⟩⏐⏐⏐    = P    ⏐⏐⏐⏐⏐⏐ ∑ i̸=k ∑ d a(i) d a(k) d x(i) d Ωd ⏐⏐⏐⏐⏐⏐ >α ⏐⏐⏐ ⣨ x(k),Ω ⟩⏐⏐⏐    (13) Since {a(i) d ·a(k) d }d,i̸=k for kfixed is a set of i.i.d. Rademacher random variables, we are in a position to apply Hoeffding’s inequality (see Appendix, Theorem 5) which gives P    ⏐⏐⏐⏐⏐⏐ ∑ i̸=k ∑ d a(i) d a(k) d x(i) d Ωd ⏐⏐⏐⏐⏐⏐ >α ⏐⏐⏐ ⣨ x(k),Ω ⟩⏐⏐⏐   ≤2 exp  − α2⏐⏐⟨x(k),Ω⟩ "
6926,unknown,">α ⏐⏐⏐ ⣨ x(k),Ω ⟩⏐⏐⏐   ≤2 exp  − α2⏐⏐⟨x(k),Ω⟩ ⏐⏐2 2 ∑ i̸=k ∑ d ⏐⏐⏐x(i) d Ωd ⏐⏐⏐ 2   (14) 3 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Figure A1: Depiction of a single trained MIMOConv performing dynamic inference. Instead of the fast N=2 mode (above) the same input can be inserted twice for the slow N=1 mode (below) effectively implementing an ensemble method."
6927,unknown,"A.4 Illustration of dynamic inference To explore the idea of dynamic inference, suppose only two superposition channels are used with binding keys a(1),a(2) and unbinding keys ˜a(1),˜a(2). We already know how the model performs standard computation in superposition (see Eq. (1) – (4) in the main text). Let us thus examine how a network with the same parameters can instead be used as an ensemble-me"
6928,unknown,"network with the same parameters can instead be used as an ensemble-method with higher accuracy, but lower throughput. A superposition is established of twice the same input x: s= a(1) ⊙x+ a(2) ⊙x (15) After applying the deep neural network fθ to the superposition, we may unbind as ˜a(1) ⃝∗fθ(s) ≈˜a(1) ⃝∗fθ ( a(1) ⊙x ) + ˜a(1) ⃝∗fθ ( a(2) ⊙x ) (16) ≈fθ(x) + ˜a(1) ⃝∗fθ ( a(2) ⊙x ) (17) and ˜a(2) ⃝∗"
6929,unknown,"( a(1) ⊙x ) + ˜a(2) ⃝∗fθ ( a(2) ⊙x ) (18) ≈˜a(2) ⃝∗fθ ( a(1) ⊙x ) + fθ(x) . (19) After averaging the two expressions, we get 1 2 ( ˜a(1) ⃝∗fθ(s) + ˜a(2) ⃝∗fθ(s) ) ≈fθ(x) + noise (20) where noiseis a random noise vector and fθ(x) is approximated as an average of two predictions. Owing to the introduction of stochasticity by the binding and unbinding process these predictions are decorrelated, i.e.,"
6930,unknown,"for an illustration. A.5 Alternative throughput-increasing methods Although not a focus of this work, we believe that computation in superposition can be combined with other throughput-increasing methods such as model-downsizing, quantization aware training, and pruning. 4 37th Conference on Neural Information Processing Systems (NeurIPS 2023) The Blessing of Dimensionality gives, in terms of dime"
6931,unknown,"The Blessing of Dimensionality gives, in terms of dimensionality, an exponentially decreasing probability of interference for superpositions, even for (2-bit quantized) Rademachers. The extent to which these superpositions can be kept intact as linear layers act on them depends on the conditioning of the matrix (ideally nearly-isometric) not on the fidelity of its entries. As such we suspect that "
6932,unknown,"MIMOConv can be mixed with quantization, weight pruning, etc. Regarding MIMOFormer, we can give quantitative insights. As is evident from Theorem 3, the error bounds have no dependence on the precision of projection weights, but depend only on the embedding dimensionality, the size of keys and queries, and the angles between them. Consequently, quantization, pruning, etc. are not in competition wi"
6933,unknown,"quantization, pruning, etc. are not in competition with our approach and can be easily combined. Naturally, when combining different methods not only the gains but also the errors add up. However, with diminishing returns of each method we believe the combination of several to be most effective, especially given that our method is not competing with alternatives for the same resources of a model a"
6934,unknown,"and allows it to conduct dynamic inference. 5 37th Conference on Neural Information Processing Systems (NeurIPS 2023) B MIMOConv Details B.1 Inner-product preserving activation functions Any inner-product preserving map is linear (see Appendix G Theorem 4). With activation functions being introduced to break the linearity of neural networks, they are innately at odds with inner-product preservatio"
6935,unknown,"preservation. According to [5], a trade-off can be reached between preserving inner products and introducing nonlinearities by replacing the ReLU activation function with shifted ReLU (sReLU) sReLUb(x) = ReLU(x−b) + b= max(x,b), (21) where the trainable bias bdetermines the trade-off and is initialized to −1. However, in our experi- ments (see Appendix E.5), replacing sReLU with parametric ReLU (p"
6936,unknown,"function capable of choosing the extent of nonlinearity, defined as pReLUb(x) = ReLU(x) −b·ReLU(−x) = max(x,0) + b·min(x,0) (22) gives higher performance. The trainable parameter b∈[−1,1] controls the degree of linearity, where b=1 indicates fully linear behavior. It is initialized to b=0.5 at the beginning of training. B.2 Details on isometric convolutional layers As elaborated in the main text, "
6937,unknown,"As elaborated in the main text, we strive for inner-product preserving maps. With inner-product preserving maps being norm-preserving and by extension distance-preserving if linear, and with linear distance-preserving maps being norm preserving and according to the polarization identity also inner-product preserving, it holds that inner-product preserving maps are equivalent to linear isometries. "
6938,unknown,"isometries. Hence the name of the regularization term being isometry regularization term. The adopted regularization takes the form L(W) = γ 2 ∥Conv(W,W) −δCo∥2 F , δ Co[:,:,j,l ]= ICo×Co·1j,l=⌊k 2 ⌋ (23) L(WT) = γ 2 Conv(WT,WT) −δCi 2 F , δ Ci[:,:,j,l ]= ICi×Ci·1j,l=⌊k 2 ⌋ (24) where W ∈RCo×Ci×k×k contains the weights of a convolutional layer. Co denotes the number of output feature maps, Ci "
6939,unknown,"output feature maps, Ci the number of input feature maps, and kthe (square) kernel size. WT refers to a kernel with the first two dimensions of W transposed. In the notation of Einstein summations, the 2D convolution Conv(U,V ) evaluates to Oa,b,c,d = Ua,r,c+s,d+t ·Vb,r,s,t (25) This is implemented in Pytorch by the usual zero-padded spatial 2D convolution taking an input in the first argument (wi"
6940,unknown,"the first argument (with ranks: batch size, fmaps, height, width) and a convolutional kernel in the second argument (with ranks: output fmaps, input fmaps, kernel height, kernel width). Unless the number of input fmaps and output fmaps coincide, only one of W and its adjoint WT may be isometries. Thus we use L(W) when Ci >Co and L(WT) otherwise. For more information on why such a regularization te"
6941,unknown,"where it was first proposed. B.3 Binding key regularization We use a regularization term to keep the binding vectors (a(i)) orthonormal: L(a(1),...,a (N)) = µ ( N 2 ) N∑ i=1 N∑ j=i+1 ( ⟨a(i),a(j)⟩ ∥a(i)∥∥a(j)∥)2 + µ N N∑ i=1 ( a(i) −1)2, (26) with hyperparameter µ. A grid search on the validation set found a value of µ=0.1 to give the best results. Alternatively, the binding keys may be froz"
6942,unknown,"best results. Alternatively, the binding keys may be frozen after random (Gaussian) initialization, guaranteeing orthogonality in the limit of high key dimension (see Appendix A.2). 6 37th Conference on Neural Information Processing Systems (NeurIPS 2023) C MIMOFormer Details C.1 FA VOR+ in the Performer Here, we revisit the Performer’s FA VOR+ attention block [8] and in the next subsection we val"
6943,unknown,"Here, we revisit the Performer’s FA VOR+ attention block [8] and in the next subsection we validate the use of the ReLU activation in the projection. FA VOR+ takes advantage of the fact thata,b ↦→ exp ( aTb/ √ D ) is a kernel and can be represented as an explicit inner product (inverse kernel trick) in an infinite-dimensional space of transformed inputs. The mapping to this infinite-dimensional sp"
6944,unknown,"space is approximated with a randomized feature map ϕ: RD →RR + of finitely many entries. More explicitly, since Ew∼N(0,ID) [ exp ( wT i q 4√ D −∥q∥2 2 2 √ D ) ·exp ( wT i k 4√ D −∥k∥2 2 2 √ D )] (27) = exp ( −∥q∥2 2−∥k∥2 2 2 √ D ) ·Ew∼N(0,ID)[exp ( wT i q+k 4√ D ) ] (28) = exp ( −∥q∥2 2−∥k∥2 2 2 √ D ) ·exp ( ∥q+k∥2 2 2 √ D ) = exp ( qTk/ √ D ) , (29) drawing w1,...,w R ∼N(0,ID) i.i.d. induces a f"
6945,unknown,"+ with components given by ϕi(x) = exp ( wT i x 4√ D −∥x∥2 2 2 √ D ) / √ Rthat, by the law of large number, approximates exp ( qTk/ √ D ) , i.e. ⟨ϕ(k), ϕ(q)⟩≈ exp ( qTk/ √ D ) . (30) Alternatively, by partitioning w1,...,w R into subsets of cardinality Dand drawing each such subset from the orthogonal group before rescaling each wi according to the χD-distribution it still holds wi ∼N (0,ID), but "
6946,unknown,"wi ∼N (0,ID), but the entries are no longer independent. Using such orthogonal features, one obtains an unbiased estimate of exp ( qTk/ √ D ) with lower variance than independently drawing w1,...,w R ∼N(0,ID), see [8]. The inverse kernel trick then allows FA VOR+ to take advantage of the associativity of matrix multiplication to give the following factored expression of dot-product attention: oi ="
6947,unknown,"L∑ j=1 vj ⟨ϕ(kj),ϕ(qi)⟩ L∑ j=1 ⟨ϕ(kj),ϕ(qi)⟩ = L∑ j=1 vj ( ϕ(kj)Tϕ(qi) ) L∑ j=1 ( ϕ(kj)Tϕ(qi) )    Bi = A   L∑ j=1 vj ϕ(kj)T ×ϕ(qi) L∑ j=1 ϕ(kj)T    C ×ϕ(qi) , (31) where Aand C must only be computed once. With the computational complexity of evaluating ϕ being O(DR), this takes in both cases O(LDR). Computing ϕ(qi) for all i requires another O(LDR). Finally, the matrix-vector product "
6948,unknown,"O(LDR). Finally, the matrix-vector product × for a given query position itakes O(DR) for the numerator and O(R) for the denominator. Thus, computing outputs oi for all itakes only O(LDR), which can be a considerable improvement over the usual O(L2D) for long sequence lengths (L). C.2 ReLU activation in FA VOR+S To increase training stability, we use a ReLU-based projection ϕi(x) = ReLU(wT i x)/ √ "
6949,unknown,"R √ D instead of the above-mentioned unbiased approximation of softmax through ϕi(x) = exp ( wT i x 4√ D −∥x∥2 2 2 √ D ) / √ R. Such an approach was already mentioned in the Performer [8]. Here, we derive (new to our knowledge) a theoretical justification. As the following theorem expresses, the ReLU approximation leads to a (bi-)linear dependency on the norm of keys and queries while retain- 7 37"
6950,unknown,7 37th Conference on Neural Information Processing Systems (NeurIPS 2023) 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 0.0 0.1 0.2 0.3 0.4 0.5f( ) f( ) = + g( ) 4 f( ) = ( + 1)log2( ) 2 f( ) = exp(ln( ) )) 2 Figure A2: Approximation of E[ReLU(wTx)·ReLU(wTy)]/(∥x∥∥y∥) (red line) with a polynomial (blue line). Softmax function is shown with green dashed line. ing a roughly polynomial dependency (wit
6951,unknown,"ρ= ⟨x,y⟩/(∥x∥∥y∥). Theorem 2 (ReLU approximation). Let w∼N(0,ID). Then E[ReLU(wTx) ·ReLU(wTy)] = ⟨x,y⟩+ ∥x∥∥y∥g( ⟨x,y⟩ ∥x∥∥y∥) 4 ≈∥x∥∥y∥(ρ+1)log2(π) 2π (32) for g(ρ) = 2 π [√ 1 −ρ2 + |ρ|arctan ( |ρ|√ 1−ρ2 )] with ρ= ⟨x,y⟩/(∥x∥∥y∥) (33) Figure A2 illustrates the validity of interpreting E[ReLU(wTx) ·ReLU(wTy)]/(∥x∥∥y∥) as a polynomial. The general behaviour is also similar to the usual softmax func"
6952,unknown,"although the norm of x,y no longer affecting the exponent gives greater stability. Proof. E[ReLU(wTx) ·ReLU(wTy)] = 1 4 E[(wTx+ ⏐⏐wTx ⏐⏐)(wTy+ ⏐⏐wTy ⏐⏐)] (34) =E[wTx·wTy] +(((((((E[wTx· ⏐⏐wTy ⏐⏐] +(((((((E[ ⏐⏐wTx ⏐⏐·wTy] + E[ ⏐⏐wTx·wTy ⏐⏐] 4 (35) =E[xTw·wTy] + E[ ⏐⏐wTx·wTy ⏐⏐] 4 = xTE[wwT]y+ E[ ⏐⏐wTx·wTy ⏐⏐] 4 (36) =⟨x,y⟩+ E[ ⏐⏐wTx·wTy ⏐⏐] 4 (37) Let us now proceed to evaluate E[ ⏐⏐wTx·wTy ⏐⏐]. Fi"
6953,unknown,"Gaussian with [ X Y ] ∼N ( 0, [ σ2 X ρσXσY ρσXσY σ2 Y ]) = N ( 0, [ ∥x∥2 ⟨x,y⟩ ⟨x,y⟩ ∥y∥2 ]) . (38) where ρ= Cov(X,Y) σX σY = ⟨x,y⟩/(∥x∥∥y∥) measures the alignment between xand y. Hence, by the innovations form for centered jointly Gaussians it holds (X|Y = y) ∼N(µx|y(y),σ2 x|y(y)) = N(ρσXσ−1 Y y,(1 −ρ2)σ2 X). (39) Thus, the expectation of the folded normal distribution is given by E[|X||Y = y] =σ"
6954,unknown,√ 2 π exp ( − µ2 x|y(y) 2σ2 x|y(y) ) + µx|y(y)(1 −2Φ(− µx|y(y) σx|y(y) )) (40) = √ 1 −ρ2σX √ 2 π exp ( −ρ2σ2 Xσ−2 Y y2 2(1−ρ2)σ2 X ) + ρσXσ−1 Y y(1 −2Φ(−ρσXσ−1 Y y√ 1−ρ2σX )) (41) 8 37th Conference on Neural Information Processing Systems (NeurIPS 2023) = √ 1 −ρ2σX √ 2 π ·exp ( − ρ2y2 2(1−ρ2)σ2 Y ) + |ρ|σX ·σ−1 Y y(2Φ( |ρ|y√ 1−ρ2σY ) −1) (42) where Φ(s) = 1√ 2π ∫s −∞exp ( −t2/2 ) dtdenotes the cum
6955,unknown,"2π ∫s −∞exp ( −t2/2 ) dtdenotes the cumulative distribution function of the standard normal distribution. Thus, we split the target expression in two: E[ ⏐⏐wTx·wTy ⏐⏐] =E[|X||Y|] = E[E[|X|·|Y| ⏐⏐Y]] = E[|Y|·E[|X| ⏐⏐Y]] (43) = √ 1 −ρ2σX √ 2 π ·E [ |Y|exp ( − ρ2Y2 2(1−ρ2)σ2 Y )] (44) +|ρ|σX ·E [ |Y|σ−1 Y Y(2Φ( |ρ|Y√ 1−ρ2σY ) −1) ] . (45) For the first term, we get E [ |Y|exp ( − ρ2Y2 2(1−ρ2)σ2 Y )] "
6956,unknown,"= ∫ |y|exp ( − ρ2y2 2(1−ρ2)σ2 Y ) 1√ 2πσY exp { −y2 2σ2 Y } dy (46) = √ 1 −ρ2 ∫ |y| 1√ 2π √ 1−ρ2σY exp ( − y2 2(1−ρ2)σ2 Y ) dy (47) = √ 1 −ρ2 E[|Z|] where Z ∼N(0,(1 −ρ2)σ2 Y) (48) = √ 1 −ρ2 √ 1 −ρ2σY √ 2 π = (1 −ρ2)σY √ 2 π, (49) using again the formula for the expectation of a folded normal distribution. The second term is a bit more involved to evaluate. We substituteS = Y σY , i.e. S ∼N(0,1): E"
6957,unknown,E[|Y|Y σY (2Φ( |ρ|Y√ 1−ρ2σY ) −1)] =σYE[|S|S(2Φ( |ρ|S√ 1−ρ2 ) −1)] (50) =σYE[S2(2Φ( |ρ||S|√ 1−ρ2 ) −1)] (51) =σY ∫ ps(s)s2 ·(2Φ( |ρ||s|√ 1−ρ2 ) −1)ds (52) =2σY ∫ ps(s)s2Φ( |ρ||s|√ 1−ρ2 )ds−σY (53) =4σY ∫∞ 0 s2ps(s)Φ(bs)ds−σY (54) =4σY(1 4 + 1 2π( b 1+b2 + arctan(b))) −σY (55) =4σY(1 4 + 1 2π ( b 1+b2 + arctan(b) ) ) −σY (56) =2σY π ( |ρ| √ 1 −ρ2 + arctan ( |ρ|√ 1−ρ2 )) . (57) where b = |ρ|√ 1−ρ2 a
6958,unknown,"Therefore, we get E[ ⏐⏐wTx·wTy ⏐⏐] =∥x∥·∥y∥· 2 π [ (1 −ρ2)3/2 + ρ2√ 1 −ρ2 + |ρ|arctan ( |ρ|√ 1−ρ2 )] (58) =∥x∥·∥y∥· 2 π [√ 1 −ρ2 + |ρ|arctan ( |ρ|√ 1−ρ2 )]    g(ρ) , (59) which yields for the full expression E[ReLU(wTx) ·ReLU(wTy)] = ⟨x,y⟩+ ∥x∥·∥y∥·g( ⟨x,y⟩ ∥x∥·∥y∥) 4 . (60) 9 37th Conference on Neural Information Processing Systems (NeurIPS 2023) C.3 Attention normalization in FA VOR+S We pre"
6959,unknown,"We present the computation of the normalization term in FA VOR+S. Recall the usual FA VOR+ formulation from Appendix C.1 oi = A   L∑ j=1 vj ϕ(kj)T ×ϕ(qi) / C   L∑ j=1 ϕ(kj)T ×ϕ(qi)    Bi . (61) As was remarked in Appendix C.1, construction of A, C, and ϕ(qi) ∀i each requires O(LDR) computational complexity. Therefore, it is central to also compute the normalization factor Bi in superpo"
6960,unknown,"C(m) s =   L∑ j=1 ϕ( N∑ w=1 k(m,w) j )T      construct ∀min O(LM(DR+ND)) Q(n) i = ϕ( M∑ t=1 q(t,n) i )    construct ∀i,nin O(LN(DR+MD)) (62) Since C(m) s ∈R1×R, the evaluation of × for all query positions iand channels (m,n) is relatively inexpensive, demanding only O(LMNR) operations. According to the two approximations ( P) from FA VOR+ and (H), which is explored thoroughly in Append"
6961,unknown,"B(m,n) i = C(m) s ×Q(n) i (63) = L∑ j=1 ϕ ( N∑ w=1 k(m,w) j )T ϕ (M∑ t=1 q(t,n) i ) (64) P ≈ L∑ j=1 exp ( ⟨ N∑ w=1 k(m,w) j , M∑ t=1 q(t,n) i ⟩/ √ D ) (65) H ≈ L∑ j=1 exp ( ⟨k(m,n) j , q(m,n) i ⟩/ √ D ) (66) In total, while still setting M=N to balance the load, computing B(m,n) i for all channels (m,n) and query positions idemands a runtime of O(LNDR + LN2D+ LN2R), a significant improvement over "
6962,unknown,"over O(LN2DR) for computing Bi separately for each channel (m,n) ∈{1,...,N }2. How the normalization is incorporated depends on the MIMOFormer instantiation. In the first case using superposition exclusively for the attention block (att.), the normalization scalar is directly applied on the output tokens after unbinding, i.e., o(m,n) i = S(n) i ⊙˜a(m,n) B(m,n) i , (67) where ˜a(m,n) is the unbindi"
6963,unknown,"in superposition (att.+MLP), we jointly normalize the output by the sum of all normalization scalars over m(where we enjoy additional computational savings by in fact already summing over min the construction of Cs = ∑ mC(m) s ): ¯S(n) i = S(n) i ∑M m=1 B(m,n) i . (68) 10 37th Conference on Neural Information Processing Systems (NeurIPS 2023) D Theoretical Basis for Noise Mitigation in FA VOR+S As"
6964,unknown,"ϕ(k)Tϕ(q) P ≈exp ( ⟨k,q⟩/ √ D ) ⟨ N∑ w=1 k(u,w) j , M∑ t=1 q(t,n) i ⟩ H ≈⟨k(u,n) j , q(u,n) i ⟩    intended signal (69) The approximation P, which improves with increasing R= dim(ϕ(qi)), is due to FA VOR+ and is quantified in [8] whereas the approximation H follows from: Inter-channel distortion. The probability that inter-channel attention distorts the intended signal of the dot product by a "
6965,unknown,"the dot product by a factor outside [1 −α,1 + α] has various upper bounds, most notably decaying exponentially w.r.t.Dα2 cos2(∡(k (u,n) j ,q(u,n) i ))/(NM −1)2. We proceed to make this statement exact: Theorem 3 (FA VOR+S Inter-Channel Noise). The probability that inter-channel attention distorts the true signal by a factor outside [1 −α,1 + α] shows the following tail bounds. Denote by P = P {⟨ N"
6966,unknown,"w=1 k(u,w) j , M∑ t=1 q(t,n) i ⟩/ √ D̸∈[1 −α,1 + α] ·⟨k (u,n) j , q(u,n) i ⟩/ √ D } (70) where k(m,n) j := k (m,n) j ⊙a(m,n) q(m,n) j := q(m,n) j ⊙a(m,n) (71) with a(m,n) being i.i.d. bipolar vectors of Rademachers and (m,n) denoting a channel. It holds P ≤ N∑ w=1 √ (u,w)̸=(t,n)∑ t=1,...,M 1/Ξ(t,n) (u,w) P ≤ M∑ t=1 √ (u,w)̸=(t,n)∑ w=1,...,N 1/Ξ(t,n) (u,w) (72) P ≤ (u,w)̸=(t,n)∑ w=1,...,N t=1"
6967,unknown,"1/Ξ(t,n) (u,w) P ≤2 (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M exp ( − Ξ(t,n) (u,w) 2(NM−1)2 ) (73) for Ξ(t,n) (u,w) = α2 ⏐⏐⏐⟨k (u,n) j ,q(u,n) i ⟩ ⏐⏐⏐ 2 ∑D p=1 ( k (u,w) j )2 p ( q(t,n) i )2 p = α2 cos2(∡(k (u,n) j ,q(u,n) i )) k (u,n) j  2 2 q(u,n) i  2 2k (u,w) j ⊙q(t,n) i  2 2 (74) which for keys/queries of similar size according to Theorem 6 in Appendix G typically scales as Ξ(t,n) ("
6968,unknown,"(u,n) j ,q(u,n) i )) (75) Proof. First notice that since ∑ w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ̸∈[1 −α,1 + α] ·⟨k (u,n) j , q(u,n) i ⟩ (76) ⇐⇒ (77)⏐⏐⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ⏐⏐⏐⏐⏐⏐⏐⏐ >α ⏐⏐⏐⟨k (u,n) j , q(u,n) i ⟩ ⏐⏐⏐, (78) we may instead derive tail bounds on P    ⏐⏐⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ⏐⏐⏐⏐⏐⏐⏐⏐ >α ⏐"
6969,unknown,"t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ⏐⏐⏐⏐⏐⏐⏐⏐ >α ⏐⏐⏐⟨k (u,n) j , q(u,n) i ⟩ ⏐⏐⏐    . (79) 11 37th Conference on Neural Information Processing Systems (NeurIPS 2023) We shall derive the tail bounds for a threshold αand only replace it by α ⏐⏐⏐⟨k (u,n) j , q(u,n) i ⟩ ⏐⏐⏐in a final step. Applying Markov, the triangle inequality, and linearity of expectation gives P    ⏐⏐⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)"
6970,unknown,"w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ⏐⏐⏐⏐⏐⏐⏐⏐ >α    (80) ≤E   ⏐⏐⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ⏐⏐⏐⏐⏐⏐⏐⏐   / α (81) ≤ N∑ w=1 E   ⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ⏐⏐⏐⏐⏐⏐   / α (82) = N∑ w=1 E   ⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ t=1,...,M ⣨ k (u,w) j ⊙a(u,w), q(t,n) i ⊙a(t,n) ⟩ ⏐⏐⏐⏐⏐⏐   / α, (83) where a(·,·) denotes one of the "
6971,unknown,"where a(·,·) denotes one of the (independent) binding vectors with entries given by independent Rademacher random variables and ¯k(·,·) (·) , ¯q(·,·) (·) the unbound keys and queries respectively of a given channel and token position. For ϵ(t) p denoting independent Rademacher random variables we may simplify to E   ⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ t=1,...,M ⣨ k (u,w) j ⊙a(u,w), q(t,n) i ⊙a(t,n) ⟩ ⏐⏐⏐⏐⏐⏐  "
6972,unknown," (84) = E   ⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ t=1,...,M D∑ p=1 ( k (u,w) j ) p a(u,w) p ( q(t,n) i ) p a(t,n) p ⏐⏐⏐⏐⏐⏐   (85) = E   ⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ t=1,...,M D∑ p=1 ( k (u,w) j ) p ( q(t,n) i ) p ϵ(t) p ⏐⏐⏐⏐⏐⏐  . (86) The famous Khintchine inequality determines up to a constant the behavior of the expectation as 1√ 2 √ (u,w)̸=(t,n)∑ t=1,...,M D∑ p=1 (( k (u,w) j ) p ( q(t,n) i ) p )2 ≤E   ⏐⏐"
6973,unknown,"⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ t=1,...,M D∑ p=1 ( k (u,w) j ) p ( q(t,n) i ) p ϵ(t) p ⏐⏐⏐⏐⏐⏐   (87) ≤ √ (u,w)̸=(t,n)∑ t=1,...,M D∑ p=1 (( k (u,w) j ) p ( q(t,n) i ) p )2 . (88) Hence, the Markov bound gives P    ⏐⏐⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ⏐⏐⏐⏐⏐⏐⏐⏐ >α    ≤ N∑ w=1 √ (u,w)̸=(t,n)∑ t=1,...,M D∑ p=1 ( k (u,w) j )2 p ( q(t,n) i )2 p / α (89) and (90) P"
6974,unknown,"   ⏐⏐⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ⏐⏐⏐⏐⏐⏐⏐⏐ >α    ≤ M∑ t=1 √ (u,w)̸=(t,n)∑ w=1,...,N D∑ p=1 ( k (u,w) j )2 p ( q(t,n) i )2 p / α (91) 12 37th Conference on Neural Information Processing Systems (NeurIPS 2023) where the latter follows by symmetry. Alternatively, we could also apply Chebyshev to the problem, i.e. P    ⏐⏐⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ "
6975,unknown,"w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ⏐⏐⏐⏐⏐⏐⏐⏐ >α    (92) = P      (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩   2 >α2    (93) ≤E     (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩   2  / α2 (94) = E     (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M ⣨ k (u,w) j ⊙a(u,w), q(t,n) i ⊙a(t,n) ⟩   2  / α2 (95) = E    "
6976,unknown,"(u,w)̸=(t,n)∑ w=1,...,N t=1,...,M D∑ p=1 ( k (u,w) j ) p a(u,w) p ( q(t,n) i ) p a(t,n) p   2  / α2 (96) = (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M D∑ p=1 (u,w′)̸=(t′,n)∑ w′=1,...,N t′=1,...,M D∑ p′=1 ( k (u,w) j ) p ( q(t,n) i ) p ( k (u,w′) j ) p′ ( q(t′,n) i ) p′ · ·E [ a(u,w) p a(t,n) p a(u,w′) p′ a(t′,n) p′ ]/ α2 (97) Now, since for (u,w) ̸= ( t,n), (u,w′) ̸= ( t′,n) and (t,w) ̸= ( t′,w′) th"
6977,unknown,"α2 (97) Now, since for (u,w) ̸= ( t,n), (u,w′) ̸= ( t′,n) and (t,w) ̸= ( t′,w′) the cardinality of {(u,w),(t,n),(u,w′),(t′,n)}is at least three, at least one entry has no duplicate. Let w.l.o.g. be that entry a(u,w) p . Then by independence, one has E [ a(u,w) p a(t,n) p a(u,w′) p′ a(t′,n) p′ ] = E [ a(u,w) p ] E [ a(t,n) p a(u,w′) p′ a(t′,n) p′ ] = 0 (98) Consequently, all terms with (t,w) ̸= (t′"
6978,unknown,"entries are independent, i.e. E [ a(u,w) p a(t,n) p a(u,w) p′ a(t,n) p ] = E [ a(u,w) p ] E [ a(t,n) p ] E [ a(u,w) p ] E [ a(t,n) p ] = 0 (99) Hence, we have that all cross-terms vanish, which gives P    ⏐⏐⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ⏐⏐⏐⏐⏐⏐⏐⏐ >α    (100) ≤ (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M D∑ p=1 ( k (u,w) j )2 p ( q(t,n) i )2 p E [( a(u,w) p )2 "
6979,unknown,"a(t,n) p )2]/ α2 (101) = (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M D∑ p=1 ( k (u,w) j )2 p ( q(t,n) i )2 p / α2 (102) 13 37th Conference on Neural Information Processing Systems (NeurIPS 2023) For higher orders than two, we can no longer rely on the absence of duplicates in the set of multiplied binding vectors. For example, a cross-term such as E  a(u,w) p a(t,n) p   1 a(u,w′) p′ a(t′,n) p′   "
6980,unknown,"p′ a(t′,n) p′    2 a(u,w) p a(t,n) p   3 a(u,w′) p′ a(t′,n) p′    4  = 1 (103) is non-vanishing even for (t,w,p ) ̸= (t′,w′,p′). Hence, for higher orders, we will have to work with P    ⏐⏐⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ⏐⏐⏐⏐⏐⏐⏐⏐ >α    ≤P    (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M ⏐⏐⏐ ⣨ k(u,w) j , q(t,n) i ⟩⏐⏐⏐>α    (104) ≤P { ∃("
6981,unknown,"≤P { ∃(u,w) ̸= (t,n) : ⏐⏐⏐ ⣨ k(u,w) j , q(t,n) i ⟩⏐⏐⏐>α/(NM −1) } (105) ≤ (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M P {⏐⏐⏐ ⣨ k(u,w) j , q(t,n) i ⟩⏐⏐⏐>α/(NM −1) } (106) = (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M P {⏐⏐⏐⏐⏐ D∑ p=1 ( k (u,w) j ) p a(u,w) p ( q(t,n) i ) p a(t,n) p ⏐⏐⏐⏐⏐>α/(NM −1) } (107) = (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M P {⏐⏐⏐⏐⏐ D∑ p=1 ( k (u,w) j ) p ( q(t,n) i ) p ϵp ⏐⏐⏐⏐⏐>α/(NM −1) } (108) for {"
6982,unknown,"for {ϵp}D p=1 independent Rademacher random variables. Finally, we may apply Hoeffding’s inequality (Theorem 5), which gives P    ⏐⏐⏐⏐⏐⏐⏐⏐ (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M ⣨ k(u,w) j , q(t,n) i ⟩ ⏐⏐⏐⏐⏐⏐⏐⏐ ≥α    ≤2 (u,w)̸=(t,n)∑ w=1,...,N t=1,...,M exp ( − α2 2(NM−1)2 ∑D p=1 ( k (u,w) j )2 p ( q(t,n) i )2 p ) (109) 14 37th Conference on Neural Information Processing Systems (NeurIPS "
6983,unknown,"E Experimental Setup and Ablation Study on MIMOConv E.1 Experimental setup Datasets CIFAR10 and CIFAR100. The CIFAR10 [9] dataset contains 60,000 images, each of resolution 32 ×32, divided into 50,000 training and 10,000 test images. The images are grouped into ten classes, each with 6000 examples. CIFAR100 has the same number of images and resolutions, but contains 100 classes each with 600 examp"
6984,unknown,"MNIST. The MNIST dataset [10] provides grey-scale images, each of resolution 28×28, containing hand-written digits. The 60,000 training samples are divided into a training and validation set containing 55,000 and 5000 samples, respectively. Finally, the test accuracy is reported on the test set containing 10,000 samples. SVHN. The street view house number (SVHN) dataset [11] provides cropped image"
6985,unknown,"SVHN. The street view house number (SVHN) dataset [11] provides cropped images of house number plates, each of resolution 32×32. As in MNIST, the task is to classify the printed digits (from 0 to 9). It contains 73,257 RGB images for training and 26,032 for testing. Training setups The experiments are run on an NVIDIA A100 Tensor Core GPU with 80 GB memory and 8 CPU cores. All experiments are repe"
6986,unknown,"cores. All experiments are repeated five times with different random seeds. We report the mean and standard deviations of accuracy to account for variability in training. Overall, all MIMOConv experiments together required 3290 GPU hours when accumulating all the training runs required for generating the main results and the ablations study. CIFAR10 and CIFAR100. In all experiments on CIFAR10 and "
6987,unknown,"CIFAR10 and CIFAR100. In all experiments on CIFAR10 and CIFAR100, stochastic gradient descent (SGD) with momentum is used. Unless otherwise noted, we train for 1200 epochs using the OneCycleLR policy [12] with cosine annealing for two phases (30% increase, 70% decrease of learning rate). The initial learning rate is set to 0.008, the maximal learning rate to 0.2, and the final learning rate to 2e-"
6988,unknown,"learning rate to 2e-5. Momentum is cycled inversely with base momentum set to 0.85 and maximal momentum set to 0.95. Due to overfitting, WideResNet28-10 shows higher test accuracy when trained with 200 epochs than 1200 epochs; hence, Table 1 in the main text shows WideResNet28-10’s performance with 200 training epochs. For all parameters, except for the bias in shifted ReLU, the slope in parametri"
6989,unknown,"slope in parametric ReLU, and binding/unbinding keys, weight decay with value 1e-5 is applied. The images are standardized on each color channel by subtracting the mean and dividing by the standard deviation. To augment data, the images are randomly flipped horizontally, and a random 32 ×32 crop is taken after zero padding the images on each side by four pixels. Furthermore, the data agnostic augm"
6990,unknown,"data agnostic augmentation strategy mixup [13] is employed with parameter α=1, which is decisive for obtaining high accuracy. The batch size is set to 128 elements per superposition channel (i.e., 128N). Thus, after binding, a batch of 128 superpositions traverses the CNN. Increasing superposition channels would decrease the number of update steps per epoch. To correct for that, in each epoch, the"
6991,unknown,"often as the number of superposition channels used. While the results presented in this paper come from a train/test split of the datasets, the training dataset is split into a 90/10 train/validation split for all model design and hyperparameter choices. Furthermore, to decrease the degrees of freedom in the experiments, the remaining hyperparameters (learning rate, weight decay, mixup parameters)"
6992,unknown,"the experiments, the remaining hyperparameters (learning rate, weight decay, mixup parameters) are tuned to yield good performance on the base model WideResNet28-10. Finally, to stabilize training, the average gradient norm of each epoch is recorded, and the batches of the subsequent epoch are discarded (without repetition) if their update gradient norm exceeds the recorded average of the last epo"
6993,unknown,"epoch by a factor of 10. Training MIMOConv for 1200 epochs takes 11 hours independent of the number of superposition channels owing to the batch loading corrections that account for the same number of training steps. MNIST. The experiments on MNIST use a similar setup to the ones on CIFAR: the same learning rate scheduler, batch size, weight decay, and mixup coefficients are used. In contrast to C"
6994,unknown,15 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Table A2: Millions of multiply-accumulate (MMAC) operations per sample on CIFAR-100. Number in parenthesis shows the relative share of the overall complexity. First conv. layer Binding Rest of conv. layers Unbinding FCL Total WideResNet-28-10 0.49 (0.009%) n.a. 5245 (99.99%) n.a. 0.064 (0.001%) 5251 WideIsoNet-28-10 0.49 (0
6995,unknown,"WideIsoNet-28-10 0.49 (0.009%) n.a. 5245 (99.99%) n.a. 0.064 (0.001%) 5251 MIMOConv (N=1) 1.97 (0.04%) 4.19 (0.08%) 5329 (99.88%) 0.41 (0.008%) 0.064 (0.001%)5335 MIMOConv (N=2) 1.97 (0.07%) 4.19 (0.15%) 2664 (99.75%) 0.41 (0.015%) 0.064 (0.002%)2671 MIMOConv (N=4) 1.97 (0.15%) 4.19 (0.31%) 1332 (99.50%) 0.41 (0.031%) 0.064 (0.005%)1339 number of training epochs is reduced to 50. Moreover, the ima"
6996,unknown,"number of training epochs is reduced to 50. Moreover, the images are center-cropped to 20×20 pixels both in training and testing. A random horizontal flip serves as data augmentation during training. We reduce the depth of MIMOConv from 28 to 10 layers, and the width factor from 10×to 1×, hence we call it MIMOConv-10-1. Moreover, the initial width factor of the first convolutional layer is also se"
6997,unknown,"SVHN. In the SVHN experiments, we train the standard MIMOConv-28-10 architecture for 200 epochs. The remaining hyperparameters are kept the same as in CIFAR. During training, random crop with padding is used. E.2 Computational complexity Table A2 breaks down MIMOConv’s computational benefits (in MMAC) on CIFAR100, as N is increased from 1 to 4. As shown, the integration of variable binding mechani"
6998,unknown,"increased from 1 to 4. As shown, the integration of variable binding mechanisms via binding and unbinding operations is inconsequential, amounting to only between 0.008% ( N=1) and 0.031% (N=4) of the total MACs for MIMOConv. On MNIST, MIMOConv-10-1 has a computational cost of 5.10 MMAC per sample atN=1 super- position channels and manages to reduce the cost to 0.47 MMAC per sample atN=16, an effe"
6999,unknown,"position channels and manages to reduce the cost to 0.47 MMAC per sample atN=16, an effective reduction of 10.9×. The reduction is smaller than N due to the computationally dominating first convolutional layer, which is not operating in superposition. Yet, MIMOConv shows a notably higher reduction than the LeNet-like model (CNN+nonlinear (8×)) from DataMUX [14], a key competitor, which requires 0."
7000,unknown,"which requires 0.88 MMAC and 0.65 MMAC per sample atN=1 and N=16, respectively. E.3 The effectiveness of position-wise binding (PWHRR) and isometry regularization DataMux [14] also explored CNNs that compute in superposition and reported its findings on MNIST. Even with a trivial downsizing for fair comparison from a 28-layer very-wide (10×) MIMOConv to a 10-layer narrow (1×) MIMOConv, our method "
7001,unknown,"10-layer narrow (1×) MIMOConv, our method scales much better to high superposition channels (N) than DataMUX does. Indeed, our model shows an accuracy of 80.4% against their 52.9% in case of N=16 superposition channels (highest number of channels reported by DataMUX for vision tasks), despite being computationally cheaper (0.47 MMAC/s vs. 0.65 MMAC/s). DataMux’s binding overhead results in a mere "
7002,unknown,"overhead results in a mere 1.35×reduction in MACs compared to our 10.9×as N goes from 1 to 16 demonstrating superior scaling of our method. We attribute the improved performance to a set of innovations which we reiterate here: MIMOConv applies position-wise binding (PWHRR), thus retaining the locality property present in natural images and vital for CNNs, whilst as discussed by Murahari, Vishvak, "
7003,unknown,"and vital for CNNs, whilst as discussed by Murahari, Vishvak, et al. their primary binding does not. As a workaround they proposed binding via two layers CNNs each outputting 8 feature maps. The resulting (pixel-wise) superposition in a low-dimensional space (8-D) leads to high interference. Additionally to using an expensive binding mechanism, it also makes the first layer of the model 8 times as"
7004,unknown,"times as expensive no matter the number of superpositions. We are able to circumvent this issue by applying the first layer of the CNN before the pixel-wise binding, increasing the dimensionality of each pixel in an easy-to-understand manner. Another difference to their work is our use of isometric neural networks to further reduce interference during the processing of superposed images. 16 37th C"
7005,unknown,88 90 92 94 96 98 100Accuracy (%) 94.16 94.73 95.38 95.43 95.56 95.56 90.28 95.99 96.33 96.41 96.36 96.22 97.52 97.39 97.25 97.13 96.93 96.46 slow mode (1 query/pass) normal mode (2 queries/pass) fast mode (4 queries/pass) 0.5 0.6 0.7 0.8 0.9 1.0 Fast mode training frequency 70 72 71.16 (a) CIFAR10 72 74 76 78 80 82Accuracy (%) 71.89 75.06 77.59 78.19 78.28 78.24 72.86 78.71 79.84 80.48 80.28 79.9
7006,unknown,"79.91 81.27 81.56 82.42 82.52 81.59 80.43 slow mode (1 query/pass) normal mode (2 queries/pass) fast mode (4 queries/pass) 0.5 0.6 0.7 0.8 0.9 1.0 Fast mode training frequency 58 60 59.7 (b) CIFAR 100 Figure A3: Dynamic inference with MIMOConv trained for 1200 epochs in slow and fast mode depending on the fast mode training frequency. Each model is evaluated in slow (1 input/pass), normal (2 input"
7007,unknown,"(2 inputs/pass), and fast mode (4 inputs/pass). We report the average accuracy and the standard deviation (error bars) over five runs with different seeds. Outliers in normal mode (indicated with ⋆) are not used for standard deviation computation. E.4 Dynamic inference Dynamic inference enables the instantaneous on-demand partitioning of the superposition channels to select an operating point with"
7008,unknown,"select an operating point with a suitable speed/accuracy trade-off. Even though every MIMOConv can be configured to perform dynamic inference at any time, exposing MIMOConv to dynamic switching between different modes during training is beneficial. We set up a model with four channels and consider a fast (4 inputs/pass), normal (2 inputs/pass), and slow mode (1 input/pass). The fast mode maps each"
7009,unknown,maps each input to one channel; the normal mode distributes two inputs over pairs of channels; and the slow mode uses all channels for the same input. We then train the models for different frequencies in fast and slow modes. The normal mode is not used during training of the model. The potential switching between the modes happens between every batch. Figure A3 shows the classification accuracy o
7010,unknown,"Figure A3 shows the classification accuracy on CIFAR10 (a) and CIFAR100 (b) when using dynamic models trained with varying fast mode frequencies. The models, which are trained with a different fraction of inputs in fast mode, are evaluated in slow, normal, and fast modes. As is expected, increasing the fast mode training frequency is beneficial for both datasets when only looking at the fast mode "
7011,unknown,"fast mode inference. Conversely, the normal inference mode benefits from a mixture of fast and slow mode training, whereby a fast mode training frequency of 80% achieves the highest accuracy on both datasets. There is a notable volatility in the performance of normal mode at a fast mode training frequency of 0.5, which could be due to the optimization getting stuck in a local optimum exclusively l"
7012,unknown,"learning a slow mode instance. E.5 Ablation study on CIFAR10/100 Isometry regularization of CNN weights. We evaluate the impact of isometry regularization to the CNN weights by varying the orthogonal regularization coefficient (γ), described in Eq. (23) and Eq. (24). All models are trained for 200 epochs to reduce the training time. As can be observed in Figure A4, orthogonal regularization enable"
7013,unknown,"in Figure A4, orthogonal regularization enables the network to perform notably better both for configurations with a single superposition channel and two superposition channels. However, a strong regularization hinders the ability of the network to adapt to the task. With two superposition channels, 17 37th Conference on Neural Information Processing Systems (NeurIPS 2023) CIFAR10 CIFAR10070727476"
7014,unknown,96.53 81.12 96.88 82.71 96.94 83.08 96.56 82.54 96.34 80.89 Accuracy (%) γ= 0(no regularization)γ=1e-5γ=1e-4γ=3e-4γ=1e-3 (a) N=1 image superposed CIFAR10 CIFAR100707274767880828486889092949698100 95.13 77.37 95.65 79.84 95.68 79.68 95.02 78.3 94.2 76.01 Accuracy (%) γ= 0(no regularization)γ=1e-5γ=1e-4γ=3e-4γ=1e-3 (b) N=2 images superposed Figure A4: MIMOConv with varying orthogonal regularization 
7015,unknown,Figure A4: MIMOConv with varying orthogonal regularization coefficient ( γ) for N=1 and N=2 superpositions. All models are trained for 200 epochs. We report the average accuracy and the standard deviation (error bars) over five runs with different seeds. CIFAR10 CIFAR10070 75 80 85 90 95 100 95.73 80.37 95.53 79.61 95.05 77.88 Accuracy (%) ReLUparametricReLUshiftedReLU (a) N=2 and 200 training epo
7016,unknown,"CIFAR10 CIFAR10070 75 80 85 90 95 100 94.84 76.39 94.93 76.38 94.37 75.38 Accuracy (%) ReLUparametricReLUshiftedReLU (b) N=5 and 1200 training epochs Figure A5: MIMOConv with different activation functions. We report the average accuracy and the standard deviation (error bars) over five runs with different seeds. the performance difference is more striking, i.e., orthogonal regularization is more "
7017,unknown,"other experiments, an orthogonal regularization coefficient of γ=1e-4 was used. Isometry at activation functions. We investigate the ReLU, shifted ReLU, and parametric ReLU activation functions to give the model more control over the extent of isometry. Each activation function owns separate, trainable parameters which are not shared between the feature maps and layers. Experimental results are sh"
7018,unknown,"layers. Experimental results are shown in Figure A5. When using N=2 superposition channels and 200 training epochs, ReLU outperforms both parametric ReLU and shifted ReLU. For longer training times (1200 epochs) and more superposition channels (N=5), the network prefers parametric ReLU. Surprisingly, in the case of low superposition counts, the model develops highly non-isometric parametric ReLU a"
7019,unknown,"parametric ReLU activation functions, as seen in Figure A6. With the convolutional layers being pushed toward isometry due to the isometry regularization term and residual skip connections increasing isometry further, the network seeks balance through strongly non-isometric activation functions. Nevertheless, increasing the number of images superposed incentivizes the network to learn isometric ac"
7020,unknown,"originates in interference or in the attempt of the network to reduce interference through isometry, and it is likely that some balance between the two is reached. Further research will be needed to gain more insight into the benefits and drawbacks of isometry. We employ parametric ReLU in all other experiments as its performance is comparable to ReLU, but it allows more degrees of freedom and hen"
7021,unknown,hence could give additional performance benefits under different network configurations. 18 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Figure A6: Average parametric ReLU parameter (b) during training on CIFAR10 for different number of superposition channels (N). CIFAR10 CIFAR10070 75 80 85 90 95 100 95.53 79.61 95.4 78.84 95.23 78.3 Accuracy (%) no skipInitskipInitskip
7022,unknown,"90 95 100 95.53 79.61 95.4 78.84 95.23 78.3 Accuracy (%) no skipInitskipInitskipInit and diracInit Figure A7: Effect of skipInit and diracInit on accuracy in superposition mode N=2, 200 epochs. We report the average accuracy and the standard deviation (error bars) over five runs with different seeds. SkipInit and DiracInit. In [7], the benefit of skip initialization [15] via inducing maximum isome"
7023,unknown,"layers as an identity, called diracInit, was promoted. In our experiments, both additions worsen the performance on CIFAR10 and CIFAR100, as seen in Figure A7. Number of training epochs. Training a neural network model to simultaneously handle multiple images while providing high accuracy is a more difficult task than without superposition present. Figure A8 shows that the performance gap between "
7024,unknown,"Figure A8 shows that the performance gap between single-image mode and multiple-image mode narrows when training for more epochs. For each epoch, the training set is passed through the network as many times as superposition channels were used: this corrects for the larger batch size used in superposition modes and to have roughly equal training time for each epoch. Number of feature maps in the fi"
7025,unknown,"Number of feature maps in the first layer. Compared to a standard ResNet, the WideResNet architecture [16] increases the number of feature maps of every layer by a width factor. The only exception is the first layer, which has 16 feature maps in WideResNet-28-10. However, binding takes place after the first layer in MIMOConv. In order to enter the regime of high dimensionality and to benefit from "
7026,unknown,"and to benefit from the Blessing of Dimensionality, we experiment with an additional parameter termed initial width factor, which increases the number of feature maps of the output of the first convolutional layer. For example, an initial width factor of 4 yields 4 ·16=64 feature maps after the first convolutional layer. The initial width factor can be configured independently from the general wid"
7027,unknown,"width factor. Figure A9 shows the performance against variable initial width factors, while the general width factor is fixed to 10. Initial width factors 2 and 4 give satisfactory results, while factors 1 and 8 yield very unstable training (as indicated by the large variance). We attribute this to the large step either from 3 19 37th Conference on Neural Information Processing Systems (NeurIPS 20"
7028,unknown,100200 800 1200 1600 Epochs 82 84 86 88 90 92 94 96 98 100Accuracy (%) 96.11 96.81 97.47 97.48 97.44 94.08 95.53 96.69 96.88 96.93 91.82 94.28 96.13 96.38 96.39 89.49 92.57 95.32 95.63 95.83 N=1 N=2 N=3 N=4 (a) CIFAR10 100200 800 1200 1600 Epochs 66 68 70 72 74 76 78 80 82 84Accuracy (%) 81.36 83.01 83.4 83.15 83.1 76.13 79.61 82.09 82.33 82.34 71.71 76.18 80.03 80.52 80.9 66.97 72.73 77.78 78.37 
7029,unknown,71.71 76.18 80.03 80.52 80.9 66.97 72.73 77.78 78.37 78.81 N=1 N=2 N=3 N=4 (b) CIFAR100 Figure A8: Accuracy of MIMOConv when trained with a variable number of epochs. We report the average accuracy and the standard deviation (error bars) over five runs with different seeds. CIFAR10 CIFAR1000 10 20 30 40 50 60 70 80 90 10095.57 63.59 95.49 79.63 95.53 79.61 95.73 42.17 Accuracy (%) width factor 10/
7030,unknown,"79.61 95.73 42.17 Accuracy (%) width factor 10/1width factor 10/2width factor 10/4width factor 10/8 Figure A9: Initial width majorly affects stability in MIMOConv (N=2, 200 epochs). Configurations are labeled with width factor/initial width factor. Standard initial width factor of vanilla WideResNet would be 1. We report the average accuracy and the standard deviation (error bars) over five runs w"
7031,unknown,"with different seeds. feature maps to 16 ·8 in the first layer (initial width factor 8), or from 1 ·16 feature maps to 16 ·10 in the second layer (initial factor 1). An initial width factor of 4 strikes a good balance and provides stability; hence we will use this value for all other experiments. MIMOConv width factor. WideResNet architectures identify the number of feature maps as the most crucia"
7032,unknown,"most crucial factor in determining the capacity of a model. At the same time, a higher number of feature maps means less interference between bound vectors. In Figure A10, the benefits of large width factors can be observed. Notice that width enters both the number of parameters and the computational complexity quadratically unless grouped convolutions are used, where the number of groups increase"
7033,unknown,"groups increases with the channel width. Not exploring grouped convolutions, a trade-off between performance and accuracy has to be struck. We shall go with a width factor of 10 for all other experiments. 20 37th Conference on Neural Information Processing Systems (NeurIPS 2023) CIFAR10 CIFAR100707274767880828486889092949698100 94.36 75.33 95.41 78.89 95.53 79.61 95.65 80.09 Accuracy (%) width fac"
7034,unknown,"79.61 95.65 80.09 Accuracy (%) width factor 4/2width factor 8/4width factor 10/4width factor 12/4 Figure A10: Performance effect of MIMOConv width factor ( N=2, 200 epochs). Configurations are labeled with width factor/inital width factor. We report the average accuracy and the standard deviation (error bars) over five runs with different seeds. N=1 N=2 N=3 N=4 N=5666870727476788082848688909294969"
7035,unknown,96.95 95.58 94.18 92.61 91.13 Accuracy (%) trainable keysfixed keys (a) CIFAR10 N=1 N=2 N=3 N=4 N=56668707274767880828486889092949698100 83.01 79.61 76.18 72.73 69.64 82.86 79.7 76.48 72.72 69.54 Accuracy (%) trainable keysfixed keys (b) CIFAR100 Figure A11: MIMOConv with trainable or frozen binding keys for different numbers of superposition channels (N). Models are trained for 200 epochs. We rep
7036,unknown,"channels (N). Models are trained for 200 epochs. We report the average accuracy and the standard deviation (error bars) over five runs with different seeds. Freezing of binding keys. We investigate, if the binding keys can be frozen during training. As can be observed in Figure A11, keys do not need to be trainable while still maintaining a high accuracy across a wide range of superposition channe"
7037,unknown,"trainable. 21 37th Conference on Neural Information Processing Systems (NeurIPS 2023) F Experimental Setup and Evaluations on MIMOFormer F.1 Experimental setup Datasets LRA. The long range arena (LRA) benchmark [17] is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, covering a wide range of data types and modalities. Below, we list the LRA tasks used in this work: • ListOps"
7038,unknown,"• ListOps: This dataset [18] tests the capability of modeling hierarchically structured data comprised of long sequences with operators (max, mean, median, modulo sum). The sequence length is up to 2K, and the task is to perform a ten-way classification. ListOps is released under an MIT license. • Text: The IMDb reviews dataset [19] is a document classification benchmark. The bench- mark uses byte"
7039,unknown,"mark uses byte-level sequences of up to 4K, entailing a binary classification task. • Retrieval: This task measures the model’s capability of storing text document information into a compressed representation and matching or retrieving it against other documents. The ACL Anthology Network [20] is a byte/character level dataset with sequence lengths of up to 4K. It is a binary classification task. "
7040,unknown,"BY-NC 4.0 license. • Image: In this task, RGB images of resolution 32 ×32 from the CIFAR10 [9] dataset are flattened and classified by the sequence classification model. As a result, this is a ten-way classification task with sequences of length 1024. • Pathfinder: This task [21] requires a model to decide if two points (circles) are connected by a path of dashes on a black and white 2D image of d"
7041,unknown,"by a path of dashes on a black and white 2D image of dimension 32 ×32. The 2D images are flattened to sequences of length 1024, and classified by the sequence model as a binary classification. Pathfinder is released under MIT license. LRA further contains a more fine-grained version of Pathfinder, called Pathfinder-X, with 128 ×128 image resolution leading to a sequence length of 16K. However, all"
7042,unknown,"image resolution leading to a sequence length of 16K. However, all Transformer variants considered in [17] (including the Performer) failed to learn this task. Since this work demonstrates the MIMO- capability of self-attention models rather than increasing their sequence length, we do not consider Pathfinder-X. Synthetic sequence benchmarks. We use two synthetic benchmarks [22] which measure the "
7043,unknown,"reasoning capability of neural sequence models. • Associative recall: The task is to remember associations between pairs of tokens. For example, given a sequence of tokens a 2 c 4 b 3 d 1, if the model is prompted with a, the expected output is 2, the token following a in the input sequence. If it were prompted with b, the correct output would be 3, etc. Each sequence contains 40 characters, where"
7044,unknown,"dictionary with 20 different characters is used. • Induction head: The task is to recall content after a special token (e.g., ⊢). For example, the string a d b ⊢g f ... h c ⊢would expect the response g. Each sequence contains 30 characters, whereby a dictionary with 20 different characters is used. Both tasks provide a training set (5000 examples) and a test set (500 examples). Training setup As w"
7045,unknown,"As with MIMOConv, the experiments are performed on an NVIDIA A100 Tensor Core GPU with 80 GB memory and 8 CPU cores. All experiments are repeated five times with a different random seed. We report the mean and standard deviations of accuracy to account for variability in training. Overall, the training and evaluation of all MIMOFormer models, including the ablation of the number of training steps,"
7046,unknown,"of training steps, consumed 3112 GPU hours. 22 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Table A3: Architecture and training setup on LRA. L=number of layers; Nhead=number of heads; Dhead=head dimension; E=embedding dimension; Dhidden=hidden dimension in MLP; Bs=batch size; Lr=learning rate. Model Training L Nhead Dhead E D hidden Bs Lr Warmup steps Train steps Dropou"
7047,unknown,"steps Train steps Dropout MIMOFormer Listops 6 8 64 512 2048 64 1e-4 1000 20,000 0.1 Text 6 8 64 512 2048 32 1e-4 8000 40,000 0.1 Retrieval 4 4 32 128 512 32 1e-4 800 60,000 0.1 Image 3 4 64 64 128 256 1e-4 175 70,000 0.1 Pathfinder 4 8 128 128 128 256 1e-4 312 124,800 0.1 DataMUX Listops 12 12 120 120 3072 48 2e-5 0 20,000 0.1 Table A4: Training time (hours) on the long range arena (LRA) using an"
7048,unknown,"ListOps Text Retrieval Image Pathfinder Total Deep models L=6, H=8 L=6, H=8 L=4, H=4 L=3, H=4 L=4, H=8 Performer (reproduced) 3.45±0.00 5.89±0.00 5.93±0.01 3.55±0.02 26.66±0.02 45.47±0.04 MIMOFormer (N=2, att.) 3.96±0.03 7.69±0.01 5.99±0.02 3.51±0.01 25.42±0.17 46.57±0.20 MIMOFormer (N=2, att.+MLP)3.42±0.03 6.71±0.26 5.38±0.01 3.14±0.01 22.50±0.18 41.15±0.31 MIMOFormer (N=4, att.) 3.09±0.01 5.89±0"
7049,unknown,"MIMOFormer (N=4, att.) 3.09±0.01 5.89±0.01 4.35±0.02 2.73±0.29 21.26±0.21 37.32±0.27 MIMOFormer (N=4, att.+MLP)2.42±0.26 4.48±0.26 3.44±0.01 2.10±0.01 17.01±0.37 29.44±0.31 Wide models L=1, H=48 L=1, H=48 L=1, H=16 L=1, H=12 L=1, H=32 Performer (reproduced) 2.46±0.05 5.23±0.01 5.26±0.01 3.21±0.39 29.52±0.03 45.68±0.38 MIMOFormer (N=2, att.) 2.54±0.00 4.79±0.01 4.46±0.01 2.93±0.01 23.45±0.01 38.17±"
7050,unknown,"MIMOFormer (N=2, att.+MLP)2.31±0.02 4.30±0.00 4.16±0.00 2.65±0.00 20.69±0.06 34.12±0.07 MIMOFormer (N=4, att.) 1.95±0.08 4.04±0.15 3.17±0.14 2.25±0.10 19.43±0.37 30.84±0.66 MIMOFormer (N=4, att.+MLP)1.57±0.08 3.39±0.00 2.71±0.12 1.87±0.08 15.51±0.04 25.06±0.16 LRA. Table A3 lists the deep MIMOFormer architecture and the training setup for each task in the LRA benchmark [17]. Both wide and deep mod"
7051,unknown,"the LRA benchmark [17]. Both wide and deep models use the same training setup, but wide models shrink to a single layer L = 1 with inverse scaling in the number of heads Nhead. MIMOFormer uses the same base architecture (number of heads, layers, dimensions, etc.) as proposed in the initial evaluation on LRA [17]. In addition, Table A3 also summarizes the model configuration and the settings used f"
7052,unknown,"our DataMux model on the Roberta architecture as specified in [14]. We adjusted the number of heads, the number of layers, the embedding dimension, and the hidden dimension to approximately match the MIMOFormer’s number of parameters. Before training on ListOps, the scaled-down DataMux model is first pre-trained on the ""retrieval warm-up task"" as outlined in [14]. The training setup and the evalua"
7053,unknown,The training setup and the evaluation setup for MIMOFormer is based on code provided by [23]. Training uses an Adam optimizer (β1=0.9 and β2=0.99) with a OneCycleLR policy [12] and additional warmup. All MIMOFormer configurations use the same number of training steps per task; we note however that configurations with many superposition channels converge more slowly and consequently might benefit f
7054,unknown,"Finally, the output tokens are fed through average pooling and classified with a task-specific readout mechanism. MIMOFormer faces training issues when the number of superposition channels is high (e.g., N=4). To this end, we propose a curriculum learning strategy where the number of superposition is reduced to N’=N/2 at the beginning of the training. This warmup period is set to 1/6th of the tota"
7055,unknown,"training epochs. The overall training setup, including the learning rate scheduling, remains the same. Table A4 shows the training time for the reported models. Since all MIMOFormer configurations use the same number of training steps, we observe a reduced training time using a large number of superposition channels. Contrary to MIMOConv, we do not repeatedly send batches through to ensure equal t"
7056,unknown,23 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Table A5: Training for more steps improves MIMOFormer accuracy. We report the average test accuracy (%) on LRA over five runs with different seeds when training the model for 0.5×/ 1×the training steps described in Table A3. MIMOFormer uses an equal number of query superpositions (N) and value-key tensor product superpositi
7057,unknown,"(N) and value-key tensor product superpositions (M), i.e., N=M. Computation in superposition is performed either in attention only (att.) or in both attention and MLP (att.+MLP). Lis the number of layers, H the number of heads. ListOps Text Retrieval Image Pathfinder Avg. Deep models L=6,H=8 L=6,H=8 L=4,H=4 L=3,H=4 L=4,H=8 Transformer [24] 36.37 64 .27 57 .46 42 .44 71 .40 53 .39 Performer [8] 18."
7058,unknown,"Performer [8] 18.01 65 .40 53 .82 42 .77 77 .05 51 .41 Performer (reproduced) 37.93/38.94 65.45/65.70 81.37/81.58 40.04/40.14 73.01/73.82 59.56/60.04 MIMOFormer (N=2, att.) 38.07/38.08 64.47/65.00 77.16/79.37 37.33/38.21 68.19/72.36 57.04/58.61 MIMOFormer (N=2, att.+MLP)37.28/37.65 64.30/64.39 73.33/76.02 31.62/33.85 56.31/67.98 52.57/55.98 MIMOFormer (N=4, att.) 31.39/37.22 64.73/64.59 57.67/60.9"
7059,unknown,"MIMOFormer (N=4, att.) 31.39/37.22 64.73/64.59 57.67/60.99 27.48/28.16 49.86/55.50 46.23/49.29 MIMOFormer (N=4, att.+MLP)17.91/17.74 53.97/60.71 66.24/72.20 23.30/24.01 50.26/50.33 42.33/45.00 Wide models L=1,H=48 L=1,H=48 L=1,H=16 L=1,H=12 L=1,H=32 Performer (reproduced) 39.13/39.40 65.73/65.73 83.20/83.67 41.53/41.67 73.88/74.11 60.70/60.93 MIMOFormer (N=2, att.) 38.31/38.90 65.40/65.39 78.71/81"
7060,unknown,"MIMOFormer (N=2, att.) 38.31/38.90 65.40/65.39 78.71/81.27 39.98/40.25 71.97/73.51 58.87/59.86 MIMOFormer (N=2, att.+MLP)37.76/37.59 64.73/64.64 75.26/78.30 35.14/36.69 67.60/68.22 56.10/57.09 MIMOFormer (N=4, att.) 36.97/37.71 64.61/64.22 71.50/74.99 31.13/35.43 67.56/69.52 54.35/56.37 MIMOFormer (N=4, att.+MLP)17.41/18.52 64.24/63.53 68.91/74.30 24.21/26.54 53.36/56.33 45.63/47.84 Table A6: Bill"
7061,unknown,"Table A6: Billions of multiply-accumulate (GMAC) operations per sample on Text (subtask of LRA). Model configuration reads L[ayers] = 6 ,Nhead = 8,Dhead = 64,E[mbedding] = 512 ,Dhidden = 2048, and R = 256 where R determines the fidelity of the FA VOR+ attention approximation. Number in parenthesis shows the relative share of the overall complexity. K/Q/V Projections Attention Binding & Unbinding M"
7062,unknown,"K/Q/V Projections Attention Binding & Unbinding MLPs Readout Layer Total Transformer 19.34 (14.9%) 58.80 (45.3%) n.a. 51.62 (39.8%) 0.001 (0.001%) 129.8 Performer 19.34 (21.4%) 19.58 (21.6%) n.a. 51.62 (57.0%) 0.001 (0.001%) 90.5 MIMOFormer (N=2, att.) 19.34 (23.0%) 13.05 (15.5%) 0.050 (0.06%) 51.62 (61.4%) 0.001 (0.001%)84.1 MIMOFormer (N=2, att.+MLP) 19.34 (35.2%) 9.80 (17.8%) 0.050 (0.09%) 25.8"
7063,unknown,"MIMOFormer (N=4, att.) 19.34 (23.9%) 9.78 (12.1%) 0.050 (0.06%) 51.62 (63.9%) 0.001 (0.001%)80.8 MIMOFormer (N=4, att.+MLP) 19.34 (52.0%) 4.90 (13.2%) 0.050 (0.14%) 12.90 (34.7%) 0.001 (0.003%)37.2 Synthetic sequence benchmarks. We use a light-weight MIMOFormer with two layers, one head, an embedding dimension of 32, and a hidden dimension of 128. The model is trained with SGD for 400 epochs using"
7064,unknown,"400 epochs using a learning rate of 5e-4, batch size 32, and a weight decay of 0.1. To configure DataMux for associative recall, we use the SimpleLM language model with 30.5K trainable parameters specified in the Safari repository 1 and insert multiplexing and demultiplexing layers at the input and the output of the model as specified in [14]. Before experimenting with N=2 channels, we first teste"
7065,unknown,"F.2 Number of training steps In our standard training setup, we train the Performer and MIMOFormer models for a large number of training steps (≈2×of what was described in [23]). Here, we show the benefit of a longer training procedure. Table A5 compares the performance of the models when trained with0.5×or 1×as many steps as the training setup reported in Table A3. Note that the test accuracies r"
7066,unknown,"the main text also used the standard training setup (1×). The longer training procedure improves the Performer’s accuracy marginally (0.23–0.48% gain). Conversely, MIMOFormer notably benefits from the longer training in both deep (1.57–3.06% gain) and wide models (0.99–2.21% gain). F.3 Computational complexity As can be deduced from Table A6, the integration of variable binding mechanisms via bind"
7067,unknown,"and unbinding operations is inconsequential. It amounts to only between 0.06% and 0.14% of the computational complexity for MIMOFormer despite being performed at each attention layer. The 1https://github.com/HazyResearch/safari 24 37th Conference on Neural Information Processing Systems (NeurIPS 2023) K/Q/V projections make up a considerable part of the overall computational complexity; hence, com"
7068,unknown,"computing them in superposition would further reduce the number of computes per input. F.4 The importance of faithful attention scores DataMux [14], claims to retain high performance for subsets of the GLUE [25] and CoNLL-2003 [26] benchmarks, despite using up to 40 inputs in superposition. However, as discussed in [27], none of the tasks reported on require attention layers at all. Indeed, DataMU"
7069,unknown,"algorithm, but keeps a single scalar attention score Ai,j for each pair of token positions, which effectively multiplies the (unnormalized) attention score of each (protected) superposition channel: Ai,j = exp ( ⟨ N∑ w=1 k(w) j , N∑ t=1 q(t) i ⟩/ √ D ) ≈exp ( N∑ w=1 ⟨k(w) j , q(w) i ⟩/ √ D ) = N∏ t=1 A(w) i,j (110) As our experiments confirm (see Section 5.2), on more nuanced tasks in NLP such as "
7070,unknown,"As our experiments confirm (see Section 5.2), on more nuanced tasks in NLP such as “associative recall” and “induction head”, which require faithful attention, their method drops to 20.04% and 6.06% for N=2, while ours, at a score of 96.52% and 99.40% respectively, succeeds. Despite investing significant efforts in the training of DataMUX, it cannot perform on these synthetic tasks. This is in"
7071,unknown,"line with the findings of [22] which identifies the lack of attention as the reason that the Structured State Space Sequence (S4) model [28] is able to completely outperform state of the art in LRA [17], but is not competetive for large language models. In contrast to DataMUX, our work approximates true attention and our theoretical derivations show convergence to actual dot-product attention as t"
7072,unknown,"the hidden dimension increases, giving us an even stronger case for applicability to large language models (for instance, GPT-3 uses embedding dimension 12,888, far exceeding the maximum of 512 we report on). 25 37th Conference on Neural Information Processing Systems (NeurIPS 2023) G Supporting Theorems The theorems presented in this section are of general nature and stated for completeness. Theo"
7073,unknown,"Theorem 4. Any inner-product preserving map T : X →Y between two inner-product spaces X,Y is linear Proof. Let u,v ∈X and λ∈C. Then ∥T(λu+ v) −λTu −Tv∥2 = ⟨T(λu+ v) −λTu −Tv,T (λu+ v) −λTu −Tv⟩ (111) = ⟨T(λu+ v),T(λu+ v)⟩−2λ⟨T(λu+ v),Tu⟩−2⟨T(λu+ v),Tv ⟩ + λ2⟨Tu,Tu ⟩+ 2λ⟨Tu,Tv ⟩+ ⟨Tv,Tv ⟩ (112) = ⟨λu+ v,λu + v⟩−2λ⟨λu+ v,u⟩−2⟨λu+ v,v⟩+ λ2⟨u,u⟩+ 2λ⟨u,v⟩+ ⟨v,v⟩ (113) = 2⟨λu+ v,λu + v⟩−2λ⟨λu+ v,u⟩−2⟨λu"
7074,unknown,"= 2⟨λu+ v,λu + v⟩−2⟨λu+ v,λu + v⟩ (115) = 0 (116) which implies T(λu+ v) = λTu + Tv (117) Theorem 5 (Hoeffding’s Inequality). Let X1,...,X n be independent bound random variables satisfying |Xi|≤ ai and E[Xi] = 0. Then, P {⏐⏐⏐⏐⏐ n∑ i=1 Xi ⏐⏐⏐⏐⏐>t } ≤2 exp ( − t2 2 ∑n i=1 a2 i ) (118) Proof. Following [29], we shall prove that P { n∑ i=1 Xi >t } ≤exp ( − t2 2 ∑n i=1 a2 i ) (119) from which by symme"
7075,unknown,"λ> 0 P { n∑ i=1 Xi >t } = P { λ n∑ i=1 Xi >λt } = P { exp ( λ n∑ i=1 Xi ) >exp(λt) } (120) ≤E [ exp ( λ n∑ i=1 Xi )]/ exp(λt) = exp(−λt) n∏ i=1 E[exp(λXi)] (121) where the last equality follows from independence of {Xi}n i=1. Because the function x↦→exp(λx) is convex it holds exp(λx) ≤ai+x 2ai exp(λai) + ai−x 2ai exp(−λai) (122) for all x ∈[−ai,ai]. Thus, since |Xi|≤ ai we may use the above and th"
7076,unknown,"E[exp(λXi)] E[exp(λXi)] ≤E [ ai+x 2ai exp(λai) + ai−x 2ai exp(−λai) ] = 1 2 (eλai + e−λai) = cosh(λai) (123) = ∞∑ n=0 (λai)2n (2n)! ≤ ∞∑ n=0 (λai)2n 2nn! = exp ( (λai)2/2 ) (124) where the Taylor expansion of cosh(·) and exp ( (·)2/2 ) were used and the penultimate step is given by (2n)! ≥2nn!. Hence, we get for any λ> 0 P { n∑ i=1 Xi >t } ≤exp(−λt) n∏ i=1 exp ( (λai)2/2 ) = exp ( −λt+ λ2 2 n∑ i=1"
7077,unknown,2 n∑ i=1 a2 i ) (125) 26 37th Conference on Neural Information Processing Systems (NeurIPS 2023) and in particular for λ= t/(∑n i=1 a2 i) >0 one gets P { n∑ i=1 Xi >t } ≤exp ( − t2 2 ∑n i=1 a2 i ) (126) Theorem 6 (On the Norm of Hadamard Products). Let X ∈SD−1 follow an arbitrary distribution and let Y ∈SD−1 be uniformly distributed and independent from X. Then E [ ∥X⊙Y∥2 2 ] = 1 D (127) and P { ∥
7078,unknown,"[ ∥X⊙Y∥2 2 ] = 1 D (127) and P { ∥X⊙Y∥2 2 ≤1+β D } ≥ 1 1+1/β (128) Proof. Since by definition ∥Y∥2 2 = 1, it follows by rotational symmetry and linearity of expectation that E [ Y2 q ] = D∑ p=1 E [ Y2 p ]/ D= E [D∑ p=1 Y2 p ]/ D= 1 D (129) Also, by linearity of expectation and independence E [ ∥X⊙Y∥2 2 ] = E [D∑ p=1 X2 pY2 p ] = D∑ p=1 E [ X2 p ] E [ Y2 p ] = 1 D D∑ p=1 E [ X2 p ] = 1 DE [D∑ p=1 X"
7079,unknown,"X2 p ] = 1 D (130) Hence, we can apply Markov to get P { ∥X⊙Y∥2 2 ≤1+β D } = 1 −P { ∥X⊙Y∥2 2 ≥1+β D } ≥1 − D·E[∥X⊙Y∥2 2] 1+β = 1 − 1 1+β = 1 1+1/β (131) 27 37th Conference on Neural Information Processing Systems (NeurIPS 2023) H Limitations MIMONets exploit the Blessing of Dimensionality, that with high probability exponentially many (in dimension D) vectors are almost orthogonal. Although the co"
7080,unknown,"dimension D) vectors are almost orthogonal. Although the components of MIMONet are made near isometric through regularization, a certain number of (hidden) dimensions is still necessary. This naturally limits MIMONets to large (oftentimes over-parametrized) models or models employing low-rank decompositions. The number of inputs that can be superposed without incurring heavy losses in accuracy is "
7081,unknown,"The number of inputs that can be superposed without incurring heavy losses in accuracy is limited given a fixed neural network due to increasingly strong interference between the superposition channels. The proposed superposition capable attention mechanism converges to faithful attention (without interference between channels) as the embedding dimension increases, but at the price of only a speed"
7082,unknown,"speedup of N when using N2 superposition channels. Being built on linearized attention such as FA VOR+, it further inherits all their benefits (linear scaling) and drawbacks (limited parallelization and increased memory accesses for autoregressive training (see Section 3.1 in [30]). On the other hand, trivial superposition would yield a speedup of N2 instead, but at the cost of blurring the attent"
7083,unknown,"scores with each token-token score summarizing attention in all superposition channels at once. Such models employing blurry attention are limited to application where imprecise “summarizing” information suffices. 28 37th Conference on Neural Information Processing Systems (NeurIPS 2023) References [1] D. Kleyko, D. Rachkovskij, E. Osipov, and A. Rahimi, “A survey on hyperdimensional comput- ing a"
7084,unknown,"ing aka vector symbolic architectures, part I: Models and data transformations,”ACM Computing Surveys, vol. 55, no. 6, 2022. [2] T. A. Plate, “Holographic reduced representations,” IEEE Transactions on Neural Networks, vol. 6, no. 3, pp. 623–641, 1995. [3] S. I. Gallant and T. W. Okaywe, “Representing objects, relations, and sequences,” Neural Computation, vol. 25, no. 8, pp. 2038–2078, 2013. [4] "
7085,unknown,"[4] R. W. Gayler, “Multiplicative binding, representation operators & analogy,” in Advances in Analogy Research: Integration of Theory and Data from the Cognitive, Computational, and Neural Sciences, 1998. [5] W. Hu, L. Xiao, and J. Pennington, “Provable benefit of orthogonal initialization in optimizing deep linear networks,” in International Conference on Learning Representations, 2020. [6] K. H"
7086,unknown,"[6] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification,” in Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015, pp. 1026–1034. [7] H. Qi, C. You, X. Wang, Y . Ma, and J. Malik, “Deep isometric learning for visual recognition,” in International Conference on Machine Learning (ICML). PMLR, 20"
7087,unknown,"in International Conference on Machine Learning (ICML). PMLR, 2020, pp. 7824–7835. [8] K. M. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser et al., “Rethinking attention with performers,” in International Conference on Learning Representations (ICLR), 2020. [9] A. Krizhevsky, “Learning multiple layers of features from tiny im"
7088,unknown,"2009. [10] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998. [11] N. Yuval, “Reading digits in natural images with unsupervised feature learning,” inProceedings of the NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. [12] L. N. Smith and N. Topin, “Sup"
7089,unknown,"[12] L. N. Smith and N. Topin, “Super-convergence: Very fast training of neural networks using large learning rates,” inArtificial intelligence and machine learning for multi-domain operations applications, vol. 11006, 2019, pp. 369–386. [13] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical risk minimization,” in International Conference on Learning Representations (I"
7090,unknown,"minimization,” in International Conference on Learning Representations (ICLR), 2018. [14] V . Murahari, C. Jimenez, R. Yang, and K. Narasimhan, “DataMUX: Data multiplexing for neural networks,” Advances in Neural Information Processing Systems (NeurIPS), vol. 35, pp. 17 515–17 527, 2022. [15] S. De and S. Smith, “Batch normalization biases residual blocks towards the identity function in deep netw"
7091,unknown,"in deep networks,” Advances in Neural Information Processing Systems (NeurIPS), vol. 33, pp. 19 964–19 975, 2020. [16] S. Zagoruyko and N. Komodakis, “Wide residual networks,” in Proceedings of the British Machine Vision Conference (BMVC). BMV A Press, September 2016, pp. 87.1–87.12. [17] Y . Tay, M. Dehghani, S. Abnar, Y . Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler, “Long "
7092,unknown,"D. Metzler, “Long range arena: A benchmark for efficient transformers,” in International Conference on Learning Representations (ICLR), 2021. [18] N. Nangia and S. R. Bowman, “Listops: A diagnostic dataset for latent tree learning,” arXiv preprint arXiv:1804.06028, 2018. [19] A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts, “Learning word vectors for sentiment analysis,” in Proc"
7093,unknown,"for sentiment analysis,” in Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, 2011, pp. 142–150. [20] D. R. Radev, P. Muthukrishnan, V . Qazvinian, and A. Abu-Jbara, Language Resources and Evaluation, vol. 47, pp. 919–944, 2013. 29 37th Conference on Neural Information Processing Systems (NeurIPS 2023) [21] D. Linsley, J. Kim, V ."
7094,unknown,"[21] D. Linsley, J. Kim, V . Veerabadran, C. Windolf, and T. Serre, “Learning long-range spatial dependencies with horizontal gated recurrent units,” Advances in neural information processing systems, vol. 31, 2018. [22] D. Y . Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re, “Hungry hungry hippos: Towards language modeling with state space models,” inThe Eleventh International Conferenc"
7095,unknown,"on Learning Representations (ICLR), 2022. [23] Y . Xiong, Z. Zeng, R. Chakraborty, M. Tan, G. Fung, Y . Li, and V . Singh, “Nyströmformer: A nyström-based algorithm for approximating self-attention,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 16, 2021, pp. 14 138–14 148. [24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and "
7096,unknown,"I. Polosukhin, “Attention is all you need,”Advances in Neural Information Processing Systems (NeurIPS), vol. 30, 2017. [25] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, “GLUE: A multi- task benchmark and analysis platform for natural language understanding,” in International Conference on Learning Representations (ICLR), 2019. [26] E. F. Sang and F. De Meulder, “Introduction "
7097,unknown,"[26] E. F. Sang and F. De Meulder, “Introduction to the CoNLL-2003 shared task: Language- independent named entity recognition,” arXiv preprint cs/0306050, 2003. [27] M. Hassid, H. Peng, D. Rotem, J. Kasai, I. Montero, N. A. Smith, and R. Schwartz, “How much does attention actually attend? Questioning the importance of attention in pretrained transformers,” in Findings of the Association for Compu"
7098,unknown,"2022, pp. 1403–1416. [28] A. Gu, K. Goel, and C. Re, “Efficiently modeling long sequences with structured state spaces,” in International Conference on Learning Representations (ICLR), 2022. [29] A. S. Bandeira, Mathematics of Data Science, 2020, book draft version 0.1. [Online]. Available: https://people.math.ethz.ch/~abandeira/teaching.html [30] W. Hua, Z. Dai, H. Liu, and Q. Le, “Transformer qu"
7099,unknown,"[30] W. Hua, Z. Dai, H. Liu, and Q. Le, “Transformer quality in linear time,” in International Conference on Machine Learning (ICML). PMLR, 2022, pp. 9099–9117. 30 Multi-Target XGBoostLSS Regression Alexander M¨ arzℵ0 Abstract— Current implementations of Gradient Boosting Machines are mostly designed for single- target regression tasks and commonly assume inde- pendence between responses when used"
7100,unknown,"ate settings. As such, these models are not well suited if non-negligible dependencies exist between targets. To overcome this limitation, we present an exten- sion of XGBoostLSS that models multiple targets and their dependencies in a probabilistic regression setting. Empirical results show that our approach outperforms existing GBMs with respect to runtime and compares well in terms of accuracy."
7101,unknown,"and compares well in terms of accuracy. Keywords: Compositional Data Analysis· Multi-Target Dis- tributional Regression· Probabilistic Modelling· XGBoostLSS I. Introduction The recent M5 forecasting competition demonstrated that tree-based models are highly competitive beyond cross-sectional tabular data. 1 Yet, despite their wide- spread use, when applied in a multivariate setting, cur- rent impl"
7102,unknown,"(GBMs) commonly assume (conditional) independence between target variables.2 However, modelling of mutual dependencies in a probabilistic regression setting has been shown to increase accuracy and to also lead to added insight into the data generating process (Schmid et al., 2023). Therefore, models tailored to single-target regression tasks are not well suited when applied in en- vironments where"
7103,unknown,"vironments where non-negligible inter-target co-relations exist.3 While high-dimensional multivariate forecasting is an active area of research in Deep Learning (see, e.g., Kan et al. (2022); Rasul et al. (2021a,b); Wu et al. (2020); Salinas et al. (2019)) with applications ranging ℵ0Author for correspondence: alex.maerz@gmx.net 1For details on the M5 competition, see Makridakis et al. (2021a,b). "
7104,unknown,"(2021a,b). For a good overview of tree-based methods and their use in the M5 competition, see Januschowski et al. (2021). 2While XGBoost (Chen and Guestrin, 2016) and CatBoost (Prokhorenkova et al., 2018; Dorogush et al., 2017) allow modelling of several responses, with a separate model trained for each target, LightGBM (Ke et al., 2017) currently does not support multi-target regression. A workar"
7105,unknown,"regression. A workaround often suggested for extending models that do not natively support multi-target regression is to use scikit- learn’s Multi-Output-Regressor. However, since a separate model is trained per target, this does not allow modelling of dependencies between multiple responses. 3In the following, we use the terms multi-target and multivariate regression interchangeably for denoting "
7106,unknown,"a N×D response matrix y = (yi1,...,yiD)T,i = 1,...,N with D denoting the target dimension. from anomaly detection to causal analysis or retail sales forecasting, where modelling of dependencies between articles is crucial to account for cannibalization eﬀects, tree-based approaches have received comparatively few multivariate extensions. Recent advances include Pande et al. (2022) who introduce a "
7107,unknown,"to model multivariate longitudinal responses, as well as Nespoli and Medici (2022) who present a computation- ally eﬃcient algorithm for ﬁtting multivariate boosted trees. To address the problem of low generalization abil- ity and tree redundancy when dependencies between sev- eral targets are ignored, Zhang and Jung (2021) propose a general method to train GBMs with multiple targets. O’Malley et "
7108,unknown,"Duan et al. (2020) to a multivariate Gaussian setting and ´Cevid et al. (2021) introduce a distributional regression forest for multivariate responses. Based on Bayesian ad- ditive regression trees (BARTs) of Chipman et al. (2010), Clark et al. (2021) develop multivariate time series mod- els and Um et al. (2020) extend BARTs to allow mod- elling of multivariate skewed responses. Lang et al. (2020"
7109,unknown,"introduce multivariate distributional regression forests to probabilistically predict wind proﬁles. Quan and Valdez (2018) use multivariate trees to model insurance claims data with correlated responses and Miller et al. (2016) introduce a multivariate extension of gradient boosted regression trees for continuous multivariate responses. With this paper, we contribute to the emerging lit- erature o"
7110,unknown,"a multivariate extension of the univariate XGBoostLSS introduced by M¨ arz (2019). Our approach leverages automatic diﬀerentiation using PyTorch (Paszke et al., 2019), which facilitates implementation of distributions for which Gradients and Hessians are diﬃcult to derive analytically. The remainder of this paper is organized as fol- lows: Section II introduces our multivariate XGBoostLSS framewor"
7111,unknown,"study and real world examples. Section IV concludes. 4 II. Multi-Target XGBoostLSS In its original formulation, distributional modelling re- lates all parameters of a univariate response distribution to covariates x. In particular, it assumes the response to follow a distribution D ( θθθ(x) ) that depends on up 4The code of the implementation will be made available on /githubStatMixedML/XGBoostLSS"
7112,unknown,"of the paper. arXiv:2210.06831v1 [cs.LG] 13 Oct 2022 2 to four parameters, i.e., yi ind∼ D(µix,σ2 ix,νix,τix),i = 1,...,N, where µix and σ2 ix are often location and scale parameters, respectively, while νix and τix correspond to shape parameters such as skewness and kurtosis. 5 More generally, univariate distributional modelling can be formulated as follows yi ind∼ D   h1 ( θi1(xi) ) = ηi1"
7113,unknown,"yi ind∼ D   h1 ( θi1(xi) ) = ηi1 h2 ( θi2(xi) ) = ηi2 ... hK ( θiK(xi) ) = ηiK   for i= 1,...,N, where D(·) denotes a parametric distri- bution for the response y = (y1,...,yN)T that depends on Kdistributional parameters θk, k= 1,...,K, and with hk(·) denoting a known function relating distributional parameters to predictors ηηηk. The predictor speciﬁcation ηηηk = fk(x),k = 1,...,K i"
7114,unknown,"GAM-type, Deep Learning or GBMs as in our case. To allow for a more ﬂexible framework that explicitly models the dependencies of a D-dimensional response y = (yi1,...,yiD)T,i= 1,...,N, Klein et al. (2015) intro- duce a multivariate version of distributional regression. Similar to the univariate case, multivariate distributional regression relates all K parameters of a multivariate density fi ( yi1"
7115,unknown,"yi1,...,yiD|θi1(x ) ,...,θiK(x) ) to a set of co- variates x.6 A. Multivariate Gaussian Regression A common choice for multivariate probabilistic regres- sion is to assume a multivariate Gaussian distribution, with the density given as 7 f ( y|θθθx ) = 1√ (2π)D|ΣΣΣ x| exp ( −1 2(y−µµµx)TΣΣΣ −1 x (y−µµµx) ) where µµµx ∈RD represents a vector of conditional means, ΣΣΣ x is a positive deﬁnite symmetr"
7116,unknown,"matrix and |·|denotes the determinant. For the bivariate case D= 2, ΣΣΣ x can be expressed as 5While the original formulation of GAMLSS in Rigby and Stasinopoulos (2005) suggests that any distribution can be de- scribed by location, scale and shape parameters, it is not necessarily true that the observed data distribution can actually be character- ized by all of these parameters. Hence, we prefer"
7117,unknown,"ized by all of these parameters. Hence, we prefer to use the term distributional modelling. 6To improve on the convergence and stability of XGBoostLSS estimation, unconditional Maximum Likelihood estimates of the parameters θk, k= 1,...,K are used as oﬀset values. Also, since XGBoostLSS updates the parameters by optimizing Gradients and Hessians, it is important that these are comparable in magnit"
7118,unknown,"for all distributional parameters. Due to variability regarding the ranges, the estimation of Gradients and Hessians might become unstable so that XGBoostLSS might not converge or might con- verge very slowly. To mitigate these eﬀects, we have implemented a stabilization of Gradients and Hessians. 7In the further course of this section, we follow the notation of Muschinski et al. (2022); O’Malley "
7119,unknown,"(2019). ΣΣΣ ix = [ σ2 i,1(x) ρi(x)σi,1(x)σi,2(x) ρi(x)σi,2(x)σi,1(x) σ2 i,2(x) ] with the variances on the diagonal and the covariances on the oﬀ-diagonal, for i= 1,...,N. Cholesky Decomposition of Covariance Matrix To ensure positive deﬁniteness of ΣΣΣ, the D(D+ 1)/2 entries of the covariance matrix must satisfy speciﬁc conditions.8 For the bivariate case, this can be ensured by applying exponent"
7120,unknown,"a suitable transformation to restrict the coeﬃcient of correlation ρ∈[−1,1]. However, in high-dimensional set- tings, where all moments are modelled as functions of covariates, ensuring positive deﬁniteness of the covari- ance matrix becomes challenging, since joint restrictions for the elements of ΣΣΣ are necessary (Muschinski et al., 2022). A computationally more tractable approach to ensure pos"
7121,unknown,"decomposition, that uniquely decomposes the covariance matrix as follows ΣΣΣ = LLT where L ∈RD×D is a lower triangular matrix.9 To ensure ΣΣΣ to be positive deﬁnite, the D diagonal elements ℓii of L need to be strictly positive, whereas all D(D−1)/2 oﬀ diagonal elements ℓij can take on any value in R, leaving them untransformed. Illustrative for the bivariate case, the Cholesky factor L is given a"
7122,unknown,"L = [exp(ℓ11) 0 ℓ21 exp(ℓ22) ] In addition to reparameterizing the covariance matrix, the Cholesky decomposition is also computationally eﬃ- cient, since only the determinant of a triangular matrix needs to be calculated (Salinas et al., 2019). Low-Rank Covariance Approximation While eﬃcient for low to medium dimensions of D, the computational cost of the Cholesky-decomposition be- comes prohibiti"
7123,unknown,"comes prohibitive in high-dimensional settings. To reduce the computational overhead, the covariance matrix ΣΣΣ can be approximated via the sum of a diagonal matrix K ∈ RD×D + and a unrestricted low-rank matrix V ∈RD×r ΣΣΣ = K+ VVT =  ]) exp(K1) ... 0 ... ... ... 0 ... exp(KD) ( [+  ]) V1 ... VD ( [  ]) V1 ... VD ( [ T 8Without loss of generality, we notationally omit the explicit dependency"
7124,unknown,"dependency of all parameters on x and i= 1,...,Nin the following. 9For the precision matrix, the Cholesky-decomposition as given as ΣΣΣ −1 = (L−1)TL−1. 10In contrast to the original formulation of ΣΣΣ, the elements in L do not have any direct interpretation. 3 where exp( ·) ensures all diagonal entries of K to be strictly positive and the rank parameter r governs the quality of the approximation. "
7125,unknown,"ciency of this approach results from the fact that the rank parameter r<<D can typically be chosen much smaller than the number of target variables D (Salinas et al., 2019). Showing the relationship between the response dimension D and the number of parameters K, Table 1 indicates that the number of parameters increases ex- ponentially for the Cholesky-decomposition, while the re- lationship is on"
7126,unknown,"making it more suitable for high-dimensional settings. [Table 1 about here.] B. Multivariate Student-T Regression As a generalization of the multivariate Gaussian, the multivariate Student-T is suitable when modelling heavy-tailed data, i.e., when there is more mass in the tails of the distribution. The density is given as f ( y|θθθx ) = Γ [νννx+D 2 ] Γ[νννx 2 ](πνννx)D/2|ΣΣΣ x|1/2 [ 1 +(y−µµµx)TΣ"
7127,unknown,"x (y−µµµx) νxνxνx ]−νννx+D 2 with covariance matrixνννx(νννx −2)−1ΣΣΣ x and Γ[·] denoting the gamma function. µµµx ∈RD and ΣΣΣ x are deﬁned as for the multivariate Gaussian. The multivariate Student-T distribution has an additional degrees of freedom param- eter νννx >2 that governs the tail behaviour, where for νννx →∞ the Student-T converges in distribution to the multivariate Normal. Similar to"
7128,unknown,"sian, we use the Cholesky-decomposition ΣΣΣ x = LxLT x to ensure the covariance matrix to be positive deﬁnite. 11 C. Dirichlet Regression While the multivariate Gaussian and Student-T are deﬁned for y ∈RD, the Dirichlet distribution is com- monly used for modelling non-negative compositional data, i.e., data that consist of sub-sets that are frac- tions of some total. Compositional data are typica"
7129,unknown,"represented as proportions or percentages summing to 100%, so that the Dirichlet extends the univariate beta- distribution to the multivariate case (Klein et al., 2015). Dating back to the seminal paper of Aitchison (1982), compositional data analysis (CoDa) is a branch of statis- tics that deals with multivariate observations carrying relative information and ﬁnds widespread use in ecology (Douma"
7130,unknown,"(Douma and Weedon, 2019), economics Fry et al. (2000) or political science (Katz and King, 1999). As a result of the unit-sum constraint, models that use distributions designed for unconstrained data typically suﬀer from 11Unlike the multivariate Normal, a low-rank approximation of the covariance matrix is not yet implemented for the multivariate Student-T, but is planned in future releases. the p"
7131,unknown,"the problem of spurious correlation when applied to compositional data (Aitchison, 2003). The density of the Dirichlet distribution with parame- ters αααx = (αx,1,...,αx,D) ∈RD + with ∑D d=1 yd= 1 for all yd∈[0,1] is given by f ( y|θθθx ) = 1 B(αααx) D∏ d=1 y αx,d−1 d where the normalizing constant is expressed as the multi- nomial beta-function B(αααx) = D d=1 Γ(αx,d) Γ (D∑ d=1 αx,d ) To ensure "
7132,unknown,"D d=1 Γ(αx,d) Γ (D∑ d=1 αx,d ) To ensure positivity, we use exp(αx,d) for all d= 1,...,D. The estimated parameters have the interpretation of providing the probability of an event falling into category d, i.e., E(yd) = αd α0 , with α0 = ∑D d=1 αd (Klein et al., 2015). III. Applications In this section, we present simulation studies and real- world examples to illustrate the functionality of our a"
7133,unknown,"approach. All hyper-parameters of the models presented in this paper are selected using Optuna of Akiba et al. (2019). For all models, Table 2 shows the space of the tuneable hyper-parameter search. [Table 2 about here.] A. Simulation Multivariate Gaussian Regression We start with a trivariate Gaussian scenario ( N = 10,000), where all moments of the distribution are al- lowed to vary with covaria"
7134,unknown,"[Figure 1 about here.] Figure 1 shows that the estimated parameters of the multivariate Gaussian, with the covariance matrix being parametrized using either the Cholesky-decomposition (Panel 1a) or the low-rank approximation (Panel 1b), closely match the true parameters, even for a rank as low as 2. Yet, for ρρρ21 and ρρρ32 speciﬁcally, the low-rank approximation shows some deviations and generall"
7135,unknown,"somewhat more erratic behaviour of the estimates as compared to the Cholesky-decomposition. 12Hyper-parameters for each of the models in the simulation study are optimized running 100 hyper-parameter trails each. 4 Multivariate Student-T Regression Figure 2 presents the simulation results for the trivari- ate Student-T distribution, where the covariance ma- trix is parametrized via the estimated C"
7136,unknown,"trix is parametrized via the estimated Cholesky-factors ˆLˆLT.13 [Figure 2 about here.] The estimates presented in Figure 2 are still quite close to the true shapes, but not as accurate as for the mul- tivariate Gaussian. In general, the Student-T estimates exhibit a somewhat more erratic behaviour than the Gaussian Cholesky-decomposition. This is especially true for the degrees of freedom paramet"
7137,unknown,"approximate the U-shape well. Also, parameter estimates of µµµ3 and ρρρ21 deviate slightly from the true values. Dirichlet Regression To evaluate the ability of our approach of inferring the relationship between covariates and a set of Dirichlet- distributed responses, we apply our model to the widely used Arctic-Lake dataset of (Aitchison, 2003) that con- tains information on sediment composition"
7138,unknown,"clay) of an Arctic lake. The data are shown in Figure 3. [Figure 3 about here.] The dataset includes 39 measurements and we model the sediment composition as a function of water depth in meters. Figure 4 compares the results of our model to a scatter-smooth estimate. To facilitate visual comparison, we use smoothed estimates of our model. [Figure 4 about here.] The scatter-smooth and our model est"
7139,unknown,"close agreement, showing that with increasing depth, the relative frequency of sand decreases while the proportion of silt and clay increases. In summary, despite some deviations, our multivariate XGBoostLSS models approximate well the conditional moments of the underlying data generating processes for all distributions studied. For the simulated datasets, one could further improve the accuracy by"
7140,unknown,"trails of the hyper-parameter search or the number of boosting iterations, which we set to 100. B. Regression Datasets We benchmark our model, which we refer to as mXG- BoostLSS, against the multivariate NGBoost (mNG- Boost) model of O’Malley et al. (2021) and our univariate XGBoostLSS implementation (M¨ arz, 2019) using a sub- set of the datasets of Spyromitros-Xiouﬁs et al. (2016), as well as th"
7141,unknown,"well as the California housing dataset of Pace and Barry (1997):14 13Similar to the Gaussian scenario, we set N= 10,000. 14We restrict the datasets in Spyromitros-Xiouﬁs et al. (2016) to continuous or close-to continuous responses only. We use the Cali- fornia housing dataset of scikit-learn. All other datasets are publicly available at http://mulan.sourceforge.net/datasets-mtr.html. We run all ex"
7142,unknown,"We run all experiments on a 8-core Intel(R) i7-7700T CPU with 32 GB of RAM. • Airline Ticket Price (1d): This dataset is concerned with the prediction of airline ticket prices and the target variables are the next day price for 6 ﬂight preferences: (1) any airline with any number of stops, (2) any airline non-stop only, (3) Delta Air- lines, (4) Continental Airlines, (5) Airtrain Airlines, and (6)"
7143,unknown,"and (6) United Airlines. • California Housing: This dataset was derived from the 1990 U.S. census and contains information with respect to demography, location, as well as general information regarding the house in Californian dis- tricts. As responses, we use the median income and the median house value. • Jura: The dataset consists of measurements of con- centrations of seven heavy metals record"
7144,unknown,"locations in the topsoil of a region of the Swiss Jura. The type of land use and rock type are also recorded for each location. Cadmium, copper and lead are treated as targets, while the remaining metals along with land use type, rock type and the coordinates of each location are used as features. • Occupational Employment Survey (2010): The Oc- cupational Employment Survey data were obtained from"
7145,unknown,"from the annual Occupational Employment Survey compiled by the US Bureau of Labor Statistics in 2010. Each target denotes the estimated num- ber of full-time equivalent employees across many employment types (e.g., doctor, dentist, car repair technician, etc.) across 403 cities. • River Flow 1: The dataset, obtained from the US National Weather Service and collected between September 2011 and Sept"
7146,unknown,"from hourly river ﬂow observations for 8 sites in the Mississippi River network and the task is to predict river network ﬂows at speciﬁc locations. • Supply Chain Management (1d) : This dataset is derived from the Trading Agent Competition in Sup- ply Chain Management (TAC SCM) tournament from 2010 and contains 16 targets, each correspond- ing to the next day mean price for each product in the com"
7147,unknown,"the competition. • Slump: This dataset is concerned with the prediction of three properties of concrete (slump, ﬂow and compressive strength) as a function of the content of seven concrete ingredients. We also include a subset of the simulated trivariate Gaussian and Student-T datasets presented in Section III-A, with additional noise features added. Table 3 presents the data and its characteristi"
7148,unknown,"[Table 3 about here.] From Table 3 it follows that there is a non-negligible dependence between targets in all datasets. It is in- teresting to see how the univariate baseline, which as- sumes independence between targets, compares to the 5 multivariate models. For conducting the experiments, we create 11 randomly shuﬄed folds for all datasets. Each is split into train and test, where 80% is used "
7149,unknown,"training and the remaining 20% for evaluation. The ﬁrst fold is used for hyper-parameter tuning only, where we run 100 hyper-parameter trails for each model-dataset combination and select the best set of hyper-parameters. All models are initialized with 500 boosting rounds, with the optimal number of iterations based on early-stopping. Once optimized, we keep the optimal set of hyper- parameters c"
7150,unknown,"model evaluation. 15 All models except the Student-T assume a Gaussian distribution, with the accuracy being evaluated using the negative log-likelihood (NLL). Table 4 reports the results. [Table 4 about here.] Table 4 shows that our models compare well with ex- isting implementations. With an average rank of 2.2, mXGBoostLSS-G-C has the highest overall accuracy, closely followed by mXGBoostLSS-T-"
7151,unknown,"G-C with an average rank of 2.4 each. For kurtotic datasets, the mXGBoostLSS-T-C outperforms its Gaus- sian counterparts due to its additional degrees of free- dom parameter. Probably due to its ﬁxed and non- optimized rank parameter r, the LRA model ranks last in our comparison. For the comparatively high-dimensional scm1d and oes10-datasets, the LRA model shows some diverging behaviour. We will "
7152,unknown,"of the rank on the LRA model in Section III-C. The comparison between models also shows that explicitly modelling dependencies between targets tends to increase accuracy: for 7 out of 9 datasets, the multivariate models have a higher accuracy than the univariate model. This is consistent with the results of Schmid et al. (2023) who report that the performances of multivariate approaches were subst"
7153,unknown,"were substantially better than the univariate ones for some of the simulation settings considered. To further benchmark our models, Table 5 reports the variability (NLLmax−NLLmin) of the NLL-scores across folds and datasets, demonstrating that explicit modelling of dependencies tends to stabilize estimation. [Table 5 about here.] Compared to its multivariate counterparts, the univari- ate model sh"
7154,unknown,"datasets and folds. As an example, consider the sl- dataset, for which the univariate XGBoostLSS model shows some divergent predictions. The lower overall vari- ability might be attributed to the fact that joint mod- elling of all targets tends to stabilize the estimation in the multivariate setting. Within the class of multivariate 15For the high-dimensional scm1d and oes10-datasets, we were not "
7155,unknown,"not able to complete the hyper-parameter search for mNGBoost, mainly due to out-of-memory issues, non-positive deﬁnite covari- ance matrices, or unrealistically long runtimes for a single iteration. For these reasons, we had to re-initialize the hyper-parameter search several times and also reduce the number of hyper-parameter trails to 20. models, mXGBoostLSS-G-C shows the least variation, closel"
7156,unknown,"closely followed by mXGBoostLSS-T-C. In addition to assessing the accuracy, Table 6 presents an overview of normalized runtimes. To ensure a fair comparison, we set all hyper-parameters of the models to the same values. All models are estimated using CPUs and all XGBoostLSS models, both univariate and multivariate, are trained without leveraging its fast his- togram tree-growing method. We exclude"
7157,unknown,"XGBoostLSS model from the analysis since it has less parameters compared to its multivariate counterparts and therefore always lower runtimes. [Table 6 about here.] From Table 6 it follows that, while mNGBoost-G-C has the lowest runtime for the smallest sl-dataset, the Cholesky-based mXGBoostLSS models scale well with the number of observations and with the mXGBoostLSS- G-C model being the most eﬃ"
7158,unknown,"across datasets. The eﬃciency is likely to increase further if we leverage XGBoostLSS’s GPU-histogram training. Also, for fairly large datasets, one can use distributed training with Dask for even better scalability. Table 6 also shows that the LRA-model beneﬁts from its linear increase in parameters which results in the lowest run- time for the high-dimensional oes10-dataset. C. Ablation Study Fo"
7159,unknown,"C. Ablation Study Following the discussion in the previous section, it remains to be investigated why the LRA-model does not perform as well as the other models, especially for datasets with small D and N. One reason might be the low number of observations relative to the number of estimated parameters. Recall that for the Gaussian, the Cholesky factorization of the covariance matrix requires esti"
7160,unknown,"need to be estimated for the low-rank covariance ap- proximation. As an example, take the relatively small sl-dataset with D= 3: while only 9 parameters have to be estimated for the Cholesky model, there are already 21 parameters for the LRA-model with r= 5. Another reason might be related to the choice of the rank parameter r. While all hyper-parameters of the Cholesky models are optimized, we se"
7161,unknown,"datasets, mainly to keep the computational cost low while still maintaining a reasonable ﬁt. To further inves- tigate the eﬀect of r on the accuracy, we run additional experiments using a small subset of the datasets with varying values of r. Table 7 reports the results. [Table 7 about here.] Table 7 shows that the quality of the covariance approxi- mation varies with the rank parameter. However, "
7162,unknown,"no general recommendation that higher values of r tend to increase accuracy. This can be seen from the atp1d- dataset, where the model tends to overﬁt with increasing values of r, resulting in lower accuracy. For the relatively 6 low-dimensional ju-dataset, higher values of r increase accuracy, while for the sl-dataset, a moderately low rank gives the best results. Since the order of r is dependin"
7163,unknown,"on the size of the dataset and the characteristics de- termining the covariance structure, our recommendation would be to add the rank parameter as a tuneable hyper- parameter. IV. Conclusion, Limitations and Future Research While most implementations of Gradient Boosting Machines are tailored to single-target settings, this pa- per presents an extension of XGBoostLSS that models multi-targets and"
7164,unknown,"regression environment. Using simulation studies and real-world data, we have shown that our approach out- performs existing GBMs with respect to runtime and is competitive in terms of accuracy. We have also demon- strated that explicit modelling of dependencies between targets can lead to an increase in accuracy. Despite its ﬂexibility, we acknowledge some limita- tions of our approach that requi"
7165,unknown,"Although the base XGBoost implementation accepts a N×D array of responses, model training is still op- timized for single-target models. This implies that for our XGBoostLSS approach, which is based on multi- parameter training, with a separate tree grown for each parameter, estimating many parameters for a large dataset can become computationally expensive, with the computational cost growing O(K"
7166,unknown,"emphasize, though, that high-dimensional multi-target regression and multiclass-classiﬁcation is a known scaling problem of current GBMs implementations and therefore not a unique limitation of our approach. An interesting scope for future implementation and research would be a more runtime eﬃcient version of our framework, where multiple parameters can be estimated with a single tree. In addition"
7167,unknown,"to allow for a more ﬂexible choice of multivariate re- sponse distributions beyond the Gaussian, Dirichlet and Student-T to be an interesting reﬁnement. References Aitchison, J. (1982). The Statistical Analysis of Compo- sitional Data. Journal of the Royal Statistical Society: Series B (Methodological), 44(2):139–160. Aitchison, J. (2003). The statistical analysis of composi- tional data. Blackbur"
7168,unknown,"tional data. Blackburn Press, Caldwell, N.J. Akiba, T., Sano, S., Yanase, T., Ohta, T., and Koyama, M. (2019). Optuna: A Next-generation Hyperpa- rameter Optimization Framework. In Teredesai, A., editor, Proceedings of the 25th ACM SIGKDD Inter- national Conference on Knowledge Discovery & Data Mining, ACM Digital Library, pages 2623–2631, New York,NY,United States. Association for Computing Machi"
7169,unknown,"Machinery. ´Cevid, D., Michel, L., N¨ af, J., Meinshausen, N., and B¨ uhlmann, P. (2021). Distributional Random Forests: Heterogeneity Adjustment and Multivariate Distribu- tional Regression. arXiv Pre-Print. Chen, T. and Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining, KDD ’16, p"
7170,unknown,"794, New York, NY, USA. Association for Computing Machinery. Chipman, H. A., George, E. I., and McCulloch, R. E. (2010). BART: Bayesian additive regression trees. The Annals of Applied Statistics, 4(1):266–298. Clark, T. E., Huber, F., Koop, G., Marcellino, M., and Pfarrhofer, M. (2021). Tail Forecasting with Multi- variate Bayesian Additive Regression Trees. Federal Reserve Bank of Cleveland, Wor"
7171,unknown,"Dorogush, A. V., Ershov, V., and Gulin, A. (2017). CatBoost: gradient boosting with categorical features support. Workshop on ML Systems at NIPS. Douma, J. C. and Weedon, J. T. (2019). Analysing continuous proportions in ecology and evolution: A practical introduction to beta and Dirichlet regression. Methods in Ecology and Evolution, 10(9):1412–1430. Duan, T., Anand, A., Ding, D. Y., Thai, K. K.,"
7172,unknown,"S., Ng, A. Y., and Schuler, A. (2020). NGBoost: Nat- ural Gradient Boosting for Probabilistic Prediction. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 2690–2700. PMLR. Fry, J. M., Fry, T. R. L., and McLaren, K. R. (2000). Compositional data analysis and zeros in micr"
7173,unknown,"Applied Economics, 32(8):953–959. Januschowski, T., Wang, Y., Torkkola, K., Erkkil¨ a, T., Hasson, H., and Gasthaus, J. (2021). Forecasting with trees. International Journal of Forecasting. Kan, K., Aubet, F.-X., Januschowski, T., Park, Y., Benidis, K., Ruthotto, L., and Gasthaus, J. (2022). Multivariate Quantile Function Forecaster. arXiv Pre- Print. Katz, J. N. and King, G. (1999). A Statistical"
7174,unknown,"for Multiparty Electoral Data. The American Political Science Review, 93(1):15–32. Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., and Liu, T.-Y. (2017). LightGBM: A Highly Eﬃcient Gradient Boosting Decision Tree. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, pages 3149–3157, Red Hook, NY, USA. Curran Asso- ciates Inc. Klei"
7175,unknown,"ciates Inc. Klein, N., Kneib, T., Klasen, S., and Lang, S. (2015). Bayesian structured additive distributional regression for multivariate responses. Journal of the Royal Statis- tical Society: Series C (Applied Statistics), 64(4):569– 591. Lang, M. N., Mayr, G. J., Schlosser, L., Simon, T., Stauﬀer, R., and Zeileis, A. (2020). Multivariate Dis- tributional Regression Forests for Probabilistic Now"
7176,unknown,"casting of Wind Proﬁles. In Irigoien, I., Lee, D.-J., 7 Mart´ ınez-Minaya, J., and Rodr´ ıguez-´Alvarez, M. X., editors, Proceedings of the 35th International Workshop on Statistical Modelling, pages 142–147, Bilbao. Makridakis, S., Spiliotis, E., and Assimakopoulos, V. (2021a). The M5 competition: Background, organi- zation, and implementation. International Journal of Forecasting. Makridakis, S."
7177,unknown,"Z., Gaba, A., Tsetlin, I., and Winkler, R. L. (2021b). The M5 uncertainty competition: Results, ﬁndings and conclusions. International Journal of Forecasting. M¨ arz, A. (2019). XGBoostLSS - An extension of XG- Boost to probabilistic forecasting. arXiv Pre-Print, pages 1–23. Miller, P. J., Lubke, G. H., McArtor, D. B., and Berge- man, C. S. (2016). Finding structure in data using multivariate tree"
7178,unknown,"multivariate tree boosting. Psychological methods, 21(4):583–602. Muschinski, T., Mayr, G. J., Simon, T., Umlauf, N., and Zeileis, A. (2022). Cholesky-based multivariate Gaussian regression. Econometrics and Statistics. Nespoli, L. and Medici, V. (2022). Multivariate Boosted Trees and Applications to Forecasting and Control. arXiv Pre-Print. O’Malley, M., Sykulski, A. M., Lumpkin, R., and Schuler,"
7179,unknown,"A. (2021). Multivariate Probabilistic Regression with Natural Gradient Boosting. arXiv Pre-Print, pages 1– 19. Pace, K. R. and Barry, R. (1997). Sparse spatial autore- gressions. Statistics & Probability Letters, 33(3):291– 297. Pande, A., Ishwaran, H., and Blackstone, E. (2022). Boosting for Multivariate Longitudinal Responses. SN Computer Science, 3(3):186. Paszke, A., Gross, S., Massa, F., Lere"
7180,unknown,"J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K¨ opf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Proceedings of the 33rd International Conference on Neural Information Processing Systems. Curran Associates Inc,"
7181,unknown,"Curran Associates Inc, Red Hook, NY, USA. Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., and Gulin, A. (2018). CatBoost: unbiased boosting with categorical features. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa- Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc. Quan, Z. and Valdez, E. A. (2018). Pred"
7182,unknown,"of insurance claims using multivariate decision trees. Dependence Modeling, 6(1):377–407. Rasul, K., Seward, C., Schuster, I., and Vollgraf, R. (2021a). Autoregressive Denoising Diﬀusion Models for Multivariate Probabilistic Time Series Forecasting. In Meila, M. and Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learn- ing, volume 139 of Proceedings of Machine Lear"
7183,unknown,"Research, pages 8857–8868. PMLR. Rasul, K., Sheikh, A.-S., Schuster, I., Bergmann, U. M., and Vollgraf, R. (2021b). Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows. In International Conference on Learning Rep- resentations. Rigby, R. A. and Stasinopoulos, D. M. (2005). Gener- alized additive models for location, scale and shape. Journal of the Royal Statistical"
7184,unknown,"Journal of the Royal Statistical Society: Series C (Ap- plied Statistics), 54(3):507–554. Salinas, D., Bohlke-Schneider, M., Callot, L., Medico, R., and Gasthaus, J. (2019). High-dimensional mul- tivariate forecasting with low-rank Gaussian Copula Processes. In H. Wallach, H. Larochelle, A. Beygelz- imer, F. d \textquotesingle Alch´ e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Inform"
7185,unknown,"Processing Systems, volume 32. Curran Associates, Inc. Schmid, L., Gerharz, A., Groll, A., and Pauly, M. (2023). Tree-based ensembles for multi-output regres- sion: Comparing multivariate approaches with sepa- rate univariate ones. Computational Statistics & Data Analysis, 179:107628. Spyromitros-Xiouﬁs, E., Tsoumakas, G., Groves, W., and Vlahavas, I. (2016). Multi-target regression via input spac"
7186,unknown,"space expansion: treating targets as inputs. Machine Learning, 104(1):55–98. Um, S., Linero, A., Sinha, D., and Bandyopadhyay, D. (2020). Bayesian Additive Regression Trees for Multivariate Skewed Responses. In American Statisti- cal Association, editor, 2020 JSM Proceedings: Papers Presented at the Virtual Joint Statistical Meetings, Au- gust 2-6, 2020, and Other ASA-sponsored Conferences. Americ"
7187,unknown,"American Stastistical Association. Wu, Z., Pan, S., Long, G., Jiang, J., Chang, X., and Zhang, C. (2020). Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks. In Gupta, R., editor, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Dis- covery & Data Mining, ACM Digital Library, pages 753–763, New York,NY,United States. Association for Computi"
7188,unknown,"Computing Machinery. Zhang, Z. and Jung, C. (2021). GBDT-MO: Gradient- Boosted Decision Trees for Multiple Outputs. IEEE Transactions on Neural Networks and Learning Sys- tems, 32(7):3156–3167. TABLES 8 TABLE 1: Number of parameters for Cholesky and Low- Rank Approximation (LRA) YD Cholesky LRA(r=5) LRA(r=10) LRA(r=20) 2 5 14 24 44 5 20 35 60 110 10 65 70 120 220 50 1,325 350 600 1,100 100 5,150 7"
7189,unknown,"100 5,150 700 1,200 2,200 500 125,750 3,500 6,000 11,000 1,000 501,500 7,000 12,000 22,000 10,000 50,015,000 70,000 120,000 220,000 The table shows the number of parametersKto estimate for a Multivari- ate Gaussian for the Cholesky D(D+ 3)/2 and the low-rank covariance matrix approximation D(2 +r) as functions of the response dimension YD. TABLE 2: Hyper-Parameter Search-Space Range learning-rate "
7190,unknown,"max-depth [2, 10] gamma [0, 100] sub-sample [0.4, 1.0] col-sample [0.4, 1.0] min-child-weight [0, 500] boosting-iterations [500] early-stopping-rounds [2] TABLE 3: Dataset Overview Dataset Abbreviation Observations YD Features Dependency-Strength Airline Ticket Price (1d) atp1d 337 6 411 0.8013 [0.7305, 0.9166] California Housing ch 20,640 2 7 0.6881 [0.6881, 0.6881] Jura ju 359 3 15 0.1907 [0.156"
7191,unknown,"Occupational Employment Survey (2010) oes10 403 16 298 0.549 [0.3195, 0.6928] River Flow 1 rf1 9,125 7 65 0.5028 [0.0799, 0.8208] Supply Chain Management (1d) scm1d 9,803 16 280 0.6256 [0.4857, 0.8386] Simulated Trivariate Gaussian stg 2,000 3 5 0.4585 [0.4542, 0.5035] Simulated Trivariate Student-T stt 3,000 3 6 0.5524 [0.5261, 0.6780] Slump sl 103 3 7 -0.124 [-0.2035, 0.7001]"
7192,unknown,"Slump sl 103 3 7 -0.124 [-0.2035, 0.7001] Due to its high level of skewness that caused instabilities for all models, we removed one target (target NASI2 48H 0) from the rf1 dataset. Further, also for stability reasons, we applied a Box-Cox transformation to all responses of the oes10 dataset. The last column measures the median strength of dependency between responses using the Pearson coeﬃcient "
7193,unknown,"quantiles in parentheses, i.e., q0.5(q0.1,q0.9). TABLES 9 TABLE 4: NLL scores mNGBoost-G-C mXGBoostLSS-G-C mXGBoostLSS-G-LRA(5) mXGBoostLSS-T-C uXGBoostLSS-G atp1d 32.1131 [31.1064, 33.3593]33.1507 [32.4934, 33.8251] 35.2945 [34.7998, 37.9187] 34.2416 [32.8878, 35.8722] 36.4295 [35.6846, 37.9606] ch 1.5875 [1.5602, 1.656] 1.6921 [1.6512, 1.7364] 2.2747 [2.2289, 2.3244] 1.6679 [1.6375, 1.7526] 1.56"
7194,unknown,"ju 6.9913 [6.054, 7.6863] 6.9372 [6.6957, 7.489] 7.0017 [6.5767, 7.1895] 7.4975 [7.1694, 7.8141] 7.5602 [7.1841, 8.068] oes10 4.9139 [3.8811, 5.8206] 5.763 [5.3078, 6.6205] 9.1977 [8.3772, 10.0111] 4.0237 [3.5981, 5.3306] 4.4383 [3.798, 6.6723] rf1 26.5513 [26.3026, 26.8497] 21.3728 [21.2832, 21.4263] 24.9533 [24.337, 25.1423] 23.3522 [23.1668, 24.2478] 19.1992 [19.0538, 20.0179]"
7195,unknown,"scm1d 97.5695 [97.3239, 97.9193] 95.5774 [95.4184, 95.8474] 128.814 [128.1463, 129.0811] 95.124 [95.014, 95.3738]107.1911 [107.0608, 107.3768] stg 3.706 [3.5851, 3.8114] 3.664 [3.5365, 3.7215] 3.9296 [3.7848, 4.0291] 3.8374 [3.7553, 3.898] 4.6199 [4.5147, 4.7021] stt 5.1247 [4.7432, 5.6858] 4.8221 [4.7306, 5.7158] 5.1441 [4.8861, 5.4495] 4.4243 [4.3407, 4.4598] 5.7544 [5.5818, 6.8563]"
7196,unknown,"sl 10.0841 [9.4764, 10.9282]10.4098 [10.1858, 11.065] 10.7407 [10.5641, 11.2718] 10.6563 [10.3614, 11.573] 12.0785 [11.4485, 13.5503] Average Rank 2.4 2.2 4.2 2.4 3.7 The table shows median NLL scores across models, datasets and folds, with additional quantiles in parentheses, i.e.,q0.5(q0.1,q0.9). Lower is better, with best results in bold. At the bottom,"
7197,unknown,"we also report average ranks across datasets. Again, lower is better. The columns are to be read as follows: Model-Distribution-Covariance Approximation, where G: Gaussian, T: Student-T, C: Cholesky, LRA: Low-Rank Approximation(r), m: multivariate, u: univariate. TABLE 5: NLL Variability mNGBoost-G-C mXGBoostLSS-G-C mXGBoostLSS-G-LRA(5) mXGBoostLSS-T-C uXGBoostLSS-G atp1d 2.6763 1.8471 5.4083 3.86"
7198,unknown,ch 0.1212 0.1120 0.1290 0.1227 0.1106 ju 2.2576 1.2288 1.0296 0.6547 1.0135 oes10 2.6402 1.7893 2.6149 2.3389 3.8587 rf1 1.3828 0.3413 1.1937 2.4426 1.1768 scm1d 0.8980 0.6472 1.4308 0.6590 0.5307 stg 0.3263 0.2021 0.3171 0.1691 0.3044 stt 1.7744 1.0664 1.0759 0.2368 1.5770 sl 1.6586 1.2184 0.9302 1.3211 12.1062 Average 1.5262 0.9392 1.5699 1.3119 2.6661 The table shows the distance between maximu
7199,unknown,"Lower is better, with best results in bold. At the bottom, we also report average distances across datasets. Again, lower is better. The columns are to be read as follows: Model-Distribution-Covariance Approximation, where G: Gaussian, T: Student-T, C: Cholesky, LRA: Low-Rank Approximation(r), m: multivariate, u: univariate. TABLE 6: Relative Median Runtimes mNGBoost-G-C mXGBoostLSS-G-C mXGBoostLS"
7200,unknown,"atp1d 5.4236 1.0000 2.1106 1.0881 ch 6.5707 1.0000 10.8340 1.4833 ju 1.2184 1.0733 3.8502 1.0000 oes10 5.5625 1.1406 1.0000 1.4297 rf1 10.0112 1.0413 2.6890 1.0000 scm1d 45.1237 1.1741 2.0985 1.0000 stg 3.1539 1.0352 7.4486 1.0000 stt 3.5399 1.0000 9.0137 1.3316 sl 1.0000 2.0964 8.9662 3.0045 Average Rank 3.2 1.7 3.3 1.8 The table shows relative median runtimes, with entries normalized to the mode"
7201,unknown,"with best results in bold. The following hyper-parameters are used: learning-rate=0.1, max-depth=6, iterations=100. All other hyper-parameters are set to their default values. The columns are to be read as follows: Model-Distribution-Covariance Approximation, where G: Gaussian, T: Student-T, C: Cholesky, LRA: Low-Rank Approximation( r), m: multivariate, u: univariate. TABLE 7: Ablation Results of "
7202,unknown,"univariate. TABLE 7: Ablation Results of the LRA( r) model Rank (r) atp1d ju sl 2 34.9707 [34.3182, 35.9444] 7.2583 [6.9075, 7.6592] 14.9027 [14.6824, 16.4902] 4 35.0910 [34.3140, 36.8900] 7.5869 [6.9829, 7.9232] 12.8737 [12.2232, 12.9751] 5 35.2945 [34.7998, 37.9187] 7.0017 [6.5767, 7.1895] 10.7407 [10.5641, 11.2718] 6 48.0852 [47.1251, 48.4539] 8.1454 [7.8824, 8.3371] 16.8487 [14.7109, 23.4578] "
7203,unknown,"8 37.8567 [37.1523, 38.7453] 6.9445 [6.2847, 7.4682] 13.4723 [13.2427, 13.6850] 10 70.3046 [48.6155, 79.8918] 7.2275 [6.8551, 7.5264] 14.7997 [14.4707, 18.2046] The table shows median NLL scores across datasets, folds and varying values ofrfor the LRA model, with additional quantiles in parentheses, i.e., q0.5(q0.1,q0.9). Lower is better, with best results in bold. 10 Fig. 1: Estimated Parameters "
7204,unknown,(a) Estimated Parameters using Cholesky-Decomposition of Covariance-Matrix. 0 0.25 0.50 0.75 1 1 0.5 0 0.5 1 LOCATION_1 ACTUAL FIT 0 0.25 0.50 0.75 1 0 0.5 1 1.5 2 LOCATION_2 0 0.25 0.50 0.75 1 1 1.25 1.50 1.75 2 LOCATION_3 0 0.25 0.50 0.75 1 0.6 0.7 0.8 0.9 RHO_21 0 0.25 0.50 0.75 1 0 0.2 0.4 0.6 RHO_31 0 0.25 0.50 0.75 1 0.4 0.5 0.6 0.7 0.8 RHO_32 0 0.25 0.50 0.75 1 0.6 0.7 0.8 0.9 1 1.1 SCALE_1
7205,unknown,1 1.1 SCALE_1 0 0.25 0.50 0.75 1 0.5 1 1.5 2 SCALE_2 0 0.25 0.50 0.75 1 1 1.2 1.4 1.6 1.8 SCALE_3 (b) Estimated Parameters using a Low-Rank-Approximation (r = 2) of Covariance-Matrix. 0 0.25 0.50 0.75 1 0.5 0 0.5 1 LOCATION_1 ACTUAL FIT 0 0.25 0.50 0.75 1 0 0.5 1 1.5 2 LOCATION_2 0 0.25 0.50 0.75 1 0.9 1.2 1.5 1.8 LOCATION_3 0 0.25 0.50 0.75 1 0.6 0.7 0.8 0.9 RHO_21 0 0.25 0.50 0.75 1 0 0.2 0.4 0.
7206,unknown,0.6 0.8 RHO_31 0 0.25 0.50 0.75 1 0.5 0.6 0.7 0.8 RHO_32 0 0.25 0.50 0.75 1 0.5 0.6 0.7 0.8 0.9 1 SCALE_1 0 0.25 0.50 0.75 1 0.5 1 1.5 2 SCALE_2 0 0.25 0.50 0.75 1 1 1.2 1.4 1.6 SCALE_3 11 Fig. 2: Estimated Parameters of trivariate Student-T. 2 0 2 5 10 15 20 DF ACTUAL FIT 0 0.25 0.50 0.75 1 0 0.5 1 1.5 2 LOCATION_1 0 0.25 0.50 0.75 1 1 0.5 0 0.5 1 LOCATION_2 2 0 2 1 1.5 2 2.5 3 LOCATION_3 0 0.25 
7207,unknown,"0.4 0.5 0.6 0.7 0.8 RHO_21 2 0 2 0.4 0.6 0.8 RHO_31 0 0.25 0.50 0.75 1 0 0.2 0.4 0.6 0.8 RHO_32 0 0.25 0.50 0.75 1 1.5 2 2.5 SCALE_1 0 0.25 0.50 0.75 1 1 1.5 SCALE_2 0 0.25 0.50 0.75 1 1 1.5 2 2.5 3 SCALE_3 Fig. 3: Relative Frequencies of Sand, Silt, and Clay in Arctic-Lake Data. Fig. 4: Sediment Composition of Arctic-Lake Data. CLAY SAND SILT 25 50 75 100 25 50 75 100 25 50 75 100 0% 20% 40% 60% "
7208,unknown,"0% 20% 40% 60% 80% Depth in meters Relative Frequency MODEL FIT SCATTER−SMOOTH Multi-task Gaussian Process Prediction Edwin V . Bonilla, Kian Ming A. Chai, Christopher K. I. Williams School of Informatics, University of Edinburgh, 5 Forrest Hill, Edinburgh EH1 2QL, UK edwin.bonilla@ed.ac.uk, K.M.A.Chai@sms.ed.ac.uk, c.k.i.williams@ed.ac.uk Abstract In this paper we investigate multi-task learning "
7209,unknown,Abstract In this paper we investigate multi-task learning in the context of Gaussian Pro- cesses (GP). We propose a model that learns a shared covariance function on input-dependent features and a “free-form” covariance matrix over tasks. This al- lows for good ﬂexibility when modelling inter-task dependencies while avoiding the need for large amounts of data for training. We show that under the a
7210,unknown,"tion of noise-free observations and a block design, predictions for a given task only depend on its target values and therefore a cancellation of inter-task trans- fer occurs. We evaluate the beneﬁts of our model on two practical applications: a compiler performance prediction problem and an exam score prediction task. Additionally, we make use of GP approximations and properties of our model in o"
7211,unknown,order to provide scalability to large data sets. 1 Introduction Multi-task learning is an area of active research in machine learning and has received a lot of at- tention over the past few years. A common set up is that there are multiple related tasks for which we want to avoid tabula rasa learning by sharing information across the different tasks. The hope is that by learning these tasks simult
7212,unknown,"that by learning these tasks simultaneously one can improve performance over the “no transfer” case (i.e. when each task is learnt in isolation). However, as pointed out in [1] and supported empirically by [2], assuming relatedness in a set of tasks and simply learning them together can be detrimental. It is therefore important to have models that will generally beneﬁt related tasks and will not h"
7213,unknown,"It is therefore important to have models that will generally beneﬁt related tasks and will not hurt performance when these tasks are unrelated. We investigate this in the context of Gaussian Process (GP) prediction. We propose a model that attempts to learn inter-task dependencies based solely on the task identities and the observed data for each task. This contrasts with approaches in [3, 4] wher"
7214,unknown,"and the observed data for each task. This contrasts with approaches in [3, 4] where task-descriptor features t were used in a parametric covariance function over different tasks—such a function may be too constrained by both its parametric form and the task descriptors to model task similarities effectively. In addition, for many real-life scenarios task-descriptor features are either unavailable "
7215,unknown,"or difﬁcult to deﬁne correctly. Hence we propose a model that learns a “free-form” task-similarity matrix, which is used in conjunction with a parameterized covariance function over the input features x. For scenarios where the number of input observations is small, multi-task learning augments the data set with a number of different tasks, so that model parameters can be estimated more conﬁdently"
7216,unknown,"this helps to minimize over-ﬁtting. In our model, this is achieved by having a common covariance function over the features x of the input observations. This contrasts with the semiparametric latent factor model [5] where, with the same set of input observations, one has to estimate the parameters of several covariance functions belonging to different latent processes. For our model we can show th"
7217,unknown,"task transfer in the speciﬁc case of noise-free observations and a block design. We have investigated both gradient-based and EM-based optimization of the marginal likelihood for learning the hyper- parameters of the GP. Finally, we make use of GP approximations and properties of our model in order to scale our approach to large multi-task data sets, and evaluate the beneﬁts of our model on two pr"
7218,unknown,"two practical multi-task applications: a compiler performance prediction problem and a exam score prediction task. The structure of the paper is as follows: in section 2 we outline our model for multi-task learning, and discuss some approximations to speed up computations in section 3. Related work is described in section 4. We describe our experimental setup in section 5 and give results in secti"
7219,unknown,"2 The Model Given a set X of N distinct inputs x1, . . . , xN we deﬁne the complete set of responses for M tasks as y = ( y11, . . . , yN1, . . . , y12, . . . , yN2, . . . , y1M , . . . , yNM )T, where yil is the response for the lth task on the ith input xi. Let us also denote the N × M matrix Y such that y = vec Y . Given a set of observations yo, which is a subset of y, we want to predict some "
7220,unknown,"response-values yu at some input locations for certain tasks. We approach this problem by placing a GP prior over the latent functions {fl} so that we directly induce correlations between tasks. Assuming that the GPs have zero mean we set ⟨fl(x)fk(x′)⟩ = Kf lkkx(x, x′) yil ∼ N (fl(xi), σ2 l ), (1) where Kf is a positive semi-deﬁnite (PSD) matrix that speciﬁes the inter-task similarities, kx is a c"
7221,unknown,"covariance function over inputs, and σ2 l is the noise variance for the lth task. Below we focus on stationary covariance functions kx; hence, to avoid redundancy in the parametrization, we further let kx be only a correlation function (i.e. it is constrained to have unit variance), since the variance can be explained fully by Kf . The important property of this model is that the joint Gaussian di"
7222,unknown,"The important property of this model is that the joint Gaussian distribution over y is not block- diagonal wrt tasks, so that observations of one task can affect the predictions on another task. In [4, 3] this property also holds, but instead of specifying a general PSD matrix Kf , these authors set Kf lk = kf (tl, tk), where kf (·, ·) is a covariance function over the task-descriptor features t."
7223,unknown,"Kf lk = kf (tl, tk), where kf (·, ·) is a covariance function over the task-descriptor features t. One popular setup for multi-task learning is to assume that tasks can be clustered, and that there are inter-task correlations between tasks in the same cluster. This can be easily modelled with a general task-similarity Kf matrix: if we assume that the tasks are ordered with respect to the clusters,"
7224,unknown,"Kf will have a block diagonal structure. Of course, as we are learning a “free form” Kf the ordering of the tasks is irrelevant in practice (and is only useful for explanatory purposes). 2.1 Inference Inference in our model can be done by using the standard GP formulae for the mean and variance of the predictive distribution with the covariance function given in equation (1). For example, the mean"
7225,unknown,"mean prediction on a new data-point x∗ for task l is given by ¯fl(x∗) = ( kf l ⊗ kx ∗)T Σ−1y Σ = Kf ⊗ Kx + D ⊗ I (2) where ⊗ denotes the Kronecker product, kf l selects the lth column of Kf , kx ∗ is the vector of covariances between the test point x∗ and the training points, Kx is the matrix of covariances between all pairs of training points, D is an M × M diagonal matrix in which the (l, l)th e"
7226,unknown,"σ2 l , and Σ is an MN × MN matrix. In section 2.3 we show that when there is no noise in the data (i.e. D = 0), there will be no transfer between tasks. 2.2 Learning Hyperparameters Given the set of observations yo, we wish to learn the parameters θx of kx and the matrix Kf to maximize the marginal likelihood p(yo|X, θx, Kf ). One way to achieve this is to use the fact that y|X ∼ N (0, Σ). Therefo"
7227,unknown,"fact that y|X ∼ N (0, Σ). Therefore, gradient-based methods can be readily applied to maxi- mize the marginal likelihood. In order to guarantee positive-semideﬁniteness of Kf , one possible parametrization is to use the Cholesky decomposition Kf = LLT where L is lower triangular. Computing the derivatives of the marginal likelihood with respect to L and θx is straightforward. A drawback of this ap"
7228,unknown,"tial size MN × MN (or solving an MN × MN linear system) at each optimization step. Note, however, that one only needs to actually compute the Gram matrix and its inverse at the visible locations corresponding to yo. Alternatively, it is possible to exploit the Kronecker product structure of the full covariance matrix as in [6], where an EM algorithm is proposed such that learning of θx and Kf in t"
7229,unknown,"as in [6], where an EM algorithm is proposed such that learning of θx and Kf in the M-step is decoupled. This has the advantage that closed-form updates for Kf and D can be obtained (see equation (5)), and that Kf is guaranteed to be positive-semideﬁnite. The details of the EM algorithm are as follows: Let f be the vector of function values corresponding to y, and similarly for F wrt Y . Further, "
7230,unknown,"which in this case is f, the complete-data log-likelihood is Lcomp = −N 2 log |Kf | − M 2 log |Kx| − 1 2 tr [( Kf )−1 FT (Kx)−1 F ] − N 2 M∑ l=1 log σ2 l − 1 2 tr [ (Y − F)D−1(Y − F)T] − MN 2 log 2π (3) from which we have following updates: ˆθx = arg min θx ( N log ⏐⏐⏐ ⣨ FT (Kx(θx))−1 F ⟩⏐⏐⏐ + M log |Kx(θx)| ) (4) ˆKf = N−1 ⟨ FT ( Kx(ˆθx) )−1 F ⟩ ˆσ2 l = N−1 ⣨ (y·l − f·l)T (y·l − f·l) ⟩ (5) where "
7231,unknown,"( f|yo, θx, Kf ) , and ˆ· denotes the updated parameters. For clarity, let us consider the case where yo = y, i.e. a block design. Then p ( f|y, θx, Kf ) = N ( (Kf ⊗ Kx)Σ−1y, (Kf ⊗ Kx) − (Kf ⊗ Kx)Σ−1(Kf ⊗ Kx) ) . We have seen that Σ needs to be inverted (in time O(M3N3)) for both making predictions and learning the hyperparameters (when considering noisy observations). This can lead to computation"
7232,unknown,"problems if MN is large. In section 3 we give some approximations that can help speed up these computations. 2.3 Noiseless observations and the cancellation of inter-task transfer One particularly interesting case to consider is noise-free observations at the same locations for all tasks (i.e. a block-design) so that y|X ∼ Normal(0, Kf ⊗ Kx). In this case maximizing the marginal likelihood p(y|X) "
7233,unknown,"marginal likelihood p(y|X) wrt the parameters θx of kx reduces to maximizing −M log |Kx| − N log |Y T (Kx)−1Y |, an expression that does not depend on Kf . After convergence we can obtain Kf as ˆKf = 1 N Y T (Kx)−1Y . The intuition behind is this: The responses Y are correlated via Kf and Kx. We can learn Kf by decorrelating Y with (Kx)−1 ﬁrst so that only correlation with respect to Kf is left. T"
7234,unknown,"to Kf is left. Then Kf is simply the sample covariance of the de-correlated Y . Unfortunately, in this case there is effectively no transfer between the tasks (given the kernels). To see this, consider making predictions at a new location x∗ for all tasks. We have (using the mixed- product property of Kronecker products) that f(x∗) = ( Kf ⊗ kx ∗ )T ( Kf ⊗ Kx)−1 y (6) = ( (Kf )T ⊗ (kx ∗)T) ( (Kf )−"
7235,unknown,"(Kf )−1 ⊗ (Kx)−1) y (7) = [( Kf (Kf )−1) ⊗ ( (kx ∗)T(Kx)−1)] y (8) =   (kx ∗)T(Kx)−1y·1 ... (kx ∗)T(Kx)−1y·M  , (9) and similarly for the covariances. Thus, in the noiseless case with a block design, the predictions for task l depend only on the targets y·l. In other words, there is a cancellation of transfer. One can in fact generalize this result to show that the cancellation of transfer f"
7236,unknown,"if the observations are only sparsely observed at locations X = ( x1, . . . , xN ) on the other tasks. After having derived this result we learned that it is known as autokrigeability in the geostatistics literature [7], and is also related to the symmetric Markov propertyof covariance functions that is discussed in [8]. We emphasize that if the observations are noisy, or if there is not a block d"
7237,unknown,"then this result on cancellation of transfer will not hold. This result can also be generalized to multidimensional tensor product covariance functions and grids [9]. 3 Approximations to speed up computations The issue of dealing with large N has been much studied in the GP literature, see [10, ch. 8] and [11] for overviews. In particular, one can use sparse approximations where only Q out of N da"
7238,unknown,"[11] for overviews. In particular, one can use sparse approximations where only Q out of N data points are selected as inducing inputs[11]. Here, we use the Nystr ¨om approximation of Kx in the marginal likelihood, so that Kx ≈ ˜Kx def = Kx ·I(Kx II )−1Kx I·, where I indexes Q rows/columns of Kx. In fact for the posterior at the training points this result is obtained from both the subset of regre"
7239,unknown,"regressors (SoR) and projected process (PP) approximations described in [10, ch. 8]. Specifying a full rank Kf requires M(M + 1)/2 parameters, and for large M this would be a lot of parameters to estimate. One parametrization of Kf that reduces this problem is to use a PPCA model [12] Kf ≈ ˜Kf def = UΛUT + s2IM , where U is an M × P matrix of the P principal eigenvectors of Kf , Λ is a P × P diago"
7240,unknown,"of Kf , Λ is a P × P diagonal matrix of the corresponding eigenvalues, and s2 can be determined analytically from the eigenvalues of Kf (see [12] and references therein). For numerical stability, we may further use the incomplete-Cholesky decomposition setting UΛUT = ˜L˜LT, where ˜L is a M × P matrix. Below we consider the case s = 0 , i.e. a rank- P approximation to Kf . Applying both approximati"
7241,unknown,"Applying both approximations to get Σ ≈ ˜Σ def = ˜Kf ⊗ ˜Kx + D ⊗ IN , we have, after using the Woodbury identity, ˜Σ−1 = ∆ −1 − ∆−1B [ I ⊗ Kx II + BT∆−1B ]−1 BT∆−1 where B def = ( ˜L ⊗ Kx ·I), and ∆ def = D ⊗ IN is a diagonal matrix. As ˜Kf ⊗ ˜Kx has rank PQ, we have that computation of ˜Σ−1y takes O(MNP 2Q2). For the EM algorithm, the approximation of ˜Kx poses a problem in (4) because for the ra"
7242,unknown,"For the EM algorithm, the approximation of ˜Kx poses a problem in (4) because for the rank-deﬁcient matrix ˜Kx, its log-determinant is negative inﬁnity, and its matrix inverse is undeﬁned. We overcome this by considering ˜Kx = lim ξ→0(Kx ·I(Kx II )−1Kx I· +ξ2I), so that we solve an equivalent optimiza- tion problem where the log-determinant is replaced by the well-deﬁned log |Kx I·Kx ·I| − log |Kx"
7243,unknown,"I·Kx ·I| − log |Kx II |, and the matrix inverse is replaced by the pseudo-inverse. With these approximations the compu- tational complexity of hyperparameter learning can be reduced to O(MNP 2Q2) per iteration for both the Cholesky and EM methods. 4 Related work There has been a lot of work in recent years on multi-task learning (or inductive transfer) using methods such as Neural Networks, Gaussi"
7244,unknown,"methods such as Neural Networks, Gaussian Processes, Dirichlet Processes and Support Vector Machines, see e.g. [2, 13] for early references. The key issue concerns what properties or aspects should be shared across tasks. Within the GP literature, [14, 15, 16, 17, 18] give models where the covariance matrix of the full (noiseless) system is block diagonal, and each of the M blocks is induced from "
7245,unknown,"induced from the same kernel function. Under these models each y·i is conditionally independent, but inter-task tying takes place by sharing the kernel function across tasks. In contrast, in our model and in [5, 3, 4] the covariance is not block diagonal. The semiparametric latent factor model (SLFM) of Teh et al [5] involves having P latent processes (where P ≤ M) and each of these latent process"
7246,unknown,"(where P ≤ M) and each of these latent processes has its own covariance function. The noiseless outputs are obtained by linear mixing of these processes with a M × P matrix Φ. The covariance matrix of the system under this model has rank at most PN , so that when P < M the system corresponds to a degenerate GP. Our model is similar to [5] but simpler, in that all of the P latent processes share th"
7247,unknown,"and should help to minimize overﬁtting. With a common covariance function kx, it turns out that Kf is equal to ΦΦT, so a Kf that is strictly positive deﬁnite corresponds to using P = M latent processes. Note that if P > M one can always ﬁnd an M × M matrix Φ′ such that Φ′Φ′T = ΦΦ T. We note also that the approximation methods used in [5] are different to ours, and were based on the subset of data "
7248,unknown,"subset of data (SoD) method using the informative vector machine (IVM) selection heuristic. In the geostatistics literature, the prior model for f· given in eq. (1) is known as the intrinsic cor- relation model[7], a speciﬁc case of co-kriging. A sum of such processes is known as the linear coregionalization model(LCM) [7] for which [6] gives an EM-based algorithm for parameter es- timation. Our m"
7249,unknown,"timation. Our model for the observations corresponds to an LCM model with two processes: the process for f· and the noise process. Note that SLFM can also be seen as an instance of the LCM model. To see this, let Epp be a P ×P diagonal matrix with 1 at (p, p) and zero elsewhere. Then we can write the covariance in SLFM as (Φ⊗I)(∑P p=1 Epp ⊗Kx p )(Φ⊗I)T = ∑P p=1(ΦEppΦT)⊗Kx p , where ΦEppΦT is of ra"
7250,unknown,"p=1 Epp ⊗Kx p )(Φ⊗I)T = ∑P p=1(ΦEppΦT)⊗Kx p , where ΦEppΦT is of rank 1. Evgeniou et al. [19] consider methods for inducing correlations between tasks based on a correlated prior over linear regression parameters. In fact this corresponds to a GP prior using the kernel k(x, x′) = xT Ax′ for some positive deﬁnite matrix A. In their experiments they use a restricted form of Kf with Kf lk = (1 − λ) +"
7251,unknown,"form of Kf with Kf lk = (1 − λ) + λMδlk (their eq. 25), i.e. a convex combination of a rank-1 matrix of ones and a multiple of the identity. Notice the similarity to the PPCA form of Kf given in section 3. 5 Experiments We evaluate our model on two different applications. The ﬁrst application is a compiler performance prediction problem where the goal is to predict the speed-up obtained in a given"
7252,unknown,"applying a sequence of code transformations x. The second application is an exam score prediction problem where the goal is to predict the exam scoreobtained by a student x belonging to a speciﬁc school (task). In the sequel, we will refer to the data related to the ﬁrst problem as the compiler data and the data related to the second problem as the school data. We are interested in assessing the b"
7253,unknown,"We are interested in assessing the beneﬁts of our approach not only with respect to the no-transfer case but also with respect to the case when a parametric GP is used on the joint input-dependent and task-dependent space as in [3]. To train the parametric model note that the parameters of the covari- ance function over task descriptors kf (t, t′) can be tuned by maximizing the marginal likelihood"
7254,unknown,"ance function over task descriptors kf (t, t′) can be tuned by maximizing the marginal likelihood, as in [3]. For the free-form Kf we initialize this (given kx(·, ·)) by using the noise-free expression ˆKf = 1 N Y T (Kx)−1Y given in section 2.3 (or the appropriate generalization when the design is not complete). For both applications we have used a squared-exponential (or Gaussian) covariance func"
7255,unknown,"function kx and a non-parametric form for Kf . Where relevant the parametric covariance function kf was also taken to be of squared-exponential form. Both kx and kf used an automatic relevance determination (ARD) parameterization, i.e. having a length scale for each feature dimension. All the length scales in kx and kf were initialized to 1, and all σ2 l were constrained to be equal for all tasks "
7256,unknown,l were constrained to be equal for all tasks and initialized to 0.01. 5.1 Description of the Data Compiler Data. This data set consists of 11 C programs for which an exhaustive set of 88214 sequences of code transformations have been applied and their corresponding speed-ups have been recorded. Each task is to predict the speed-up on a given program when applying a speciﬁc trans- formation sequenc
7257,unknown,formation sequence. The speed-up after applying a transformation sequence on a given program is deﬁned as the ratio of the execution time of the original program (baseline) over the execution time of the transformed program. Each transformation sequence is described as a 13-dimensional vector x that records the absence/presence of one-out-of 13 single transformations. In [3] the task- descriptor f
7258,unknown,"descriptor features (for each program) are based on the speed-ups obtained on a pre-selected set of 8 transformations sequences, so-called “canonical responses”. The reader is referred to [3, section 3] for a more detailed description of the data. School Data. This data set comes from the Inner London Education Authority (ILEA) and has been used to study the effectiveness of schools. It is publicl"
7259,unknown,"has been used to study the effectiveness of schools. It is publicly available under the name of “school effectiveness” at http://www.cmm.bristol.ac.uk/learning-training/ multilevel-m-support/datasets.shtml. It consists of examination records from 139 secondary schools in years 1985, 1986 and 1987. It is a random 50% sample with 15362 students. This data has also been used in the context of multi-t"
7260,unknown,"This data has also been used in the context of multi-task learning by Bakker and Heskes [20] and Evgeniou et al. [19]. In [20] each task is deﬁned as the prediction of the exam score of a student belonging to a speciﬁc school based on four student-dependent features (year of the exam, gen- der, VR band and ethnic group) and four school-dependent features (percentage of students eligible for free s"
7261,unknown,"for free school meals, percentage of students in VR band 1, school gender and school denomina- tion). For comparison with [20, 19] we evaluate our model following the set up described above and similarly, we have created dummy variables for those features that are categorical forming a total of 19 student-dependent features and 8 school-dependent features. However, we note that school-descriptor f"
7262,unknown,"percentage of students in VR band 1 actually depend on the year the particular sample was taken. It is important to emphasize that for both data sets there are task-descriptor features available. How- ever, as we have described throughout this paper, our approach learns task similarity directly without the need for task-dependent features. Hence, we have neglected these features in the application"
7263,unknown,"our free-form Kf method. 6 Results For the compiler data we have M = 11 tasks and we have used a Cholesky decomposition Kf = LLT . For the school data we have M = 139 tasks and we have preferred a reduced rank parameterization of Kf ≈ ˜Kf = ˜L˜LT , with ranks 1, 2, 3 and 5. We have learnt the parame- ters of the models so as to maximize the marginal likelihood p(yo|X, Kf , θx) using gradient-based"
7264,unknown,"search in MATLAB with Carl Rasmussen’s minimize.m. In our experiments this method usually outperformed EM in the quality of solutions found and in the speed of convergence. Compiler Data:For this particular application, in a real-life scenario it is critical to achieve good performance with a low number of training data-points per task given that a training data-point requires the compilation and "
7265,unknown,"although there are a total of 88214 training points per program we have followed a similar set up to [3] by considering N = 16 , 32, 64 and 128 transformation sequences per program for training. All the M = 11 programs (tasks) have been used for training, and predictions have been done at the (unobserved) remaining 88214 − N inputs. For comparison with [3] the mean absolute error (between the actu"
7266,unknown,(between the actual speed-ups of a program and the predictions) has been used as the measure of performance. Due to the variability of the results depending on training set selection we have considered 10 different replications. Figure 1 shows the mean absolute errors obtained on the compiler data for some of the tasks (top row and bottom left) and on average for all the tasks (bottom right). Samp
7267,unknown,"row and bottom left) and on average for all the tasks (bottom right). Sample task 1 (histogram) is an example where learning the tasks simultaneously brings major beneﬁts over the no transfer case. Here, multi-task GP (transfer free-form) provides a reduction on the mean absolute error of up to 6 times. Additionally, it is consistently (although only marginally) superior to the parametric approach"
7268,unknown,"approach. For sample task2 (ﬁr), our approach not only signiﬁcantly outperforms the no transfer case but also provides greater beneﬁts over the parametric method (which for N = 64 and 128 is worse than no transfer). Sample task 3 (adpcm) is the only case out of all 11 tasks where our approach degrades performance, although it should be noted that all the methods perform similarly. Further analysis"
7269,unknown,"that cannot be explained by the 1-out-of-13 encoding used for the input features. Finally, for all tasks on average (bottom right) our approach brings signiﬁcant improvements over single task learning and consistently outperforms the parametric method. For all tasks except one our model provides better or roughly equal performance than the non-transfer case and the parametric model. School Data:Fo"
7270,unknown,"School Data:For comparison with [20, 19] we have made 10 random splits of the data into training (75%) data and test (25%) data. Due to the categorical nature of the data there are a maximum of N = 202 different student-dependent feature vectors x. Given that there can be multiple ob- servations of a target value for a given task at a speciﬁc input x, we have taken the mean of these observations a"
7271,unknown,"observations and corrected the noise variances by dividing them over the corresponding number of observations. As in [19], the percentage explained variance is used as the measure of performance. This measure can be seen as the percentage version of the well known coefﬁcient of determination r2 between the actual target values and the predictions. 16 32 64 1280 0.04 0.08 0.12 0.16 0.2 SAMPLE TASK "
7272,unknown,N MAE NO TRANSFER TRANSFER PARAMETRIC TRANSFER FREE−FORM 16 32 64 1280 0.05 0.1 0.15 0.2 0.25 0.3 0.35 SAMPLE TASK 2 N MAE NO TRANSFER TRANSFER PARAMETRIC TRANSFER FREE−FORM (a) (b) 16 32 64 1280 0.02 0.04 0.06 0.08 0.1 0.12 SAMPLE TASK 3 N MAE NO TRANSFER TRANSFER PARAMETRIC TRANSFER FREE−FORM 16 32 64 1280 0.02 0.04 0.06 0.08 0.1 0.12 0.14 ALL TASKS N MAE NO TRANSFER TRANSFER PARAMETRIC TRANSFER
7273,unknown,"0.08 0.1 0.12 0.14 ALL TASKS N MAE NO TRANSFER TRANSFER PARAMETRIC TRANSFER FREE−FORM (c) (d) Figure 1: Panels (a), (b) and (c) show the average mean absolute error on the compiler data as a function of the number of training points for speciﬁc tasks. no transferstands for the use of a single GP for each task separately; transfer parametric is the use of a GP with a joint parametric (SE) covarianc"
7274,unknown,"covariance function as in [3]; and transfer free-formis multi-task GP with a “free form” covariance matrix over tasks. The error bars show ± one standard deviation taken over the 10 replications. Panel (d) shows the average MAE over all 11 tasks, and the error bars show the average of the standard deviations over all 11 tasks. The results are shown in Table 1; note that larger ﬁgures are better. T"
7275,unknown,"the table was obtained from the school-descriptor features; in the cases where these features varied for a given school over the years, an average was taken. The results show that better results can be obtained by using multi-task learning than without. For the non-parametric Kf , we see that the rank-2 model gives best performance. This performance is also comparable with the best (29.5%) found i"
7276,unknown,found in [20]. We also note that our no transferresult of 21.1% is much better than the baseline of 9.7% found in [20] using neural networks. no transfer parametric rank 1 rank 2 rank 3 rank 5 21.05 (1.15) 31.57 (1.61) 27.02 (2.03) 29.20 (1.60) 24.88 (1.62) 21.00 (2.42) Table 1: Percentage variance explained on the school dataset for various situations. The ﬁgures in brackets are standard deviatio
7277,unknown,"brackets are standard deviations obtained from the ten replications. On the school data the parametric approach for Kf slightly outperforms the non-parametric method, probably due to the large size of this matrix relative to the amount of data. One can also run the parametric approach creating a task for every unique school-features descriptor 1; this gives rise to 288 tasks rather than 139 school"
7278,unknown,"288 tasks rather than 139 schools, and a performance of 33.08% ( ±1.57). Evgeniou et al [19] use a linear predictor on all 8 features (i.e. they combine both student and school features into x) and then introduce inter-task correlations as described in section 4. This approach uses the same information as our 288 task case, and gives similar performance of around 34% (as shown in Figure 3 of [19])"
7279,unknown,"1Recall from section 5.1 that the school features can vary over different years. 7 Conclusion In this paper we have described a method for multi-task learning based on a GP prior which has inter-task correlations speciﬁed by the task similarity matrix Kf . We have shown that in a noise- free block design, there is actually a cancellation of transfer in this model, but not in general. We have succe"
7280,unknown,"have successfully applied the method to the compiler and school problems. An advantage of our method is that task-descriptor features are not required (c.f. [3, 4]). However, such features might be beneﬁcial if we consider a setup where there are only few datapoints for a new task, and where the task-descriptor features convey useful information about the tasks. Acknowledgments CW thanks Dan Cornf"
7281,unknown,"This work is supported under EPSRC grant GR/S71118/01 , EU FP6 STREP MILEPOST IST-035307, and in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST- 2002-506778. This publication only reﬂects the authors’ views. References [1] Jonathan Baxter. A Model of Inductive Bias Learning. JAIR, 12:149–198, March 2000. [2] Rich Caruana. Multitask Learning. Machin"
7282,unknown,"[2] Rich Caruana. Multitask Learning. Machine Learning, 28(1):41–75, July 1997. [3] Edwin V . Bonilla, Felix V . Agakov, and Christopher K. I. Williams. Kernel Multi-task Learning using Task-speciﬁc Features. In Proceedings of the 11th AISTATS, March 2007. [4] Kai Yu, Wei Chu, Shipeng Yu, V olker Tresp, and Zhao Xu. Stochastic Relational Models for Discrimina- tive Link Prediction. In NIPS 19, Cam"
7283,unknown,"tive Link Prediction. In NIPS 19, Cambridge, MA, 2007. MIT Press. [5] Yee Whye Teh, Matthias Seeger, and Michael I. Jordan. Semiparametric latent factor models. In Pro- ceedings of the 10th AISTATS, pages 333–340, January 2005. [6] Hao Zhang. Maximum-likelihood estimation for multivariate spatial linear coregionalization models. Environmetrics, 18(2):125–139, 2007. [7] Hans Wackernagel. Multivaria"
7284,unknown,"Berlin, 2nd edition, 1998. [8] A. O’Hagan. A Markov property for covariance structures. Statistics Research Report 98-13, Nottingham University, 1998. [9] C. K. I. Williams, K. M. A. Chai, and E. V . Bonilla. A note on noise-free Gaussian process prediction with separable covariance functions and grid designs. Technical report, University of Edinburgh, 2007. [10] C. E. Rasmussen and C. K. I. Willi"
7285,unknown,"bridge, Massachusetts, 2006. [11] Joaquin Qui ˜nonero-Candela, Carl Edward Rasmussen, and Christopher K. I. Williams. Approximation Methods for Gaussian Process Regression. In Large Scale Kernel Machines. MIT Press, 2007. To appear. [12] Michael E. Tipping and Christopher M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society, Series B, 61(3):611–622, 1999."
7286,unknown,"[13] S. Thrun. Is Learning the n-th Thing Any Easier Than Learning the First? In NIPS 8, 1996. [14] Thomas P. Minka and Rosalind W. Picard. Learning How to Learn is Learning with Point Sets. 1999. [15] Neil D. Lawrence and John C. Platt. Learning to learn with the Informative Vector Machine. In Proceed- ings of the 21st International Conference on Machine Learning, July 2004. [16] Kai Yu, V olker "
7287,unknown,"Proceedings of the 22nd International Conference on Machine Learning, 2005. [17] Anton Schwaighofer, V olker Tresp, and Kai Yu. Learning Gaussian Process Kernels via Hierarchical Bayes. In NIPS 17, Cambridge, MA, 2005. MIT Press. [18] Shipeng Yu, Kai Yu, V olker Tresp, and Hans-Peter Kriegel. Collaborative Ordinal Regression. In Pro- ceedings of the 23rd International Conference on Machine Learnin"
7288,unknown,"ceedings of the 23rd International Conference on Machine Learning, June 2006. [19] Theodoros Evgeniou, Charles A. Micchelli, and Massimiliano Pontil. Learning Multiple Tasks with Kernel Methods. Journal of Machine Learning Research, 6:615–537, April 2005. [20] Bart Bakker and Tom Heskes. Task Clustering and Gating for Bayesian Multitask Learning. Journal of Machine Learning Research, 4:83–99, May "
7289,unknown,"Machine Learning Research, 4:83–99, May 2003. Journal of Machine Learning Research 23 (2022) 1-47 Submitted 3/21; Revised 8/22; Published 8/22 Multivariate Boosted Trees and Applications to Forecasting and Control Lorenzo Nespoli1,2 lorenzo.nespoli@supsi.ch Vasco Medici1 vasco.medici@supsi.ch 1ISAAC, SUPSI, Mendrisio, CH, 2Hive Power SA, Manno, CH Editor: Lorenzo Rosasco Abstract Gradient boosted "
7290,unknown,"Abstract Gradient boosted trees are competition-winning, general-purpose, non-parametric regres- sors, which exploit sequential model ﬁtting and gradient descent to minimize a speciﬁc loss function. The most popular implementations are tailored to univariate regression and classiﬁcation tasks, precluding the possibility of capturing multivariate target cross- correlations and applying structured p"
7291,unknown,"correlations and applying structured penalties to the predictions. In this paper, we present a computationally eﬃcient algorithm for ﬁtting multivariate boosted trees. We show that multivariate trees can outperform their univariate counterpart when the predictions are correlated. Furthermore, the algorithm allows to arbitrarily regularize the predictions, so that properties like smoothness, consis"
7292,unknown,"that properties like smoothness, consistency and functional relations can be enforced. We present applications and numerical results related to forecasting and control. Keywords: boosted trees, multivariate regression, forecasting, control, statistical learning 1. Introduction We propose the use of multivariate boosted trees (MBTs) to induce arbitrary regularization and consistency properties in t"
7293,unknown,"and consistency properties in the tree output. This can be done both via penalization of the multivariate output or requiring it to be a superposition of basis functions. Inducing regularization in multivariate output is not new, but while this is common for example in neural network architectures (Oreshkin et al., 2019; Belharbi et al., 2018; Bronstein et al., 2017), they are currently not exploi"
7294,unknown,"et al., 2017), they are currently not exploited in tree-based algorithms. One exception is the possibility of LightGBM and XGBoost to express monotonicity conditions of the univariate prediction, with respect to a given input (LightGBM, 2020). This is obtained by inhibiting the tree growth if the new leaf causes a non-monotonic split in the selected feature. However, this may produce unnecessarily"
7295,unknown,"However, this may produce unnecessarily shallow trees if not enough split candidates are tested, which could be the case if the tree is grown using histogram search, one of the most popular methods for ﬁnding candidate splits. 1.1 Related work In Pande et al. (2017), an MBT tailored to predicting longitudinal data is presented. This kind of data is typically generated in medical studies, sampling "
7296,unknown,"c⃝2022 Lorenzo Nespoli and Vasco Medici. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v23/21-0247.html. Nespoli and Medici Nomenclature Acronyms cdf cumulative density function CV cross validation DDC data driven control GBT gradient boosted tree MAPE mean absolute percentage er- ror MBT multivariate boosted t"
7297,unknown,"MIMO multiple-input multiple- output MISO multiple-input single-output MPC model predictive control PCC point of common coupling pdf probability density function RMSE root mean square error VSC voltage sensitivity coeﬃcients Variables and Functions ϵ prediction error ˆyb,˜yb forecasted and reconciled bot- tom time series ˆyu,˜yu forecasted and reconciled up- per levels time series 1x indicator fun"
7298,unknown,"tion x E expectation operator L loss function χ average number of quantile crossings τ quantile level ˜Gk ∑ i∈Il ˜gi,k ˜Hk ∑ i∈Il ˜hi,k ε boosted model training loss F boosted model f weak learner FY|x cdf of random variable Y gk, ˜gk loss gradient w.r.t. Fk, wk hk, ˜hk loss Hessian w.r.t. Fk, wk kp i,j,kq i,j VSC for node i w.r.t. node j, for power and reactive power p probability Qs quantile sco"
7299,unknown,p probability Qs quantile score r response function rτ reliability of quantile τ x feature matrix xlr feature matrix for linear re- sponse y target variable matrix Parameters Λ quadratic regularization ma- trix λ quadratic regularization coef- ﬁcient In identity matrix of size n Ω error covariance matrix ρ learning rate Θ BT parameters θ tree parameters D second order diﬀerence matrix N number of 
7300,unknown,nb number of bottom time series nf features dimension ni number of boosting rounds nk number of wavenumbers nl number of leaves nq number of predicted quantiles nt targets dimension nu number of upper level time se- ries nw leaf’s weights dimension nlf linear features dimension nmin minimum number of observa- tions per leaf nqs number of quantile splits for histogram search S summation matrix W re
7301,unknown,"W response function parameters wl leaf-speciﬁc parameters Sets D dataset Il observations in leaf l K set of wavenumbers2 Multivariate Boosted Trees and Applications to Forecasting and Control points in time. Typically, the amount of available data to model the temporal relation is limited. The authors developed MBTs and trained them in function space, using B-Splines to model time interactions. Th"
7302,unknown,"to model time interactions. The algorithm is tested on a synthetic dataset, generated using simple algebraic formulae to model the target dependence over features and time. In Li et al. (2019), a single tree is ﬁtted using a multivariate linear regressor as weak learner. The tree is grown such that in each leaf the dataset is divided into two classes, based on the points for which the tree returne"
7303,unknown,"the points for which the tree returned an overshot or undershot prediction. Despite the interesting idea, splitting points are not chosen with a variance reduction criterion, and only one model is ﬁtted, thus not exploiting gradient boosting. The algorithm is found to perform better than linear regression on 3 machine learning datasets, while performance against LightGBM is datasets dependent. Rec"
7304,unknown,"against LightGBM is datasets dependent. Recently, the authors in Zhang and Jung (2019) proposed a multivariate version of the XGBoost algorithm, introduced a new histogram algorithm for datasets with sparse features and implemented a performance tailored C++ library. In this work, we make use of the same approach to ﬁt MBTs, coupling it with non-constant response functions. 1.2 Contributions We ha"
7305,unknown,"We have extended the formulation of boosted trees to the multivariate and non-constant response cases. This goes beyond popular gradient boosting libraries, which adopt a uni- variate and constant response paradigm. To the best of our knowledge, no one has ever presented a non-constant response MBT. This new method allows us to arbitrarily regu- larize the covariance structure of the outputs and i"
7306,unknown,"features for many applications. In section 3.3, we introduce a smoothed formulation of the quantile loss and show its supe- riority in terms of expected quantile loss and crossings of the predicted quantile. In section 3.2, we introduce a new approach for hierarchical forecasting, which takes into account previous forecast error, and show that this method is better compared to other state-of- the-"
7307,unknown,"the-art algorithms for the ﬁrst prediction steps. This is possible thanks to the introduction of a consistent non-constant response function. Finally, in section 3.4, as an example of application, we present a way to ﬁt voltage sensitivity coeﬃcients for electrical distribution networks through boosted trees, while retaining their linear form w.r.t. the active and re- active powers. The ﬁt is base"
7308,unknown,"active powers. The ﬁt is based on few exogenous variables, and we show that robustness to input variable noise makes this approach suitable for control application. The algorithm has been released as a python package under MIT license, and it is freely available at https://github.com/supsi-dacd-isaac/mbtr. All the code used for running the exper- iments presented in the paper, including the code f"
7309,unknown,"iments presented in the paper, including the code for generating the ﬁgures, is available at https://github.com/supsi-dacd-isaac/mbtr_experiments. All the used datasets are freely accessible, and directly downloaded by the experiment’s code. The dataset used for the numerical experiments can be downloaded from https://zenodo.org/record/ 4108561#.YEeukVmYWV5 and https://zenodo.org/record/4549296#.Y"
7310,unknown,"4108561#.YEeukVmYWV5 and https://zenodo.org/record/4549296#.YEeuvFmYWV4. 2. Background Given a matrix of targets y ∈RN×nt, where N is the number of observations and nt the dimensionality of the target, and a set of features (or covariates, or explanatory variables) 3 Nespoli and Medici x∈RN×nf, we call the union of their observations a dataset D= {(xi,yi)N i=1}. Our goal is to ﬁt a learnable model"
7311,unknown,"to ﬁt a learnable model F(x,Θ), where Θ is the set of model’s parameters, on dataset D, such that it minimizes the expected loss on unseen data. To achieve this, we minimize the empirical expectation of the loss function ℓ(yi,F(xi,Θ)) : Rnt →R, also known as empirical risk, on the observed dataset D: Θ∗= argmin Θ N∑ i=1 ℓ(yi,F(xi,Θ)) = argmin Θ L(y,F (x,Θ)) (1) 2.1 Decision trees Since GBTs use re"
7312,unknown,"Since GBTs use regression trees as weak learners, we recall here their formal description and ﬁtting strategy. A regression tree is a function partitioning the input spaceRnf into diﬀerent regions, or leaves, each of which contains a response function r(wl) : Rnw →Rnt, nt = 1 corresponding to a univariate tree, parametrized by weights wl ∈Rnw. Formally, a tree can be described as a function f(x,θ)"
7313,unknown,"can be described as a function f(x,θ) ∈F where F= { f(x,θ) = r(wq(x)) } (q: Rnf →nl) where q represents the structure of the tree which maps observations into leaf indexes and θ is the set of the tree’s parameters. Equivalently, a tree can be described as the sum of the leaves’ response functions, weighted by the indicator function 1l(xi) = {i|q(xi) = l}: Rnf →{0,1}, returning 1 if xi belongs to t"
7314,unknown,"f(xi,θ) = nl∑ l=1 r(wl)1l(xi) (2) In this paper we will only consider trees applying a recursive binary partitioning (or splits) of the input to construct their leaves, resulting in leaves that are disjoint and orthogonal w.r.t. the features under consideration. In this case, θ= {S,W}consists of the ordered set of vari- ables and levels deﬁning the splits for each of the nn nodes of the tree, S = "
7315,unknown,"ables and levels deﬁning the splits for each of the nn nodes of the tree, S = {(vn,ln)nn n=1}, and the parameter set of the response functions for each leaf of the tree, W = {wl}nl l=1. While in this paper we will make use of diﬀerent response functions, in the standard case this is a constant, thus r(wl) = wl, wl ∈Rnw. In order to ﬁt both univariate and multivari- ate trees, we can rely on the fo"
7316,unknown,"ate trees, we can rely on the following remark: Remark Since the functional form r(wl) is the same for each leaf, wl is constant for a given leaf, and since the leaves are disjoint regions of the feature space, we only need to know the functional form of the leaves’ loss function in order to ﬁt a tree. We can then write the total loss, as a summation of the leaf losses: L(y,f(x,θ)) = N∑ i=1 ℓ(yi,f"
7317,unknown,"= N∑ i=1 ℓ ( yi, L∑ l=1 r(wl) 1l(xi) ) = nl∑ l=1 ∑ i∈Il ℓ(yi,r (wl)) = nl∑ l=1 ℓl (3) 4 Multivariate Boosted Trees and Applications to Forecasting and Control where Il = {i : 1l(xi) = 1 }. To ﬁt the tree, we must ﬁnd both the optimal values of wl inside a given leaf, and the leaf partitions Il. While the ﬁrst task is straightforward, the second one is much harder; in fact, since the latter is usua"
7318,unknown,"second one is much harder; in fact, since the latter is usually computationally infeasible, greedy algorithms are used to ﬁnd the best splits. Basically, at each iteration, a leaf with dataset Dl is split if the sum of the loss computed on the partial datasets Dl,s1 and Dl,s2 is lower than the leaf loss. It is easy to see that the splitting criterion (that is, how to divide Dl), must be only depen"
7319,unknown,"Dl), must be only dependent on the features x since at prediction time we won’t know the values of y. Even if this approach is simple, it can result in high computational costs; in the extreme case in which all the points are regarded as splitting candidates, the computational cost of the algorithm is O(N×nf) for the ﬁrst splitting decision. In this paper, we restrict splitting candidates using hi"
7320,unknown,"splitting candidates using histograms, as done in LightGBM (Ke et al., 2017). This reduces the cost of ﬁnding the optimal split to O(nqs ×nf) where nqs is the number of considered bins. Note that if conditions stated in the remark were not met, it would be harder to optimize the tree’s parameters θ. If the reward function was not the same in all the leaves, we should decide which response to use i"
7321,unknown,"we should decide which response to use in each leaf, based on some optimization strategy. If the leaves were not disjoint, we would end up with overlapping sets Il, which would be harder to optimize even using greedy algorithms. Finally, having non constant wl in a given leaf would be equivalent to have a tree with further splits. 2.2 Boosted trees Boosting algorithms have progressively gained pop"
7322,unknown,"Boosting algorithms have progressively gained popularity among the machine learning and statistics community, starting from the introduction in the 90s of the famous AdaBoost classiﬁcation algorithm (Freund and Schapire, 1997). Originally introduced as an ensemble method (B¨ uhlmann and Hothorn, 2007), boosting was later interpreted as a gradient descent in function space (Breiman, 1998), opening "
7323,unknown,"in function space (Breiman, 1998), opening up the possibility of using it for optimizing a wide variety of smooth and non-smooth objective functions. In this paper, we follow the interpretation of boosting as an iterative optimization strategy for statistical learning. In this section, we review the original gradient descent interpretation in function space presented in Friedman (2001). A boosted "
7324,unknown,"weak learners, each of which is a tree: FK(x) = K∑ k=1 f(x,θk), f k(x,θk) ∈F (4) Under the hypothesis that L(y,F ) is continuous and smooth almost everywhere, we can seek its minimizer F∗through gradient descent iterations. To simplify the notation, we refer to ∂ℓ(y,Fk(x,Θ)) ∂Fk(x,Θ) as gk, that is, the gradient of the loss with respect to the model’s predictions at iteration k. As it is known, ap"
7325,unknown,"at iteration k. As it is known, applying gradient descent to L(y,Fk(x,Θ)) in the Fk(x,Θ) argument is equivalent to solve the following minimization problem (where Fk,i = Fk(xi,Θ) for sake of notation) at each iteration k: 5 Nespoli and Medici Fk+1 = argmin F L(y,Fk) + ∂L(y,Fk) ∂Fk T (F −Fk) + 1 2ρ∥F −Fk∥2 2 (5) = argmin F L(y,Fk) + N∑ i=1 gi(Fi −Fk,i) + 1 2ρ∥F −Fk∥2 2 (6) where ∥·∥ 2 2 denotes the"
7326,unknown,"2 (6) where ∥·∥ 2 2 denotes the sum of squares over all the predictions, ρ is a hyper-parameter and the last equality holds under the assumption of suﬃcient regularity, so that one can interchange diﬀerentiation and integration. Equation (5) can be interpreted as the act of minimizing the ﬁrst order approximation of the loss function in its argument F, while trying not to deviate too much from the"
7327,unknown,"trying not to deviate too much from the predictions of the previous ﬁtted model Fk. In order to ﬁnd the minimizer of (5), we apply the ﬁrst order optimality condition, w.r.t. each observation, and we ﬁnd: Fk+1(x,Θ) = Fk(x,Θ) −ρgk (7) which is the gradient descent step. The loss gradientgk is easily computed for the datasetD. However, as pointed out in Friedman (2001), our goal is to minimize L(y,F"
7328,unknown,"However, as pointed out in Friedman (2001), our goal is to minimize L(y,F ) not only for the dataset D, but also on unseen data, in order to perform statistical learning and achieve model generalization. For this reason, boosting replaces gk with the gradient learned by a base model f(x,θ), also known as weak learner. The iterative model ﬁtting becomes: Fk+1(x,Θ) = Fk(x,Θ) −ρfk(x,θ) (8) where fk(x"
7329,unknown,"where fk(x,θ) has been ﬁtted under least squares criterion on gk. Boosting in function space is a building block of many other machine learning algorithms. For example, it has been recently adopted, in combination with parametric probabilistic modelling and the concept of natural gradient, in the NGBoost library (Duan et al., 2019). In this paper, we will follow the method adopted by XGboost and L"
7330,unknown,"will follow the method adopted by XGboost and LightGBM, which optimizes the boosted tree using a second-order approximation of the loss function. We retain only the additive stage-wise strategy deﬁned by the iteration (8), assuming it to be coercive with respect to the prediction error. Indeed the presence of the learning rate ρ helps in dampening the response of the current iteration model, avoid"
7331,unknown,"the response of the current iteration model, avoiding overshooting of the ﬁnal model Fk+1. Under a stage-wise strategy, we can write the second order approximation of L(y,Fk) with respect to the new weak-learner as: L(y,F ) ≃L(y,Fk) + N∑ i=1 gk,if(xi,θ) + 1 2hk,if2(xi,θ) + λ 2 nl∑ l=1 w2 l (9) where h = ∂2ℓ(y,Fk(x,Θ)) ∂F(x,Θ)2 is the second order derivative of the loss w.r.t. the predictions and t"
7332,unknown,"and the last term is a regularization term. At each stage we want to ﬁnd the optimal set of parameters θ∗ k which includes both the split points and the weights. To ﬁnd θ∗ k we can follow the same strategy to ﬁt a tree introduced in section 2.1, using the second order approximation of the loss function. At ﬁrst, (9) is used to estimate the loss in each leaf, given the current splits {1l}nl 1 , and"
7333,unknown,"leaf, given the current splits {1l}nl 1 , and secondly, a greedy strategy is applied to ﬁnd the optimal splits. In the case in which the model response is constant in each leaf, and equal to 6 Multivariate Boosted Trees and Applications to Forecasting and Control r(wl) = wl, we can rewrite (3) using the second order approximation (9); the loss function (disregarding the constant term) can be deﬁne"
7334,unknown,"L(y,fk) ≃ nl∑ l=1  ∑ i∈Il ( gk,iwl + 1 2hk,iw2 l ) + λ 2 w2 l   (10) Thus, for the lth leaf, the optimal wl given the split is: w∗ l = −∑ i∈Il gk,i λ+ ∑ i∈Il hk,i (11) The optimal approximated leaf loss becomes: ˜ℓ∗= −1 2 ∑ i∈Il g2 k,i λ+ ∑ i∈Il hk,i (12) This is the same procedure used by XGboost an LightGBM, for instance. In order to consider non-constant responses, two strategies can be fol"
7335,unknown,"consider non-constant responses, two strategies can be followed: the ﬁrst is to replace wl in the inner summation of (10) with r(wl). We can then compute the optimal response as: r(wl)∗= −∑ i∈Il gk,i λ+ ∑ i∈Il hk,i (13) In order to ﬁnd the optimal parametersw∗ l, this requires the responser(wl) to be analytically known and invertible. Since this is not true for some interesting applications, as in"
7336,unknown,"in which the response is in the form r(wl) = Aw with Rna×nt and na > nt, we propose to replace the approximation of the loss function w.r.t. the model’s prediction with the approximation w.r.t. the models’ weights w. Deﬁning ˜g and ˜h as the gradient and the second derivative of the loss function, with respect to the model weights, for the chain rule, we can write for each leaf: ˜gk,i = gk,i ∂r(wl"
7337,unknown,"˜gk,i = gk,i ∂r(wl) ∂wl ˜hk,i = gk,i ∂2r(wl) ∂w2 l + hk,i (∂r(wl) ∂wl )2 ∀i∈Il (14) Note that for the usual case in which the leaf response is constant, ˜ g = g and ˜h = h. We can now use equations (10), (11) and (12) replacing gk,i and hk,i with ˜gk,i and ˜hk,i. This allows us to keep the same procedure for ﬁtting the tree while just requiring r(w) to be diﬀerentiable w.r.t. w. 2.3 MBTs Multivari"
7338,unknown,"2.3 MBTs Multivariate GBTs can be ﬁtted by following the same procedure described in the previous section. The only diﬀerence relies on the dimensionality of the target variable y ∈RN×nt where nt is strictly greater than 1, and the use of the Hessian matrix instead of the second derivative for the the computation of the approximated loss and optimal weights. For 7 Nespoli and Medici clarity, we re"
7339,unknown,"clarity, we report the matrix form of ˜g and ˜h in the multivariate case, for which (14) are the univariate analogous: ˜g= g (∂r(wl) ∂wl ) (15) ˜h= g∂2r(wl) ∂w2 l + ∂r(wl) ∂wl T h∂r(wl) ∂wl (16) where g ∈ RN×nt, h ∈ RN×nt×nt, ∂r(wl) ∂wl ∈ Rnt×nw, ∂2r(wl) ∂w2 l ∈ Rnt×nw×nw. Note that the number of dimensions of the leaf parameter vector, nw, may be diﬀerent from the dimensionality of the target, nt"
7340,unknown,"dimensionality of the target, nt. For example, this is the case of hierarchical forecasting, presented in section 3.2. We stress out that in the multivariate case, the second derivative of the response function is a 3-order tensor. However, as we will see, for many combinations of objective function and responses, MBT ﬁtting won’t require to store or compute the whole tensor, considerably simplify"
7341,unknown,"i∈Il ˜gi,k with ˜G and ∑ i∈Il ˜hi,k with ˜H, the optimal response (11) and the optimal loss (12) can be rewritten as: w∗ l = − ( Λ + ˜H )−1 ˜G (17) L∗ l = ˜GT ( Λ + ˜H )−1 ˜G (18) where Λ ∈ Rnr×nr is the quadratic regularization matrix, which weights the L2 norm penalization of the model parameters, ∥wl∥2 Λ. The complete procedure for ﬁtting the MBT is described in algorithm 1 and 2. Algorithm 1 d"
7342,unknown,"is described in algorithm 1 and 2. Algorithm 1 describes the boosting procedure: starting from an initial guess for ˆy, which in this case corresponds to the column-expectations of y, we retrieve the gradient ˜gand hessian matrices ˜hfor all the observations of the dataset (line 3), given the loss function L and the leaf response function r. At line 4 the weak learner at iteration kis ﬁtted using "
7343,unknown,"iteration kis ﬁtted using the fit-tree algorithm described in 2. Then the overall model Fk is updated (line 5) along with the training loss (line 7). This is computed through the exact formulation of the loss function and includes a term for the penalization of the number of leaves T = ∑K k=1 nl,k in the ﬁnal model Fk: ε= L(y,Fk(x)) + ρTT (19) The procedure ends if the training loss is not decreas"
7344,unknown,"The procedure ends if the training loss is not decreasing or the iterations exceeded the maximum number ni. Algorithm 2 describes the recursive procedure to ﬁt the multivariate tree. At line 1-2 the algorithm halts if the number of observations is lower than a threshold, nmin. If this is not the case, the total leaf loss is computed (line 3), and the best split point search is carried out for all "
7345,unknown,"point search is carried out for all the variables in x (line 4). As anticipated, we use the same histogram search adopted in XGboost and LightGBT, see algorithm 2 in Chen and Guestrin (2016) and algorithm 1 in Ke et al. (2017). Brieﬂy speaking, instead of enumerating all the possible split points as done by the pre-sorting algorithm (Mehta et al., 1996), only a few numbers of quantiles are tested "
7346,unknown,"a few numbers of quantiles are tested for each feature. This does not reduce too much the ﬁnal regressor accuracy; on the other hand, since ﬁnding the best split takes most of 8 Multivariate Boosted Trees and Applications to Forecasting and Control the computational time of boosted tree algorithms, this procedure substantially speeds up the ﬁtting process. At line 5, the quantiles for the jth feat"
7347,unknown,"the ﬁtting process. At line 5, the quantiles for the jth feature are retrieved and are then used at line 7 to obtain the partial sums of the gradient and Hessian, based on the split point q and variable j. The split-loss La + Lb is then computed using equation (18); if this value is lower than the current minimum, the latter and the best split candidate are updated (line 10-11). Finally, if a spli"
7348,unknown,"updated (line 10-11). Finally, if a split with a total loss lower than L0 has been found, the procedure is called recursively, with partial datasets, gradients and Hessian, based on the best split. Otherwise, the current node is considered a terminal leaf, and the optimal response is computed based on equation (17). Algorithm 1: MBT training Input: training dataset: Dtr = {(xi,yi)N i=1},L,ni,r(w) "
7349,unknown,"Output: boosted tree F 1 ˆy←[Ejyj,i]nt i=1 ⊿ initial guess 2 while k<n i and εk <εk−1 ,k++ do 3 ˜g,˜h←ˆy,y, L ⊿ using (15) and (16) 4 fk ←fit-tree ( x,y, ˜g,˜h ) 5 Fk ←Fk−1 + ρfk 6 ˆy←Fk(x) 7 εk−1 ←y,Fk(x) ⊿ using (19) 3. Multivariate Regularization In this section, we introduce some of the most relevant loss functions and multivariate responses that can be modelled through the proposed MBT. 3.1 C"
7350,unknown,"responses that can be modelled through the proposed MBT. 3.1 Covariance structure and Smoothing Generally speaking, imposing a learning bias on the covariance structure of the target can be beneﬁcial for any machine learning algorithm. The most known example of this is linear regression ﬁtting under generalized least squares; in this case, the estimated covariance matrix of the errors ˆΩ is used t"
7351,unknown,"matrix of the errors ˆΩ is used to penalize the model’s errors diﬀerently. This can be readily integrated using a linear response function (as explained in section 3.2). Under a constant model response, r(wl) = wl, the covariance structure of the data can be taken into account by means of the quadratic regularization matrix Λ. For example, we can impose a given smoothness of the response using a ﬁ"
7352,unknown,"smoothness of the response using a ﬁltering approach (Kim et al., 2009) such as an Hodrick- Prescott ﬁlter (de Jong and Sakarya, 2016), punishing the discrete second-order derivative of r. This can be obtained setting Λ = λDTD where D ∈R(nt−2)×nt is the second-order diﬀerence matrix: D=   1 −2 1 ... ... ... 1 −2 1 1 −2 1   (20) 9 Nespoli and Medici Algorithm 2: fit-tree Input: x,y, ˜g,˜h"
7353,unknown,"Input: x,y, ˜g,˜h,f,node Output: tree f 1 if length(x) <nmin then 2 return 3 L0 = L∗←G,H 4 for xj ∈xT do ⊿ find best split 5 qj ←xj,nqs 6 for q∈qj do ⊿ histogram search 7 ˜Ga, ˜Ha, ˜Gb, ˜Hb ←˜g,˜h,q,j 8 La,Lb ← ˜Ga, ˜Ha, ˜Gb, ˜Hb 9 if La + Lb <L∗then 10 f[node] .split←(q,j) 11 L∗←La + Lb 12 if L0 <L∗then ⊿ recursive split 13 ˜ga,˜ha,˜gb,˜hb ←˜g,˜h,f [node] .split 14 xa,ya,xb,yb ←x,y,f [node] .spli"
7354,unknown,"15 fit-tree ( xa,ya,˜ga,˜ha,f,node a ) 16 fit-tree ( xb,yb,˜gb,˜hb,f,node b ) 17 else ⊿ compute best response 18 f[node] .ropt ←r ( ( ˜H+ Λ)−1 ˜G ) 10 Multivariate Boosted Trees and Applications to Forecasting and Control Since under constant response h= Int where Int is the identity matrix of dimension nt, we have: Λ + H = λDTD+ nlInt (21) where nl is the number of observations in the current lea"
7355,unknown,"where nl is the number of observations in the current leaf. The previous expression can be replaced in (17) and (18) to retrieve the optimal response and loss of MBT, respectively. Imposing a condition on the derivative smoothness of the response can be seen as a way to perform signal denoising. If the Hodrick-Prescott ﬁlter is applied in a forecasting task, the approach becomes similar to denoisi"
7356,unknown,"task, the approach becomes similar to denoising the time series with an a priori smoothing. However, imposing smoothness of the forecasted signal gives the regressor a chance to predict statistically signiﬁcant peaks, that would have been smoothed out in the pre-processing phase. A second approach to induce prediction regularization is through smoothing via basis function (Ramsay et al., 2009). As"
7357,unknown,"function (Ramsay et al., 2009). As recently proposed in Oreshkin et al. (2019) in the context of forecasting with neural networks, we can couple a Fourier expansion with the MBT algorithm. We deﬁne the response as r = Pw where P ∈Rnt×2nk, is a projection matrix onto sine and cosine function space with nk diﬀerent wavenumbers: P = [{( cos ( k2πt nt ) ,sin ( k2πt nt ))nt t=1 }] k∈K (22) where Kis th"
7358,unknown,"}] k∈K (22) where Kis the set of considered wave numbers. Under L2 loss, the ith component of the loss function gradient and Hessian can be written as: ˜gi = −PTgi (23) ˜hi = PTP = Inr (24) where the last equality holds due to the fact that P is orthonormal. Under these conditions (17) then becomes: w∗ l = −(Λ + nlInr)−1 PTG (25) where G= ∑ i∈Il gi, and (18) becomes: L∗ l = GTP(Λ + nlInr)−1 PTG= G"
7359,unknown,"where G= ∑ i∈Il gi, and (18) becomes: L∗ l = GTP(Λ + nlInr)−1 PTG= GT (Λ + nlInr)−1 G (26) where the last equality holds again for the orthonormality of P, and Λ being diagonal. 3.2 Latent variables and hierarchical forecasting In several applications, we are interested in responses that are linear combinations of a ﬁxed matrix S ∈Rnt×nr. That is, S is kept constant through leaves and boosting rou"
7360,unknown,"matrix S ∈Rnt×nr. That is, S is kept constant through leaves and boosting rounds, while the response r= Swl can change conditionally to the observations. This procedure restricts the response to lie in the span of S. When the dimensionality of wl is smaller than the response (nw <nt), wl can be seen as latent variables generating the full response. Latent variables are usually used to induce regul"
7361,unknown,"variables are usually used to induce regularization in regression (Izenman, 1975). Loosely speaking, it is easy to see that all the (conditional) information which is needed to generate y∈RN×nt is already present in x∈RN×nf if yT = CxT +ε, where C ∈Rnt×nf is constant, and ε∈RN×nt is the realization of a Gaussian random variable. A notable application of 11 Nespoli and Medici this approach is what "
7362,unknown,"this approach is what is known as hierarchical forecasting; this method tries to reconcile previously produced point forecasts for hierarchically structured signals, by ensuring that the corrected forecasts are consistent under addition. In brief, every time we want to predict a set of base or bottom signals and their groupings (aggregations), we face the problem of making the forecasts aggregate-"
7363,unknown,"making the forecasts aggregate-consistent. Consistency under aggregation is not guaranteed if we separately forecast the bottom time series, call them yb ∈RN×nb, and their groupings generated by aggregations yu ∈RN×nu. The simplest method to have a set of aggregate- consistent forecasts is apply the so-called bottom-up approach, in which only the bottom time series are forecasted, and the forecast"
7364,unknown,"time series are forecasted, and the forecasts for the aggregated time series are generated by summing them up according to the grouping. This naive approach has been shown to be in general worse than generating forecasts for the aggregated time series by optimally combine the bottom forecasts, which is the concept behind hierarchical forecasting. Denoting the whole set of original forecasts as ˆy "
7365,unknown,"whole set of original forecasts as ˆy = [ yT u,yT b ]T ∈RN×nt, where nt = nb + nu and nb and nu are the number of the bottom and upper time series, hierarchical forecasting consists in ﬁnding a set of corrected bottom forecasts, ˜yb, which minimize the overall forecast error and such that the following equation holds: ˜yT = S˜yT b (27) where ˜y are the corrected signals for the whole hierarchy and"
7366,unknown,"matrix. An example of a three-level summation matrix is the following: S =   1 1 1 1 1 1 0 0 0 0 1 1 I4   (28) In Hyndman et al. (2011), the authors used ordinary least squares regression to recon- cile the forecasts in the hierarchy. Elaborating on this approach, in Wickramasuriya and Athanasopoulos (2017) and in Wickramasuriya et al. (2018), the authors proposed a trace minimization meth"
7367,unknown,minimization method (called minT) in which the covariance matrix of the forecasters’ error is estimated to perform a weighted least squares regression. The basic idea exploited in all the aforementioned works is that forecasts can be reconciled solving a generalized least squares problem with error covariance matrix ˆΩ ∈Rnt×nt: ˜yT = S argmin z ∥ˆyT −SzT∥2 Ω−1 (29) which has an analytical solution
7368,unknown,"˜yT = S ( STΩ†S )−1 STΩ†ˆyT (30) where †denotes the pseudo-inverse, since Ω is typically near-singular. Diﬀerent hierarchical reconciliation methods basically diﬀer in the choice and estimation of the error covariance matrix Ω. We can see how (30) exploits only information of the originally forecasted signals, and of Ω. The latter is usually estimated using forecast errors from a training set (or "
7369,unknown,"all the available observations), and as such, can be considered invariant. We propose to 12 Multivariate Boosted Trees and Applications to Forecasting and Control use a MBT to estimate the reconciled signals starting from ˆ y. This is easily obtained by setting the response to r= Sw. Since S is ﬁxed, following the same reasoning of the Fourier decomposition approach introduced in 3.1, equations (2"
7370,unknown,"w∗ l = −(Λ + nlSTS)−1STG (31) L∗ l = GTS(Λ + nlSTS)−1STG (32) The advantage of using a MBT over computing ˜y is that we can use additional features to build the trees. We propose to ﬁt the MBT on the residual between the observed signals and the bottom-up reconciliation, y−ˆybST, such that the ﬁnal reconciled time series can be written as: ˜ymbt = f ( {( ˆyi,ϵi,xt,i)N i=1} ) + ˆybST (33) where ϵi "
7371,unknown,"+ ˆybST (33) where ϵi = ˆyt−1 −yt−1 ∈RN×nt contains the forecast error at the timestep prior to the reconciliation and xt contains categorical encoding of the weekday and the day-hour. In- cluding ϵi in the tree features gives a possibility to the MBT to trust the forecast of the ith predictor, based on its current performances. 3.3 Quantile loss and its relaxations Quantile estimation in the cont"
7372,unknown,"called quantile loss function, deﬁned as: lq(ϵτi) = ( τi −1ϵτi<0 ) ϵτi (34) where ϵτi = y−ˆqτi is the distance between the observations and the predictions for the τi quantile. It can be shown that the expectation of (30) is minimized when ˆ qτi is the τi quantile of FY, qY(τi) = F−1 y (τi) = inf {y : FY(y) > τi}, for any cdf FY. The quantile loss (34) is linear and asymmetric, with an undeﬁned de"
7373,unknown,"loss (34) is linear and asymmetric, with an undeﬁned derivative at ϵτi = 0 and constant 0 Hessian. These characteristics make it hard to exploit the second-order approximation strategy. Indeed, relying only on the ﬁrst-order approximation reduces the boosting strategy to ﬁtting a classiﬁer on the sign of ϵτi at each iteration k. Some popular boosted tree packages, like XGBoost, relax the loss func"
7374,unknown,"packages, like XGBoost, relax the loss function (34) considering a constant second derivative equal to 1. This has the practical eﬀect of ﬁtting the kth model fk to the leaf-average binary response Iϵτi>0. We propose a further relaxation of the problem, approximating the discontinuous gradient of the quantile loss function with a smooth function. The idea of smoothing the quantile loss for ﬁtting "
7375,unknown,"smoothing the quantile loss for ﬁtting boosted models was already introduced in Zheng (2012), where the authors propose to use the cumulative density function of the Gaussian distribution (erf(ϵτi)) as a smoothed version of the gradient of (34). The rationale behind smoothing lq is that the MBT will have additional information on how far the observations are from the predicted quantile, which can "
7376,unknown,"are from the predicted quantile, which can help in building the tree. In this paper, we decided to use the (scaled and shifted) inverse logit function as a smoothed version of ˜lq derivative, due to its relation with logistic regression literature and the AdaBoost algorithm (see appendix B). This choice can be explained by the fact that the distance of the predicted τi quantile from the observatio"
7377,unknown,"τi quantile from the observation, i.e. ϵτi, is interpreted as the re-weighted log-odds of the condition ϵτi > 0. That is, if we describe y as the observation drawn from the random 13 Nespoli and Medici variable Y(x), given the prediction ˆqτi(x), we assume: ϵτi(x) = y−ˆqτi(x) = log ((1 −τi)FY|x τi(1 −FY|x) ) (35) where FY|x is the conditional cdf of Y. Inverting (35) we obtain: FY|x = eϵτi(x)+s 1 "
7378,unknown,"FY|x = eϵτi(x)+s 1 + eϵτi(x)+s (36) where s is logit(τi). It can be easily veriﬁed that FY|x = τi when ϵτi = 0. In other words, we are implicitly assuming that the estimated quantile ˆ qτi is the correct one, under the hypothesis of Y having a logistic pdf: hi,i = dFY|x = eϵτi+s (1 + eϵτi+s)2 (37) where hi,i is the ith diagonal element of the Hessian of the loss function. We can now deﬁne the smoo"
7379,unknown,"the smoothed derivative of lq(ϵτi) as: −gk = −∂˜lq(ϵτi) ∂fk(x) = ∂˜lq(ϵτi) ∂ϵτi = FY|x −1 + τi (38) and we can now see that its second derivative is equal to the probability density function (37). Since −1+ τi is a constant, and at each iteration we ﬁt fk(x) on −gk, we can interpret the boosting procedure under the smoothed loss function as an iterative ﬁtting on the probability p{Y<ˆqτi|x}. We ca"
7380,unknown,"probability p{Y<ˆqτi|x}. We can see how the hypothesis on the distribution of the residuals we made in (35), and especially the τi re-weighting, has the eﬀect of shifting ˜lq such that its minimum is located in ϵτi = 0. The eﬀect of changing scan be seen in Fig. 1. As shown in Fig. 1, ˜lq(ϵτi) and its derivatives are now smooth functions, thus we can apply the same second-order approximation for ﬁ"
7381,unknown,"Linear-quadratic quantile loss function Smoothing lq(ϵτi) has two main drawbacks. First, we cannot guarantee anymore its min- imizer being the τi quantile of FY|x, independently from its distribution. In fact, any minimizer of E(lq(ϵτi)) must zero its derivative, and this is true for any distribution FY|x only if the derivative is independent from FY|x. The second drawback is that, as we try to mi"
7382,unknown,"mitigate the ﬁrst eﬀect by narrowing the pdf, the objective function becomes closer to the original quantile loss, turning the regression problem again in a classiﬁcation one. Here we introduce a linear-quadratic quantile loss function which is consistent for any target pdf. We exploit the learning peculiarities of trees to approximate Fl,Y|x in each leaf with the em- pirical one, ˆFl,Y|x, and cra"
7383,unknown,"quantile of the ˆFl,Y|x. Theorem 1 Given a sample population D = {(xi,yi)N i=1}, ϵτ,i = yi −ˆqτ(xi) being the distance between the ith observed target and its predicted τ quantile, k being a constant, the 14 Multivariate Boosted Trees and Applications to Forecasting and Control 10 5 0 5 10 0 2 4 6 8 q q ′ q ′′ q 0.8 0.6 0.4 0.2 0.0 0.2 ′ q, ′′ q Figure 1: Continuous lines: smoothed quantile loss ˜"
7384,unknown,"q, ′′ q Figure 1: Continuous lines: smoothed quantile loss ˜lq(ϵτi) and its ﬁrst and second deriva- tives for τi = 0.2. Dashed lines: the same functions, for s= 0. 15 Nespoli and Medici following loss function: lqs,i(ϵ,ˆqτ(xi),τ) = ( (τ −1)ϵτ,i + kϵ2 τ,i 2¯ϵτ,l ) 1y<ˆqτ + ( τϵτ,i + kϵ2 τ,i 2¯ϵτ,r ) 1y≥ˆqτ −ϵτ,i2 k N (39) where ¯ϵτ,l = ∑ i∈Il ϵτ,i, Il = {i : yi < ˆqτ(x)}, ¯ϵτ,r = ∑ i∈Ir ϵτ,i,Ir = {"
7385,unknown,"i∈Ir ϵτ,i,Ir = {i : yi ≥ˆqτ(x)}, is minimized by the empirical quantile of Y = (yi)N i=1. The proof is reported in appendix A. The diagonal entries of the Hessian are then: hi,i = ∂2lqs,i ∂ϵ2 τ,i = ( k ¯ϵτ,l ) 1y≤ˆqτ (40) + ( k ¯ϵτ,r ) 1y≥ˆqτ As recently introduced in the LightGBM implementation, we also consider the case of reﬁtting the leaf responses wl. After ﬁtting the weak learner fk using on"
7386,unknown,"reﬁtting the leaf responses wl. After ﬁtting the weak learner fk using one of the approxi- mated previously introduced losses, we replace wl with the exact minimizers of (34), given the identiﬁed tree regions. That is, for each τi: wl,i = ˆF−1 Y|xl (ϵk,l,τi) (41) where ϵk,l,τi is the error at iteration k for the current leaf and quantile τi, while ˆF−1 Y|xl is the inverse of the empirical conditio"
7387,unknown,"Y|xl is the inverse of the empirical conditional cdf of the current leaf. 3.4 Data driven control Standard control methods rely on a model of the controlled system, which is usually iden- tiﬁed through system identiﬁcation techniques (Ljung, 1998). One standard description of the controlled system is the so called linear state-space representation, which in its discrete time-invariant form is desc"
7388,unknown,"time-invariant form is described by: ξt+1 = Aξt + But + Gwt (42) γt+1 = Cξt + Dut + Hwt + vt (43) where ξt ∈Rns is the vector of system states, γt ∈Rno is the vector of measured system’s outputs, ut ∈Rno is the vector of system’s controlled inputs and wt ∈Rns and vt ∈Rno are two vector of (usually) uncorrelated Gaussian disturbances, taking into account discrep- ancy between the system’s model and"
7389,unknown,"ancy between the system’s model and the real one and measurement noise, respectively. Model (42)-(43) is then used to optimally control the target system, usually coupling it with feedback controllers or with model predictive control (MPC) (Morari et al., 1988). Data-driven control (DDC) has been introduced in the last years as a way to overcome identiﬁcation issues in MPC. For many systems of int"
7390,unknown,"identiﬁcation issues in MPC. For many systems of interest, a single linear system could not provide enough accuracy, while increasing the number of states or switching to a non-linear 16 Multivariate Boosted Trees and Applications to Forecasting and Control Section L r ˜G ˜H+ Λ 3.1 L2 w ϵ nlInr + λDTD 3.1 L2 Pw PTϵ nlInr + Λ 3.2 L2 Sw STϵ nlSTS+ Λ 3.4 L2 xlr,lw ϵxT lr,l xT lr,lxlr,l + Λ 3.3 lq(ϵτ)"
7391,unknown,"3.2 L2 Sw STϵ nlSTS+ Λ 3.4 L2 xlr,lw ϵxT lr,l xT lr,lxlr,l + Λ 3.3 lq(ϵτ) w (35) / (39) (37) / (40) Table 1: List of combinations of loss and response functions, with their gradients and Hessians. First row: constant response with second derivative regularization. In rows 2 ,3,4 responses are linear functions of: nonlinear basis function, constant summation matrix, feature space. Last row: diﬀeren"
7392,unknown,"feature space. Last row: diﬀerent quantile loss approximations with a constant response. system can introduce identiﬁcation issues and increase the computational time of the con- troller. The authors in Jain et al. (2017); Smarra et al. (2018) introduce the idea of ﬁtting a tree f(x,θ), which responses are linearized dynamics of the controllable system. If the fea- tures used for growing the tree "
7393,unknown,"tures used for growing the tree do not include control actions and system states, the linear dynamics identiﬁed in the leaves can be regarded as independent from the system and thus be directly used for control. Overcoming identiﬁability issues for control application is of great practical interest, and as such DDC gained popularity in the last year (issue Energies, 2019). Here we propose to apply"
7394,unknown,"2019). Here we propose to apply MBTs to increase the accuracy of the identiﬁed linear models, with respect to the one identiﬁable with a single tree. In this case, the weak learner f(x,xlr,θ) requires two sets of features: the one used to grow the tree and choose the best split x∈RN×nf, and the one used to ﬁt the linear model in each leaf xlr ∈RN×nlf. Note that, due to the additive nature of boost"
7395,unknown,"that, due to the additive nature of boosting, the ﬁnal model will still be a linear system in the tree’s inputs. In this case, the second-order approximation is not helpful to reduce the calculation eﬀort, because it corresponds to the exact solution of a linear system. We have, in fact: w∗ l = − ( Λ + xT lr,lxlr,l )−1 xT lr,lgl (44) where xlr,l ∈Rnl×nf is the feature matrix in the current leaf, a"
7396,unknown,"matrix in the current leaf. 3.5 Consistency The additive nature of boosting guarantees consistency in the properties encoded in the weak learners, if they are invariant under summation. The two smoothing approaches presented in section 3.1 show diﬀerent levels of consistency under boosting. For the Hodrick-Perscott ﬁlter, at each iteration, a curve with penalized second derivative is added in each"
7397,unknown,"ﬁlter, at each iteration, a curve with penalized second derivative is added in each leaf, such that the ﬁnal curve is still smooth. However, if we compute the quadratic loss for the ﬁnal response, (∑ni k=1 wl,k )T DTD (∑ni k=1 wl,k ) could be higher than the same loss from a single weak learner. This means that the ﬁnal level of smoothness could depend on the number of ﬁtting rounds ni. For the Fo"
7398,unknown,"of ﬁtting rounds ni. For the Fourier expansion case, the ﬁnal response will be a summation over Fourier coeﬃcients in the chosen wave numbersk∈K, which means the ﬁnal signal will 17 Nespoli and Medici be a superposition of columns of P. This means that the Fourier decomposition property of identifying a signal composed only by harmonics with Kwave numbers is fully retained. The single ﬁtted respon"
7399,unknown,"The single ﬁtted responses in section 3.2 respects the hierarchical relationship encoded in S, that is rk = Swl,k. Since S is constant through leaves and boosting rounds, also the ﬁnal prediction retain this property, since r= ∑ni k=1 Swl,k = S∑ni k=1 wl,k. Quantile losses of section 3.3 do not generate strictly consistent responses. This is because the quantiles corrections identiﬁed at each iter"
7400,unknown,"because the quantiles corrections identiﬁed at each iteration k are not jointly constrained. However, we will see in section 4.4 that in the case of the reﬁtting strategy, consistency is respected in practice, presenting very few quantile crossing instances. Finally, the prediction of MBT with linear responses of the feature space, like the one in section 3.4, is consistently linear in x, being a "
7401,unknown,"3.6 Numerical Methods Table 1 summarizes the forms of the loss gradients and Hessian for the diﬀerent combinations of losses and responses introduced in the previous section. In particular, the last column contains the expression that needs to be inverted when computing the optimal response w∗ l and approximated loss function. Inverting ˜H+ Λ requires most of the computational time of the algorith"
7402,unknown,"of the algorithm. Thus it is important to try to simplify or speed up this computation. In Zhang and Jung (2019), the authors present an upper bound for the optimal response and loss in the case of a constant response and when the matrix ˜H+ Λ is diagonally domi- nant. Here we show how to accelerate the exact computation of ( ˜H+ Λ)−1 for three of the cases in table 1. We can see how the ﬁrst two "
7403,unknown,"cases in table 1. We can see how the ﬁrst two cases require to invert a constant (through leaves and boosts) matrix, plus the identity matrix multiplied by the number of elements in the current leaf, nl. Called A ∈Rk,k this matrix, this inversion can be reduced to a matrix multiplication in the form ( A+ nIk)−1 = QT ˜LQ where ˜L ∈Rk,k is diagonal with ˜Li,i = 1/(λAi + n) and λAi is the ith eigenve"
7404,unknown,"˜Li,i = 1/(λAi + n) and λAi is the ith eigenvector of A, thanks to lemma (2) reported in appendix C, along with its proof. Since in our case A is constant, its eigenvalues, Q and its inverse can be computed only once for the entire ﬁtting process. The only variable part is n, which in our case corresponds to the number of observations in the current leaf. This only aﬀects the diagonal entries of ˜"
7405,unknown,"only aﬀects the diagonal entries of ˜L, while all the other quantities remain unchanged. For the third case of table 1, we have to invert nlSTS+ Λ. Once again, the only non-constant term is nl. If the quadratic regularization term Λ is a multiple of the identity matrix (as is typically assumed), this can be written as nlSTS + nI, and we can use the following corollary of lemma (2): Corollary Given"
7406,unknown,"Corollary Given a symmetric invertible matrix A ∈Rk×k, (mA+ nIk)−1 can be com- puted as: (mA+ nIk)−1 = QLQ−1/m (45) where L∈Rk,k is diagonal with Li,i = 1/(λi + n/m), and λi and Q as deﬁned in (2). the latter corollary follows from lemma (2) proof, noting that mA+ nIk = m(A+ nIk/m). 18 Multivariate Boosted Trees and Applications to Forecasting and Control F1 F2 F3 0 1 2 3 4 5 6 t Figure 2: Example"
7407,unknown,"Figure 2: Example of CV for 3 folds. Rows and columns indicate diﬀerent folds and diﬀerent times, respectively. Blue: training sets. Violet: test sets. 4. Numerical results In this section, we present numerical results of the responses and loss functions introduced in section 3. For all the datasets, we obtained the results using k-fold cross-validation (CV). Since all the applications deal with t"
7408,unknown,"(CV). Since all the applications deal with temporal data, we adopted sliding-window cross- validation. An example of training and testing splits under this cross-validation is shown in Fig. 2, in the case of 3 folds. In all the experiments the hyperparameters were ﬁxed to the following values, in order to guarantee a fair comparison with the LightGBM regressors. For all the experiments, we kept th"
7409,unknown,"For all the experiments, we kept the LightGBM’s number of iterations ﬁxed to 100 and a learning rate of 0.1, as for the MBT models. Table 4 shows the most important parameters for the diﬀerent experiments carried out in the paper. The minl parameter speciﬁes the minimum number of observation in one leaf. We set a minimum number of 10 observations per feature for the VSC experiment, since in this c"
7410,unknown,"per feature for the VSC experiment, since in this case we need to solve a linear regression in each leaf. At the same time, we lower the value of λ to 0.01 in this case, since we didn’t expect presence of noise in the simulated dataset. nboost learning rate minl λ Fourier (4.1) 100 0.1 300 1 Hierarhical (4.2) 100 0.1 400 1 Quantiles (4.4) 100 0.1 300 1 VSC (4.3) 100 0.1 10 nf 0.01 Table 2: Valuse "
7411,unknown,"Table 2: Valuse for the most important hyperparameters, as a function of the numerical experiment (and the corresponding section in brackets). 4.1 Forecasting via Fourier decomposition We applied the Fourier-based MBT introduced in 3.1 to two public datasets, available at (D1, 2022) and (M4, 2022). The ﬁrst one consists of about 1 year of electrical load mea- surements of secondary substations and"
7412,unknown,"surements of secondary substations and cabinets located in a low voltage distribution grid, and additional numerical weather predictions for the temperature and the irradiance. The signals have a sampling frequency of 10 minutes. In total, 31 time series are provided, show- ing hierarchical relationships, that is, 7 time series are the algebraic summation of speciﬁc subgroups. Called Pi the power "
7413,unknown,"subgroups. Called Pi the power measurement of the ith time series, we aim at forecasting the day-ahead signal (144 steps), given historical values of the power, the numerical weather 19 Nespoli and Medici predictions of temperature and irradiance, and time-related covariates: ˆPi,t = f(Pt−j,xt,xf,t+z) (46) where xt contains categorical encodings of the weekday and the day-hour,xf,t+z contains the "
7414,unknown,"numerical weather predictions of temperature and irradiance at timet+zand z,j ∈[1,144], meaning that we pass to the forecaster all the numerical weather predictions and an history of the power signal of 24 hours. We compared the MBT with two baselines using LightGBM and two diﬀerent multi step-ahead strategies (Ben Taieb et al., 2012). The ﬁrst one mimics a multiple-input multiple-output approach "
7415,unknown,"a multiple-input multiple-output approach (MIMO). This is obtained, similarly to what is done in Sampathirao et al. (2014) with support vector machines, by adding an auxiliary feature xc,i to the dataset, which represents a categorical encoding of the step ahead to which yi corresponds. The second one adopts a multiple-input single-output (MISO) approach: 144 diﬀerent models are trained, each of t"
7416,unknown,"144 diﬀerent models are trained, each of them predicting a given step ahead. This strategy has the advantage of increasing the ﬁnal forecaster ﬂexibility, at the price of disregarding time correlations in the predictions. An example of 24 hours ahead Fourier forecasting using an increasing number of har- monics is shown in Fig. 3. The top panel shows the aggregated time series, while the second pa"
7417,unknown,"panel shows one of the bottom (more variable) time series. It can be seen how increasing the number of harmonics (from dark to light colours) increases the ﬂexibility of the fore- caster while keeping potential useful time correlations. However, in this case, the targets present a degree of correlation which depends on the hour of the day. In the top panel of Fig. 3 it can be seen how the target i"
7418,unknown,"Fig. 3 it can be seen how the target is strongly correlated in the early morning and during evening hours, while correlation is less obvious in during the day. This pattern is recurrent in all the days of the dataset. To see the eﬀect of the number of harmonics on the accuracy of the MBT, we retrieve the forecasts for all the 31 time series using a 3 fold CV, for an increasing number of wavenumber"
7419,unknown,"increasing number of wavenumbers. This investigation is reported in Fig. 4, where the CV fold-mediated and normalized RMSE and MAPE are reported. The ﬁrst column uses the values of the RMSE and MAPE from the MIMO strategy benchmark for the normaliza- tion of the results, while the second one normalizes the MBT key performance indicators (KPIs) with the one obtained with the MISO strategy. Dots hig"
7420,unknown,"(KPIs) with the one obtained with the MISO strategy. Dots highlights the best normalized performance for the various time series, while colours represent the MAPE obtained with MIMO (ﬁrst column) and MISO (second column) strategies. We can see how the MBT is strictly better than the MIMO strategy in terms of RMSE, for almost all the number of harmonics, while achieving better results in terms of M"
7421,unknown,"harmonics, while achieving better results in terms of MAPE for all but one case. Despite the lack of inter-temporal information, the MISO strategy performs better than the other two on average. The MBT provides higher accuracy for 14 time series in terms of RMSE and for 11 in terms of MAPE. However, no evident correlation with respect to the MISO strategy MAPE (line colour) is observed. In all the"
7422,unknown,"In all the cases, we can observe an initial improvement of performances with respect to increasing wavenumber. Results show that the minimum of the KPIs lies in what looks like a plateau for all the considered cases, as the wavenumber increases. This means that while considering more harmonics than the one highlighted by the dots, the accuracy does not increase or decrease signiﬁcantly. This sugge"
7423,unknown,increase or decrease signiﬁcantly. This suggests that including a priori information on the smoothness (and time correlation structure) of the curve doesn’t seem to be particularly helpful for this dataset. This is possibly due to the fact that the available features are 20 Multivariate Boosted Trees and Applications to Forecasting and Control 0 20 40 60 80 100 120 140 500 600 700 800 900 P [kW] o
7424,unknown,"P [kW] obs mimo miso MBT3 MBT6 MBT14 MBT31 MBT70 0 20 40 60 80 100 120 140 step ahead [600s] 6 8 10 12 14 16 18 P [kW] Figure 3: Example of forecasting via Fourier decomposition on the aggregated time series (top) and on time series belonging to the lower aggregation level (bottom). Thick blue line represents the ground truth, while the dotted and dashed lines represents the mimo and miso benchmar"
7425,unknown,"the mimo and miso benchmarks. The other lines are forecasts obtained with MBT, the color indicating an increasing number of considered frequencies, from darker to lighter. 21 Nespoli and Medici 0.8 0.9 1.0 1.1 1.2 1.3 1.4 MAPE ratio [-] mimo miso 0.8 0.9 1.0 1.1 1.2 1.3 RMSE ratio [-] 0 25 50 75 100 0 25 50 75 100 0 25 50 75 100 n freq [-] 0 25 50 75 100 n freq [-] 0.063 0.099 0.14 0.17 0.21 0.24 "
7426,unknown,"MAPE [-] Figure 4: RMSE and MAPE, mediated over CV folds and step ahead, for a diﬀerent number of harmonics ﬁtted by the forecaster based on Fourier decomposition. In the ﬁrst column, the KPIs are normalized with the KPIs of the MIMO forecaster, while in the second one they are normalized with the KPIs of MISO strategy. Colours in the ﬁrst and second column refer to the MAPE of the MIMO and MISO s"
7427,unknown,"respectively. Histograms show the distributions of the best KPIs as a function of the number of ﬁtted harmonics, marked as a dot for each case. 22 Multivariate Boosted Trees and Applications to Forecasting and Control already very informative for the prediction of the power signal, and further imposing a regularization on the temporal shape of the prediction doesn’t help the regression. In order t"
7428,unknown,"In order to test this hypothesis, we applied the same methods on the hourly dataset of the M4 competition (M4, 2022), which do not have associate exogenous features. We discarded those time series containing missing values and tested the method on a total of 414 signals. In this case the predictions at time t for the ith time series are given by ˆyi,t = f(yi,t−j) where j ∈[1,48], meaning that we p"
7429,unknown,"ˆyi,t = f(yi,t−j) where j ∈[1,48], meaning that we passed a two days history of the signal to predict the next day ahead. The results w.r.t. the MIMO and MISO strategy are shown in ﬁgure 5. We can see how the distribution of the best performing number of harmonics is skewed towards high numbers, as opposed to the much more uniform distributions of ﬁgure 4. At the same time, for most of the time se"
7430,unknown,"ﬁgure 4. At the same time, for most of the time series MBT obtains a better performance in both MAPE and RMSE, as can be seen from the vertical distributions of ﬁgure 5. To actually see if the increase of performance is due to the Fourier regularization, in ﬁgure 6 we compared the MBT model and the MBT model using Fourier regularization w.r.t. the normalized MAPE and RMSE of the MIMO and MISO stra"
7431,unknown,"normalized MAPE and RMSE of the MIMO and MISO strategies, in terms of distributions for the 414 time series. Switching from the base MBT model to the Fourier regularized one causes the distributions of the MAPE and the RMSE to shift towards smaller values, both when normalized with the MISO and the MIMO results. 4.2 Hierarchical forecasting Using the same dataset of the previous section, we obtain"
7432,unknown,"Using the same dataset of the previous section, we obtained the baseline 24 hours ahead forecasts for all the 31 time series, using 3 fold CV. In this dataset we have 3 aggregation levels, so that the summation matrix S can be written as: S =   1nb I2 ⊗1nb/2 I4 ⊗1nb/4 Inb   (47) where 1nb is the unit row vector with the size equal to the number of bottom-level time series, in this case, nb"
7433,unknown,"in this case, nb = 24 and ⊗is the Kronecker product. The forecasts are then reconciled using the minT strategy (Wickramasuriya et al., 2018), coupled with the graphical Lasso approach (Friedman et al., 2008) for the error covariance estimation and a bottom-up strategy. The latter consist in retrieving consistent forecast summing up bottom level forecasts. Formally, we obtain the set of reconciled "
7434,unknown,"we obtain the set of reconciled forecasts as ˜ y = Sˆyb. We then compare the results with a MBT using information about the forecast error of the previous timestep, as described in section 3.2. The results as a function of the step ahead, and divided by aggregation groups, are presented in Fig. 7. We can see how the additional information that MBT can exploit signiﬁcantly decrease the forecast err"
7435,unknown,"can exploit signiﬁcantly decrease the forecast error for the ﬁrst hours ahead. On the other hand, the advantage over standard reconciliation approaches vanishes with the increase of the step-ahead. Since the MBT requires substantially more computational time, an eﬀective strategy would be to ﬁt this model only for the initial steps ahead and then switch to the standard reconciliation strategy. 23 "
7436,unknown,"23 Nespoli and Medici 0.5 1.0 1.5 2.0 MAPE ratio [-] mimo miso 0.5 1.0 1.5 2.0 2.5 RMSE ratio [-] 5 10 15 20 5 10 15 20 5 10 15 20 n freq [-] 5 10 15 20 n freq [-] 0.0034 0.099 0.2 0.29 0.39 0.48 MAPE [-] Figure 5: Same of ﬁgure 4, but for the M4 hourly dataset. 24 Multivariate Boosted Trees and Applications to Forecasting and Control 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Density mape wrt mimo mape wrt "
7437,unknown,"mape w.r.t. mimo, Fourier mape w.r.t. miso, Fourier 0.4 0.6 0.8 1.0 1.2 1.4 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Density rmse wrt mimo rmse wrt miso rmse w.r.t. mimo, Fourier rmse w.r.t. miso, Fourier Figure 6: Comparison of the MBT model and the MBT model using Fourier regularization w.r.t. the normalized MAPE and RMSE of the MIMO and MISO strategies, in terms of distributions for the 414 time series "
7438,unknown,"25 Nespoli and Medici 0 5 10 15 20 step ahead [h] 10 20 30 RMSE [kW] type top aggs_1 aggs_2 bottoms method bu rec mbt 0 5 10 15 20 step ahead [h] 0.02 0.04 0.06 0.08 0.10 MAPE [-] type top aggs_1 aggs_2 bottoms method bu rec mbt Figure 7: RMSE as a function of the step ahead, grouped by hierarchical levels, mediated over the CV folds, for diﬀerent reconciliation techniques. Blue, orange, green and"
7439,unknown,"and red lines refer to the overall aggregated proﬁle, the ﬁrst and second level of aggregates, and the bottom time series, respectively. Continuous lines: bottom- up reconciliation. Dashed lines: reconciled forecasts using the shrink strategy and glasso covariance estimation. Dotted lines: MBT with history of reconciliation errors. 26 Multivariate Boosted Trees and Applications to Forecasting and "
7440,unknown,"4.3 Boosted voltage sensitivity coeﬃcients While DDC has been mainly applied to the control of heating systems, here we propose an application for the control in the electrical distribution grid. When performing optimal power ﬂow, a distribution system operator (DSO) must take into account the nonlinear power ﬂow equation, which includes the nonlinear relation: S = V ⊙I∗ (48) where S, V and I are "
7441,unknown,"where S, V and I are the vectors of complex powers, voltages and currents in the buses of the network, ∗ denotes the complex conjugate and ⊙the Hadamard product. Diﬀerent relaxations of power ﬂow equation exist (Molzahn et al., 2017). Usually, either the knowl- edge of phasors’ angles (e.g. DC approximation) or the knowledge of the lines’ parameters and topology (e.g. the DistFlow model) are requi"
7442,unknown,"and topology (e.g. the DistFlow model) are required inputs to this approximation. How- ever, this information is not always available. For example, the network topology of the low-voltage grid, where most residential users are located, is usually unknown or diﬃcult to access. In the absence of network topology, one could opt for an approximate formulation of the power ﬂow, whose parameters can be "
7443,unknown,"of the power ﬂow, whose parameters can be estimated using smart meter data. One of these formulations consists of the ﬁrst-order linearization of the power ﬂow equations. The linear coeﬃcients of this formulation are known as the voltage sensitivity coeﬃcients (VSCs): kp i,j = ∂|Vj| ∂Pk kq i,j = ∂|Vj| ∂Qk (49) where P and Qare the active and reactive power, respectively, andkp i,j,kq i,j are the s"
7444,unknown,"coeﬃcients between node i and node j. The analytical expression of voltage sensitivity coeﬃcients, and an eﬃcient method to compute them based on the state of the grid and admittance matrix, is provided in Christakou et al. (2013). In Mugnier et al. (2016), it has been shown that the voltage sensitivity coeﬃcients can be estimated by least-squares regression of the time derivatives of voltage magn"
7445,unknown,"regression of the time derivatives of voltage magnitudes,P and Q. We follow their approach to ﬁnd sets of VSCs, conditional to the state of the grid. However, knowing the latter would require to know all the voltages of the buses’ grid. As discussed in section 3.4, we aim at building the MBT without using the state of the system, we use the power measurements at the point of common coupling (PCC) "
7446,unknown,"state of the grid. In order to compare the approach in Mugnier et al. (2016) with the MBT one, we simulated 3 months of data for a low voltage grid located in Switzerland. Fig. 8 show the topology of the grid and the QP buses locations. This information, along with parameters for the grid’s cables, were retrieved from the local DSO. Power proﬁles of uncontrollable loads were generated with the Loa"
7447,unknown,"loads were generated with the LoadProﬁleGenerator (Pﬂugradt et al., 2013); power proﬁles of photovoltaic roof-mounted power plants were obtained through the PVlib python library (Andrews et al., 2012), while the electrical loads due to heat pumps was retrieved simulating domestic heating systems and buildings thermal dynamics, modelling them starting from building’s metadata. The grid was then sim"
7448,unknown,"building’s metadata. The grid was then simulated with OpenDSS (Dugan, 2012), and the 3 phases voltages, power and currents retrieved for all the QP nodes of the grid, with a 1 minute sampling time. The results were then obtained by applying a 10 fold CV. As an additional comparison, we considered Ridge regression for the VSCs. Since xlr and y both have a high number of 27 Nespoli and Medici Figure"
7449,unknown,"Figure 8: Schematics of the simulated low voltage grid for the VSC computation. Grey circles indicates single households, while the double circle indicate the power transformer. dimensions, quadratic regularization could help in ﬁnding a better solution. The regular- ization coeﬃcient for the Ridge regression was found using an inner CV for each fold. The dataset for the linear regression was D= {"
7450,unknown,"dataset for the linear regression was D= {(xlr,i,yi)N i=1}is the same for all the three models, where xlr ∈RN×6n contains the discrete-time derivatives of P and Q values for the 3 phases of all the buses, while y ∈RN×3n contains the time derivatives of the voltages. The MBT was built using x∈RN×3, which contains PPCC , which is the power measured at the PCC (the double circle in Fig. 8), the hour "
7451,unknown,"(the double circle in Fig. 8), the hour of the day and the weekday. In this case, the tree growth is not independent from the control action, since the power at PCC includes the power of controlled appliances in the grid. Under these conditions, the MBT can still be applied to build an oracle for checking voltage violations, using a ”proxy-Lagrangian” for- mulation of the optimization problem (Cot"
7452,unknown,"mulation of the optimization problem (Cotter et al., 2019). However, this results in a more complex formulation, being the constraints non-convex. We compare this solution to one in which the MBT is only ﬁtted using meteorological variables, i.e. the ambient temperature Ta and the solar irradiance Girr, the hour of the day and the weekday. In this case the identiﬁed leaves are independent from sys"
7453,unknown,"MBT can be employed in standard convex optimization. Fig. 9 shows results in terms of mean RMSE over folds and grids’ nodes, and of normal- ized RMSE. The normalization of the latter is done with the mean RMSE obtained with a constant prediction of the voltage. This is a meaningful normalization because in Europe voltage signals in low voltage networks have a nominal value of 230V, and usually the"
7454,unknown,"not deviate more than the 10%. Ridge regularization slightly increase the accuracy, while the MBT does it signiﬁcantly. As expected, the MBT using power at PCC is more accurate with respect to its counterpart using only disturbances for the growth of the trees. This means that the power at the PCC is a better proxy for the state of the electrical grid than 28 Multivariate Boosted Trees and Applica"
7455,unknown,"linear ridge linear mbt pcc linear mbt model 0.000 0.005 0.010 0.015 0.020 0.025 0.030 RMSE [V] metric rmse rmse norm Figure 9: RMSE for diﬀerent regressors, mediated over the CV folds and nodes. The red columns show the RMSE normalized with the predictions using the sample mean values. 29 Nespoli and Medici 0.0 0.024 0.047 0.071 0.095 0.12 0.14 0.17 0.19 0.21 input MAPE [-] 0.020 0.022 0.024 0.02"
7456,unknown,"input MAPE [-] 0.020 0.022 0.024 0.026 0.028 0.030 normalized RMSE [-] ridge norm RMSE Figure 10: Boxplots for the 10 fold CV of the normalized RMSE of the MBT predictor for increasing levels of noise in the tree inputs, in terms of MAPE. the meteorological variables. However, since these models are meant to be used in control applications, the models must be accurate for the whole decision horizo"
7457,unknown,"applications, the models must be accurate for the whole decision horizon (typically 24 hours ahead for demand-side management applications). Since the ﬁrst two models are constant, they do not need any further investigation. On the other hand, the ﬁnal MBT model de- pends on the features used to build the tree, x. In the following we restrict the analysis to the MBT ﬁtted on the meteorological var"
7458,unknown,"to the MBT ﬁtted on the meteorological variables; the one ﬁtted on P at PCC shows a very similar behavior. Indeed, we only need to investigate the accuracy degradation with respect to the forecasted Ta and Girr, since the other two variables in x are deterministic. We thus applied increasing levels of multiplicative noise from a (3 σ) truncated Gaussian distribution to Ta and Girr, to mimic accura"
7459,unknown,"distribution to Ta and Girr, to mimic accuracy degradation in its forecasts, and retrieved the MBT normalized RMSE on the CV folds. The results are shown in Fig. 10 in terms of increasing MAPE on the forecasted Ta and Girr signals, as box plots containing the 10 CV folds measurements. We can conclude that the degradation of the MBT is negligible up to a MAPE of 21%, which corresponds to very bad f"
7460,unknown,"a MAPE of 21%, which corresponds to very bad forecasts for this kind of applications. 30 Multivariate Boosted Trees and Applications to Forecasting and Control 4.4 Quantile prediction We tested the diﬀerent quantile loss relaxations and ﬁtting strategies presented in section 3.3 on the aggregated power proﬁle of the hourly-resampled dataset (D1, 2022). In particular, we seek to retrieve the quanti"
7461,unknown,"we seek to retrieve the quantile predictions tensor ˆ qτi ∈RN×nt×nq where τi ∈T , T is a set of nq = 11 equispaced quantiles and nt = 24. For all the methods, we kept the same features and target matrix xand y as speciﬁed in section 4.1. The benchmark to which we compare the MBT-based solutions are 24 sets of nq LightGBMs, that is, we ﬁtted a diﬀerent LightGBM for each combination of step-ahead an"
7462,unknown,"LightGBM for each combination of step-ahead and quantile. Other three models are then compared: the MBT using the smoothed version of quantile loss ˜lq, deﬁned by its gradient (38) and Hessian (37); the same model with quantile reﬁtting, as explained in section 3.3; the model using the linear-quadratic quantile loss deﬁned by its gradient (39) and Hessian (40), with quantile reﬁtting. Quality of q"
7463,unknown,"Quality of quantile forecasts is harder to assess compared to point forecasts since dif- ferent desirable properties of the forecasted prediction interval must be evaluated. For this comparison we relied on 4 KPIs. The ﬁrst one is the time average of the quantile loss (34), ¯lq = ∑T t=1 lq(ϵτi,t). The second one is the quantile score Qs(ˆqτi,y), which is a proper scoring rule (Gneiting and Raftery"
7464,unknown,"rule (Gneiting and Raftery, 2007; Golestaneh et al., 2016; Bentzien and Friederichs, 2014), and it’s deﬁned as the expected quantile loss (34): Qs= ∫ 1 0 ¯lq(ϵτi)dτi ≃ ∑ τi∈T ¯lq(ϵτi)dτi (50) where ˆqτi is the predicted τi-quantile, while y is the observed ground truth. This score is strictly connected with the continuous rank probability score (Gneiting and Raftery, 2007), both being total variat"
7465,unknown,"2007), both being total variation measurements between the forecasted pdf and a Heaviside distribution centered on the observation y. For these scores, lower values indicate better performances. The third KPI is based on the reliability (Pinson et al., 2010), which is the average number of times the observed signal was actually below the predicted τ quantile. rτi = 1 N N∑ j=1 1{yj<ˆqτi,j} (51) Whe"
7466,unknown,"When plotted against T , the perfect reliability aligns with the bisector of the ﬁrst quadrant. Because all the models provided highly reliable quantiles, to ease the comparison of the performance, we deﬁned the following KPI: Rs= |rτi(Fb) −τ|−|rτi(Fm) −τ| (52) that is, the diﬀerence of absolute deviation from the perfect reliability, between a benchmark forecasting model Fb and the considered one"
7467,unknown,"quantiles. We deﬁne it as: χ= 1 nq nq∑ i=1 1 N N∑ j=1 1ˆqτi>ˆqτi+1 (53) that is, the average over quantiles of the mean number of times ˆqτi violates the monotonicity of ˆF(Y). 31 Nespoli and Medici 0 5 10 15 20 80 100 120 Qs [kW] miso mbt mbt refit mbt lin-quad refit 0 5 10 15 20 step ahead [h] 0.0 0.5 1.0 1.5 2.0 [ ] miso mbt mbt refit mbt lin-quad refit Figure 11: CRPS and mean quantile crossin"
7468,unknown,"Figure 11: CRPS and mean quantile crossings, as a function of step-ahead, mediated over the CV folds. The reﬁtted MBT with logistic and quadratic losses show simi- lar performances with the MISO strategy while achieving a signiﬁcantly lower number of crossings. 32 Multivariate Boosted Trees and Applications to Forecasting and Control 0.09 0.17 0.25 0.34 0.42 0.50 0.58 0.66 0.75 0.83 0.91 [-] 1.0 0"
7469,unknown,"1.0 0.5 0.0 0.5 1.0 1.5 lq [kW] mbt 0.09 0.17 0.25 0.34 0.42 0.50 0.58 0.66 0.75 0.83 0.91 [-] mbt refit 0.09 0.17 0.25 0.34 0.42 0.50 0.58 0.66 0.75 0.83 0.91 [-] mbt lin-quad refit 1 5 10 14 19 24 step ahead [h] Figure 12: Diﬀerences between ¯lq for diﬀerent models, with respect to the benchmark case, as a function of τi and the step ahead (line colour, from blue to yellow). Lines above the grey"
7470,unknown,"above the grey dashed line denotes better performances. In Fig. 12 we compare the quantile loss lq(ϵτi) as a function of τi and the step ahead. To ease the comparison, we plot the diﬀerences between the lq(ϵτi) of the benchmark and the other models. The original quantile loss plots can be found in the appendix E. All the MBT models are consistently better at modelling the tails of the distribution"
7471,unknown,"models are consistently better at modelling the tails of the distribution, while applying reﬁtting to the linear quantile loss function doesn’t show any improvement. In terms of reliability, Fig. 13 shows how both the base model and the one using the lin-quantile loss have similar reliability with respect to the benchmark. The ﬁrst panel of Fig. 11 shows the quantile score Qs as a function of step"
7472,unknown,"quantile score Qs as a function of step-ahead for the four diﬀerent models. All the MBT based models show a Qs score lower than the benchmark, the base MBT model and the one using the linear-quadratic formulation being strictly better for all the steps ahead. The second panel shows χ for increasing steps ahead. It is evident how using diﬀerent BTs for diﬀerent quantiles leads the benchmark model t"
7473,unknown,diﬀerent quantiles leads the benchmark model to inconsistent results. The quantile crossing is negligible for all the MBT based models when compared to the benchmark. 33 Nespoli and Medici 0.09 0.17 0.25 0.34 0.42 0.50 0.58 0.66 0.75 0.83 0.91 [-] 0.08 0.06 0.04 0.02 0.00 0.02 Rs [-] mbt 0.09 0.17 0.25 0.34 0.42 0.50 0.58 0.66 0.75 0.83 0.91 [-] mbt refit 0.09 0.17 0.25 0.34 0.42 0.50 0.58 0.66 0.
7474,unknown,"0.25 0.34 0.42 0.50 0.58 0.66 0.75 0.83 0.91 [-] mbt lin-quad refit 1 5 10 14 19 24 step ahead [h] Figure 13: Rs for diﬀerent models, with respect to the benchmark case, as a function of τi and the step ahead (line colour, from blue to yellow). 34 Multivariate Boosted Trees and Applications to Forecasting and Control 5. Conclusions In this paper, we have presented a multivariate boosted tree algor"
7475,unknown,"In this paper, we have presented a multivariate boosted tree algorithm, ﬁtted using the same second-order Taylor expansion used by LightGBM and XGboost. The algorithm allows to arbitrarily regularize the predictions, through the use of multivariate penalization and basis functions. We have shown how, for a relevant class of applications, the Hessian inversion required for ﬁtting the underlying tre"
7476,unknown,"required for ﬁtting the underlying tree models can be reduced to a matrix multiplication, making the algorithm computationally appealing. Unlike its univariate counterpart, the MBT is particularly useful when properties like smoothness, consistency and functional re- lations are required. We have shown this through numerical examples on four diﬀerent tasks, namely: time series forecasting, hierarc"
7477,unknown,"namely: time series forecasting, hierarchical reconciliation, data-driven control and quantile forecasting. While including a priori regularization on the smoothness of a forecasted time series doesn’t seem to increase accuracy against univariate BTs with a MISO strategy, for the other presented applications, where some consistency is explicitly required, the algo- rithm showed clear advantages. W"
7478,unknown,"rithm showed clear advantages. We conclude by noting that the presented MBT algorithm only used histogram-based split search since we did not make use of very large datasets in our experiments. Computational time can be readily reduced through the adoption of numerical techniques tailored to tree ﬁtting, such as gradient-based one-side sampling and exclusive feature bundling (Ke et al., 2017). Ack"
7479,unknown,"exclusive feature bundling (Ke et al., 2017). Acknowledgments This project is carried out within the frame of the Swiss Centre for Competence in Energy Research on the Future Swiss Electrical Infrastructure(SCCER-FURIES) with the ﬁnancial support of the Swiss Innovation Agency (Innosuisse - SCCER program) and of the Swiss Federal Oﬃce of Energy (project SI/501523). References Robert W Andrews, Jos"
7480,unknown,"References Robert W Andrews, Joshua S Stein, Cliﬀord Hansen, Daniel Riley, Calama Consulting, and Sandia National Laboratories. Introduction to the open source PV LIB for python photovoltaic system modelling package. 2012. Souﬁane Belharbi, Romain H´ erault, Cl´ ement Chatelain, and S´ ebastien Adam. Deep neural networks regularization for structured output prediction. Neurocomputing, 281:169–177,"
7481,unknown,"2018. ISSN 18728286. doi: 10.1016/j.neucom.2017.12.002. Souhaib Ben Taieb, Gianluca Bontempi, Amir F. Atiya, and Antti Sorjamaa. A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition. Expert Systems with Applications , 39(8):7067–7083, 2012. ISSN 09574174. doi: 10.1016/j.eswa.2012.01.039. Sabrina Bentzien and Petra Friederichs. "
7482,unknown,"Sabrina Bentzien and Petra Friederichs. Decomposition and graphical portrayal of the quantile score. Quarterly Journal of the Royal Meteorological Society , 140(683):1924– 1934, 2014. ISSN 1477870X. doi: 10.1002/qj.2284. Leo Breiman. Arcing classiﬁers. Annals of Statistics , 1998. ISSN 00905364. doi: 10.1214/ aos/1024691079. 35 Nespoli and Medici Michael M. Bronstein, Joan Bruna, Yann Lecun, Arthu"
7483,unknown,"Michael M. Bronstein, Joan Bruna, Yann Lecun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: Going beyond euclidean data. IEEE Signal Processing Maga- zine, 34(4):18–42, 2017. ISSN 10535888. doi: 10.1109/MSP.2017.2693418. Peter B¨ uhlmann and Torsten Hothorn. Boosting algorithms: Regularization, prediction and model ﬁtting. Statistical Science, 2007. ISSN 08834237. doi: 10.1214/0"
7484,unknown,"Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785–794, 2016. doi: 10.1145/2939672.2939785. Konstantina Christakou, Jean Yves Leboudec, Mario Paolone, and Dan Cristian Tomozei. Eﬃcient computation of sensitivity coeﬃcients of node voltages and line currents in un- b"
7485,unknown,"balanced radial electrical distribution networks. IEEE Transactions on Smart Grid, 4(2): 741–750, 2013. ISSN 19493053. doi: 10.1109/TSG.2012.2221751. Andrew Cotter, Heinrich Jiang, and Karthik Sridharan. Two-player games for eﬃcient non-convex constrained optimization. 98(1):1–33, 2019. D1. https://zenodo.org/record/3463137#.XY3GqvexWV4, 2022. Robert M. de Jong and Neslihan Sakarya. The econometri"
7486,unknown,"Robert M. de Jong and Neslihan Sakarya. The econometrics of the Hodrick-Prescott ﬁlter. Review of Economics and Statistics , 2016. ISSN 15309142. doi: 10.1162/REST a 00523. Janez Demsar. Statistical comparisons of classiﬁers over multiple data sets. Journal of Machine Learning Research, 7(1):1–30, 2006. Tony Duan, Anand Avati, Daisy Yi Ding, Sanjay Basu, Andrew Y. Ng, and Alejandro Schuler. NGBoos"
7487,unknown,"Schuler. NGBoost: Natural gradient boosting for probabilistic prediction. 2019. Roger C Dugan. The open distribution system simulator ( OpenDSS ). Technical report, 2012. Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences , 1997. ISSN 00220000. doi: 10.1006/jcss.1997.1504. Jerome Friedm"
7488,unknown,"Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: A statistical view of boosting, 2000. ISSN 00905364. Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estima- tion with the graphical lasso. Biostatistics (Oxford, England), 2008. ISSN 14654644. doi: 10.1093/biostatistics/kxm045. Jerome H. Friedman. Greedy function approximation: A "
7489,unknown,"of Statistics, 2001. ISSN 00905364. doi: 10.2307/2699986. Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association , 102(477):359–378, 2007. ISSN 01621459. doi: 10.1198/016214506000001437. 36 Multivariate Boosted Trees and Applications to Forecasting and Control Faranak Golestaneh, Pierre Pinson, and H. B. Go"
7490,unknown,"Faranak Golestaneh, Pierre Pinson, and H. B. Gooi. Very short-term nonparametric prob- abilistic forecasting of renewable energy generation - With application to solar energy. IEEE Transactions on Power Systems , 2016. ISSN 08858950. doi: 10.1109/TPWRS. 2015.2502423. Myles Hollander and Douglas Wolfe. Nonparametric statistical methods, 2nd edition. In A Volume in the Wiley Series in Probability an"
7491,unknown,"A Volume in the Wiley Series in Probability and Mathematical Statistics . 1999. ISBN 0-471-19045-4. Rob J. Hyndman, Roman A. Ahmed, George Athanasopoulos, and Han Lin Shang. Optimal combination forecasts for hierarchical time series. Computational Statistics and Data Analysis, 2011. ISSN 01679473. doi: 10.1016/j.csda.2011.03.006. Special issue Energies. Special issue ”Energy Eﬃciency and Data-Driv"
7492,unknown,"(ISSN 1996-1073), 2019. Alan Julian Izenman. Reduced-rank regression for the multivariate linear model. Journal of Multivariate Analysis, 5(2):248–264, 1975. ISSN 10957243. doi: 10.1016/0047-259X(75) 90042-1. Achin Jain, Madhur Behl, and Rahul Mangharam. Data Predictive Control for building energy management. Proceedings of the American Control Conference , (May):44–49, 2017. ISSN 07431619. doi: 1"
7493,unknown,"2017. ISSN 07431619. doi: 10.23919/ACC.2017.7962928. Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. LightGBM: A highly eﬃcient gradient boosting decision tree. Nips ’17, (Nips):9, 2017. Seung-Jean Kim, Kwangmoo Koh, Stephen Boyd, and Dimitry Gorinevsky. Trend ﬁltering. SIAM Review, 51(2):339–360, 2009. ISSN 00361445. doi: Doi10.1137/070690274. N K"
7494,unknown,"N Kourentzes, I. Svetunkov, and O. Schaer. tsutils. https://github.com/trnnick/tsutils/, 2022. Wen Li, Wei Wang, and Wenjun Huo. RegBoost : A gradient boosted multivariate regression algorithm. International Journal of Crowd Science , (61672384), 2019. doi: 10.1108/ IJCS-10-2019-0029. manual LightGBM. LightGBM - release 2.3.2. 2020. Lennart Ljung. System Identiﬁcation. In Ales Proch´ azka, Jan Uhl"
7495,unknown,"Lennart Ljung. System Identiﬁcation. In Ales Proch´ azka, Jan Uhl´ ıˇ r, P. W. J. Rayner, and N. G. Kingsbury, editors, Signal Analysis and Prediction , Applied and Numerical Harmonic Analysis, pages 163–173. Birkh¨ auser, Boston, MA, 1998. ISBN 978-1-4612- 1768-8. doi: 10.1007/978-1-4612-1768-8 11. M4. M4-datasets, https://github.com/mcompetitions/m4-methods/tree/master/dataset, July 2022. 37 Nes"
7496,unknown,"July 2022. 37 Nespoli and Medici Manish Mehta, Rakesh Agrawal, and Jorma Rissanen. SLIQ: A fast scalable classiﬁer for data mining. In Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics) , 1996. ISBN 3-540-61057-X. doi: 10.1007/bfb0014141. Daniel K Molzahn, Florian Dorﬂer, Henrik Sandberg, Steven H Low, Sambuddha Chakr"
7497,unknown,"Chakrabarti, Ross Baldick, and Javad Lavaei. A survey of distributed optimization and control algorithms for electric power systems. IEEE Transactions on Smart Grid , 3053 (c):1, 2017. ISSN 1949-3053. doi: 10.1109/TSG.2017.2720471. Manfred Morari, Carlos E. Garcia, and David M. Prett. Model predictive control: Theory and practice. IFAC Proceedings Volumes, 21(4):1–12, June 1988. ISSN 1474-6670. do"
7498,unknown,"10.1016/B978-0-08-035735-5.50006-1. C Mugnier, K Christakou, J Jaton, M De Vivo, M Carpita, and M Paolone. Model- less/measurement-based computation of voltage sensitivities in unbalanced electrical dis- tribution networks. 19th Power Systems Computation Conference, PSCC 2016 , 2016. doi: 10.1109/PSCC.2016.7540852. Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neu"
7499,unknown,"Neural basis expansion analysis for interpretable time series forecasting. pages 1–31, 2019. Amol Pande, Liang Li, Jeevanantham Rajeswaran, John Ehrlinger, Udaya B. Kogalur, Eugene H. Blackstone, and Hemant Ishwaran. Boosted multivariate trees for longitu- dinal data. Machine Learning, 106(2):277–305, 2017. ISSN 15730565. doi: 10.1007/ s10994-016-5597-1. N. Pﬂugradt, J. Teuscher, B. Platzer, and W"
7500,unknown,"N. Pﬂugradt, J. Teuscher, B. Platzer, and W. Schuﬀt. Analysing low-voltage grids using a behaviour based load proﬁle generator. Renewable Energy and Power Quality Journal , 2013. ISSN 2172038X. doi: 10.24084/repqj11.308. Pierre Pinson, Patrick McSharry, and Henrik Madsen. Reliability diagrams for non- parametric density forecasts of continuous variables: Accounting for serial correlation. Quarterl"
7501,unknown,"Quarterly Journal of the Royal Meteorological Society , 136(646):77–90, 2010. ISSN 00359009. doi: 10.1002/qj.559. J O Ramsay, Giles Hooker, and Spencer Graves. Smoothing: Computing curves from noisy data. In Functional Data Analysis with R and MATLAB , pages 59–82. Springer New York, New York, NY, 2009. ISBN 978-0-387-98185-7. doi: 10.1007/978-0-387-98185-7 5. Ajay Kumar Sampathirao, Juan Manuel G"
7502,unknown,"Ajay Kumar Sampathirao, Juan Manuel Grosso, Pantelis Sopasakis, Carlos Ocampo- Martinez, Alberto Bemporad, and Vicen¸ c Puig. Water demand forecasting for the op- timal operation of large-scale Drinking Water Networks: The barcelona case study. In IFAC Proceedings Volumes (IFAC-PapersOnline), 2014. ISBN 978-3-902823-62-5. doi: 10.3182/20140824-6-za-1003.01343. Francesco Smarra, Achin Jain, Tullio "
7503,unknown,"Francesco Smarra, Achin Jain, Tullio de Rubeis, Dario Ambrosini, Alessandro D’Innocenzo, and Rahul Mangharam. Data-driven model predictive control using random forests for 38 Multivariate Boosted Trees and Applications to Forecasting and Control building energy optimization and climate control. Applied Energy, 226:1252–1272, 2018. ISSN 03062619. doi: 10.1016/j.apenergy.2018.02.126. Shanika L Wickr"
7504,unknown,"Shanika L Wickramasuriya and George Athanasopoulos. Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization.Journal of the American Statistical Association, 2017. Shanika L Wickramasuriya, George Athanasopoulos, and Rob J Hyndman. Forecasting hierarchical and grouped time series through trace minimization.Journal of the American Statistical Association, "
7505,unknown,"Statistical Association, (November), 2018. ISSN 0162-1459. doi: 10.1080/01621459.2018. 1448825. Zhendong Zhang and Cheolkon Jung. GBDT-MO: Gradient boosted decision trees for multiple outputs. pages 1–13, 2019. Songfeng Zheng. QBoost: Predicting quantiles with boosting for regression and binary classiﬁcation. Expert Systems with Applications , 39(2):1687–1697, 2012. ISSN 09574174. doi: 10.1016/j.e"
7506,unknown,"doi: 10.1016/j.eswa.2011.06.060. Appendix Appendix A. Proof of theorem 1 Proof The loss function (35) is minimized in expectation, with respect to the empirical distribution of the target y in D= {(xi,yi)N i=1}, if the expectation of its derivative is zeroed by its minimizer: q∗= argmin q EDlqs(x,q,τ ) (54) q∗s.t. ∂EDlqs(x,q∗,τ) ∂q = 0 (55) Keeping the same nomenclature in theorem (1), we retrieve"
7507,unknown,"∂q = 0 (55) Keeping the same nomenclature in theorem (1), we retrieve q∗ by solving (55). We recall that the derivative of the set membership function 1z>0 is zero almost everywhere, and by the chain rule, deriving f(z)1z>0 results in ∂f(z) ∂z 1z>0. Since we want the derivative of the expectation over the dataset D, we have ∂EDlqs ∂q = 1 N N∑ i=1 [( (τ −1) + kϵτ,i ¯ϵτ,l ) 1y<ˆqτ + ( τ + kϵτ,i ¯ϵτ,"
7508,unknown,"∂EDlqs ∂q = 1 N N∑ i=1 [( (τ −1) + kϵτ,i ¯ϵτ,l ) 1y<ˆqτ + ( τ + kϵτ,i ¯ϵτ,r ) 1y≥ˆqτ −2 k N ] summation over the N elements of the dataset and set membership functions can be turned into partial summations over the sets Il = {i: yi ≤ˆqτ(x)}and Ir = {i: yi ≥ˆqτ(x)}: ∂EDlqs ∂q = 1 N [∑ i∈Il ( (τ −1) + kϵτ,i ¯ϵτ,l ) + ∑ i∈Ir ( τ + kϵτ,i ¯ϵτ,r ) −2k ] (56) 39 Nespoli and Medici by the deﬁnition of ¯ϵτ"
7509,unknown,"∂EDlqs ∂q = 1 N [ nl(τ −1) + k+ nrτ + k−2k ] (57) Given that nr = N −nl, where nl and nr are the cardinalities of the Il and Ir sets, respectively, we get: ∂EDlqs ∂q = Nτ −nl N (58) and ﬁnally, zeroing it we get: τ = nl N (59) That is, the optimal q∗minimizing EDlqs must be greater than exactly a fraction of τ obser- vations of ycontained in the dataset D, which is the deﬁnition of the empirical τ"
7510,unknown,"Appendix B. Connections with AdaBoost At each iteration, AdaBoost employs an exponential loss function in order to solve a binary classiﬁcation problem. It can be shown that the minimizer f∗ k(x) of this loss minimizes also the logit loss associated to the classiﬁcation probabilities Friedman et al. (2000) : f∗ k(x) = argmin fk(x) lA(y,fk(x)) = log ( P{y=1|x} P{y=−1|x} ) (60) and therefore, invert"
7511,unknown,"as: p(y= 1|x) = ef∗ k(x) e−f∗ k(x) + ef∗ k(x) = e2f∗ k(x) 1 + e2f∗ k(x) (61) which means that AdaBoost algorithm can be explained in terms of an additive logistic regression model. Appendix C. Matrix inverses Lemma 2 Given a symmetric invertible matrix A ∈Rk×k, (A+ nIk)−1 can be computed as: (A+ nIk)−1 = QLQ−1 (62) where L ∈Rk,k is diagonal with Li,i = 1/(λi + n), λi is the ith eigenvalue of A and"
7512,unknown,"the matrix whose columns are the eigenvectors of A. Proof Considering the eigenequation of matrix A: Ax= λx (63) and adding a multiple of the identity matrix: (A+ nI)x= (λ+ n)x (64) 40 Multivariate Boosted Trees and Applications to Forecasting and Control calling A+ nI = ˜A, this means that λ˜A,i = λA,i + n, where λA,i denotes the ith eigenvalue of A. Since adding a multiple of I to Ajust inﬂuence"
7513,unknown,"of A. Since adding a multiple of I to Ajust inﬂuences the magnitude of the vector to which the ﬁnal matrix is applied, the eigenvectors of A and A+ nI are the same. Thus, since A is symmetric and invertible, and its inverse can be obtained as: A−1 = QTLQ (65) ˜A−1 can be obtained as ˜A−1 = QT ˜LQ (66) where L ∈ Rk,k is diagonal with Li,i = 1 /λAi and ˜L ∈ Rk,k is diagonal with ˜Li,i = 1/(λAi + n)."
7514,unknown,"1/(λAi + n). Appendix D. Statistical analysis We performed Nemenyi tests Hollander and Wolfe (1999) on the experiments presented in the paper to statistically compare the performances of the diﬀerent models. The Nemenyi test is a post-hoc pairwise test, which is used to compare a set of m diﬀerent models on a group of n independent experiments. Firstly, a matrix R ∈Rn×m whose elements ri,j are the"
7515,unknown,"the ranks for experiment iand model j, is obtained. Then, the mean rank for each model is retrieved through column-wise averages of R. The performance of two models is identiﬁed as signiﬁcantly diﬀerent by the Nemenyi test if the corresponding average ranks diﬀer by at least the critical diﬀerence: CD = qα,m √ m(m+ 1) 12n (67) where qα is the quantile α of the Studentized range statistic with m sa"
7516,unknown,"12n (67) where qα is the quantile α of the Studentized range statistic with m samples. We imple- mented the Nemenyi test in python following the implementation in thetsutils R package Kourentzes et al. (2022). The Nemenyi test is usually performed after a Friedman’s test, which is a non-parametric analog of variance for a randomized block design; this can be considered as non-parametric version of"
7517,unknown,"considered as non-parametric version of a one-way ANOVA with repeated measures. More details on the diﬀerence and implementation of the two tests can be found in Demsar (2006). Since we run several experiments through the paper, it is of interest to perform not just one test, but several ones, to assess under which conditions the MBT regressor is better. In the following and in the ﬁgures, we refe"
7518,unknown,"the following and in the ﬁgures, we refer to the KPI used for building the ranking matrix R as the target variable, and to the parameter or property we have changed through diﬀerent tests as the independent variable. In the following we present the results of the statistical tests on the experiments presented in the paper. All the preliminary Friedman’s tests con- futed the null hypothesis that th"
7519,unknown,"futed the null hypothesis that the compared algorithms have the same distribution for the target variable; the only exception was the reliability of the quantile forecasting experiment for the α = 0.5 quantile, which means that the compared models where considered to be statistically equally reliable. In Fig. 14 the column-wise means of the R matrix and the conﬁdence bands obtained through the CD "
7520,unknown,"41 Nespoli and Medici 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100105110115 n freq [-] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 rank [-] MAPE miso mimo mbt Figure 14: Nemenyi tests for the Fourier forecasting using the (D1, 2022) dataset. The tests are grouped by time series, while the independent variable is the number of the harmonics used. The target variable is the MAPE. (D1, 2022). In this "
7521,unknown,"(D1, 2022). In this case the population of the reference experiments is composed by the 31 time series, so that in this case we have n = 31. The target variable is the MAPE of the MIMO, MISO and the MBT models, while the independent variable is the number of harmonics used by the MBT model. The MIMO model is consistently worse than the other two. The MBT model is always better than the MIMO model;"
7522,unknown,"other two. The MBT model is always better than the MIMO model; while it is worse than the MISO model when using few number of harmonics, its performances gets statistically indistinguishable from the MISO model for a number of harmonics higher than 25. This conﬁrms that inducing smoothness in the multiple step ahead forecasting task doesn’t help in reducing the MAPE. Fig. 15 shows the same analysi"
7523,unknown,"in reducing the MAPE. Fig. 15 shows the same analysis but for the dataset (M4, 2022). In this case it’s clear that the Fourier smoothing helps decreasing the MAPE compared to the MISO and MIMO models. Fig. 16 refers to the hierarchical forecast experiments, with the ﬁrst three steps ahead as population, MAPE as target variable and level of aggregation as independent variable. For each level of agg"
7524,unknown,"independent variable. For each level of aggregation we see that the MBT regressor perform better w.r.t. the bottom up aggregation and the hierarchical reconciliation method. For one aggregation group, the bottom time series, the hierarchical reconciliation worsen the base forecast results, while the MBT regressor consistently performs better also in this case. Fig. 17 and 18 refer to the quantile "
7525,unknown,"Fig. 17 and 18 refer to the quantile forecast experiments, with the ﬁrst three steps ahead as population, level of aggregation as independent variable and quantile score and reliability deviations, deﬁned as |rτi(Fm) −τ|, as target variable. For the quantile score we see that while the MISO strategy perform better for the central quantiles, both the MBT models (with the normal quantile loss and wi"
7526,unknown,"(with the normal quantile loss and with the linear-quadratic one) perform better for the 42 Multivariate Boosted Trees and Applications to Forecasting and Control 4 6 8 10 12 14 16 18 20 22 n freq [-] 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 rank [-] MAPE miso mimo mbt Figure 15: Nemenyi tests for the Fourier forecasting using the (M4, 2022) dataset. The tests are grouped by time series, while the independ"
7527,unknown,"of the harmonics used. The target variable is the MAPE. extreme quantiles. On the other hand, when considering reliability, the MISO strategy is better for extreme quantiles. 43 Nespoli and Medici aggs_1 aggs_2 bottoms top aggregation group [-] 0 1 2 3 4 rank [-] MAPE bu rec mbt Figure 16: Nemenyi tests for the hierarchical forecasting. The tests are grouped by the ﬁrst three steps ahead, while th"
7528,unknown,"variable is the MAPE. 0.09 0.17 0.25 0.34 0.42 0.5 0.58 0.66 0.75 0.83 0.91 [-] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 rank [-] QS miso mbt refit mbt lin-quad refit Figure 17: Nemenyi tests for the quantile forecasting. The tests are grouped by step ahead, while the independent variable is the α quantile. The target variable is the quantile score. 44 Multivariate Boosted Trees and Applications to Forecasting"
7529,unknown,"0.09 0.17 0.25 0.34 0.42 0.5 0.58 0.66 0.75 0.83 0.91 [-] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 rank [-] reliability miso mbt refit mbt lin-quad refit Figure 18: Nemenyi tests for the quantile forecasting. The tests are grouped by step ahead, while the independent variable is the α quantile. The target variable is the reliability. 45 Nespoli and Medici Appendix E. Additional ﬁgures 0.090.170.250.340.420.500"
7530,unknown,"[-] 4 6 8 10 12 14 lq [kW] miso 0.090.170.250.340.420.500.580.660.750.830.91 [-] mbt 0.090.170.250.340.420.500.580.660.750.830.91 [-] mbt refit 0.090.170.250.340.420.500.580.660.750.830.91 [-] mbt lin-quad refit 1 5 10 14 19 24 step ahead [h] Figure 19: ¯lq for diﬀerent models, as a function of τi and the step ahead (line color, from blue to yellow). 46 Multivariate Boosted Trees and Applications "
7531,unknown,"0.090.170.250.340.420.500.580.660.750.830.91 [-] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 r i [-] miso 0.090.170.250.340.420.500.580.660.750.830.91 [-] mbt 0.090.170.250.340.420.500.580.660.750.830.91 [-] mbt refit 0.090.170.250.340.420.500.580.660.750.830.91 [-] mbt lin-quad refit 1 5 10 14 19 24 step ahead [h] Figure 20: Reliability plots for diﬀerent models, as a function of τi and the step ahead (l"
7532,unknown,"color, from blue to yellow). 47 Multivariate Regression (Chapter 10) This week we’ll cover multivariate regression and maybe a bit of canonical correlation. Today we’ll mostly review univariate multivariate regression. With multivariate regression, there are typically multiple dependent variables as well as multiple independent or explanatory variables. A special case of this is when the explanato"
7533,unknown,"special case of this is when the explanatory variables are categorical and the dependent variables are continuous (particularly multivariate normal), in which case we have MANOVA. For multivariate regression, we allow the explanatory variables to be continuous. This approach generalizes multiple regression much as MANOVA generalizes ANOVA. Typically in regression, we think of the y variables as ra"
7534,unknown,"variables as ﬁxed. For multivariate regression, we’ll consider x variables as either ﬁxed or random. We’ll start with them being treated as ﬁxed. April 29, 2015 1 / 35 Multivariate regression First, we’ll review multiple (univariate) regression with ﬁxed x variables. For this model, we have y1 = β0 + p∑ j=1 βj x1j + ε1 y2 = β0 + p∑ j=1 βj x2j + ε2 ... yn = β0 + p∑ j=1 βj xnj + εn April 29, 2015 2 "
7535,unknown,"Multivariate regression The standard assumptions for multiple regression are E(εi ) = 0 Var(εi ) = σ2 cov(εi ,εj ) = 0 Equivalently, you can write E(ε) = 0 Cov(ε) = σ2I April 29, 2015 3 / 35 Multivariate regression Under the assumption that the xs are ﬁxed, we have E(yi ) = β0 + p∑ j=1 βj x1j Var(yi ) = σ2 Cov(yi ,yj ) = Cov(εi ,εj ) = 0 Equivalently, E(y) = Xβ Cov(y) = σ2I April 29, 2015 4 / 35 M"
7536,unknown,"Multivariate regression The regression model using matrix notation is y = Xβ+ ε When I was an undergrad, my Calc III professor suggested that we get tattoos of f = ma, but if you are a statistics, y = Xβ+ ε would be better.... April 29, 2015 5 / 35 Multivariate regression April 29, 2015 6 / 35 Multivariate regression Written out, the matrix form looks like this April 29, 2015 7 / 35 Multivariate r"
7537,unknown,"Multivariate regression The X is called the design matrix and recall that it has a column of 1s which is necessary for the β0 term. For estimation and hypothesis testing (for which variances are needed), you need n >q + 1 April 29, 2015 8 / 35 Multivariate regression The least squares approach for estimating βis to minimize the following SSE = n∑ i=1 ˆε2 i = n∑ i=1 (yi −ˆyi )2 = n∑ i=1 (yi −ˆβ0 + "
7538,unknown,"This problem can be solved with calculus, or with less eﬀort, using matrix algebra: y = Xβ If you set ˆy equal to its expectation and to solve for β, then get ˆβ= (X′X)−1X′y April 29, 2015 9 / 35 Multivariate regression The previous solution for estimating βis the least squares solution regardless of the distribution of the error term. If the error terms are independent and identically distributed"
7539,unknown,"solution is also the maximum likelihood solution. April 29, 2015 10 / 35 Multivariate regression An unbiased estimator for σ2 is s2 = SSE n −q −1 = 1 n −q −1(y −Xˆβ)′(y −Xˆβ) = y′y −ˆβX′y April 29, 2015 11 / 35 Multivariate regression Another way of writing the model is to center the xs, so you have x1 = n∑ i=1 xi1, ··· ,xq = n∑ i=1 xiq Then we write (next slide) April 29, 2015 12 / 35 April 29, 2"
7540,unknown,"Multivariate regression This approach is equivalent, and corresponds to the model yi = α+ q∑ j=1 βj (xij −xj ) so the xs are centered and the intercept term is changed and becomes ˆα= y The term ˆβ1 is (q −1) ×1 rather than q ×1, so we have ˆβ= ( ˆβ0,ˆβ1)′ where ˆβ0 = ˆα− q∑ j=1 ˆβj xj April 29, 2015 14 / 35 Multivariate regression To do hypothesis tests, the total sums of squares for y is partiti"
7541,unknown,"SSE and SSR. This is done as follows y′y = y′y −ˆβ ′ Xy + ˆβ ′ Xy = SSE + ˆβ ′ Xy = SSE + ˆβ ′ Xy + ny2 −ny2 = SSE + SSR −ny2 ⇒y′y + ny2 = SSE + SSR April 29, 2015 15 / 35 Multivariate regression A test for the nonintercept coeﬃcients H0 : β1 = 0 is F = SSR/q SSE/(n −q −1) which has an Fq,n−q−1 distribution under the null (and assuming normally distributed y values). April 29, 2015 16 / 35 Multiva"
7542,unknown,"Multivariate regression You can also test whether a subset of coeﬃcients is 0. To do this, let βd be the subset of interest so that the null is H0 : βd = 0 Have the betas arranged so that β= (βr βd ) The reduced model is y = Xr βr + εr The idea is that the reduced model has only the variables with nonzero coeﬃcients. April 29, 2015 17 / 35 Multivariate regression The term βr is estimated by ˆβr = "
7543,unknown,"r Xr )−1Xr y The reduced model is tested against the full model using F = (ˆβ ′ X′y −ˆβ ′ r X′ r y)/h (y′y −ˆβ ′ X′y)/(n −q −1) = SSRf −SSRr )/h SSEf /(n −q −1) = MSR MSE where the subscript f refers to the full model and h is the number of parameters in βd . The test statistic is compared to a Fh,n−q−1 distirbution. April 29, 2015 18 / 35 Multivariate regression A special case is testing individu"
7544,unknown,"A special case is testing individual predictor variables, in which case h = 1, but the formulas hold for this case as well. In this particular case (with numerator degrees of freedom equal to 1), the F statistic is the square of a t statistic. The R2 value gives the proportion of variance “explained” by the model, which is R2 = regression sum of squares total sum of squares = ˆβ ′ X′y −ny2 y′y −ny"
7545,unknown,"total sum of squares = ˆβ ′ X′y −ny2 y′y −ny2 April 29, 2015 19 / 35 Multivariate regression For multivariate regression, we have p variables for y, so that Y = (yij ) is an n ×p matrix. The observation vectors are y′ i , i = 1,..., n. As usual, observation vectors are considered as column vectors even though they are written horizontally in the data ﬁle and even though they correspond to rows of "
7546,unknown,"rows of Y. April 29, 2015 20 / 35 Multivariate regression The design matrix X is as before with a column of 1s and q columns corresponding to x variables. However, there is now a column of q β coeﬃcients for each of the p response variables. The model now has B = (β1,..., βp) = (βij ), which is a ( q + 1) ×p matrix. The model can be written as Y = XB + Ξ The model for an individual column of Y is "
7547,unknown,"multiple regression model. (It so happens that B is the capital of β in Greek. However Ξ is not the capital of ε, so this choice of notation seems a bit inconsistent. However E is used as ˆΞ ′ˆΞ, which is the matrix analogue of SSE. April 29, 2015 21 / 35 April 29, 2015 22 / 35 Multivariate regression The assumptions of the model are 1. E(Y) = XB,E(Ξ) = O 2. Cov(y)i ) = Σ, for i = 1,..., n, where "
7548,unknown,"i is the ith row of Y 3. Cov(yi ,yj ) = O for i ̸= j Note that Cov(y)i is p ×p. April 29, 2015 23 / 35 Multivariate regression Similar to univrariate multiple regression, ˆB = (X′X)−1X′Y so y was replaced with Y in the formula. April 29, 2015 24 / 35 Multivariate regression An estimator for the covariance matrix of yi is Se = E n −q −1 = (Y −XˆB)′(Y −XˆB) n −q −1 The B can be partitioned so that t"
7549,unknown,"n −q −1 The B can be partitioned so that there is essentially a vector of intercept terms, one for each response variable, and a matrix of other non-intercept coeﬃcients. April 29, 2015 25 / 35 April 29, 2015 26 / 35 Multivariate regression You can also express ˆB as ˆB = S−1 xx Sxy where we use an estimated covariance matrix of all variables (whether or not they are really random): y1,..., yp,x1,"
7550,unknown,"y1,..., yp,x1,..., xq S = (Syy Syx Sxy Sxx ) Here S is (p + q) ×(p + q). April 29, 2015 27 / 35 Multivariate regression We typically wish to test H0 : B1 = 0 against HA : B1 ̸= 0. This only requires that one βij ̸= 0 for some i ≥1 and some j ≥1. Similar to MANOVA, we deﬁne matrices E and H. To total sum of squares can be partitioned into these two matrices: Y′Y −ny′y = (Y′Y −ˆB′X′Y) + ( ˆB′X′Y −ny"
7551,unknown,"Y′Y −ny′y = (Y′Y −ˆB′X′Y) + ( ˆB′X′Y −ny′y) = E + H April 29, 2015 28 / 35 Multivariate regression Similar to MANOVA, the eigenvalues of E−1H can be used to create test statistics for testing the null hypothesis. Wilk’s Lambda: |Λ = min(p,q)∏ i=1 1 1 + λi Roy’s greatest root: λ1 1 + λ1 Pillai’s test: min(p,q)∑ i=1 1 1 + λi Lawley-Hotelling test: min(p,q)∑ i=1 λi April 29, 2015 29 / 35 Multivariate"
7552,unknown,"If you don’t want to use specialized tables of critical values in the book for these statistics, you can use the same F approximations that we used for MANOVA for Wilk’s Lambda, where Λ = Λ q,p,n−p−1, so that the degrees of freedom for the F test are a function of q, p, and n −p −1. April 29, 2015 30 / 35 Multivariate regression As in the univariate, multiple regression case, you can whether subse"
7553,unknown,"the x variables have coeﬃcients of 0. In this case, there is a matrix in the null hypothesis, H0 : Bd = 0. The E and H matrices are given by E = Y′Y −ˆB′X′Y H = ˆB′X′Y −ˆB′ r X′ r Y And the test statistics are given as before. It is also possible to try to pick a subset of the y variables if some of the y variables are not well-explained by the x variables. This can also be done with stepwise proc"
7554,unknown,"with stepwise procedures. April 29, 2015 31 / 35 Canonical correlation analysis Correlation between two variables measure the linear relationship between those two variables. In canonical correlation, we measure the linear relationship between two sets of variables. Typically, variables within each set will be related in some way, for example a set of student aptitudes or qualiﬁcations (high schoo"
7555,unknown,"qualiﬁcations (high school GPA, SAT scores) and outcomes (college GPA, GRE scores), or variables on a child and similar variables on their parent. April 29, 2015 32 / 35 Canonical correlation analysis If you only have one variable in one set, y, and q variables in the other set, x1,..., xq, then you can deﬁne S = (s2 y s′ yx sxy Sxx ) R = (1 r′ yx rxy Rxx ) where r′ yx is a vectorwith sample corre"
7556,unknown,"(1 r′ yx rxy Rxx ) where r′ yx is a vectorwith sample correlations between y and xi , i = 1,..., q. The squared multiple correlation between y and x1,..., xq is R2 = r′ yx R−1 xx rxy April 29, 2015 33 / 35 Canonical correlation analysis When there are multiple y variables, we use S = (Syy Syx Sxy Sxx ) A measure of association is R2 M = |Syx S−1 xx Sxy | |Syy | = |S−1 yy Syx S−1 xx Sxy |= min(p,q)"
7557,unknown,"xx Sxy |= min(p,q)∏ i=1 r2 i where the r2 i terms are the eigenvalues of S−1 yy Syx S−1 xx Sxy . The values ri , i = 1,..., min(p,q) are called the canonical correlations. April 29, 2015 34 / 35 Canonical correlation analysis The largest canonical correlation r1, is used as a measure of association of the two sets of variables. An interpretation of r2 1 is that it is the maximum squared correlatio"
7558,unknown,"linear combination of the x variables. With each canonical correlation, there is a set of associated linear combinations so that there exist ai and bi such that ri = cor(a′y,b′x) April 29, 2015 35 / 35 Canonical correlation analysis There is some interesting discussion in the book about how the author thinks that canonical correlation is often misapplied in practice. If you are ever asked to use c"
7559,unknown,"If you are ever asked to use canonical correlation, try looking this up! April 29, 2015 36 / 35 Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics Alex Kendall University of Cambridge agk34@cam.ac.uk Yarin Gal University of Oxford yarin@cs.ox.ac.uk Roberto Cipolla University of Cambridge rc10001@cam.ac.uk Abstract Numerous deep learning applications beneﬁt from "
7560,unknown,"task learning with multiple regression and classiﬁcation ob- jectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task’s loss. Tuning these weights by hand is a difﬁcult and expensive process, mak- ing multi-task learning prohibitive in practice. We pro- pose a principled approach to multi-task deep learni"
7561,unknown,"which weighs multiple loss functions by considering the ho- moscedastic uncertainty of each task. This allows us to si- multaneously learn various quantities with different units or scales in both classiﬁcation and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular in- put image. Perhaps surprisingly, we show our m"
7562,unknown,learn multi-task weightings and outperform separate mod- els trained individually on each task. 1. Introduction Multi-task learning aims to improve learning efﬁciency and prediction accuracy by learning multiple objectives from a shared representation [7]. Multi-task learning is prevalent in many applications of machine learning – from computer vision [27] to natural language processing [11] to sp
7563,unknown,speech recognition [23]. We explore multi-task learning within the setting of vi- sual scene understanding in computer vision. Scene under- standing algorithms must understand both the geometry and semantics of the scene at the same time. This forms an in- teresting multi-task learning problem because scene under- standing involves joint learning of various regression and classiﬁcation tasks with 
7564,unknown,"task learning of visual scene understanding is of crucial importance in systems where long computation run-time is prohibitive, such as the ones used in robotics. Combining all tasks into a single model reduces computation and allows these systems to run in real-time. Prior approaches to simultaneously learning multiple tasks use a na ¨ıve weighted sum of losses, where the loss weights are uniform"
7565,unknown,"ever, we show that performance is highly dependent on an appropriate choice of weighting between each task’s loss. Searching for an optimal weighting is prohibitively expen- sive and difﬁcult to resolve with manual tuning. We observe that the optimal weighting of each task is dependent on the measurement scale (e.g. meters, centimetres or millimetres) and ultimately the magnitude of the task’s noi"
7566,unknown,In this work we propose a principled way of combining multiple loss functions to simultaneously learn multiple ob- jectives using homoscedastic uncertainty. We interpret ho- moscedastic uncertainty as task-dependent weighting and show how to derive a principled multi-task loss function which can learn to balance various regression and classiﬁca- tion losses. Our method can learn to balance these w
7567,unknown,"ings optimally, resulting in superior performance, compared with learning each task individually. Speciﬁcally, we demonstrate our method in learning scene geometry and semantics with three tasks. Firstly, we learn to classify objects at a pixel level, also known as se- mantic segmentation [32, 3, 42, 8, 45]. Secondly, our model performs instance segmentation, which is the harder task of segmenting"
7568,unknown,"segmenting separate masks for each individual object in an image (for example, a separate, precise mask for each in- dividual car on the road) [37, 18, 14, 4]. This is a more difﬁcult task than semantic segmentation, as it requires not only an estimate of each pixel’s class, but also which object that pixel belongs to. It is also more complicated than ob- ject detection, which often predicts objec"
7569,unknown,"alone [17]. Finally, our model predicts pixel-wise metric depth. Depth by recognition has been demonstrated using dense prediction networks with supervised [15] and unsu- pervised [16] deep learning. However it is very hard to esti- mate depth in a way which generalises well. We show that we can improve our estimation of geometry and depth by using semantic labels and multi-task deep learning. In "
7570,unknown,"In existing literature, separate deep learning models 1 arXiv:1705.07115v3 [cs.CV] 24 Apr 2018 Encoder Semantic Decoder Input Image Multi-Task Loss Instance Decoder Depth Decoder Semantic Task Uncertainty Instance Task Uncertainty Depth Task Uncertainty Σ Figure 1: Multi-task deep learning. We derive a principled way of combining multiple regression and classiﬁcation loss functions for"
7571,unknown,"multi-task learning. Our architecture takes a single monocular RGB image as input and produces a pixel-wise classiﬁcation, an instance semantic segmentation and an estimate of per pixel depth. Multi-task learning can improve accuracy over separately trained models because cues from one task, such as depth, are used to regularize and improve the generalization of another domain, such as segmentatio"
7572,unknown,"would be used to learn depth regression, semantic segmen- tation and instance segmentation to create a complete scene understanding system. Given a single monocular input im- age, our system is the ﬁrst to produce a semantic segmenta- tion, a dense estimate of metric depth and an instance level segmentation jointly (Figure 1). While other vision mod- els have demonstrated multi-task learning, we s"
7573,unknown,"learn to combine semantics and geometry. Combining these tasks into a single model ensures that the model agrees be- tween the separate task outputs while reducing computa- tion. Finally, we show that using a shared representation with multi-task learning improves performance on various metrics, making the models more effective. In summary, the key contributions of this paper are: 1. a novel and p"
7574,unknown,"ously learn various classiﬁcation and regression losses of varying quantities and units using homoscedastic task uncertainty, 2. a uniﬁed architecture for semantic segmentation, in- stance segmentation and depth regression, 3. demonstrating the importance of loss weighting in multi-task deep learning and how to obtain superior performance compared to equivalent separately trained models. 2. Relate"
7575,unknown,"models. 2. Related Work Multi-task learning aims to improve learning efﬁciency and prediction accuracy for each task, when compared to training a separate model for each task [40, 5]. It can be con- sidered an approach to inductive knowledge transfer which improves generalisation by sharing the domain information between complimentary tasks. It does this by using a shared representation to learn m"
7576,unknown,"one task can help learn other tasks [7]. Fine-tuning [1, 36] is a basic example of multi-task learning, where we can leverage different learning tasks by considering them as a pre-training step. Other models al- ternate learning between each training task, for example in natural language processing [11]. Multi-task learning can also be used in a data streaming setting [40], or to prevent forgettin"
7577,unknown,"forgetting previously learned tasks in reinforcement learn- ing [26]. It can also be used to learn unsupervised features from various data sources with an auto-encoder [35]. In computer vision there are many examples of methods for multi-task learning. Many focus on semantic tasks, such as classiﬁcation and semantic segmentation [30] or classiﬁ- cation and detection [38]. MultiNet [39] proposes an"
7578,unknown,"tecture for detection, classiﬁcation and semantic segmenta- tion. CrossStitch networks [34] explore methods to com- bine multi-task neural activations. Uhrig et al. [41] learn semantic and instance segmentations under a classiﬁcation setting. Multi-task deep learning has also been used for ge- ometry and regression tasks. [15] show how to learn se- mantic segmentation, depth and surface normals. P"
7579,unknown,"[25] is a model which learns camera position and orienta- tion. UberNet [27] learns a number of different regression and classiﬁcation tasks under a single architecture. In this work we are the ﬁrst to propose a method for jointly learn- ing depth regression, semantic and instance segmentation. Like the model of [15], our model learns both semantic and geometry representations, which is important "
7580,unknown,"derstanding. However, our model learns the much harder task of instance segmentation which requires knowledge of both semantics and geometry. This is because our model must determine the class and spatial relationship for each pixel in each object for instance segmentation. 2 00.10.20.30.40.50.60.70.80.91 45 50 55 60 Classiﬁcation Weight IoU Classiﬁcation (%) Classiﬁcation 0 0.1 0.2 0.3 0.4 0.5 0."
7581,unknown,0.58 0.6 0.62 0.64 Depth Weight RMS Inverse Depth Error (m−1) Classiﬁcation Depth Regression Task Weights Class Depth Class Depth IoU [%] Err. [px] 1.0 0.0 59.4 - 0.975 0.025 59.5 0.664 0.95 0.05 59.9 0.603 0.9 0.1 60.1 0.586 0.85 0.15 60.4 0.582 0.8 0.2 59.6 0.577 0.7 0.3 59.0 0.573 0.5 0.5 56.3 0.602 0.2 0.8 47.2 0.625 0.1 0.9 42.7 0.628 0.0 1.0 - 0.640 Learned weights 62.7 0.533 with task uncer
7582,unknown,"0.0 1.0 - 0.640 Learned weights 62.7 0.533 with task uncertainty (this work, Section 3.2) (a) Comparing loss weightings when learning semantic classiﬁcation and depth regression 00.10.20.30.40.50.60.70.80.91 3.8 4 4.2 4.4 4.6 4.8 5 Instance Weight RMS Instance (px) Instance Regression 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.6 0.62 0.64 0.66 0.68 0.7 Depth Weight RMS Inverse Depth Error (m−1)Inst"
7583,unknown,"Depth Regression Task Weights Instance Depth Instance Depth Err. [px] Err. [px] 1.0 0.0 4.61 0.75 0.25 4.52 0.692 0.5 0.5 4.30 0.655 0.4 0.6 4.14 0.641 0.3 0.7 4.04 0.615 0.2 0.8 3.83 0.607 0.1 0.9 3.91 0.600 0.05 0.95 4.27 0.607 0.025 0.975 4.31 0.624 0.0 1.0 0.640 Learned weights 3.54 0.539 with task uncertainty (this work, Section 3.2) (b) Comparing loss weightings when learning instance regres"
7584,unknown,(b) Comparing loss weightings when learning instance regression and depth regression Figure 2: Learning multiple tasks improves the model’s representation and individual task performance . These ﬁgures and tables illustrate the advantages of multi-task learning for (a) semantic classiﬁcation and depth regression and (b) instance and depth regression. Performance of the model in individual tasks is
7585,unknown,"between each task, we observe improved performance for both tasks. All models were trained with a learning rate of 0.01 with the respective weightings applied to the losses using the loss function in (1). Results are shown using the Tiny CityScapes validation dataset using a down-sampled resolution of 128 × 256. More importantly, all previous methods which learn mul- tiple tasks simultaneously use"
7586,unknown,"losses, where the loss weights are uniform, or crudely and manually tuned. In this work we propose a principled way of combining multiple loss functions to simultaneously learn multiple objectives using homoscedastic task uncer- tainty. We illustrate the importance of appropriately weight- ing each task in deep learning to achieve good performance and show that our method can learn to balance thes"
7587,unknown,ings optimally. 3. Multi Task Learning with Homoscedastic Uncertainty Multi-task learning concerns the problem of optimising a model with respect to multiple objectives. It is prevalent in many deep learning problems. The naive approach to com- bining multi objective losses would be to simply perform a weighted linear sum of the losses for each individual task: Ltotal = ∑ i wiLi. (1) This is the d
7588,unknown,"38, 30, 41], for example for dense prediction tasks [27], for scene understanding tasks [15] and for rotation (in quaternions) and translation (in meters) for camera pose [25]. However, there are a number of issues with this method. Namely, model performance is extremely sensitive to weight selection, wi, as illustrated in Figure 2. These weight hyper-parameters are expensive to tune, often taking"
7589,unknown,"many days for each trial. Therefore, it is desirable to ﬁnd a more convenient approach which is able to learn the optimal weights. More concretely, let us consider a network which learns to predict pixel-wise depth and semantic class from an in- put image. In Figure 2 the two boundaries of each plot show models trained on individual tasks, with the curves showing 3 performance for varying weights "
7590,unknown,"serve that at some optimal weighting, the joint network per- forms better than separate networks trained on each task in- dividually (performance of the model in individual tasks is seen at both edges of the plot: w= 0and w= 1). At near- by values to the optimal weight the network performs worse on one of the tasks. However, searching for these optimal weightings is expensive and increasingly difﬁ"
7591,unknown,"models with numerous tasks. Figure 2 also shows a similar result for two regression tasks; instance segmentation and depth regression. We next show how to learn optimal task weightings using ideas from probabilistic modelling. 3.1. Homoscedastic uncertainty as task-dependent uncertainty In Bayesian modelling, there are two main types of un- certainty one can model [24]. •Epistemic uncertainty is u"
7592,unknown,which captures what our model does not know due to lack of training data. It can be explained away with increased training data. •Aleatoric uncertainty captures our uncertainty with re- spect to information which our data cannot explain. Aleatoric uncertainty can be explained away with the ability to observe all explanatory variables with in- creasing precision. Aleatoric uncertainty can again be 
7593,unknown,"categories. •Data-dependent or Heteroscedastic uncertainty is aleatoric uncertainty which depends on the input data and is predicted as a model output. •Task-dependent or Homoscedastic uncertainty is aleatoric uncertainty which is not dependent on the in- put data. It is not a model output, rather it is a quantity which stays constant for all input data and varies be- tween different tasks. It can"
7594,unknown,"tween different tasks. It can therefore be described as task-dependent uncertainty. In a multi-task setting, we show that the task uncertainty captures the relative conﬁdence between tasks, reﬂecting the uncertainty inherent to the regression or classiﬁcation task. It will also depend on the task’s representation or unit of measure. We propose that we can use homoscedastic uncertainty as a basis f"
7595,unknown,learning problem. 3.2. Multi-task likelihoods In this section we derive a multi-task loss function based on maximising the Gaussian likelihood with homoscedastic uncertainty. Let fW(x) be the output of a neural network with weights W on input x. We deﬁne the following proba- bilistic model. For regression tasks we deﬁne our likelihood as a Gaussian with mean given by the model output: p(y|fW(x)) =
7596,unknown,"p(y|fW(x)) =N(fW(x),σ2) (2) with an observation noise scalar σ. For classiﬁcation we often squash the model output through a softmax function, and sample from the resulting probability vector: p(y|fW(x)) =Softmax(fW(x)). (3) In the case of multiple model outputs, we often deﬁne the likelihood to factorise over the outputs, given some sufﬁ- cient statistics. We deﬁne fW(x) as our sufﬁcient statisti"
7597,unknown,"and obtain the following multi-task likelihood: p(y1,..., yK|fW(x)) =p(y1|fW(x))...p(yK|fW(x)) (4) with model outputs y1,..., yK (such as semantic segmenta- tion, depth regression, etc). In maximum likelihood inference, we maximise the log likelihood of the model. In regression, for example, the log likelihood can be written as log p(y|fW(x)) ∝− 1 2σ2 ||y −fW(x)||2 −log σ (5) for a Gaussian likeli"
7598,unknown,"lihood) with σ the model’s observation noise parameter – capturing how much noise we have in the outputs. We then maximise the log likelihood with respect to the model pa- rameters W and observation noise parameter σ. Let us now assume that our model output is composed of two vectors y1 and y2, each following a Gaussian distribu- tion: p(y1,y2|fW(x)) =p(y1|fW(x)) ·p(y2|fW(x)) = N(y1; fW(x),σ2 1) ·"
7599,unknown,"= N(y1; fW(x),σ2 1) ·N(y2; fW(x),σ2 2). (6) This leads to the minimisation objective, L(W,σ1,σ2), (our loss) for our multi-output model: = −log p(y1,y2|fW(x)) ∝ 1 2σ2 1 ||y1 −fW(x)||2 + 1 2σ2 2 ||y2 −fW(x)||2 + logσ1σ2 = 1 2σ2 1 L1(W) + 1 2σ2 2 L2(W) + logσ1σ2 (7) Where we wrote L1(W) =||y1 −fW(x)||2 for the loss of the ﬁrst output variable, and similarly for L2(W). We interpret minimising this la"
7600,unknown,"to σ1 and σ2 as learning the relative weight of the losses 4 L1(W) and L2(W) adaptively, based on the data. As σ1 – the noise parameter for the variable y1 – increases, we have that the weight of L1(W) decreases. On the other hand, as the noise decreases, we have that the weight of the respective objective increases. The noise is discouraged from increasing too much (effectively ignoring the data)"
7601,unknown,"the last term in the objective, which acts as a regulariser for the noise terms. This construction can be trivially extended to multiple regression outputs. However, the extension to classiﬁcation likelihoods is more interesting. We adapt the classiﬁcation likelihood to squash a scaled version of the model output through a softmax function: p(y|fW(x),σ) =Softmax( 1 σ2 fW(x)) (8) with a positive sc"
7602,unknown,"σ2 fW(x)) (8) with a positive scalar σ. This can be interpreted as a Boltz- mann distribution (also called Gibbs distribution) where the input is scaled byσ2 (often referred to astemperature). This scalar is either ﬁxed or can be learnt, where the parameter’s magnitude determines how ‘uniform’ (ﬂat) the discrete dis- tribution is. This relates to its uncertainty, as measured in entropy. The log li"
7603,unknown,"ten as log p(y = c|fW(x),σ) = 1 σ2 fW c (x) −log ∑ c′ exp ( 1 σ2 fW c′ (x) ) (9) with fW c (x) the c’th element of the vectorfW(x). Next, assume that a model’s multiple outputs are com- posed of a continuous output y1 and a discrete out- put y2, modelled with a Gaussian likelihood and a soft- max likelihood, respectively. Like before, the joint loss, L(W,σ1,σ2), is given as: = −log p(y1,y2 = c|fW("
7604,unknown,"= −log p(y1,y2 = c|fW(x)) = −log N(y1; fW(x),σ2 1) ·Softmax(y2 = c; fW(x),σ2) = 1 2σ2 1 ||y1 −fW(x)||2 + logσ1 −log p(y2 = c|fW(x),σ2) = 1 2σ2 1 L1(W) + 1 σ2 2 L2(W) + logσ1 + log ∑ c′ exp ( 1 σ2 2 fW c′ (x) ) (∑ c′ exp ( fW c′ (x) ))1 σ2 2 ≈ 1 2σ2 1 L1(W) + 1 σ2 2 L2(W) + logσ1 + logσ2, (10) where again we write L1(W) = ||y1 − fW(x)||2 for the Euclidean loss of y1, write L2(W) = −log Softmax(y2,f"
7605,unknown,"(with fW(x) not scaled), and optimise with respect to W as well as σ1, σ2. In the last transition we introduced the ex- plicit simplifying assumption 1 σ2 ∑ c′ exp ( 1 σ2 2 fW c′ (x) ) ≈ (∑ c′ exp ( fW c′ (x) ))1 σ2 2 which becomes an equality when σ2 →1. This has the advantage of simplifying the optimisation objective, as well as empirically improving re- sults. This last objective can be seen as"
7606,unknown,"weights of the losses for each output. Large scale values σ2 will decrease the contribution of L2(W), whereas small scale σ2 will increase its contribution. The scale is regulated by the last term in the equation. The objective is penalised when setting σ2 too large. This construction can be trivially extended to arbitrary combinations of discrete and continuous loss functions, al- lowing us to le"
7607,unknown,"lowing us to learn the relative weights of each loss in a principled and well-founded way. This loss is smoothly dif- ferentiable, and is well formed such that the task weights will not converge to zero. In contrast, directly learning the weights using a simple linear sum of losses (1) would result in weights which quickly converge to zero. In the following sections we introduce our experimental m"
7608,unknown,"empirical results. In practice, we train the network to predict the log vari- ance, s := logσ2. This is because it is more numerically stable than regressing the variance, σ2, as the loss avoids any division by zero. The exponential mapping also allows us to regress unconstrained scalar values, where exp(−s) is resolved to the positive domain giving valid values for variance. 4. Scene Understandin"
7609,unknown,"variance. 4. Scene Understanding Model To understand semantics and geometry we ﬁrst propose an architecture which can learn regression and classiﬁcation outputs, at a pixel level. Our architecture is a deep con- volutional encoder decoder network [3]. Our model con- sists of a number of convolutional encoders which produce a shared representation, followed by a corresponding num- ber of task-speci"
7610,unknown,"ber of task-speciﬁc convolutional decoders. A high level summary is shown in Figure 1. The purpose of the encoder is to learn a deep mapping to produce rich, contextual features, using domain knowledge from a number of related tasks. Our encoder is based on DeepLabV3 [10], which is a state of the art semantic seg- mentation framework. We use ResNet101 [20] as the base feature encoder, followed by "
7611,unknown,"Pooling (ASPP) module [10] to increase contextual aware- ness. We apply dilated convolutions in this encoder, such that the resulting feature map is sub-sampled by a factor of 5 (a) Input Image (b) Semantic Segmentation (c) Instance vector regression (d) Instance Segmentation Figure 3: Instance centroid regression method. For each pixel, we regress a vector pointing to the instance’s centroid. The"
7612,unknown,"only computed over pixels which are from instances. We visualise (c) by representing colour as the orientation of the instance vector, and intensity as the magnitude of the vector. 8 compared to the input image dimensions. We then split the network into separate decoders (with separate weights) for each task. The purpose of the decoder is to learn a mapping from the shared features to an output. E"
7613,unknown,"Each decoder consists of a 3 ×3 convolutional layer with output feature size 256, followed by a1×1 layer regressing the task’s output. Further architectural details are described in Appendix A. Semantic Segmentation. We use the cross-entropy loss to learn pixel-wise class probabilities, averaging the loss over the pixels with semantic labels in each mini-batch. Instance Segmentation. An intuitive "
7614,unknown,"ing which instance a pixel belongs to is an association to the instance’s centroid. We use a regression approach for in- stance segmentation [29]. This approach is inspired by [28] which identiﬁes instances using Hough votes from object parts. In this work we extend this idea by using votes from individual pixels using deep learning. We learn an instance vector, ˆxn, for each pixel coordinate,cn, "
7615,unknown,"centroid of the pixel’s instance,in, such that in = ˆxn+ cn. We train this regression with an L1 loss using ground truth labels xn, averaged over all labelled pixels, NI, in a mini- batch: LInstance = 1 |NI| ∑ NI ∥xn −ˆxn∥1. Figure 3 details the representation we use for instance segmentation. Figure 3(a) shows the input image and a mask of the pixels which are of an instance class (at test time i"
7616,unknown,"time inferred from the predicted semantic segmentation). Figure 3(b) and Figure 3(c) show the ground truth and pre- dicted instance vectors for both x and y coordinates. We then cluster these votes using OPTICS [2], resulting in the predicted instance segmentation output in Figure 3(d). One of the most difﬁcult cases for instance segmentation algorithms to handle is when the instance mask is split"
7617,unknown,"(a) Input Image (b) Instance Segmentation Figure 4: This example shows two cars which are occluded by trees and lampposts, making the instance segmentation challeng- ing. Our instance segmentation method can handle occlusions ef- fectively. We can correctly handle segmentation masks which are split by occlusion, yet part of the same instance, by incorporating semantics and geometry. to occlusion. "
7618,unknown,"semantics and geometry. to occlusion. Figure 4 shows that our method can handle these situations, by allowing pixels to vote for their instance centroid with geometry. Methods which rely on watershed approaches [4], or instance edge identiﬁcation approaches fail in these scenarios. To obtain segmentations for each instance, we now need to estimate the instance centres, ˆin. We propose to con- side"
7619,unknown,"sider the estimated instance vectors,ˆxn, as votes in a Hough parameter space and use a clustering algorithm to identify these instance centres. OPTICS [2], is an efﬁcient density based clustering algorithm. It is able to identify an unknown number of multi-scale clusters with varying density from a given set of samples. We chose OPICS for two reasons. Crucially, it does not assume knowledge of th"
7620,unknown,"clusters like algorithms such as k-means [33]. Secondly, it does not assume a canonical instance size or density like discretised binning approaches [12]. Using OPTICS, we cluster the points cn + ˆxn into a number of estimated in- stances, ˆi. We can then assign each pixel, pn to the instance closest to its estimated instance vector, cn + ˆxn. Depth Regression. We train with supervised labels us- "
7621,unknown,"ing pixel-wise metric inverse depth using aL1 loss function: LDepth = 1 |ND| ∑ ND dn − ˆdn  1 . Our architecture esti- mates inverse depth, ˆdn, because it can represent points at inﬁnite distance (such as sky). We can obtain inverse depth labels, dn, from a RGBD sensor or stereo imagery. Pixels which do not have an inverse depth label are ignored in the loss. 5. Experiments We demonstrate t"
7622,unknown,"CityScapes [13], a large dataset for road scene understand- ing. It comprises of stereo imagery, from automotive grade stereo cameras with a 22cmbaseline, labelled with instance and semantic segmentations from 20 classes. Depth images are also provided, labelled using SGM [22], which we treat as pseudo ground truth. Additionally, we assign zero in- verse depth to pixels labelled as sky. The datase"
7623,unknown,6 Task Weights Segmentation Instance Inverse Depth Loss Seg. Inst. Depth IoU [%] Mean Error [px] Mean Error [px] Segmentation only 1 0 0 59.4% - - Instance only 0 1 0 - 4.61 - Depth only 0 0 1 - - 0.640 Unweighted sum of losses 0.333 0.333 0.333 50.1% 3.79 0.592 Approx. optimal weights 0.89 0.01 0.1 62.8% 3.61 0.549 2 task uncertainty weighting ✓ ✓ 61.0% 3.42 - 2 task uncertainty weighting ✓ ✓ 62.
7624,unknown,"2 task uncertainty weighting ✓ ✓ 62.7% - 0.533 2 task uncertainty weighting ✓ ✓ - 3.54 0.539 3 task uncertainty weighting ✓ ✓ ✓ 63.4% 3.50 0.522 Table 1: Quantitative improvement when learning semantic segmentation, instance segmentation and depth with our multi-task loss. Experiments were conducted on the Tiny CityScapes dataset (sub-sampled to a resolution of 128 × 256). Results are shown from t"
7625,unknown,"validation set. We observe an improvement in performance when training with our multi-task loss, over both single-task models and weighted losses. Additionally, we observe an improvement when training on all three tasks ( 3 × ✓) using our multi-task loss, compared with all pairs of tasks alone (denoted by 2 × ✓). This shows that our loss function can automatically learn a better performing weighti"
7626,unknown,"between the tasks than the baselines. lected from a number of cities in ﬁne weather and consists of 2,975 training and 500 validation images at 2048 ×1024 resolution. 1,525 images are withheld for testing on an on- line evaluation server. Further training details, and optimisation hyperparame- ters, are provided in Appendix A. 5.1. Model Analysis In Table 1 we compare individual models to multi-ta"
7627,unknown,"learning models using a na¨ıve weighted loss or the task un- certainty weighting we propose in this paper. To reduce the computational burden, we train each model at a reduced res- olution of 128 ×256 pixels, over 50,000 iterations. When we downsample the data by a factor of four, we also need to scale the disparity labels accordingly. Table 1 clearly il- lustrates the beneﬁt of multi-task learnin"
7628,unknown,"signiﬁcantly better performing results than individual task models. For example, using our method we improve classi- ﬁcation results from 59.4% to 63.4%. We also compare to a number of na¨ıve multi-task losses. We compare weighting each task equally and using approx- imately optimal weights. Using a uniform weighting results in poor performance, in some cases not even improving on the results from"
7629,unknown,"the results from the single task model. Obtaining approxi- mately optimal weights is difﬁcult with increasing number of tasks as it requires an expensive grid search over param- eters. However, even these weights perform worse com- pared with our proposed method. Figure 2 shows that using task uncertainty weights can even perform better compared to optimal weights found through ﬁne-grained grid se"
7630,unknown,"We believe that this is due to two reasons. First, grid search is restricted in accuracy by the resolution of the search. Second, optimising the task weights using a homoscedas- tic noise term allows for the weights to be dynamic during training. In general, we observe that the uncertainty term decreases during training which improves the optimisation process. In Appendix B we ﬁnd that our task-un"
7631,unknown,"robust to the initialisation chosen for the parameters. These quickly converge to a similar optima in a few hundred train- ing iterations. We also ﬁnd the resulting task weightings varies throughout the course of training. For our ﬁnal model (in Table 2), at the end of training, the losses are weighted with the ratio 43 : 1 : 0.16 for semantic segmentation, depth regression and instance segmentati"
7632,unknown,"Finally, we benchmark our model using the full-size CityScapes dataset. In Table 2 we compare to a number of other state of the art methods in all three tasks. Our method is the ﬁrst model which completes all three tasks with a sin- gle model. We compare favourably with other approaches, outperforming many which use comparable training data and inference tools. Figure 5 shows some qualitative ex- "
7633,unknown,amples of our model. 6. Conclusions We have shown that correctly weighting loss terms is of paramount importance for multi-task learning problems. We demonstrated that homoscedastic (task) uncertainty is an effective way to weight losses. We derived a principled loss function which can learn a relative weighting automati- cally from the data and is robust to the weight initialization. We showed th
7634,unknown,"understanding tasks with a uniﬁed architecture for seman- 7 Semantic Segmentation Instance Segmentation Monocular Disparity Estimation Method IoU class iIoU class IoU cat iIoU cat AP AP 50% AP 100m AP 50m Mean Error [px] RMS Error [px] Semantic segmentation, instance segmentation and depth regression methods (this work) Multi-Task Learning 78.5 57.4 89.9 77.7 21.6 39.0 35.0 37.0 2.92 5.88 Semantic"
7635,unknown,Semantic segmentation and instance segmentation methods Uhrig et al. [41] 64.3 41.6 85.9 73.9 8.9 21.1 15.3 16.7 - - Instance segmentation only methods Mask R-CNN [19] - - - - 26.2 49.9 37.6 40.1 - - Deep Watershed [4] - - - - 19.4 35.3 31.4 36.8 - - R-CNN + MCG [13] - - - - 4.6 12.9 7.7 10.3 - - Semantic segmentation only methods DeepLab V3 [10] 81.3 60.9 91.6 81.7 - - - - - - PSPNet [44] 81.2 59
7636,unknown,"PSPNet [44] 81.2 59.6 91.2 79.2 - - - - - - Adelaide [31] 71.6 51.7 87.3 74.1 - - - - - - Table 2: CityScapes Benchmark [13]. We show results from the test dataset using the full resolution of 1024 × 2048 pixels. For the full leaderboard, please see www.cityscapes-dataset.com/benchmarks. The disparity (inverse depth) metrics were computed against the CityScapes depth maps, which are sparse and com"
7637,unknown,as many methods use ensembles of different training datasets. Our method is the ﬁrst to address all three tasks with a single model. (a) Input image (b) Segmentation output (c) Instance output (d) Depth output Figure 5: Qualitative results for multi-task learning of geometry and semantics for road scene understanding . Results are shown on test images from the CityScapes dataset using our multi-ta
7638,unknown,"multi-task learning improves the smoothness and accuracy for depth perception because it learns a representation that uses cues from other tasks, such as segmentation (and vice versa). tic segmentation, instance segmentation and per-pixel depth regression. We demonstrated modelling task-dependent ho- moscedastic uncertainty improves the model’s representa- tion and each task’s performance when com"
7639,unknown,"rate models trained on each task individually. There are many interesting questions left unanswered. Firstly, our results show that there is usually not a single op- timal weighting for all tasks. Therefore, what is the optimal weighting? Is multitask learning is an ill-posed optimisa- tion problem without a single higher-level goal? A second interesting question is where the optimal loca- tion is"
7640,unknown,"tion is for splitting the shared encoder network into separate decoders for each task? And, what network depth is best for the shared multi-task representation? Finally, why do the semantics and depth tasks out- perform the semantics and instance tasks results in Table 1? Clearly the three tasks explored in this paper are compli- mentary and useful for learning a rich representation about the scen"
7641,unknown,"the scene. It would be beneﬁcial to be able to quantify the relationship between tasks and how useful they would be for multitask representation learning. 8 References [1] P. Agrawal, J. Carreira, and J. Malik. Learning to see by moving. In Proceedings of the IEEE International Confer- ence on Computer Vision, pages 37–45, 2015. 2 [2] M. Ankerst, M. M. Breunig, H.-P. Kriegel, and J. Sander. Optics"
7642,unknown,"Optics: ordering points to identify the clustering structure. In ACM Sigmod Record, volume 28, pages 49–60. ACM, 1999. 6 [3] V . Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A deep convolutional encoder-decoder architecture for scene segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. 1, 5 [4] M. Bai and R. Urtasun. Deep watershed transform for instance segme"
7643,unknown,"instance segmentation. arXiv preprint arXiv:1611.08303 , 2016. 1, 6, 8 [5] J. Baxter et al. A model of inductive bias learning. J. Artif. Intell. Res.(JAIR), 12(149-198):3, 2000. 2 [6] S. R. Bul `o, L. Porzi, and P. Kontschieder. In-place activated batchnorm for memory-optimized training of dnns. arXiv preprint arXiv:1712.02616, 2017. [7] R. Caruana. Multitask learning. In Learning to learn, pages"
7644,unknown,"95–133. Springer, 1998. 1, 2 [8] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep con- volutional nets and fully connected crfs. In ICLR, 2015. 1 [9] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully con- nected crfs. arXiv pr"
7645,unknown,"[10] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Re- thinking atrous convolution for semantic image segmenta- tion. arXiv preprint arXiv:1706.05587, 2017. 5, 8, 11 [11] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM, 2"
7646,unknown,"[12] D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. IEEE Transactions on pattern analysis and machine intelligence, 24(5):603–619, 2002. 6 [13] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In In Proc. IEEE Conf. on Computer Vision and P"
7647,unknown,"Recognition, 2016. 6, 8 [14] J. Dai, K. He, and J. Sun. Instance-aware semantic segmenta- tion via multi-task network cascades. In In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2016. 1 [15] D. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolu- tional architecture. In Proceedings of the IEEE International Conference on Com"
7648,unknown,"Conference on Computer Vision, pages 2650–2658, 2015. 1, 2, 3 [16] R. Garg and I. Reid. Unsupervised cnn for single view depth estimation: Geometry to the rescue. Computer Vision–ECCV 2016, pages 740–756, 2016. 1 [17] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea- ture hierarchies for accurate object detection and semantic segmentation. In In Proc. IEEE Conf. on Computer Vision and P"
7649,unknown,"and Pattern Recognition, pages 580–587, 2014. 1 [18] B. Hariharan, P. Arbel ´aez, R. Girshick, and J. Malik. Hyper- columns for object segmentation and ﬁne-grained localiza- tion. In In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, pages 447–456. IEEE, 2014. 1 [19] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick. Mask r-cnn. arXiv preprint arXiv:1703.06870, 2017. 8 [20] K. He, X. Z"
7650,unknown,"[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2016. 5, 11 [21] H. Hirschmuller. Accurate and efﬁcient stereo processing by semi-global matching and mutual information. In In Proc. IEEE Conf. on Computer Vision and Pattern Recognition , volume 2, pages 807–814. IEEE, 2005. 8 [22] H. Hirschmu"
7651,unknown,"[22] H. Hirschmuller. Stereo processing by semiglobal matching and mutual information. IEEE Transactions on pattern anal- ysis and machine intelligence, 30(2):328–341, 2008. 6 [23] J.-T. Huang, J. Li, D. Yu, L. Deng, and Y . Gong. Cross- language knowledge transfer using multilingual deep neural network with shared hidden layers. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE Inter"
7652,unknown,"Signal Processing (ICASSP), 2013 IEEE International Con- ference on, pages 7304–7308. IEEE, 2013. 1 [24] A. Kendall and Y . Gal. What uncertainties do we need in bayesian deep learning for computer vision? arXiv preprint arXiv:1703.04977, 2017. 4 [25] A. Kendall, M. Grimes, and R. Cipolla. Convolutional net- works for real-time 6-dof camera relocalization. In Pro- ceedings of the International Con"
7653,unknown,"ceedings of the International Conference on Computer Vi- sion (ICCV), 2015. 2, 3 [26] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Des- jardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic for- getting in neural networks. Proceedings of the National Academy of Sciences, page 201611835, 2017. 2 [27] I. Kokkinos. Ubernet: Training auniv"
7654,unknown,"neural network for low-, mid-, and high-level vision us- ing diverse datasets and limited memory. arXiv preprint arXiv:1609.02132, 2016. 1, 2, 3 [28] B. Leibe, A. Leonardis, and B. Schiele. Robust object detec- tion with interleaved categorization and segmentation. Inter- national Journal of Computer Vision (IJCV) , 77(1-3):259– 289, 2008. 6 [29] X. Liang, Y . Wei, X. Shen, J. Yang, L. Lin, and S."
7655,unknown,"Proposal-free network for instance-level object segmenta- tion. arXiv preprint arXiv:1509.02636, 2015. 6 [30] Y . Liao, S. Kodagoda, Y . Wang, L. Shi, and Y . Liu. Un- derstand scene categories by objects: A semantic regularized scene classiﬁer using convolutional neural networks. In2016 IEEE International Conference on Robotics and Automation (ICRA), pages 2318–2325. IEEE, 2016. 2, 3 [31] G. Lin,"
7656,unknown,"[31] G. Lin, C. Shen, I. Reid, et al. Efﬁcient piecewise training of deep structured models for semantic segmentation. arXiv preprint arXiv:1504.01013, 2015. 8 [32] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2015. 1 9 [33] J. MacQueen et al. Some methods for classiﬁcation and anal- y"
7657,unknown,"ysis of multivariate observations. In Proceedings of the ﬁfth Berkeley symposium on mathematical statistics and proba- bility, volume 1, pages 281–297. Oakland, CA, USA., 1967. 6 [34] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross- stitch networks for multi-task learning. InProceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 3994–4003, 2016. 2 [35] J. N"
7658,unknown,"tion, pages 3994–4003, 2016. 2 [35] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y . Ng. Multimodal deep learning. In Proceedings of the 28th inter- national conference on machine learning (ICML-11) , pages 689–696, 2011. 2 [36] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring mid-level image representations using convolu- tional neural networks. In In Proc. IEEE Conf"
7659,unknown,"Vision and Pattern Recognition , pages 1717–1724. IEEE, 2014. 2 [37] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to seg- ment object candidates. In Advances in Neural Information Processing Systems, pages 1990–1998, 2015. 1 [38] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks."
7660,unknown,"Conference on Learning Representations (ICLR), 2014. 1, 2, 3 [39] M. Teichmann, M. Weber, M. Zoellner, R. Cipolla, and R. Urtasun. Multinet: Real-time joint semantic reasoning for autonomous driving. arXiv preprint arXiv:1612.07695, 2016. 2, 3 [40] S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst? In Advances in neural information processing sys- tems, pages 640–646. MORGAN K"
7661,unknown,"ERS, 1996. 2 [41] J. Uhrig, M. Cordts, U. Franke, and T. Brox. Pixel-level encoding and depth layering for instance-level semantic la- beling. arXiv preprint arXiv:1604.05096, 2016. 2, 3, 8 [42] F. Yu and V . Koltun. Multi-scale context aggregation by di- lated convolutions. In ICLR, 2016. 1 [43] S. Zagoruyko and N. Komodakis. Wide residual networks. In E. R. H. Richard C. Wilson and W. A. P. Smit"
7662,unknown,"ceedings of the British Machine Vision Conference (BMVC), pages 87.1–87.12. BMV A Press, September 2016. [44] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene parsing network. arXiv preprint arXiv:1612.01105, 2016. 8 [45] S. Zheng, S. Jayasumana, B. Romera-Paredes, V . Vineet, Z. Su, D. Du, C. Huang, and P. Torr. Conditional random ﬁelds as recurrent neural networks. In International Con"
7663,unknown,"ence on Computer Vision (ICCV), 2015. 1 10 A. Model Architecture Details We base our model on the recently introduced DeepLabV3 [10] segmentation architecture. We use ResNet101 [20] as our base feature encoder, with dilated convolutions, resulting in a feature map which is downsam- pled by a factor of 8 compared with the original input im- age. We then append dilated (atrous) convolutional ASPP mo"
7664,unknown,"module [10]. This module is designed to improve the con- textual reasoning of the network. We use an ASPP module comprised of four parallel convolutional layers, with 256 output channels and dilation rates (1, 12, 24, 36), with ker- nel sizes (12, 32, 32, 32). Additionally, we also apply global average pooling to the encoded features, and convolve them to 256 dimensions with a 1 ×1 kernel. We appl"
7665,unknown,"normalisation to each of these layers and concatenate the resulting 1280 features together. This produces the shared representation between each task. We then split the network, to decode this representation to a given task output. For each task, we construct a decoder consisting of two layers. First, we apply a1×1 convolution, outputting 256 features, followed by batch normalisation and a non-lin"
7666,unknown,"and a non-linear activation. Finally, we convolve this output to the required dimensions for a given task. For classiﬁca- tion, this will be equal to the number of semantic classes, otherwise the output will be 1 or 2 channels for depth or in- stance segmentation respectively. Finally, we apply bilinear upsampling to scale the output to the same resolution as the input. The majority of the model’s"
7667,unknown,"the feature encoding, with very little ﬂexibility in each task decoder. This illustrates the attraction of multitask learning; most of the compute can be shared between each task to learn a better shared representation. A.1. Optimisation For all experiments, we use an initial learning rate of 2.5 ×10−3 and polynomial learning rate decay (1 − iter maxiter )0.9. We train using stochastic gradient de"
7668,unknown,"with Nesterov updates and momentum 0.9 and weight de- cay 104. We conduct all experiments in this paper using PyTorch. For the experiments on the Tiny CityScapes validation dataset (using a down-sampled resolution of 128 ×256) we train over 50,000 iterations, using 256 ×256 crops with batch size of 8 on a single NVIDIA 1080Ti GPU. We apply random horizontal ﬂipping to the data. For the full-scale "
7669,unknown,"train over 100,000 iterations with a batch size of 16. We apply random horizontal ﬂipping (with probability 0.5) and random scaling (selected from 0.7 - 2.0) to the data dur- ing training, before making a 512 ×512 crop. The training data is sampled uniformly, and is randomly shufﬂed for each epoch. Training takes ﬁve days on a single computer with four NVIDIA 1080Ti GPUs. B. Further Analysis This "
7670,unknown,B. Further Analysis This task uncertainty loss is also robust to the value we use to initialise the task uncertainty values. One of the at- tractive properties of our approach to weighting multi-task losses is that it is robust to the initialisation choice for the homoscedastic noise parameters. Figure 6 shows that for an array of initial choices of log σ2 from −2.0 to 5.0 the ho- moscedastic nois
7671,unknown,"same minima. Additionally, the homoscedastic noise terms converges after only 100 iterations, while the network re- quires 30,000+ iterations to train. Therefore our model is robust to the choice of initial value for the weighting terms. Figure 7 shows losses and uncertainty estimates for each task during training of the ﬁnal model on the full-size CityScapes dataset. At a point 500 iterations int"
7672,unknown,"the model estimates task variance of 0.60, 62.5 and 13.5 for semantic segmentation, instance segmentation and depth re- gression, respectively. Becuase the losses are weighted by the inverse of the uncertainty estimates, this results in a task weighting ratio of approximately 23 : 0.22 : 1 between se- mantics, instance and depth, respectively. At the conclu- sion of training, the three tasks have "
7673,unknown,"of 0.075, 3.25 and 20.4, which results in effective weighting between the tasks of 43: 0.16 : 1. This shows how the task uncertainty estimates evolve over time, and the approximate ﬁnal weightings the network learns. We observe they are far from uniform, as is often assumed in previous literature. Interestingly, we observe that this loss allows the net- work to dynamically tune the weighting. Typi"
7674,unknown,"moscedastic noise terms decrease in magnitude as training progresses. This makes sense, as during training the model becomes more effective at a task. Therefore the error, and uncertainty, will decrease. This has a side-effect of increas- ing the effective learning rate – because the overall uncer- tainty decreases, the weight for each task’s loss increases. In our experiments we compensate for th"
7675,unknown,"In our experiments we compensate for this by annealing the learning rate with a power law. Finally, a comment on the model’s failure modes. The model exhibits similar failure modes to state-of-the-art single-task models. For example, failure with objects out of the training distribution, occlusion or visually challenging situations. However, we also observe our multi-task model tends to fail with "
7676,unknown,tends to fail with similar effect in all three modalities. Ie. an erroneous pixel’s prediction in one task will often be highly correlated with error in another modality. Some examples can be seen in Figure 8. 11 0 50 100 150 200 250 Training Iterations −1 0 1 2 3 4 5 Task Uncertainty (a) Semantic segmentation task 0 50 100 150 200 250 Training Iterations 1 2 3 4 5 6 7 8Task Uncertainty (b) Instan
7677,unknown,0 50 100 150 200 250 Training Iterations −1 0 1 2 3 4 5 Task Uncertainty (c) Depth regression task Figure 6: Training plots showing convergence of homoscedastic noise and task loss for an array of initialisation choices for the ho- moscedastic uncertainty terms for all three tasks. Each plot shows the the homoscedastic noise value optimises to the same solution from
7678,unknown,"a variety of initialisations. Despite the network taking 10, 000+ iterations for the training loss to converge, the task uncertainty converges very rapidly after only 100 iterations. 0 20000 40000 60000 80000 100000 Training Iterations 0.0 0.2 0.4 0.6 0.8Loss 0 20000 40000 60000 80000 100000 Training Iterations 0 20 40 60 80 100 120 140Loss 0 20000 40000 60000 80000 100000 Training Iterations 5 10"
7679,unknown,15 20Loss 0 20000 40000 60000 80000 100000 Training Iterations 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75Task Uncertainty (σ2 ) (a) Semantic segmentation task 0 20000 40000 60000 80000 100000 Training Iterations 20 30 40 50 60 70 80 90Task Uncertainty (σ2) (b) Instance segmentation task 0 20000 40000 60000 80000 100000 Training Iterations 5 10 15 20 25 30 35 40Task Uncertainty (σ2) (c) Depth regressi
7680,unknown,"Training Iterations 5 10 15 20 25 30 35 40Task Uncertainty (σ2) (c) Depth regression task Figure 7: Learning task uncertainty. These training plots show the losses and task uncertainty estimates for each task during training. Results are shown for the ﬁnal model, trained on the fullsize CityScapes dataset. 12 C. Further Qualitative Results (a) Input image (b) Semantic segmentation (c) Instance seg"
7681,unknown,(a) Input image (b) Semantic segmentation (c) Instance segmentation (d) Depth regression Figure 8: More qualitative results on test images from the CityScapes dataset. 13 D. Failure Examples (a) Input image (b) Semantic segmentation (c) Instance segmentation (d) Depth regression Figure 9: Example where our model fails on the CityScapes test data. The ﬁrst two rows show examples of challenging visu
7682,unknown,"such as reﬂection, which confuse the model. Rows three and four show the model incorrectly distinguishing between road and footpath. This is a common mistake, which we believe is due to a lack of contextual reasoning. Rows ﬁve, six and seven demonstrate incorrect classiﬁcation of a rare class (bus, fence and motorbike, respectively). Finally, the last two rows show failure due to occlusion and whe"
7683,unknown,"the object is too big for the model’s receptive ﬁeld. Additionally, we observe that failures are highly correlated between the modes, which makes sense as each output is conditioned on the same feature vector. For example, in the second row, the incorrect labelling of the reﬂection as a person causes the depth estimation to predict human geometry. 14 XGBoost: A Scalable Tree Boosting System Tianqi"
7684,unknown,"University of Washington tqchen@cs.washington.edu Carlos Guestrin University of Washington guestrin@cs.washington.edu ABSTRACT Tree boosting is a highly eﬀective and widely used machine learning method. In this paper, we describe a scalable end- to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenge"
7685,unknown,"sparsity-aware algorithm for sparse data and weighted quan- tile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compres- sion and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems. Keywords Large-scale Machine Learning "
7686,unknown,Large-scale Machine Learning 1. INTRODUCTION Machine learning and data-driven approaches are becom- ing very important in many areas. Smart spam classiﬁers protect our email by learning from massive amounts of spam data and user feedback; advertising systems learn to match the right ads with the right context; fraud detection systems protect banks from malicious attackers; anomaly event de- tectio
7687,unknown,"tection systems help experimental physicists to ﬁnd events that lead to new physics. There are two important factors that drive these successful applications: usage of eﬀective (statistical) models that capture the complex data depen- dencies and scalable learning systems that learn the model of interest from large datasets. Among the machine learning methods used in practice, gradient tree boosti"
7688,unknown,"in many applications. Tree boosting has been shown to give state-of-the-art results on many standard classiﬁcation benchmarks [16]. LambdaMART [5], a variant of tree boost- ing for ranking, achieves state-of-the-art result for ranking 1Gradient tree boosting is also known as gradient boosting machine (GBM) or gradient boosted regression tree (GBRT) Permission to make digital or hard copies of part"
7689,unknown,"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD "
7690,unknown,"KDD ’16, August 13-17, 2016, San Francisco, CA, USA c⃝2016 Copyright held by the owner/author(s). ACM ISBN . DOI: problems. Besides being used as a stand-alone predictor, it is also incorporated into real-world production pipelines for ad click through rate prediction [15]. Finally, it is the de- facto choice of ensemble method and is used in challenges such as the Netﬂix prize [3]. In this paper,"
7691,unknown,"such as the Netﬂix prize [3]. In this paper, we describe XGBoost, a scalable machine learning system for tree boosting. The system is available as an open source package2. The impact of the system has been widely recognized in a number of machine learning and data mining challenges. Take the challenges hosted by the ma- chine learning competition site Kaggle for example. Among the 29 challenge win"
7692,unknown,"blog during 2015, 17 solutions used XGBoost. Among these solutions, eight solely used XGBoost to train the model, while most others combined XGBoost with neural nets in en- sembles. For comparison, the second most popular method, deep neural nets, was used in 11 solutions. The success of the system was also witnessed in KDDCup 2015, where XGBoost was used by every winning team in the top-10. Moreo"
7693,unknown,"Moreover, the winning teams reported that ensemble meth- ods outperform a well-conﬁgured XGBoost by only a small amount [1]. These results demonstrate that our system gives state-of- the-art results on a wide range of problems. Examples of the problems in these winning solutions include: store sales prediction; high energy physics event classiﬁcation; web text classiﬁcation; customer behavior pred"
7694,unknown,"tion; ad click through rate prediction; malware classiﬁcation; product categorization; hazard risk prediction; massive on- line course dropout rate prediction. While domain depen- dent data analysis and feature engineering play an important role in these solutions, the fact that XGBoost is the consen- sus choice of learner shows the impact and importance of our system and tree boosting. The most i"
7695,unknown,our system and tree boosting. The most important factor behind the success of XGBoost is its scalability in all scenarios. The system runs more than ten times faster than existing popular solutions on a single machine and scales to billions of examples in distributed or memory-limited settings. The scalability of XGBoost is due to several important systems and algorithmic optimizations. These inno
7696,unknown,"These innovations include: a novel tree learning algorithm is for handling sparse data; a theoretically justiﬁed weighted quantile sketch procedure enables handling instance weights in approximate tree learning. Parallel and distributed com- puting makes learning faster which enables quicker model ex- ploration. More importantly, XGBoost exploits out-of-core 2https://github.com/dmlc/xgboost 3Solut"
7697,unknown,"3Solutions come from of top-3 teams of each competitions. arXiv:1603.02754v3 [cs.LG] 10 Jun 2016 computation and enables data scientists to process hundred millions of examples on a desktop. Finally, it is even more exciting to combine these techniques to make an end-to-end system that scales to even larger data with the least amount of cluster resources. The major contributions of this paper is l"
7698,unknown,is listed as follows: •We design and build a highly scalable end-to-end tree boosting system. •We propose a theoretically justiﬁed weighted quantile sketch for eﬃcient proposal calculation. •We introduce a novel sparsity-aware algorithm for par- allel tree learning. •We propose an eﬀective cache-aware block structure for out-of-core tree learning. While there are some existing works on parallel tr
7699,unknown,"ing [22, 23, 19], the directions such as out-of-core compu- tation, cache-aware and sparsity-aware learning have not been explored. More importantly, an end-to-end system that combines all of these aspects gives a novel solution for real-world use-cases. This enables data scientists as well as researchers to build powerful variants of tree boosting al- gorithms [7, 8]. Besides these major contribu"
7700,unknown,"make additional improvements in proposing a regularized learning objective, which we will include for completeness. The remainder of the paper is organized as follows. We will ﬁrst review tree boosting and introduce a regularized objective in Sec. 2. We then describe the split ﬁnding meth- ods in Sec. 3 as well as the system design in Sec. 4, including experimental results when relevant to provide"
7701,unknown,support for each optimization we describe. Related work is discussed in Sec. 5. Detailed end-to-end evaluations are included in Sec. 6. Finally we conclude the paper in Sec. 7. 2. TREE BOOSTING IN A NUTSHELL We review gradient tree boosting algorithms in this sec- tion. The derivation follows from the same idea in existing literatures in gradient boosting. Specicially the second order method is or
7702,unknown,"method is originated from Friedman et al. [12]. We make mi- nor improvements in the reguralized objective, which were found helpful in practice. 2.1 Regularized Learning Objective For a given data set with n examples and m features D= {(xi,yi)}(|D|= n,xi ∈Rm,yi ∈R), a tree ensem- ble model (shown in Fig. 1) uses K additive functions to predict the output. ˆyi = φ(xi) = K∑ k=1 fk(xi), fk ∈F, (1) wh"
7703,unknown,"ˆyi = φ(xi) = K∑ k=1 fk(xi), fk ∈F, (1) where F = {f(x) = wq(x)}(q : Rm →T,w ∈RT) is the space of regression trees (also known as CART). Here q rep- resents the structure of each tree that maps an example to the corresponding leaf index. T is the number of leaves in the tree. Each fk corresponds to an independent tree structure q and leaf weights w. Unlike decision trees, each regression tree cont"
7704,unknown,"tree contains a continuous score on each of the leaf, we use wi to represent score on i-th leaf. For a given example, we will use the decision rules in the trees (given byq) to classify Figure 1: Tree Ensemble Model. The ﬁnal predic- tion for a given example is the sum of predictions from each tree. it into the leaves and calculate the ﬁnal prediction by sum- ming up the score in the corresponding"
7705,unknown,"To learn the set of functions used in the model, we minimize the following regularized objective. L(φ) = ∑ i l(ˆyi,yi) + ∑ k Ω(fk) where Ω(f) = γT + 1 2λ∥w∥2 (2) Here l is a diﬀerentiable convex loss function that measures the diﬀerence between the prediction ˆyi and the target yi. The second term Ω penalizes the complexity of the model (i.e., the regression tree functions). The additional regular"
7706,unknown,"ization term helps to smooth the ﬁnal learnt weights to avoid over-ﬁtting. Intuitively, the regularized objective will tend to select a model employing simple and predictive functions. A similar regularization technique has been used in Regu- larized greedy forest (RGF) [25] model. Our objective and the corresponding learning algorithm is simpler than RGF and easier to parallelize. When the regula"
7707,unknown,"ter is set to zero, the objective falls back to the traditional gradient tree boosting. 2.2 Gradient Tree Boosting The tree ensemble model in Eq. (2) includes functions as parameters and cannot be optimized using traditional opti- mization methods in Euclidean space. Instead, the model is trained in an additive manner. Formally, let ˆ y(t) i be the prediction of the i-th instance at the t-th itera"
7708,unknown,"need to add ft to minimize the following objective. L(t) = n∑ i=1 l(yi, ˆyi (t−1) + ft(xi)) + Ω(ft) This means we greedily add the ft that most improves our model according to Eq. (2). Second-order approximation can be used to quickly optimize the objective in the general setting [12]. L(t) ≃ n∑ i=1 [l(yi,ˆy(t−1)) + gift(xi) + 1 2hif2 t(xi)] + Ω(ft) where gi = ∂ˆy(t−1) l(yi,ˆy(t−1)) and hi = ∂2 ˆy"
7709,unknown,"ˆy(t−1) l(yi,ˆy(t−1)) are ﬁrst and second order gradient statistics on the loss func- tion. We can remove the constant terms to obtain the fol- lowing simpliﬁed objective at step t. ˜L(t) = n∑ i=1 [gift(xi) + 1 2hif2 t(xi)] + Ω(ft) (3) Figure 2: Structure Score Calculation. We only need to sum up the gradient and second order gra- dient statistics on each leaf, then apply the scoring formula to ge"
7710,unknown,"formula to get the quality score. Deﬁne Ij = {i|q(xi) = j}as the instance set of leaf j. We can rewrite Eq (3) by expanding Ω as follows ˜L(t) = n∑ i=1 [gift(xi) + 1 2hif2 t(xi)] + γT + 1 2λ T∑ j=1 w2 j = T∑ j=1 [( ∑ i∈Ij gi)wj + 1 2( ∑ i∈Ij hi + λ)w2 j] + γT (4) For a ﬁxed structure q(x), we can compute the optimal weight w∗ j of leaf j by w∗ j = − ∑ i∈Ij gi ∑ i∈Ij hi + λ, (5) and calculate the c"
7711,unknown,"˜L(t)(q) = −1 2 T∑ j=1 (∑ i∈Ij gi)2 ∑ i∈Ij hi + λ + γT. (6) Eq (6) can be used as a scoring function to measure the quality of a tree structure q. This score is like the impurity score for evaluating decision trees, except that it is derived for a wider range of objective functions. Fig. 2 illustrates how this score can be calculated. Normally it is impossible to enumerate all the possible tree st"
7712,unknown,"tree structures q. A greedy algorithm that starts from a single leaf and iteratively adds branches to the tree is used instead. Assume that IL and IR are the instance sets of left and right nodes after the split. Lettting I = IL ∪IR, then the loss reduction after the split is given by Lsplit = 1 2 [ (∑ i∈IL gi)2 ∑ i∈IL hi + λ + (∑ i∈IR gi)2 ∑ i∈IR hi + λ − (∑ i∈I gi)2 ∑ i∈I hi + λ ] −γ (7) This fo"
7713,unknown,"∑ i∈I hi + λ ] −γ (7) This formula is usually used in practice for evaluating the split candidates. 2.3 Shrinkage and Column Subsampling Besides the regularized objective mentioned in Sec. 2.1, two additional techniques are used to further prevent over- ﬁtting. The ﬁrst technique is shrinkage introduced by Fried- man [11]. Shrinkage scales newly added weights by a factor η after each step of tree "
7714,unknown,"in tochastic optimization, shrinkage reduces the inﬂuence of each individual tree and leaves space for future trees to im- prove the model. The second technique is column (feature) subsampling. This technique is used in RandomForest [4, Algorithm 1: Exact Greedy Algorithm for Split Finding Input: I, instance set of current node Input: d, feature dimension gain←0 G←∑ i∈I gi, H ←∑ i∈I hi for k= 1 to"
7715,unknown,"GL ←0, HL ←0 for j in sorted(I, by xjk) do GL ←GL + gj, HL ←HL + hj GR ←G−GL, HR ←H−HL score←max(score, G2 L HL+λ + G2 R HR+λ − G2 H+λ) end end Output: Split with max score Algorithm 2: Approximate Algorithm for Split Finding for k= 1 to m do Propose Sk = {sk1,sk2,···skl}by percentiles on feature k. Proposal can be done per tree (global), or per split(local). end for k= 1 to m do Gkv ←= ∑ j∈{j|sk,"
7716,unknown,"end for k= 1 to m do Gkv ←= ∑ j∈{j|sk,v≥xjk>sk,v−1}gj Hkv ←= ∑ j∈{j|sk,v≥xjk>sk,v−1}hj end Follow same step as in previous section to ﬁnd max score only among proposed splits. 13], It is implemented in a commercial software TreeNet 4 for gradient boosting, but is not implemented in existing opensource packages. According to user feedback, using col- umn sub-sampling prevents over-ﬁtting even more "
7717,unknown,"the traditional row sub-sampling (which is also supported). The usage of column sub-samples also speeds up computa- tions of the parallel algorithm described later. 3. SPLIT FINDING ALGORITHMS 3.1 Basic Exact Greedy Algorithm One of the key problems in tree learning is to ﬁnd the best split as indicated by Eq (7). In order to do so, a split ﬁnding algorithm enumerates over all the possible splits "
7718,unknown,"all the features. We call this the exact greedy algorithm. Most existing single machine tree boosting implementations, such as scikit-learn [20], R’s gbm [21] as well as the single machine version of XGBoost support the exact greedy algo- rithm. The exact greedy algorithm is shown in Alg. 1. It is computationally demanding to enumerate all the possible splits for continuous features. In order to d"
7719,unknown,"the algorithm must ﬁrst sort the data according to feature values and visit the data in sorted order to accumulate the gradient statistics for the structure score in Eq (7). 3.2 Approximate Algorithm The exact greedy algorithm is very powerful since it enu- merates over all possible splitting points greedily. However, it is impossible to eﬃciently do so when the data does not ﬁt entirely into memo"
7720,unknown,4https://www.salford-systems.com/products/treenet 0 10 20 30 40 50 60 70 80 90 Number of Iterations 0.75 0.76 0.77 0.78 0.79 0.80 0.81 0.82 0.83 Test AUC exact greedy global eps=0.3 local eps=0.3 global eps=0.05 Figure 3: Comparison of test AUC convergence on Higgs 10M dataset. The eps parameter corresponds to the accuracy of the approximate sketch. This roughly translates to 1 / eps buckets in th
7721,unknown,"We ﬁnd that local proposals require fewer buckets, because it reﬁne split candidates. tributed setting. To support eﬀective gradient tree boosting in these two settings, an approximate algorithm is needed. We summarize an approximate framework, which resem- bles the ideas proposed in past literatures [17, 2, 22], in Alg. 2. To summarize, the algorithm ﬁrst proposes can- didate splitting points acc"
7722,unknown,"distribution (a speciﬁc criteria will be given in Sec. 3.3). The algorithm then maps the continuous features into buck- ets split by these candidate points, aggregates the statistics and ﬁnds the best solution among proposals based on the aggregated statistics. There are two variants of the algorithm, depending on when the proposal is given. The global variant proposes all the candidate splits dur"
7723,unknown,"the candidate splits during the initial phase of tree construc- tion, and uses the same proposals for split ﬁnding at all lev- els. The local variant re-proposes after each split. The global method requires less proposal steps than the local method. However, usually more candidate points are needed for the global proposal because candidates are not reﬁned after each split. The local proposal reﬁne"
7724,unknown,"and can potentially be more appropriate for deeper trees. A comparison of diﬀerent algorithms on a Higgs boson dataset is given by Fig. 3. We ﬁnd that the local proposal indeed requires fewer candidates. The global proposal can be as accurate as the local one given enough candidates. Most existing approximate algorithms for distributed tree learning also follow this framework. Notably, it is also "
7725,unknown,"ble to directly construct approximate histograms of gradient statistics [22]. It is also possible to use other variants of bin- ning strategies instead of quantile [17]. Quantile strategy beneﬁt from being distributable and recomputable, which we will detail in next subsection. From Fig. 3, we also ﬁnd that the quantile strategy can get the same accuracy as exact greedy given reasonable approximat"
7726,unknown,"Our system eﬃciently supports exact greedy for the single machine setting, as well as approximate algorithm with both local and global proposal methods for all settings. Users can freely choose between the methods according to their needs. 3.3 Weighted Quantile Sketch One important step in the approximate algorithm is to propose candidate split points. Usually percentiles of a fea- Figure 4: Tree "
7727,unknown,"example will be classiﬁed into the default direction when the feature needed for the split is missing. ture are used to make candidates distribute evenly on the data. Formally, let multi-setDk = {(x1k,h1),(x2k,h2) ···(xnk,hn)} represent the k-th feature values and second order gradient statistics of each training instances. We can deﬁne a rank functions rk : R →[0,+∞) as rk(z) = 1∑ (x,h)∈Dk h ∑ (x"
7728,unknown,"(x,h)∈Dk h ∑ (x,h)∈Dk,x<z h, (8) which represents the proportion of instances whose feature value k is smaller than z. The goal is to ﬁnd candidate split points {sk1,sk2,···skl}, such that |rk(sk,j) −rk(sk,j+1)|<ϵ, s k1 = min i xik,skl = max i xik. (9) Here ϵ is an approximation factor. Intuitively, this means that there is roughly 1 /ϵ candidate points. Here each data point is weighted byhi. To s"
7729,unknown,"point is weighted byhi. To see why hi represents the weight, we can rewrite Eq (3) as n∑ i=1 1 2hi(ft(xi) −gi/hi)2 + Ω(ft) + constant, which is exactly weighted squared loss with labels gi/hi and weights hi. For large datasets, it is non-trivial to ﬁnd candidate splits that satisfy the criteria. When every in- stance has equal weights, an existing algorithm called quan- tile sketch [14, 24] solves"
7730,unknown,"existing quantile sketch for the weighted datasets. There- fore, most existing approximate algorithms either resorted to sorting on a random subset of data which have a chance of failure or heuristics that do not have theoretical guarantee. To solve this problem, we introduced a novel distributed weighted quantile sketch algorithm that can handle weighted data with a provable theoretical guarantee"
7731,unknown,"is to propose a data structure that supportsmerge and prune operations, with each operation proven to maintain a certain accuracy level. A detailed description of the algorithm as well as proofs are given in the appendix. 3.4 Sparsity-aware Split Finding In many real-world problems, it is quite common for the input x to be sparse. There are multiple possible causes for sparsity: 1) presence of mis"
7732,unknown,"frequent zero entries in the statistics; and, 3) artifacts of feature engineering such as one-hot encoding. It is impor- tant to make the algorithm aware of the sparsity pattern in the data. In order to do so, we propose to add a default direction in each tree node, which is shown in Fig. 4. When a value is missing in the sparse matrix x, the instance is classiﬁed into the default direction. There"
7733,unknown,"classiﬁed into the default direction. There are two choices Figure 6: Block structure for parallel learning. Each column in a block is sorted by the corresponding feature value. A linear scan over one column in the block is suﬃcient to enumerate all the split points. Algorithm 3: Sparsity-aware Split Finding Input: I, instance set of current node Input: Ik = {i∈I|xik ̸= missing} Input: d, feature "
7734,unknown,"Input: d, feature dimension Also applies to the approximate setting, only collect statistics of non-missing entries into buckets gain←0 G←∑ i∈I,gi,H ←∑ i∈I hi for k= 1 to m do // enumerate missing value goto right GL ←0, HL ←0 for j in sorted(Ik, ascent order byxjk) do GL ←GL + gj, HL ←HL + hj GR ←G−GL, HR ←H−HL score←max(score, G2 L HL+λ + G2 R HR+λ − G2 H+λ) end // enumerate missing value goto l"
7735,unknown,"GR ←0, HR ←0 for j in sorted(Ik, descent order byxjk) do GR ←GR + gj, HR ←HR + hj GL ←G−GR, HL ←H−HR score←max(score, G2 L HL+λ + G2 R HR+λ − G2 H+λ) end end Output: Split and default directions with max gain of default direction in each branch. The optimal default di- rections are learnt from the data. The algorithm is shown in Alg. 3. The key improvement is to only visit the non-missing entries "
7736,unknown,"entries Ik. The presented algorithm treats the non-presence as a missing value and learns the best direction to handle missing values. The same algorithm can also be applied when the non-presence corresponds to a user speciﬁed value by limiting the enumeration only to consistent solutions. To the best of our knowledge, most existing tree learn- ing algorithms are either only optimized for dense da"
7737,unknown,"need speciﬁc procedures to handle limited cases such as cat- egorical encoding. XGBoost handles all sparsity patterns in a uniﬁed way. More importantly, our method exploits the sparsity to make computation complexity linear to number of non-missing entries in the input. Fig. 5 shows the com- parison of sparsity aware and a naive implementation on an Allstate-10K dataset (description of dataset giv"
7738,unknown,We ﬁnd that the sparsity aware algorithm runs 50 times faster than the naive version. This conﬁrms the importance of the sparsity aware algorithm. 1 2 4 8 16 Number of Threads 0.03125 0.0625 0.125 0.25 0.5 1 2 4 8 16 32 Time per Tree(sec) Sparsity aware algorithm Basic algorithm Figure 5: Impact of the sparsity aware algorithm on Allstate-10K. The dataset is sparse mainly due to one-hot encoding. 
7739,unknown,"is more than 50 times faster than the naive version that does not take sparsity into consideration. 4. SYSTEM DESIGN 4.1 Column Block for Parallel Learning The most time consuming part of tree learning is to get the data into sorted order. In order to reduce the cost of sorting, we propose to store the data in in-memory units, which we called block. Data in each block is stored in the compressed c"
7740,unknown,"compressed column (CSC) format, with each column sorted by the corresponding feature value. This input data layout only needs to be computed once before training, and can be reused in later iterations. In the exact greedy algorithm, we store the entire dataset in a single block and run the split search algorithm by lin- early scanning over the pre-sorted entries. We do the split ﬁnding of all leav"
7741,unknown,"ﬁnding of all leaves collectively, so one scan over the block will collect the statistics of the split candidates in all leaf branches. Fig. 6 shows how we transform a dataset into the format and ﬁnd the optimal split using the block structure. The block structure also helps when using the approxi- mate algorithms. Multiple blocks can be used in this case, with each block corresponding to subset o"
7742,unknown,"Diﬀerent blocks can be distributed across machines, or stored on disk in the out-of-core setting. Using the sorted struc- ture, the quantile ﬁnding step becomes a linear scan over the sorted columns. This is especially valuable for local pro- posal algorithms, where candidates are generated frequently at each branch. The binary search in histogram aggregation also becomes a linear time merge style"
7743,unknown,"Collecting statistics for each column can be parallelized, giving us a parallel algorithm for split ﬁnding. Importantly, the column block structure also supports column subsam- pling, as it is easy to select a subset of columns in a block. 1 2 4 8 16 Number of Threads 8 16 32 64 128Time per Tree(sec) Basic algorithm Cache-aware algorithm (a) Allstate 10M 1 2 4 8 16 Number of Threads 8 16 32 64 128"
7744,unknown,8 16 32 64 128 256Time per Tree(sec) Basic algorithm Cache-aware algorithm (b) Higgs 10M 1 2 4 8 16 Number of Threads 0.25 0.5 1 2 4 8 Time per Tree(sec) Basic algorithm Cache-aware algorithm (c) Allstate 1M 1 2 4 8 16 Number of Threads 0.25 0.5 1 2 4 8 Time per Tree(sec) Basic algorithm Cache-aware algorithm (d) Higgs 1M Figure 7: Impact of cache-aware prefetching in exact greedy algorithm. We ﬁn
7745,unknown,"impacts the performance on the large datasets (10 million instances). Using cache aware prefetching improves the performance by factor of two when the dataset is large. Figure 8: Short range data dependency pattern that can cause stall due to cache miss. Time Complexity AnalysisLet dbe the maximum depth of the tree and K be total number of trees. For the ex- act greedy algorithm, the time complexi"
7746,unknown,"aware algorithm is O(Kd∥x∥0 log n). Here we use ∥x∥0 to denote number of non-missing entries in the training data. On the other hand, tree boosting on the block structure only cost O(Kd∥x∥0 + ∥x∥0 log n). Here O(∥x∥0 log n) is the one time preprocessing cost that can be amortized. This analysis shows that the block structure helps to save an additional log n factor, which is signiﬁcant when n is l"
7747,unknown,"approximate algorithm, the time complexity of original al- gorithm with binary search is O(Kd∥x∥0 log q). Here q is the number of proposal candidates in the dataset. While q is usually between 32 and 100, the log factor still introduces overhead. Using the block structure, we can reduce the time to O(Kd∥x∥0 + ∥x∥0 log B), where B is the maximum num- ber of rows in each block. Again we can save the"
7748,unknown,"log q factor in computation. 4.2 Cache-aware Access While the proposed block structure helps optimize the computation complexity of split ﬁnding, the new algorithm requires indirect fetches of gradient statistics by row index, since these values are accessed in order of feature. This is a non-continuous memory access. A naive implementation of split enumeration introduces immediate read/write de- "
7749,unknown,"pendency between the accumulation and the non-continuous memory fetch operation (see Fig. 8). This slows down split ﬁnding when the gradient statistics do not ﬁt into CPU cache and cache miss occur. For the exact greedy algorithm, we can alleviate the prob- lem by a cache-aware prefetching algorithm. Speciﬁcally, we allocate an internal buﬀer in each thread, fetch the gra- dient statistics into it"
7750,unknown,a mini-batch manner. This prefetching changes the direct read/write dependency to a longer dependency and helps to reduce the runtime overhead when number of rows in the is large. Figure 7 gives the comparison of cache-aware vs. 1 2 4 8 16 Number of Threads 4 8 16 32 64 128Time per Tree(sec) block size=2^12 block size=2^16 block size=2^20 block size=2^24 (a) Allstate 10M 1 2 4 8 16 Number of Threa
7751,unknown,"4 8 16 32 64 128 256 512Time per Tree(sec) block size=2^12 block size=2^16 block size=2^20 block size=2^24 (b) Higgs 10M Figure 9: The impact of block size in the approxi- mate algorithm. We ﬁnd that overly small blocks re- sults in ineﬃcient parallelization, while overly large blocks also slows down training due to cache misses. non cache-aware algorithm on the the Higgs and the All- state datase"
7752,unknown,"state dataset. We ﬁnd that cache-aware implementation of the exact greedy algorithm runs twice as fast as the naive version when the dataset is large. For approximate algorithms, we solve the problem by choos- ing a correct block size. We deﬁne the block size to be max- imum number of examples in contained in a block, as this reﬂects the cache storage cost of gradient statistics. Choos- ing an ove"
7753,unknown,"ing an overly small block size results in small workload for each thread and leads to ineﬃcient parallelization. On the other hand, overly large blocks result in cache misses, as the gradient statistics do not ﬁt into the CPU cache. A good choice of block size balances these two factors. We compared various choices of block size on two data sets. The results are given in Fig. 9. This result valida"
7754,unknown,Table 1: Comparison of major tree boosting systems. System exact greedy approximate global approximate local out-of-core sparsity aware parallel XGBoost yes yes yes yes yes yes pGBRT no no yes no no yes Spark MLLib no yes no no partially yes H2O no yes no no partially yes scikit-learn yes no no no no no R GBM yes no no no partially no shows that choosing 2 16 examples per block balances the cache 
7755,unknown,"cache property and parallelization. 4.3 Blocks for Out-of-core Computation One goal of our system is to fully utilize a machine’s re- sources to achieve scalable learning. Besides processors and memory, it is important to utilize disk space to handle data that does not ﬁt into main memory. To enable out-of-core computation, we divide the data into multiple blocks and store each block on disk. Duri"
7756,unknown,"tant to use an independent thread to pre-fetch the block into a main memory buﬀer, so computation can happen in con- currence with disk reading. However, this does not entirely solve the problem since the disk reading takes most of the computation time. It is important to reduce the overhead and increase the throughput of disk IO. We mainly use two techniques to improve the out-of-core computation"
7757,unknown,"Block Compression The ﬁrst technique we use is block compression. The block is compressed by columns, and de- compressed on the ﬂy by an independent thread when load- ing into main memory. This helps to trade some of the computation in decompression with the disk reading cost. We use a general purpose compression algorithm for com- pressing the features values. For the row index, we substract the "
7758,unknown,"the row index by the begining index of the block and use a 16bit integer to store each oﬀset. This requires 216 examples per block, which is conﬁrmed to be a good setting. In most of the dataset we tested, we achieve roughly a 26% to 29% compression ratio. Block Sharding The second technique is to shard the data onto multiple disks in an alternative manner. A pre-fetcher thread is assigned to each"
7759,unknown,"in-memory buﬀer. The training thread then alternatively reads the data from each buﬀer. This helps to increase the throughput of disk reading when multiple disks are available. 5. RELATED WORKS Our system implements gradient boosting [10], which per- forms additive optimization in functional space. Gradient tree boosting has been successfully used in classiﬁcation [12], learning to rank [5], struc"
7760,unknown,"ﬁelds. XGBoost incorporates a regularized model to prevent overﬁtting. This this resembles previous work on regularized greedy forest [25], but simpliﬁes the objective and algorithm for parallelization. Column sampling is a simple but eﬀective technique borrowed from RandomForest [4]. While sparsity- aware learning is essential in other types of models such as linear models [9], few works on tree "
7761,unknown,"this topic in a principled way. The algorithm proposed in this paper is the ﬁrst uniﬁed approach to handle all kinds of sparsity patterns. There are several existing works on parallelizing tree learn- ing [22, 19]. Most of these algorithms fall into the ap- proximate framework described in this paper. Notably, it is also possible to partition data by columns [23] and ap- ply the exact greedy algor"
7762,unknown,"our framework, and the techniques such as cache-aware pre- fecthing can be used to beneﬁt this type of algorithm. While most existing works focus on the algorithmic aspect of par- allelization, our work improves in two unexplored system di- rections: out-of-core computation and cache-aware learning. This gives us insights on how the system and the algorithm can be jointly optimized and provides an"
7763,unknown,"that can handle large scale problems with very limited com- puting resources. We also summarize the comparison be- tween our system and existing opensource implementations in Table 1. Quantile summary (without weights) is a classical prob- lem in the database community [14, 24]. However, the ap- proximate tree boosting algorithm reveals a more general problem – ﬁnding quantiles on weighted data. T"
7764,unknown,"of our knowledge, the weighted quantile sketch proposed in this paper is the ﬁrst method to solve this problem. The weighted quantile summary is also not speciﬁc to the tree learning and can beneﬁt other applications in data science and machine learning in the future. 6. END TO END EV ALUATIONS 6.1 System Implementation We implemented XGBoost as an open source package 5. The package is portable an"
7765,unknown,"weighted classiﬁcation and rank objective functions, as well as user deﬁned objective function. It is available in popular languages such as python, R, Julia and integrates naturally with language native data science pipelines such as scikit- learn. The distributed version is built on top of the rabit library6 for allreduce. The portability of XGBoost makes it available in many ecosystems, instead"
7766,unknown,"a speciﬁc platform. The distributed XGBoost runs natively on Hadoop, MPI Sun Grid engine. Recently, we also enable distributed XGBoost on jvm bigdata stacks such as Flink and Spark. The distributed version has also been integrated into cloud platform Tianchi 7 of Alibaba. We believe that there will be more integrations in the future. 6.2 Dataset and Setup 5https://github.com/dmlc/xgboost 6https://"
7767,unknown,"7https://tianchi.aliyun.com Table 2: Dataset used in the Experiments. Dataset n m Task Allstate 10 M 4227 Insurance claim classiﬁcation Higgs Boson 10 M 28 Event classiﬁcation Yahoo LTRC 473K 700 Learning to Rank Criteo 1.7 B 67 Click through rate prediction We used four datasets in our experiments. A summary of these datasets is given in Table 2. In some of the experi- ments, we use a randomly se"
7768,unknown,due to slow baselines or to demonstrate the performance of the algorithm with varying dataset size. We use a suﬃx to denote the size in these cases. For example Allstate-10K means a subset of the Allstate dataset with 10K instances. The ﬁrst dataset we use is the Allstate insurance claim dataset8. The task is to predict the likelihood and cost of an insurance claim given diﬀerent risk factors. In 
7769,unknown,"iment, we simpliﬁed the task to only predict the likelihood of an insurance claim. This dataset is used to evaluate the impact of sparsity-aware algorithm in Sec. 3.4. Most of the sparse features in this data come from one-hot encoding. We randomly select 10M instances as training set and use the rest as evaluation set. The second dataset is the Higgs boson dataset9 from high energy physics. The d"
7770,unknown,simulations of physics events. It contains 21 kinematic prop- erties measured by the particle detectors in the accelerator. It also contains seven additional derived physics quantities of the particles. The task is to classify whether an event corresponds to the Higgs boson. We randomly select 10M instances as training set and use the rest as evaluation set. The third dataset is the Yahoo! learnin
7771,unknown,"dataset [6], which is one of the most commonly used bench- marks in learning to rank algorithms. The dataset contains 20K web search queries, with each query corresponding to a list of around 22 documents. The task is to rank the docu- ments according to relevance of the query. We use the oﬃcial train test split in our experiment. The last dataset is the criteo terabyte click log dataset 10. We us"
7772,unknown,"We use this dataset to evaluate the scaling property of the system in the out-of-core and the distributed settings. The data contains 13 integer features and 26 ID features of user, item and advertiser information. Since a tree based model is better at handling continuous features, we preprocess the data by calculating the statistics of average CTR and count of ID features on the ﬁrst ten days, re"
7773,unknown,"tures by the corresponding count statistics during the next ten days for training. The training set after preprocessing contains 1.7 billion instances with 67 features (13 integer, 26 average CTR statistics and 26 counts). The entire dataset is more than one terabyte in LibSVM format. We use the ﬁrst three datasets for the single machine par- allel setting, and the last dataset for the distributed"
7774,unknown,"out-of-core settings. All the single machine experiments are conducted on a Dell PowerEdge R420 with two eight-core Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory. If not speciﬁed, all the experiments are run using all the avail- 8https://www.kaggle.com/c/ClaimPredictionChallenge 9https://archive.ics.uci.edu/ml/datasets/HIGGS 10http://labs.criteo.com/downloads/download-terabyte- click-logs/ Tabl"
7775,unknown,click-logs/ Table 3: Comparison of Exact Greedy Methods with 500 trees on Higgs-1M data. Method Time per Tree (sec) Test AUC XGBoost 0.6841 0.8304 XGBoost (colsample=0.5) 0.6401 0.8245 scikit-learn 28.51 0.8302 R.gbm 1.032 0.6224 1 2 4 8 16 Number of Threads 0.5 1 2 4 8 16 32Time per Tree(sec) XGBoost pGBRT Figure 10: Comparison between XGBoost and pG- BRT on Yahoo LTRC dataset. Table 4: Compariso
7776,unknown,"trees on Yahoo! LTRC Dataset Method Time per Tree (sec) NDCG@10 XGBoost 0.826 0.7892 XGBoost (colsample=0.5) 0.506 0.7913 pGBRT [22] 2.576 0.7915 able cores in the machine. The machine settings of the dis- tributed and the out-of-core experiments will be described in the corresponding section. In all the experiments, we boost trees with a common setting of maximum depth equals 8, shrinkage equals "
7777,unknown,"plicitly speciﬁed. We can ﬁnd similar results when we use other settings of maximum depth. 6.3 Classiﬁcation In this section, we evaluate the performance of XGBoost on a single machine using the exact greedy algorithm on Higgs-1M data, by comparing it against two other commonly used exact greedy tree boosting implementations. Since scikit-learn only handles non-sparse input, we choose the dense Hi"
7778,unknown,"dense Higgs dataset for a fair comparison. We use the 1M subset to make scikit-learn ﬁnish running in reasonable time. Among the methods in comparison, R’s GBM uses a greedy approach that only expands one branch of a tree, which makes it faster but can result in lower accuracy, while both scikit-learn and XGBoost learn a full tree. The results are shown in Table 3. Both XGBoost and scikit-learn gi"
7779,unknown,"performance than R’s GBM, while XGBoost runs more than 10x faster than scikit-learn. In this experiment, we also ﬁnd column subsamples gives slightly worse performance than using all the features. This could due to the fact that there are few important features in this dataset and we can beneﬁt from greedily select from all the features. 6.4 Learning to Rank We next evaluate the performance of XGB"
7780,unknown,128 256 512 1024 2048 Number of Training Examples (million) 128 256 512 1024 2048 4096Time per Tree(sec) Basic algorithm Block compression Compression+shard Out of system file cache start from this point Figure 11: Comparison of out-of-core methods on diﬀerent subsets of criteo data. The missing data points are due to out of disk space. We can ﬁnd that basic algorithm can only handle 200M exam- pl
7781,unknown,"ples. Adding compression gives 3x speedup, and sharding into two disks gives another 2x speedup. The system runs out of ﬁle cache start from 400M examples. The algorithm really has to rely on disk after this point. The compression+shard method has a less dramatic slowdown when running out of ﬁle cache, and exhibits a linear trend afterwards. learning to rank problem. We compare against pGBRT [22],"
7782,unknown,"the best previously pubished system on this task. XGBoost runs exact greedy algorithm, while pGBRT only support an approximate algorithm. The results are shown in Table 4 and Fig. 10. We ﬁnd that XGBoost runs faster. Interest- ingly, subsampling columns not only reduces running time, and but also gives a bit higher performance for this prob- lem. This could due to the fact that the subsampling hel"
7783,unknown,"prevent overﬁtting, which is observed by many of the users. 6.5 Out-of-core Experiment We also evaluate our system in the out-of-core setting on the criteo data. We conducted the experiment on one AWS c3.8xlarge machine (32 vcores, two 320 GB SSD, 60 GB RAM). The results are shown in Figure 11. We can ﬁnd that compression helps to speed up computation by factor of three, and sharding into two disk"
7784,unknown,"For this type of experiment, it is important to use a very large dataset to drain the system ﬁle cache for a real out- of-core setting. This is indeed our setup. We can observe a transition point when the system runs out of ﬁle cache. Note that the transition in the ﬁnal method is less dramatic. This is due to larger disk throughput and better utilization of computation resources. Our ﬁnal method "
7785,unknown,"1.7 billion examples on a single machine. 6.6 Distributed Experiment Finally, we evaluate the system in the distributed setting. We set up a YARN cluster on EC2 with m3.2xlarge ma- chines, which is a very common choice for clusters. Each machine contains 8 virtual cores, 30GB of RAM and two 80GB SSD local disks. The dataset is stored on AWS S3 instead of HDFS to avoid purchasing persistent storage"
7786,unknown,We ﬁrst compare our system against two production-level distributed systems: Spark MLLib [18] and H2O 11. We use 11www.h2o.ai 128 256 512 1024 2048 Number of Training Examples (million) 128 256 512 1024 2048 4096 8192 16384 32768Total Running Time (sec) Spark MLLib H2O XGBoost (a) End-to-end time cost include data loading 128 256 512 1024 2048 Number of Training Examples (million) 8 16 32 64 128 2
7787,unknown,"8 16 32 64 128 256 512 1024 2048 4096Time per Iteration (sec) Spark MLLib H2O XGBoost (b) Per iteration cost exclude data loading Figure 12: Comparison of diﬀerent distributed sys- tems on 32 EC2 nodes for 10 iterations on diﬀerent subset of criteo data. XGBoost runs more 10x than spark per iteration and 2.2x as H2O’s optimized ver- sion (However, H2O is slow in loading the data, get- ting worse e"
7788,unknown,ting worse end-to-end time). Note that spark suﬀers from drastic slow down when running out of mem- ory. XGBoost runs faster and scales smoothly to the full 1.7 billion examples with given resources by utilizing out-of-core computation. 32 m3.2xlarge machines and test the performance of the sys- tems with various input size. Both of the baseline systems are in-memory analytics frameworks that need
7789,unknown,"data in RAM, while XGBoost can switch to out-of-core set- ting when it runs out of memory. The results are shown in Fig. 12. We can ﬁnd that XGBoost runs faster than the baseline systems. More importantly, it is able to take ad- vantage of out-of-core computing and smoothly scale to all 1.7 billion examples with the given limited computing re- sources. The baseline systems are only able to handle "
7790,unknown,"set of the data with the given resources. This experiment shows the advantage to bring all the system improvement together and solve a real-world scale problem. We also eval- uate the scaling property of XGBoost by varying the number of machines. The results are shown in Fig. 13. We can ﬁnd XGBoost’s performance scales linearly as we add more ma- chines. Importantly, XGBoost is able to handle the "
7791,unknown,"1.7 billion data with only four machines. This shows the system’s potential to handle even larger data. 4 8 16 32 Number of Machines 128 256 512 1024 2048Time per Iteration (sec) Figure 13: Scaling of XGBoost with diﬀerent num- ber of machines on criteo full 1.7 billion dataset. Using more machines results in more ﬁle cache and makes the system run faster, causing the trend to be slightly super li"
7792,unknown,"the entire dataset using as little as four machines, and scales smoothly by utilizing more available re- sources. 7. CONCLUSION In this paper, we described the lessons we learnt when building XGBoost, a scalable tree boosting system that is widely used by data scientists and provides state-of-the-art results on many problems. We proposed a novel sparsity aware algorithm for handling sparse data an"
7793,unknown,"justiﬁed weighted quantile sketch for approximate learning. Our experience shows that cache access patterns, data com- pression and sharding are essential elements for building a scalable end-to-end system for tree boosting. These lessons can be applied to other machine learning systems as well. By combining these insights, XGBoost is able to solve real- world scale problems using a minimal amount"
7794,unknown,"Acknowledgments We would like to thank Tyler B. Johnson, Marco Tulio Ribeiro, Sameer Singh, Arvind Krishnamurthy for their valuable feedback. We also sincerely thank Tong He, Bing Xu, Michael Benesty, Yuan Tang, Hongliang Liu, Qiang Kou, Nan Zhu and all other con- tributors in the XGBoost community. This work was supported in part by ONR (PECASE) N000141010672, NSF IIS 1258741 and the TerraSwarm R"
7795,unknown,"and the TerraSwarm Research Center sponsored by MARCO and DARPA. 8. REFERENCES [1] R. Bekkerman. The present and the future of the kdd cup competition: an outsider’s perspective. [2] R. Bekkerman, M. Bilenko, and J. Langford. Scaling Up Machine Learning: Parallel and Distributed Approaches . Cambridge University Press, New York, NY, USA, 2011. [3] J. Bennett and S. Lanning. The netﬂix prize. In Pr"
7796,unknown,"Proceedings of the KDD Cup Workshop 2007 , pages 3–6, New York, Aug. 2007. [4] L. Breiman. Random forests. Maching Learning, 45(1):5–32, Oct. 2001. [5] C. Burges. From ranknet to lambdarank to lambdamart: An overview. Learning, 11:23–581, 2010. [6] O. Chapelle and Y. Chang. Yahoo! Learning to Rank Challenge Overview. Journal of Machine Learning Research - W & CP , 14:1–24, 2011. [7] T. Chen, H. Li"
7797,unknown,"Research - W & CP , 14:1–24, 2011. [7] T. Chen, H. Li, Q. Yang, and Y. Yu. General functional matrix factorization using gradient boosting. In Proceeding of 30th International Conference on Machine Learning (ICML’13), volume 1, pages 436–444, 2013. [8] T. Chen, S. Singh, B. Taskar, and C. Guestrin. Eﬃcient second-order gradient boosting for conditional random ﬁelds. In Proceeding of 18th Artiﬁcial"
7798,unknown,"Statistics Conference (AISTATS’15), volume 1, 2015. [9] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classiﬁcation. Journal of Machine Learning Research , 9:1871–1874, 2008. [10] J. Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics , 29(5):1189–1232, 2001. [11] J. Friedman. Stochastic gradient boostin"
7799,unknown,"Statistics & Data Analysis , 38(4):367–378, 2002. [12] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. Annals of Statistics, 28(2):337–407, 2000. [13] J. H. Friedman and B. E. Popescu. Importance sampled learning ensembles, 2003. [14] M. Greenwald and S. Khanna. Space-eﬃcient online computation of quantile summaries. In Proceedings of the 20"
7800,unknown,"2001 ACM SIGMOD International Conference on Management of Data , pages 58–66, 2001. [15] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, and J. Q. n. Candela. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising , ADKDD’14, 2014. [16] P. Li. Robust Logitboost a"
7801,unknown,"Logitboost. In Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI’10), pages 302–311, 2010. [17] P. Li, Q. Wu, and C. J. Burges. Mcrank: Learning to rank using multiple classiﬁcation and gradient boosting. In Advances in Neural Information Processing Systems 20 , pages 897–904. 2008. [18] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkatara"
7802,unknown,"S. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh, M. Zaharia, and A. Talwalkar. MLlib: Machine learning in apache spark. Journal of Machine Learning Research , 17(34):1–7, 2016. [19] B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo. Planet: Massively parallel learning of tree ensembles with mapreduce. Proceeding of VLDB Endowment, 2(2):1426–1437, Aug. 2009. [20] F. Pedregosa, G. Varoquaux, A. "
7803,unknown,"B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. [21] G. Ridgeway. Generalized Boosted Models: A guide to the gbm package. [22] S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin. Parallel boos"
7804,unknown,"Parallel boosted regression trees for web search ranking. In Proceedings of the 20th international conference on World wide web, pages 387–396. ACM, 2011. [23] J. Ye, J.-H. Chow, J. Chen, and Z. Zheng. Stochastic gradient boosted distributed decision trees. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM ’09. [24] Q. Zhang and W. Wang. A fast algorithm for a"
7805,unknown,"quantiles in high speed data streams. In Proceedings of the 19th International Conference on Scientiﬁc and Statistical Database Management, 2007. [25] T. Zhang and R. Johnson. Learning nonlinear functions using regularized greedy forest. IEEE Transactions on Pattern Analysis and Machine Intelligence , 36(5), 2014. APPENDIX A. WEIGHTED QUANTILE SKETCH In this section, we introduce the weighted quan"
7806,unknown,"rithm. Approximate answer of quantile queries is for many real- world applications. One classical approach to this problem is GK algorithm [14] and extensions based on the GK framework [24]. The main component of these algorithms is a data structure called quantile summary, that is able to answer quantile queries with relative accuracy of ϵ. Two operations are deﬁned for a quantile summary: • A me"
7807,unknown,"summary: • A merge operation that combines two summaries with ap- proximation error ϵ1 and ϵ2 together and create a merged summary with approximation error max( ϵ1,ϵ2). • A prune operation that reduces the number of elements in the summary to b+1 and changes approximation error from ϵ to ϵ+ 1 b. A quantile summary with merge and prune operations forms basic building blocks of the distributed and s"
7808,unknown,"building blocks of the distributed and streaming quantile comput- ing algorithms [24]. In order to use quantile computation for approximate tree boost- ing, we need to ﬁnd quantiles on weighted data. This more gen- eral problem is not supported by any of the existing algorithm. In this section, we describe a non-trivial weighted quantile summary structure to solve this problem. Importantly, the ne"
7809,unknown,"contains merge and prune operations with the same guarantee as GK summary. This allows our summary to be plugged into all the frameworks used GK summary as building block and answer quantile queries over weighted data eﬃciently. A.1 Formalization and Deﬁnitions Given an input multi-set D= {(x1,w1),(x2,w2) ··· (xn,wn)} such that wi ∈[0,+∞),xi ∈X . Each xi corresponds to a po- sition of the point an"
7810,unknown,"we have a total order < deﬁned on X. Let us deﬁne two rank functions r− D,r+ D: X→ [0,+∞) r− D(y) = ∑ (x,w)∈D,x<y w (10) r+ D(y) = ∑ (x,w)∈D,x≤y w (11) We should note that since Dis deﬁned to be a multiset of the points. It can contain multiple record with exactly same position x and weight w. We also deﬁne another weight function ωD : X→ [0,+∞) as ωD(y) = r+ D(y) −r− D(y) = ∑ (x,w)∈D,x=y w. (12) "
7811,unknown,"D(y) = ∑ (x,w)∈D,x=y w. (12) Finally, we also deﬁne the weight of multi-set Dto be the sum of weights of all the points in the set ω(D) = ∑ (x,w)∈D w (13) Our task is given a series of input D, to estimate r+(y) and r−(y) for y∈X as well as ﬁnding points with speciﬁc rank. Given these notations, we deﬁne quantile summary of weighted examples as follows: Definition A.1. Quantile Summary of Weighted"
7812,unknown,"A quantile summary for Dis deﬁned to be tuple Q(D) = (S,˜r+ D,˜r− D,˜ωD), where S = {x1,x2,··· ,xk}is selected from the points in D(i.e. xi ∈{x|(x,w) ∈D}) with the following properties: 1) xi <xi+1 for all i, and x1 and xk are minimum and max- imum point in D: x1 = min (x,w)∈D x, xk = max (x,w)∈D x 2) ˜r+ D, ˜r− D and ˜ωD are functions in S →[0,+∞), that satisﬁes ˜r− D(xi) ≤r− D(xi), ˜r+ D(xi) ≥r+"
7813,unknown,"D(xi), ˜ωD(xi) ≤ωD(xi), (14) the equality sign holds for maximum and minimum point ( ˜r− D(xi) = r− D(xi), ˜r+ D(xi) = r+ D(xi) and ˜ωD(xi) = ωD(xi) for i∈{1,k}). Finally, the function value must also satisfy the following con- straints ˜r− D(xi) + ˜ωD(xi) ≤˜r− D(xi+1), ˜r+ D(xi) ≤˜r+ D(xi+1) −˜ωD(xi+1) (15) Since these functions are only deﬁned onS, it is suﬃce to use 4k record to store the summa"
7814,unknown,"each xi and the corresponding function values of each xi. Definition A.2. Extension of Function Domains Given a quantile summary Q(D) = ( S,˜r+ D,˜r− D,˜ωD) deﬁned in Deﬁnition A.1, the domain of ˜r+ D, ˜r− D and ˜ωD were deﬁned only in S. We extend the deﬁnition of these functions to X→ [0,+∞) as follows When y <x1: ˜r− D(y) = 0, ˜r+ D(y) = 0, ˜ωD(y) = 0 (16) When y >xk: ˜r− D(y) = ˜r+ D(xk), ˜r+"
7815,unknown,"D(y) = ˜r+ D(xk), ˜ωD(y) = 0 (17) When y∈(xi,xi+1) for some i: ˜r− D(y) = ˜r− D(xi) + ˜ωD(xi), ˜r+ D(y) = ˜r+ D(xi+1) −˜ωD(xi+1), ˜ωD(y) = 0 (18) Lemma A.1. Extended Constraint The extended deﬁnition of ˜r− D, ˜r+ D, ˜ωD satisﬁes the following constraints ˜r− D(y) ≤r− D(y), ˜r+ D(y) ≥r+ D(y), ˜ωD(y) ≤ωD(y) (19) ˜r− D(y) + ˜ωD(y) ≤˜r− D(x), ˜r+ D(y) ≤˜r+ D(x) −˜ωD(x), for all y <x (20) Proof. The o"
7816,unknown,"y∈(xi,xi+1): ˜r− D(y) = ˜r− D(xi) + ˜ωD(xi) ≤r− D(xi) + ωD(xi) ≤r− D(y) ˜r+ D(y) = ˜r+ D(xi+1) −˜ωD(xi+1) ≥r+ D(xi+1) −ωD(xi+1) ≥r+ D(y) This proves Eq. (19). Furthermore, we can verify that ˜r+ D(xi) ≤˜r+ D(xi+1) −˜ωD(xi+1) = ˜r+ D(y) −˜ωD(y) ˜r− D(y) + ˜ωD(y) = ˜r− D(xi) + ˜ωD(xi) + 0 ≤˜r− D(xi+1) ˜r+ D(y) = ˜r+ D(xi+1) −˜ωD(xi+1) Using these facts and transitivity of < relation, we can prove Eq"
7817,unknown,"Eq. (20) We should note that the extension is based on the ground case deﬁned in S, and we do not require extra space to store the sum- mary in order to use the extended deﬁnition. We are now ready to introduce the deﬁnition of ϵ-approximate quantile summary. Definition A.3. ϵ-Approximate Quantile Summary Given a quantile summary Q(D) = (S,˜r+ D,˜r− D,˜ωD), we call it is ϵ-approximate summary if f"
7818,unknown,"D,˜ωD), we call it is ϵ-approximate summary if for any y∈X ˜r+ D(y) −˜r− D(y) −˜ωD(y) ≤ϵω(D) (21) We use this deﬁnition since we know that r−(y) ∈[˜r− D(y),˜r+ D(y)− ˜ωD(y)] and r+(y) ∈[˜r− D(y) + ˜ωD(y),˜r+ D(y)]. Eq. (21) means the we can get estimation of r+(y) and r−(y) by error of at most ϵω(D). Lemma A.2. Quantile summary Q(D) = (S,˜r+ D,˜r− D,˜ωD) is an ϵ-approximate summary if and only if "
7819,unknown,"holds ˜r+ D(xi) −˜r− D(xi) −˜ωD(xi) ≤ϵω(D) (22) ˜r+ D(xi+1) −˜r− D(xi) −˜ωD(xi+1) −˜ωD(xi) ≤ϵω(D) (23) Proof. The key is again consider y∈(xi,xi+1) ˜r+ D(y)−˜r− D(y)−˜ωD(y) = [˜r+ D(xi+1)−˜ωD(xi+1)]−[˜r+ D(xi)+˜ωD(xi)]−0 This means the condition in Eq. (23) plus Eq. (22) can give us Eq. (21) Property of Extended FunctionIn this section, we have in- troduced the extension of function ˜r+ D,˜r− D,˜ω"
7820,unknown,"D,˜r− D,˜ωD to X →[0,+∞). The key theme discussed in this section is the relation of con- straints on the original function and constraints on the extended function. Lemma A.1 and A.2 show that the constraints on the original function can lead to in more general constraints on the extended function. This is a very useful property which will be used in the proofs in later sections. A.2 Construction"
7821,unknown,"A.2 Construction of Initial Summary Given a small multi-set D= {(x1,w1),(x2,w2),··· ,(xn,wn)}, we can construct initial summaryQ(D) = {S,˜r+ D,˜r− D,˜ωD}, with S to the set of all values in D(S = {x|(x,w) ∈D}), and ˜r+ D,˜r− D,˜ωD deﬁned to be ˜r+ D(x) = r+ D(x), ˜r− D(x) = r− D(x), ˜ωD(x) = ωD(x) for x∈S (24) The constructed summary is 0-approximate summary, since it can answer all the queries ac"
7822,unknown,"answer all the queries accurately. The constructed summary can be feed into future operations described in the latter sections. A.3 Merge Operation In this section, we deﬁne how we can merge the two summaries together. Assume we have Q(D1) = ( S1,˜r+ D1 ,˜r− D1 ,˜ωD1 ) and Q(D2) = ( S2,˜r+ D1 ,˜r− D2 ,˜ωD2 ) quantile summary of two dataset D1 and D2. Let D= D1 ∪D2, and deﬁne the merged summary Q(D"
7823,unknown,"Q(D) = (S,˜r+ D,˜r− D,˜ωD) as follows. S = {x1,x2 ··· ,xk},xi ∈S1 or xi ∈S2 (25) The points in S are combination of points in S1 and S2. And the function ˜r+ D,˜r− D,˜ωDare deﬁned to be ˜r− D(xi) = ˜r− D1 (xi) + ˜r− D2 (xi) (26) ˜r+ D(xi) = ˜r+ D1 (xi) + ˜r+ D2 (xi) (27) ˜ωD(xi) = ˜ωD1 (xi) + ˜ωD2 (xi) (28) Here we use functions deﬁned on S →[0,+∞) on the left sides of equalities and use the exten"
7824,unknown,"equalities and use the extended function deﬁnitions on the right sides. Due to additive nature of r+, r−and ω, which can be formally written as r− D(y) =r− D1 (y) + r− D2 (y), r+ D(y) =r+ D1 (y) + r+ D2 (y), ωD(y) =ωD1 (y) + ωD2 (y), (29) and the extended constraint property in Lemma A.1, we can verify that Q(D) satisﬁes all the constraints in Deﬁnition A.1. Therefore it is a valid quantile summar"
7825,unknown,"Lemma A.3. The combined quantile summary satisﬁes ˜r− D(y) = ˜r− D1 (y) + ˜r− D2 (y) (30) ˜r+ D(y) = ˜r+ D1 (y) + ˜r+ D2 (y) (31) ˜ωD(y) = ˜ωD1 (y) + ˜ωD2 (y) (32) for all y∈X Algorithm 4: Query Function g(Q,d) Input: d: 0 ≤d≤ω(D) Input: Q(D) = (S,˜r+ D,˜r− D,˜ωD) where S = x1,x2,··· ,xk if d< 1 2 [˜r− D(x1) + ˜r+ D(x1)] then return x1 ; if d≥1 2 [˜r− D(xk) + ˜r+ D(xk)] then return xk ; Find i suc"
7826,unknown,"Find i such that 1 2 [˜r− D(xi) + ˜r+ D(xi)] ≤d< 1 2 [˜r− D(xi+1) + ˜r+ D(xi+1)] if 2d< ˜r− D(xi) + ˜ωD(xi) + ˜r+ D(xi+1) −˜ωD(xi+1) then return xi else return xi+1 end This can be obtained by straight-forward application of Deﬁni- tion A.2. Theorem A.1. If Q(D1) is ϵ1-approximate summary, and Q(D2) is ϵ2-approximate summary. Then the merged summary Q(D) is max(ϵ1,ϵ2)-approximate summary. Proof. F"
7827,unknown,"˜r+ D(y) −˜r− D(y) −˜ωD(y) =[˜r+ D1 (y) + ˜r+ D2 (y)] −[˜r− D1 (y) + ˜r− D2 (y)] −[˜ωD1 (y) + ˜ωD2 (y)] ≤ϵ1ω(D1) + ϵ2ω(D2) ≤max(ϵ1,ϵ2)ω(D1 ∪D2) Here the ﬁrst inequality is due to Lemma A.3. A.4 Prune Operation Before we start discussing the prune operation, we ﬁrst in- troduce a query function g(Q,d). The deﬁnition of function is shown in Algorithm 4. For a given rank d, the function returns a x w"
7828,unknown,"a x whose rank is close to d. This property is formally described in the following Lemma. Lemma A.4. For a given ϵ-approximate summary Q(D) = (S,˜r+ D,˜r− D,˜ωD), x∗= g(Q,d) satisﬁes the following property d≥˜r+ D(x∗) −˜ωD(x∗) −ϵ 2 ω(D) d≤˜r− D(x∗) + ˜ωD(x∗) + ϵ 2 ω(D) (33) Proof. We need to discuss four possible cases • d <1 2 [˜r− D(x1) + ˜r+ D(x1)] and x∗= x1. Note that the rank information for"
7829,unknown,"D(x1) = ω(x1), ˜r− D(x1) = 0), we have d≥0 −ϵ 2 ω(D) = ˜r+ D(x1) −˜ωD(x1) −ϵ 2 ω(D) d< 1 2 [˜r− D(x1) + ˜r+ D(x1)] ≤˜r− D(x1) + ˜r+ D(x1) = ˜r− D(x1) + ˜ω+ D(x1) • d≥1 2 [˜r− D(xk) + ˜r+ D(xk)] and x∗= xk, then d≥1 2 [˜r− D(xk) + ˜r+ D(xk)] = ˜r+ D(xk) −1 2 [˜r+ D(xk) −˜r− D(xk)] = ˜r+ D(xk) −1 2 ˜ωD(xk) d<ω (D) + ϵ 2 ω(D) = ˜r− D(xk) + ˜ωD(xk) + ϵ 2 ω(D) • x∗= xi in the general case, then 2d< ˜r−"
7830,unknown,D(xi) + ˜ωD(xi) + ˜r+ D(xi+1) −˜ωD(xi+1) = 2[˜r− D(xi) + ˜ωD(xi)] + [˜r+ D(xi+1) −˜ωD(xi+1) −˜r− D(xi) −˜ωD(xi)] ≤2[˜r− D(xi) + ˜ωD(xi)] + ϵω(D) 2d≥˜r− D(xi) + ˜r+ D(xi) = 2[˜r+ D(xi) −˜ωD(xi)] −[˜r+ D(xi) −˜ωD(xi) −˜r− D(xi)] + ˜ωD(xi) ≥2[˜r+ D(xi) −˜ωD(xi)] −ϵω(D) + 0 • x∗= xi+1 in the general case 2d≥˜r− D(xi) + ˜ωD(xi) + ˜r+ D(xi+1) −˜ωD(xi+1) = 2[˜r+ D(xi+1) −˜ωD(xi+1)] −[˜r+ D(xi+1) −˜ωD(xi+
7831,unknown,"D(xi) −˜ωD(xi)] ≥2[˜r+ D(xi+1) + ˜ωD(xi+1)] −ϵω(D) 2d≤˜r− D(xi+1) + ˜r+ D(xi+1) = 2[˜r− D(xi+1) + ˜ωD(xi+1)] + [˜r+ D(xi+1) −˜ωD(xi+1) −˜r− D(xi+1)] −˜ωD(xi+1) ≤2[˜r− D(xi+1) + ˜ωD(xi+1)] + ϵω(D) −0 Now we are ready to introduce the prune operation. Given a quantile summaryQ(D) = (S,˜r+ D,˜r− D,˜ωD) with S = {x1,x2,··· ,xk} elements, and a memory budget b. The prune operation creates another summa"
7832,unknown,"another summary Q′(D) = (S′,˜r+ D,˜r− D,˜ωD) with S′= {x′ 1,x′ 2,··· ,x′ b+1}, where x′ i are selected by query the original summary such that x′ i = g ( Q,i−1 b ω(D) ) . The deﬁnition of ˜r+ D,˜r− D,˜ωD in Q′ is copied from original sum- mary Q, by restricting input domain from S to S′. There could be duplicated entries in the S′. These duplicated entries can be safely removed to further reduce t"
7833,unknown,"elements in Q′comes from Q, we can verify that Q′satisﬁes all the constraints in Deﬁnition A.1 and is a valid quantile summary. Theorem A.2. Let Q′(D) be the summary pruned from an ϵ-approximate quantile summary Q(D) with b memory budget. Then Q′(D) is a (ϵ+ 1 b)-approximate summary. Proof. We only need to prove the property in Eq. (23) forQ′. Using Lemma A.4, we have i−1 b ω(D) + ϵ 2 ω(D) ≥˜r+ D("
7834,unknown,D(x′ i) −˜ωD(x′ i) i−1 b ω(D) −ϵ 2 ω(D) ≤˜r− D(x′ i) + ˜ωD(x′ i) Combining these inequalities gives ˜r+ D(x′ i+1) −˜ωD(x′ i+1) −˜r− D(x′ i) −˜ωD(x′ i) ≤[ i bω(D) + ϵ 2 ω(D)] −[ i−1 b ω(D) −ϵ 2 ω(D)] = ( 1 b + ϵ)ω(D) Journal of Physics: Conference Series PAPER • OPEN ACCESS Implementation of random forest algorithm with parallel computing in R To cite this article: N Azizah et al 2019 J. Phys.: Con
7835,unknown,"View the article online for updates and enhancements. This content was downloaded from IP address 178.171.54.24 on 22/11/2019 at 13:22 Content from this work may be used under the terms of the Creative Commons Attribution 3.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI. Published under licence by IOP "
7836,unknown,MSCEIS 2018 Journal of Physics: Conference Series 1280 (2019) 022028 IOP Publishing doi:10.1088/1742-6596/1280/2/022028 1
7837,unknown,"Implementation of random forest algorithm with parallel computing in R N Azizah, L S Riza* and Y Wihardi Department of Computer Science Education, Universitas Pendidikan Indonesia, Indonesia, Bandung, Indonesia *Corresponding author’s email: lala.s.riza@upi.edu Abstract. Random forest is a method for building models by combining decision trees or decision trees generated from bootstrap samples and"
7838,unknown,"and random features. A common problem that often occurs when implementing random forest is long processing time because it uses a lot of data and build many tree models to form random trees because it uses single processor. This research proposes random forest method with parallel computing and implemented in R programming language. Some of the cases used in this research are Iris flower dataset, "
7839,unknown,wine quality and diabetes diagnosis data of Pima Indian woman. The results obtained from the entire study show that the computational time used when running random forest with parallel computing is shorter than when running a regular random forest using only a single processor.
7840,unknown,"1. Introduction Random forest was first introduced by Breiman in 2001. In his research shows the advantages of random forest, among others, can produce a lower error, provide good results in the classification, can handle the training data in a very large amount efficiently, and effective methods to estimate missing data [1]. Previous research on random forest was conducted by [2] conducting resea"
7841,unknown,"on web caching by comparing classification accuracy using CART, MARS, random forest and Tree Net methods. Research on the application of random forest methods in driver analysis [3]. In the research of ensemble method on poverty classification in Jombang Regency, it is found that random forest gives best classification accuracy [4]. However, the common problem that often occurs when implementing r"
7842,unknown,"forest is the long processing time when using large amounts of data to build many tree models to form random trees when using single processor. Therefore, a random forest design with parallel computing is proposed. Parallel computing is the union of several computers or servers into a single unit that can work on the process simultaneously or simultaneously. Parallel computing makes programs and p"
7843,unknown,"and processes run faster as more CPUs are used [5]. Parallel computing was once used to improve the performance of advanced encryption standards [6]. There are several studies on parallel computing, one of which is the use of parallel computing to improve computer performance [7]. The programming language R is a programming language and software environment for statistical computing and is support"
7844,unknown,"by R Foundation for Statistical Computing [8]. According to [9] R programming is an integrated software suite facility for data manipulation, calculation and graphical display."
7845,unknown,MSCEIS 2018 Journal of Physics: Conference Series 1280 (2019) 022028 IOP Publishing doi:10.1088/1742-6596/1280/2/022028 2
7846,unknown,"This research is aimed to develop random forest to be used in parallel computing in R. To achieve this objective, we utilize the “foreach” package. It is a package that supports the foreach looping construct [10]. We use this package because it can be used easily for repeating the same procedures along with all cores. Actually, the use of the foreach package for parallel computing can be found in "
7847,unknown,"in the several literature, such as, [11, 12] and it has been implemented in the gradDescent package [13]. 2. Methods 2.1. Implementation of Random Forest in R High Performance Computing The programming language R or R programming is a programming language and software environment for statistical computing and is supported by R Foundation for Statistical Computing [8]. The R language is an implemen"
7848,unknown,"of the S programming language combined with lexical scoping semantics inspired by the scheme. The first step in this research is import data. In this process the orbit element data is inserted into the R programming language. Here is how to import data directly from the website, to keep in mind is the computer must be connected to the internet. library(""httr"") a <- GET(""https://archive.ics.uci.edu"
7849,unknown,"databases/wine/wine.data"") wine <- read.csv(textConnection(content(a)), header=F) colnames(wine) <- c('Alcohol', 'Malic', 'Ash', 'Alcalinity', 'Magnesium','Phenols', 'Flavanoids', 'Nonflavanoids', 'Proanthocyanins', 'Color', 'Hue', 'Dilution', 'Proline', 'Type') Figure 1. Import the wine dataset directly from the website. Figures 1 shows the process of retrieving the dataset from the online reposi"
7850,unknown,"the dataset from the online repository and then stored in CSV form, the next being the naming of the columns of each variable because the dataset retrieved does not have an attribute name so if it is not defined it will by default be named V1, V2, V3, and so on. Once the orbital element's dataset is imported, the next step is to build the bootstrap function. ## shuffle dataset irisData <- iris[sam"
7851,unknown,"##set the first 120 rows to be training dataset iris.tra <- irisData[1:120, ] ## set testing dataset iris.tst <- irisData[121:nrow(irisData), -ncol(irisData)] ## get real values of testing dataset real.iris <- irisData[121:nrow(irisData), ncol(irisData)] Figure 2. Code of the data retrieval program at random. Figure 2 is the program code for retrieving data randomly from the available dataset, the"
7852,unknown,"the above example is data retrieval from the iris datset. The variable ""irisData"" is used to store randomly-captured data, ""iris.tra"" is the name of a variable that contains data for training data, ""iris.tst"" is a variable name containing data for test data whereas "" real.iris ""is the name of the variable that contains the original data from the test data (testing). In this step, some steps are ta"
7853,unknown,"taken to run random forest with parallel computing. There are several packages needed to run this stage ie packages ""foreach"", ""doParallel"" and ""entropy"". The first step is to install the three packages, then import or call packages as in Figure 3. # Import packages “foreach” library(forech) # Import packages “doParallel” library(doParallel) Figure 3. Import the R packages."
7854,unknown,MSCEIS 2018 Journal of Physics: Conference Series 1280 (2019) 022028 IOP Publishing doi:10.1088/1742-6596/1280/2/022028 3
7855,unknown,"Next is to build the parallel random forest function shown in Figure 4. Basically, this code contains several components, as follows: to collect parameters, to define number of cores, to perform the foreach package along with numbers of trees, to call the decision tree procedure, and to aggregate the final results. 2.2. Experimental Design In order to conduct an efficient experiment, a suitable ex"
7856,unknown,"experimental design was created so that a satisfactory trial result was obtained. In this research there are two scenarios, the first scenario is to do the calculation of random forest without parallel computing, while for the second scenario is to do the calculation of random forest with parallel computing. The first experimental scenario of computing random forest without parallel computing usin"
7857,unknown,"only one processor to test the case of Iris flower dataset, the quality of wine and diabetes of Pima Indian women. The numTree parameter value is set to 20 and 100 and numFeature is set 2 and 4. The second experimental scenario to test the case of the Iris dataset, Wine and Pima using a parallel random forest with two processors. Modify the parameters as in the first scenario of numTree 20 and 100"
7858,unknown,"100 and numFeature 2 and 4. The third experimental scenario to test the case of the Iris dataset, Wine and Pima using a parallel random forest with three processors. Modify the parameters as in the first and second scenarios of numTree 20 and 100 and numFeature 2 and 4. Recent experimental scenarios to test the case of the Iris, Wine and Pima datasets use a parallel random forest with four process"
7859,unknown,"Modify the parameters as in the previous scenario of numTree 20 and 100 and numFeature 2 and 4. Each experimental scenario uses the Iris flower species dataset consisting of 5 variables (4 input variables and 1 output variable) and 150 rows of data. Next use the wine quality dataset consisting of 14 variables (13 input variables and 1 output variable) and 178 rows of data. Lastly, by using the Pim"
7860,unknown,"Indian female diabetes dataset consisting of 9 columns (8 input variables and 1 output variable) and 798 rows of data. parRandForest <- function(dataset, typeTask = ""classification"", controlPRF = list(), controlTree = list()){ ## set default values of all parameters controlTree <- setDefaultParametersIfMissing(controlTree, list(nameFeatures = NULL, typeFeatures = NULL, paramPercent = 0.5, typeEntr"
7861,unknown,"= ""ML"", maxDepthTree = 3, typeSplitting = ""random"", minNumData = 3, sampleFeature = TRUE, nameClasses = NULL)) controlPRF <- setDefaultParametersIfMissing(controlPRF, list(numTree = 10, numProcessor = 2, numFeature = round(sqrt(ncol(dataset)-1)))) numTree <- controlPRF$numTree numProcessor <- controlPRF$numProcessor numFeature <- controlPRF$numFeature typeFeatures <- controlTree$typeFeatures param"
7862,unknown,typeFeatures <- controlTree$typeFeatures paramPercent <- controlTree$paramPercent typeEntropy <- controlTree$typeEntropy maxDepthTree <- controlTree$maxDepthTree typeSplitting <- controlTree$typeSplitting nameFeatures <- controlTree$nameFeatures minNumData <- controlTree$minNumData nameClasses <- controlTree$nameClasses ## check colnames if (is.null(nameFeatures)){ nameFeatures <- names(dataset) }
7863,unknown,"<- names(dataset) } else { names(dataset) <- nameFeatures } ## save names of input features if (any(typeTask == c(""classification"", ""regression""))){ inputFeatures <- nameFeatures[-length(nameFeatures)] } else { inputFeatures <- nameFeatures } ## set nameClasses! if ((typeTask == ""classification"") && (is.null(nameClasses))){ nameClasses <- c(as.character(sort(unique(dataset[, ncol(dataset)])))) } #"
7864,unknown,ncol(dataset)])))) } ## build trees cl <- makeCluster(numProcessor) registerDoParallel(cl)
7865,unknown,MSCEIS 2018 Journal of Physics: Conference Series 1280 (2019) 022028 IOP Publishing doi:10.1088/1742-6596/1280/2/022028 4
7866,unknown,"modelTree <- list() modelTree <- foreach(icount(numTree), .combine=append, .export=ls(envir=globalenv())) %dopar%{ ## generate bootstrapped data and features sampleData <- dataset[sample(nrow(dataset), size = nrow(dataset), replace = TRUE), ] ## building trees modelTree <- list(decTree(sampleData, typeTask = typeTask, controlTree = list(typeFeatures = typeFeatures, paramPercent = paramPercent, typ"
7867,unknown,"typeFeatures, paramPercent = paramPercent, typeEntropy = typeEntropy, maxDepthTree = maxDepthTree, typeSplitting = typeSplitting, minNumData = minNumData, sampleFeature = TRUE, numFeature = numFeature, nameClasses = nameClasses))) } stopCluster(cl) attr(modelTree, ""typeTask"") <- typeTask attr(modelTree, ""inputFeatures"") <- inputFeatures attr(modelTree, ""numTree"") <- numTree return(ObjectFactory(mo"
7868,unknown,"return(ObjectFactory(modelTree, ""randForest"")) } Figure 4. Implementation of Parallel Random Forest in R 3. Results and Discussion This chapter describes some experimental results that have been made predictions of iris species, wine quality and diagnosis of diabetes Pima Indians by using conventional random forest methods as well as random forest methods combined with parallel computing The exper"
7869,unknown,"The experimental results obtained after running the test case studies that have been determined in this study that is the case of Iris flower dataset, the dataset of the quality of wine and the dataset of diabetes Pima Indian women. The following experimental results were made to predict 3 cases of Iris flower, wine quality and diagnosis of diabetes of Pima Indian women by random forest method in "
7870,unknown,"in parallel computing with modification of number of processor used, number of tree and number of features. Table 1. Results of parallel random forest on the Iris dataset. No Numbers of Cores Numbers of Tree Numbers of Features Cost (s) Error (%) 1 2 20 2 1.368404 10.0 2 3 20 2 1.327446 10.0 3 4 20 2 1.440552 10.0 4 2 100 2 4.294907 3.3 5 3 100 2 3.351274 3.3 6 4 100 2 3.207866 3.3 7 2 20 4 1.8941"
7871,unknown,20 4 1.894165 10.0 8 3 20 4 1.788924 10.0 9 4 20 4 1.703791 10.0 10 2 100 4 6.498522 6.6 11 3 100 4 4.964609 6.6 12 4 100 4 4.470230 6.6
7872,unknown,Table 1 shows the time obtained when running a random forest method with parallel computing to predict Iris flower species. Based on the data obtained and displayed in the table can be seen that the more processors are used then the processing time becomes shorter. But that does not mean the faster the error generated smaller. The fastest time is 1.368404 seconds when using two processors with num
7873,unknown,"numTree 20 and numFeature 2 but the resulting error is 10.0%. The smallest error is 3.3% at numTree 100 and numFeature 2 using two, three and four processors. The resulting time is 4.294907 seconds,"
7874,unknown,MSCEIS 2018 Journal of Physics: Conference Series 1280 (2019) 022028 IOP Publishing doi:10.1088/1742-6596/1280/2/022028 5
7875,unknown,3.351274 seconds and 3.207866 seconds. The longest time is 6.964609 seconds when using three processors with numTree 100 and num Feature and the resulting error is 6.6%. Table 2. Results of parallel random forest on the wine dataset. No Numbers of Cores Numbers of Tree Numbers of Features Cost (s) Error (%) 1 2 20 2 3.339464 0.000 2 3 20 2 2.965031 0.000 3 4 20 2 2.570818 0.000 4 2 100 2 13.052 2.
7876,unknown,2.857 5 3 100 2 9.62039 2.857 6 4 100 2 7.812652 2.857 7 2 20 4 4.94744 2.857 8 3 20 4 3.883546 0.000 9 4 20 4 3.486539 0.000 10 2 100 4 21.5716 2.857 11 3 100 4 15.30717 0.000 12 4 100 4 12.97523 0.000
7877,unknown,"Table 2 shows the computational results obtained when running a random forest method with parallel computing to predict the quality of the wine with different parameter values. This affects the processing time spent. The fastest time is obtained when computing with four processors with numTree 20 and numFeature 2 parameters, the resulting error is 0.000%. Errors resulting from the overall wine dat"
7878,unknown,"prediction are relatively small, the highest error is only 2.857%. Overall, the computing time gets shorter when using more processors. Table 3. Results of parallel random forest on the Pima dataset. No Numbers of Cores Numbers of Tree Numbers of Features Cost (s) Error (%) 1 2 20 2 16.22235 36.364 2 3 20 2 11.97611 35.714 3 4 20 2 9.927734 29.078 4 2 100 2 74.96628 30.519 5 3 100 2 54.40325 29.87"
7879,unknown,29.870 6 4 100 2 45.03 29.221 7 2 20 4 28.772 30.519 8 3 20 4 20.83696 29.870 9 4 20 4 17.41449 30.519 10 2 100 4 147.9651 35.065 11 3 100 4 103.69848 34.416 12 4 100 4 77.15244 35.065
7880,unknown,"Table 3 shows the results obtained when running the random forest method with parallel computing to predict the quality of the wine with different parameter values. When numTree 100 and numFeature 4 the time gained becomes larger than when numTree 20 and numFeature 2. However, it can be concluded that when the value of the numProcessor parameter increases, the processing time becomes shorter. Erro"
7881,unknown,MSCEIS 2018 Journal of Physics: Conference Series 1280 (2019) 022028 IOP Publishing doi:10.1088/1742-6596/1280/2/022028 6
7882,unknown,"when computation dijalanka in four processors with numTree 20 and numFeature 2 is 29.078% with the shortest time also is 9.927734 seconds. 4. Conclusion Based on the results of random forest implementation implementation with parallel computing in R, obtained some of the following conclusions: This research has produced a system that can predict some cases of Iris flower species, wine quality and "
7883,unknown,"diabetes diagnosis of Pima Indian women by applying random forest method with parallel computing. According to the experiments, it can be stated that computational time of random forest with parallel computing is much faster than conventional random forest and predicted results have no significant difference. 5. References [1] Breiman L 2001 Random forests. Machine learning 45 1 5-32 [2] Sulaiman "
7884,unknown,"S, Shamsuddin SM and Abraham A 2011 Intelligent web caching using adaptive regression trees, splines, random forests and tree net. InData Mining and Optimization (DMO), 2011 3rd Conference on 2011 Jun 28 108-114 [3] Dewi NK, Syafitri UD, Mulyadi SY 2011 Penerapan Metode Random Forest dalam Driver Analysis. InForum Statistika dan Komputasi 16 1 [4] Muttaqin M J 2013 Metode Ensemble pada CART Untuk "
7885,unknown,"Untuk Perbaikan Klasifikasi Kemiskinan di Kabupaten Jombang (Surabaya: Institut Teknologi Sepuluh Nopember) [5] Kumar V, Grama A, Gupta A, Karypis G 1994 Introduction to parallel computing: design and analysis of algorithms. (Redwood City: Benjamin/Cummings) [6] Pachori V, Ansari G, Chaudhary N 2012 Improved performance of advance encryption standard using parallel computing. International Journal"
7886,unknown,"of Engineering Research and Applications (IJERA) 2 1 9 67-71 [7] Haynes LS, Lau RL, Siewiorek DP, Mizell DW 1982 A survey of highly parallel computing. Computer 1 1 9-24 [8] Matloff N 2011 The art of R programming: A tour of statistical software design. No Starch Press [9] Dalgaard P 2008 Introductory statistics with R (Springer Science & Business Media) [10] Analytics R, Weston S 2014 doParallel:"
7887,unknown,"Foreach parallel adaptor for the parallel package. R package version1 8 [11] Riza LS, Asyari AH, Prabawa HW, Kusnendar J, Rahman EF 2018 Parallel Particle Swarm Optimization for Determining Pressure on Water Distribution Systems in R. Advanced Science Letters 24 10 7501-7506 [12] Riza LS, Utama JA, Putra SM, Simatupang FM, Nugroho EP 2018 Parallel Exponential Smoothing Using the Bootstrap Method i"
7888,unknown,"in R for Forecasting Asteroid's Orbital Elements. Pertanika Journal of Science & Technology 26 1 441-462 [13] Riza LS, Nasrulloh IF, Junaeti E, Zain R, Nandiyanto AB 2016 gradDescentR: An R package implementing gradient descent and its variants for regression tasks. InInformation Technology, Information Systems and Electrical Engineering (ICITISEE), International Conference 23 125-129"
7889,unknown,"1 A Parallel Random Forest Algorithm for Big Data in a Spark Cloud Computing Environment Jianguo Chen, Kenli Li, Senior Member, IEEE, Zhuo Tang, Member, IEEE, Kashif Bilal, Shui Yu, Member, IEEE, Chuliang Weng, Member, IEEE, and Keqin Li, Fellow, IEEE Abstract— With the emergence of the big data age, the issue of how to obtain valuable knowledge from a dataset efﬁciently and"
7890,unknown,"accurately has attracted increasingly attention from both academia and industry. This paper presents a Parallel Random Forest (PRF) algorithm for big data on the Apache Spark platform. The PRF algorithm is optimized based on a hybrid approach combining data-parallel and task-parallel optimization. From the perspective of data-parallel optimization, a vertical data-partitioning method is performed "
7891,unknown,"reduce the data communication cost effectively, and a data-multiplexing method is performed is performed to allow the training dataset to be reused and diminish the volume of data. From the perspective of task-parallel optimization, a dual parallel approach is carried out in the training process of RF , and a task Directed Acyclic Graph (DAG) is created according to the parallel training process o"
7892,unknown,"dependence of the Resilient Distributed Datasets (RDD) objects. Then, different task schedulers are invoked for the tasks in the DAG. Moreover, to improve the algorithm’s accuracy for large, high-dimensional, and noisy data, we perform a dimension-reduction approach in the training process and a weighted voting approach in the prediction process prior to parallelization. Extensive experimental res"
7893,unknown,"indicate the superiority and notable advantages of the PRF algorithm over the relevant algorithms implemented by Spark MLlib and other studies in terms of the classiﬁcation accuracy, performance, and scalability. Index Terms—Apache Spark, Big Data, Cloud Computing, Data Parallel, Random Forest, Task Parallel. ! 1 I NTRODUCTION 1.1 Motivation W ITH the continuous emergence of a variety of new infor"
7894,unknown,"ITH the continuous emergence of a variety of new information dissemination methods, and the rise of cloud computing and Internet of Things (IoT) technologies, data increase constantly with a high speed. The scale of global data continuously increases at a rate of 2 times every two years [1]. The application value of data in every ﬁeld is becoming more important than ever. There exists a large amou"
7895,unknown,"amount of worthwhile information in available data. The emergence of the big data age also poses serious problems and challenges besides the obvious beneﬁts. Be- cause of business demands and competitive pressure, al- most every business has a high demand for data processing in real-time and validity [2]. As a result, the ﬁrst problem is how to mine valuable information from massive data efﬁcientl"
7896,unknown,"efﬁciently and accurately. At the same time, big data hold characteristics such as high dimensionality, complexity, and noise. Enormous data often hold properties found in various input variables in hundreds or thousands of levels, while • Jianguo Chen, Kenli Li, Zhuo Tang, and Keqin Li are with the College of Computer Science and Electronic Engineering, Hunan University, and the National Supercom"
7897,unknown,"the National Supercomputing Center in Changsha, Hunan, Changsha 410082, China. Corresponding author: Kenli Li, Email: lkl@hnu.edu.cn. • Kashif Bilal is with the Qatar University,Doha 2713, Qatar, and the Comsats Institute of Information Technology, Islamabad 45550, Pakistan. • Shui Yu is with the School of Information Technology, Deakin University, Melbourne, Vic.3216, Australia. • Chuliang Weng i"
7898,unknown,"Melbourne, Vic.3216, Australia. • Chuliang Weng is with the School of Computer Science and Software Engineering, Institute for Data Science and Engineering, East China Normal University, Shanghai 200241, China. • Keqin Li is also with the Department of Computer Science, State Univer- sity of New York, New Paltz, NY 12561, USA. each one of them may contain a little information. The second problem i"
7899,unknown,"may lead to good classiﬁcation performance for a high- dimensional dataset. Considering the aforementioned facts, data mining and analysis for large-scale data have become a hot topic in academia and industrial research. The speed of data mining and analysis for large-scale data has also attracted much attention from both academia and industry. Studies on distributed and parallel data min- ing bas"
7900,unknown,"ing based on cloud computing platforms have achieved abundant favorable achievements [3, 4]. Hadoop [5] is a famous cloud platform widely used in data mining. In [6, 7], some machine learning algorithms were proposed based on the MapReduce model. However, when these algorithms are implemented based on MapReduce, the intermediate results gained in each iteration are written to the Hadoop Distribute"
7901,unknown,"Distributed File System (HDFS) and loaded from it. This costs much time for disk I/O operations and also massive resources for communication and storage. Apache Spark [8] is another good cloud platform that is suitable for data mining. In comparison with Hadoop, a Resilient Distributed Datasets (RDD) model and a Directed Acyclic Graph (DAG) model built on a memory computing framework is sup- porte"
7902,unknown,"ported for Spark. It allows us to store a data cache in memory and to perform computation and iteration for the same data directly from memory. The Spark platform saves huge amounts of disk I/O operation time. Therefore, it is more suitable for data mining with iterative computation. The Random Forest (RF) algorithm [9] is a suitable data mining algorithm for big data. It is an ensemble learning a"
7903,unknown,"algorithm using feature sub-space to construct the model. Moreover, all decision trees can be trained concurrently, hence it is also suitable for parallelization. arXiv:1810.07748v1 [cs.DC] 17 Oct 2018 2 1.2 Our Contributions In this paper, we propose a Parallel Random Forest (PRF) algorithm for big data that is implemented on the Apache Spark platform. The PRF algorithm is optimized based on a hy"
7904,unknown,"hybrid approach combining data-parallel and task-parallel optimization. To improve the classiﬁcation accuracy of PRF, an optimization is proposed prior to the parallel process. Extensive experiment results indicate the superiority of PRF and depict its signiﬁcant advantages over other algorithms in terms of the classiﬁcation accuracy and performance. Our contributions in this paper are summarized "
7905,unknown,"• An optimization approach is proposed to improve the accuracy of PRF, which includes a dimension- reduction approach in the training process and a weighted voting method in the prediction process. • A hybrid parallel approach of PRF is utilized to improve the performance of the algorithm, com- bining data-parallel and task-parallel optimization. In the data-parallel optimization, a vertical data-"
7906,unknown,"partitioning method and a data-multiplexing method are performed. • Based on the data-parallel optimization, a task- parallel optimization is proposed and implemented on Spark. A training task DAG of PRF is constructed based on the RDD model, and different task sched- ulers are invoked to perform the tasks in the DAG. The performance of PRF is improved noticeably. The rest of the paper is organize"
7907,unknown,"reviews the related work. Section 3 gives the RF algorithm optimization from two aspects. The parallel implementation of the RF algorithm on Spark is developed in Section 4. Experimental results and evaluations are shown in Section 5 with respect to the classiﬁcation accuracy and performance. Finally, Section 6 presents a conclusion and future work. 2 R ELATED WORK Although traditional data proces"
7908,unknown,"achieved good performance for small-scale and low- dimensional datasets, they are difﬁcult to be applied to large-scale data efﬁciently [10–12]. When a dataset becomes more complex with characteristics of a complex structure, high dimensionality, and a large size, the accuracy and performance of traditional data mining algorithms are sig- niﬁcantly declined [13]. Due to the need to address the hig"
7909,unknown,"noisy data, various improvement methods have been in- troduced by researchers. Xu [14] proposed a dimension- reduction method for the registration of high-dimensional data. The method combines datasets to obtain an image pair with a detailed texture and results in improved image registration. Tao et al. [15] and Lin et al. [16] introduced some classiﬁcation algorithms for high-dimensional data to "
7910,unknown,address the issue of dimension-reduction. These algorithms use multiple kernel learning framework and multilevel maximum margin features and achieve efﬁcient dimension- ality reduction in binary classiﬁcation problems. Strobl [17] and Bernard [18] studied the variable importance measures of RF and proposed some improved models for it. Taghi et al. [19] compared the boosting and bagging techniques 
7911,unknown,"proposed an algorithm for noisy and imbalanced data. Yu et al. [20] and Biau [21] focused on RF for high-dimensional and noisy data and applied RF in many applications such as multi-class action detection and facial feature detection, and achieved a good effort. Based on the existing research results, we propose a new optimization approach in this paper to address the problem of high-dimensional a"
7912,unknown,"data, which reduces the dimensionality of the data accord- ing to the structure of the RF and improves the algorithm’s accuracy with a low computational cost. Focusing on the performance of classiﬁcation algorithms for large-scale data, numerous studies on the intersection of parallel/distributed computing and the learning of tree models were proposed. Basilico et al. [22] proposed a COMET algorit"
7913,unknown,"COMET algorithm based on MapReduce, in which multiple RF ensembles are built on distributed blocks of data. Svore et al. [23] proposed a boosted decision tree ranking algo- rithm, which addresses the speed and memory constraints by distributed computing. Panda et al. [24] introduced a scalable distributed framework based on MapReduce for the parallel learning of tree models over large datasets. A "
7914,unknown,"parallel boosted regression tree algorithm was proposed in [25] for web search ranking, in which a novel method for parallelizing the training of GBRT was performed based on data partitioning and distributed computing. Focusing on resource allocation and task-parallel exe- cution in a parallel and distributed environment, Warneke et al. [26] implemented a dynamic resource allocation for efﬁcient p"
7915,unknown,"efﬁcient parallel data processing in a cloud environment. Lena et al. [27] carried out an energy-aware scheduling of MapReduce jobs for big data applications. Luis et al. [28] proposed a robust resource allocation of data processing on a heterogeneous parallel system, in which the arrival time of datasets are uncertainty. Zhang et al. [29] proposed an evo- lutionary scheduling of dynamic multitask"
7916,unknown,"for big data analysis in an elastic cloud. Meanwhile, our team also focused on parallel tasks scheduling on heteroge- neous cluster and distributed systems and achieved positive results[30, 31]. Apache Spark Mllib [32] parallelized the RF algorithm (referred to Spark-MLRF in this paper) based on a data- parallel optimization to improve the performance of the algorithm. However, there exist many dr"
7917,unknown,"Spark-MLRF. First, in the stage of determining the best split segment for continuous features, a method of sampling for each partition of the dataset is used to reduce the data trans- mission operations. However, the cost of this method is its reduced accuracy. In addition, because the data-partitioning method in Spark-MLRF is a horizontal partition, the data communication of the feature variable "
7918,unknown,"is a global communication. To improve the performance of the RF algorithm and mitigate the data communication cost and workload imbal- ance problems of large-scale data in parallel and distributed environments, we propose a hybrid parallel approach for RF combining data-parallel and task-parallel optimization based on the Spark RDD and DAG models. In compari- son with the existing study results, o"
7919,unknown,"the volume of the training dataset without decreasing the algorithm’s accuracy. Moreover, our method mitigates the data communication and workload imbalance problems of 3 large-scale data in parallel and distributed environments. 3 R ANDOM FOREST ALGORITHM OPTIMIZATION Owing to the improvement of the classiﬁcation accuracy for high-dimensional and large-scale data, we propose an opti- mization app"
7920,unknown,"mization approach for the RF algorithm. First, a dimension- reduction method is performed in the training process. Second, a weighted voting method is constructed in the pre- diction process. After these optimizations, the classiﬁcation accuracy of the algorithm is evidently improved. 3.1 Random Forest Algorithm The random forest algorithm is an ensemble classiﬁer al- gorithm based on the decision"
7921,unknown,"different training data subsets from an original dataset using a bootstrap sampling approach, and then, k decision trees are built by training these subsets. A random forest is ﬁnally constructed from these decision trees. Each sample of the testing dataset is predicted by all decision trees, and the ﬁnal classiﬁcation result is returned depending on the votes of these trees. The original training"
7922,unknown,"{(xi,yj),i = 1 ,2,...,N ; j = 1 ,2,...,M }, where x is a sample and y is a feature variable of S. Namely, the orig- inal training dataset contains N samples, and there are M feature variables in each sample. The main process of the construction of the RF algorithm is presented in Fig. 1. Fig. 1. Process of the construction of the RF algorithm The steps of the construction of the random forest algo"
7923,unknown,"rithm are as follows. Step 1. Sampling ktraining subsets. In this step, k training subsets are sampled from the original training dataset S in a bootstrap sampling man- ner. Namely, N records are selected from S by a random sampling and replacement method in each sampling time. After the current step, ktraining subsets are constructed as a collection of training subsets STrain : STrain = {S1,S2,.."
7924,unknown,"STrain = {S1,S2,...,S k}. At the same time, the records that are not to be selected in each sampling period are composed as an Out-Of-Bag (OOB) dataset. In this way, kOOB sets are constructed as a collection of SOOB : SOOB = {OOB1,OOB2,...,OOB k}, where k ≪N, Si ⋂OOBi = φ and Si ⋃OOBi = S. To obtain the classiﬁcation accuracy of each tree model, these OOB sets are used as testing sets after the tr"
7925,unknown,"Step 2. Constructing each decision tree model. In an RF model, each meta decision tree is created by a C4.5 or CART algorithm from each training subset Si. In the growth process of each tree, m feature variables of dataset Si are randomly selected from M variables. In each tree node’s splitting process, the gain ratio of each feature variable is calculated, and the best one is chosen as the splitt"
7926,unknown,"splitting node. This splitting process is repeated until a leaf node is generated. Finally, kdecision trees are trained from ktraining subsets in the same way. Step 3. Collecting ktrees into an RF model. The ktrained trees are collected into an RF model, which is deﬁned in Eq. (1): H(X,Θj) = k∑ i=1 hi(x,Θj),(j = 1,2,...,m ), (1) where hi(x,Θj) is a meta decision tree classiﬁer, X are the input fea"
7927,unknown,"input feature vectors of the training dataset, and Θj is an independent and identically distributed random vector that determines the growth process of the tree. 3.2 Dimension Reduction for High-Dimensional Data To improve the accuracy of the RF algorithm for the high- dimensional data, we present a new dimension-reduction method to reduce the number of dimensions according to the importance of th"
7928,unknown,"process of each decision tree, the Gain Ratio (GR) of each feature variable of the training subset is calculated and sorted in descending order. The top k variables ( k ≪M) in the ordered list are selected as the principal variables, and then, we randomly select (m −k) further variables from the remaining (M −k) ones. Therefore, the number of dimensions of the dataset is reduced from M to m. The p"
7929,unknown,"process of dimension-reduction is presented in Fig. 2. Fig. 2. Dimension-reduction in the training process First, in the training process of each decision tree, the entropy of each feature variable is calculated prior to the node-splitting process. The entropy of the target variable in the training subset Si (i= 1,2,...,k ) is deﬁned in Eq. (2): Entropy(Si) = c1∑ a=1 −pa log pa, (2) 4 where c1 is "
7930,unknown,"Entropy(Si) = c1∑ a=1 −pa log pa, (2) 4 where c1 is the number of different values of the target variable in Si, and pa is the probability of the type of value awithin all types in the target variable subset. Second, the entropy for each input variable of Si, except for the target variable, is calculated. The entropy of each input variable yij is deﬁned in Eq. (3): Entropy(yij) = ∑ v∈V (yij) |S(v,"
7931,unknown,"|S(v,i)| |Si| Entropy(v(yij)), (3) where the elements of Eq. (3) are described in Table 1. TABLE 1 Explanation of the elements of Eq. (3) Element Description yij the j-th feature variable of Si, j = 1, 2, ..., M. V (yij) the set of all possible values of yij. |Si| the number of samples in Si. S(v,i) a sample subset in Si, where the value of yj is v. |S(v,i)|the number of the sample subset S(v,i). "
7932,unknown,"Third, the self-split information I(yij) of each input variable is calculated, as deﬁned in Eq. (4): I(yij) = c2∑ a=1 −p(a,j) log2(p(a,j)), (4) where c2 is the number of different values of yij, and p(a,j) is the probability of the type of value a within all types in variable yj. Then, the information gain of each feature variable is calculated, as deﬁned in Eq. (5): G(yij) =Entropy(Si) −Entropy(y"
7933,unknown,"=Entropy(Si) − ∑ v∈V (yij) |S(v,i)| |Si| Entropy(v(yij)), (5) where v(yj) ∈V(yj). By using the information gain to measure the feature variables, the largest value is selected easily, but it will lead to an over ﬁtting problem. To overcome this problem, a gain ratio value is taken to measure the feature variables, and the features with the maximum value are selected. The gain ratio of the feature "
7934,unknown,"ratio of the feature variable yij is deﬁned in Eq. (6): GR(yij) = G(yij) I(yij) . (6) To reduce the dimensions of the training dataset, we calculate the importance of each feature variable according to the gain ratio of the variable. Then, we select the most important features and delete the ones with less importance. The importance of each feature variable is deﬁned as fol- lows. Deﬁnition 1. The"
7935,unknown,"lows. Deﬁnition 1. The importance of each feature variable in a training subset refers to the portion of the gain ratio of the feature variable compared with the total feature variables. The importance of feature variable yij is deﬁned as VI(yij) in Eq. (7): VI(yij) = GR(yij)∑M (a=1) GR(y(i,a)) . (7) The importance values of all feature variables are sorted in descending order, and the top k (k ≪M"
7936,unknown,"are selected as the most important. We then randomly select (m−k) further feature variables from the remaining (M − k) ones. Thus, the number of dimensions of the dataset is reduced from M to m. Taking the training subset Si as an example, the detailed steps of the dimension-reduction in the training process are presented in Algorithm 3.1. Algorithm 3.1 Dimension-reduction in the training process "
7937,unknown,Input: Si: the ith training subset; k: the number of important variables selected by V I; m: the number of the selected feature variables. Output: Fi: a set of m important feature variables of Si. 1: create an empty string array Fi; 2: calculate Entropy(Si) for the target feature variable; 3: for each feature variable yij in Si do 4: calculate Entropy(yij) for each input feature variable; 5: calcu
7938,unknown,"5: calculate gain G(yij) ←Entropy(Si) −Entropy(yij); 6: calculate split information I(yij) ←∑c2 a=1 −p(a,j) log2(p(a,j)); 7: calculate gain ratio GR(yij) ← G(yij) I(yij) ; 8: end for 9: calculate variable importance V I(yij) ← GR(yij)∑M (a=1) GR(y(i,a)) for feature variable yij; 10: sort M feature variables in descending order by V I(yij); 11: put top k feature variables to Fi[0, ..., k−1]; 12: se"
7939,unknown,"12: set c ←0; 13: for j = k to M −1 do 14: while c <(m −k) do 15: select yij from (M −k) randomly; 16: put yij to Fi[k + c]; 17: c ←c + 1; 18: end while 19: end for 20: return Fi. In comparison with the original RF algorithm, our dimension-reduction method ensures that the m selected feature variables are optimal while maintaining the same computational complexity as the original algorithm. This b"
7940,unknown,"balances the accuracy and diversity of the feature selection ensemble of the RF algorithm and prevents the problem of classiﬁcation over ﬁtting. 3.3 Weighted Voting Method In the prediction and voting process, the original RF algo- rithm uses a traditional direct voting method. In this case, if the RF model contains noisy decision trees, it likely leads to a classiﬁcation or regression error for t"
7941,unknown,"Consequently, its accuracy is decreased. To address this problem, a weighted voting approach is proposed in this section to improve the classiﬁcation accuracy for the testing dataset. The accuracy of the classiﬁcation or regression of each tree is regarded as the voting weight of the tree. After the training process, each OOB set OOBi is tested by its corresponding trained tree hi. Then, the class"
7942,unknown,"accuracy CAi of each decision tree hi is computed. 5 Deﬁnition 2. The classiﬁcation accuracy of a decision tree is deﬁned as the ratio of the average number of votes in the correct classes to that in all classes, including error classes, as classiﬁed by the trained decision tree. The classi- ﬁcation accuracy is deﬁned in Eq. (8): CAi = I(hi(x) = y) I(hi(x) = y) + ∑I(hi(x) = z), (8) where I(·) is a"
7943,unknown,"I(hi(x) = y) + ∑I(hi(x) = z), (8) where I(·) is an indicator function, yis a value in the correct class, and zis a value in the error class ( z̸= y). In the prediction process, each record of the testing dataset X is predicted by all decision trees in the RF model, and then, a ﬁnal vote result is obtained for the testing record. When the target variable of X is quantitative data, the RF is trained"
7944,unknown,"trained as a regression model. The result of the prediction is the average value of ktrees. The weighted regression result Hr(X) of X is deﬁned in Eq. (9): Hr(X) = 1 k k∑ i=1 [wi ×hi(x)] = 1 k k∑ i=1 [CAi ×hi(x)], (9) where wi is the voting weight of the decision tree hi. Similarly, when the target feature of X is qualitative data, the RF is trained as a classiﬁcation model. The result of the pred"
7945,unknown,the prediction is the majority vote of the classiﬁcation results of k trees. The weighted classiﬁcation result Hc(X) of X is deﬁned in Eq. (10): Hc(X) = Majorityk i=1[wi ×hi(x)] = Majorityk i=1[CAi ×hi(x)]. (10) The steps of the weighted voting method in the predic- tion process are described in Algorithm 3.2. Algorithm 3.2 Weighted voting in the prediction process Input: X: a testing dataset; PRF
7946,unknown,PRFtrained: the trained PRF model. Output: H(X): the ﬁnal prediction result for X. 1: for each testing data x in X do 2: for each decision tree Ti in PRFtrained do 3: predict the classiﬁcation or regression result hi(x) by Ti; 4: end for 5: end for 6: set classiﬁcation accuracy CAi as the weight wi of Ti; 7: for each testing data x in X do 8: if (operation type == classiﬁcation) then 9: vote the ﬁ
7947,unknown,"9: vote the ﬁnal result Hc(x) ←Majority k i=1[wi ×hi(x)]; 10: H(X) ←Hc(x); 11: else if (operation type == regression) then 12: vote the ﬁnal result Hr(x) ←1 k ∑k i=1 [wi ×hi(x)]; 13: H(X) ←Hr(x); 14: end if 15: end for 16: return H(X). In the weighted voting method of RF, each tree classiﬁer corresponds to a speciﬁed reasonable weight for voting on the testing data. Hence, this improves the overal"
7948,unknown,"tion accuracy of RF and reduces the generalization error. 3.4 Computational Complexity The computational complexity of the original RF algorithm is O(kMN log N), where k is the number of decision trees in RF, M is the number of features, N is the number of samples, and log N is the average depth of all tree models. In our improved PRF algorithm with dimension-reduction (PRF-DR) described in Sectio"
7949,unknown,"dimension reduction is O(MN). The computational com- plexity of the splitting process for each tree node is set as one unit (1), which contains functions such as entropy(), gain(), and gainratio() for each feature subspace. After the dimen- sion reduction, the number of features is reduced from M to m(m≪M). Therefore, the computational complexity of training a meta tree classiﬁer is O(MN + mNlog N"
7950,unknown,"the total computational complexity of the PRF-DR algorithm is O(k(MN + mNlog N)). 4 P ARALLELIZATION OF THE RANDOM FOREST ALGORITHM ON SPARK To improve the performance of the RF algorithm and miti- gate the data communication cost and workload imbalance problems of large-scale data in a parallel and distributed en- vironment, we propose a Parallel Random Forest (PRF) algo- rithm on Spark. The PRF "
7951,unknown,"hybrid parallel approach combining data-parallel and task- parallel optimization. From the perspective of data-parallel optimization, a vertical data-partitioning method and a data-multiplexing method are performed. These methods reduce the volume of data and the number of data trans- mission operations in the distributed environment without reducing the accuracy of the algorithm. From the perspec"
7952,unknown,"tive of task-parallel optimization, a dual-parallel approach is carried out in the training process of the PRF algorithm, and a task DAG is created according to the dependence of the RDD objects. Then, different task schedulers are invoked to perform the tasks in the DAG. The dual-parallel training approach maximizes the parallelization of PRF and improves the performance of PRF. Then task schedul"
7953,unknown,"further minimize the data communication cost among the Spark cluster and achieve a better workload balance. 4.1 Data-Parallel Optimization We introduce the data-parallel optimization of the PRF algorithm, which includes a vertical data-partitioning and a data-multiplexing approach. First, taking advantage of the RF algorithm’s natural independence of feature vari- ables and the resource requiremen"
7954,unknown,"a vertical data-partitioning method is proposed for the training dataset. The training dataset is split into several feature subsets, and each feature subset is allocated to the Spark cluster in a static data allocation way. Second, to address the problem that the data volume increases linearly with the increase in the scale of RF, we present a data- multiplexing method for PRF by modifying the tr"
7955,unknown,"sampling method. Notably, our data-parallel optimization method reduces the volume of data and the number of data 6 transmission operations without reducing the accuracy of the algorithm. The increase in the scale of the PRF does not lead to a change in the data size and storage location. 4.1.1 Vertical Data Partitioning In the training process of PRF, the gain-ratio computing tasks of each featur"
7956,unknown,"time. However, these tasks only require the data of the current feature variable and the target feature variable. Therefore, to reduce the volume of data and the data com- munication cost in a distributed environment, we propose a vertical data-partitioning approach for PRF according to the independence of feature variables and the resource requirements of computing tasks. The training dataset is "
7957,unknown,"divided into several feature subsets. Assume that the size of training dataset Sis N and there are M feature variables in each record. y0 ∼ yM−2 are the input feature variables, and yM−1 is the target feature variable. Each input feature variable yj and the variable yM−1 of all records are selected and generated to a feature subset FSj, which is represented as: FSj =   <0,y0j,y0(M−1) >, <1,"
7958,unknown,"<1,y1j,y1(M−1) >, ..., <i,y ij,yi(M−1) >, ..., <(N −1),y(N−1)j,y(N−1)(M−1) >   , where iis the index of each record of the training dataset S, and j is the index of the current feature variable. In such a way,Sis split into (M−1) feature subsets before dimension- reduction. Each subset is loaded as an RDD object and is independent of the other subsets. The process of the vertical data-parti"
7959,unknown,"data-partitioning is presented in Fig. 3. Fig. 3. Process of the vertical data-partitioning method 4.1.2 Data-Multiplexing Method To address the problem that the volume of the sampled training dataset increases linearly with the increase in the RF scale, we present a data-multiplexing method for PRF by modifying the traditional sampling method. In each sampling period, we do not copy the sampled d"
7960,unknown,"just note down their indexes into a Data-Sampling-Index (DSI) table. Then, the DSI table is allocated to all slave nodes together with the feature subsets. The computing tasks in the training process of each decision tree load the corresponding data from the same feature subset via the DSI table. Thus, each feature subset is reused effectively, and the volume of the training dataset will not incre"
7961,unknown,"volume of the training dataset will not increase any more despite the expansion of the RF scale. First, we create a DSI table to save the data indexes generated in all sampling times. As mentioned in Section 3.1, the scale of a RF model is k. Namely, there are k sampling times for the training dataset, and N data indexes are noted down in each sampling time. An example of the DSI table of PRF is p"
7962,unknown,"TABLE 2 Example of the DSI table of PRF Data indexes of training dataset Sampling times Sample0 1 3 8 5 10 0 ... Sample1 2 4 1 9 7 8 ... Sample2 9 1 12 92 2 5 ... Sample3 3 8 87 62 0 2 ... ... ... ... ... ... ... ... ... Samplek−1 9 1 4 43 3 5 ... Second, the DSI table is allocated to all slave nodes of the Spark cluster together with all feature subsets. In the subsequent training process, the ga"
7963,unknown,"of different trees for the same feature variable are dispatched to the slaves where the required subset is located. Third, each gain-ratio computing task accesses the rele- vant data indexes from the DSI table, and obtains the feature records from the same feature subset according to these indexes. The process of the data-multiplexing method of PRF is presented in Fig. 4. Fig. 4. Process of the da"
7964,unknown,"In Fig. 4, each RDDFS refers to an RDD object of a feature subset, and each TGR refers to a gain- ratio computing task. For example, we allocate tasks {TGR1.1,TGR1.2,TGR1.3}to Slave1 for the feature subset RDDFS 1, allocate tasks {TGR2.1,TGR2.2,TGR2.3}to Slave2 for RDDFS 2, and allocate tasks {TGR3.1,TGR3.2,TGR3.3}to Slave3 for RDDFS 3. From the perspective of the decision trees, tasks in the same"
7965,unknown,"trees. For example, tasks TGR1.1, TGR1.2, and TGR1.3 in the Slave1 belong to “DecisionTree1”, “DecisionTree2”, and “DecisionTree3”, respectively. These tasks obtain records from the same feature subset according to the corresponding 7 indexes in DSI, and compute the gain ratio of the feature variable for different decision trees. After that, the interme- diate results of these tasks are submitted "
7966,unknown,"ing subsequent tasks to build meta decision trees. Results of the tasks {TGR1.1, TGR2.1, TGR3.1}are combined for the tree node splitting process of “DecisionTree1”, and results of the tasks {TGR1.2, TGR2.2, TGR3.2}are combined for that of “DecisionTree2”. 4.1.3 Static Data Allocation To achieve a better balance of data storage and workload, after the vertical data-partitioning, a static data alloc"
7967,unknown,"method is applied for the feature subsets. Namely, these subsets are allocated to a distributed Spark cluster before the training process of PRF. Moreover, because of the dif- ference of the data type and volume of each feature subset, the workloads of the relevant subsequent computing tasks will be different as well. As we know, a Spark cluster is constructed by a master node and several slave no"
7968,unknown,"deﬁne our allocation function to determine each feature subset be allocated to which nodes, and allocate each feature subset according to its volume. There are 3 scenarios in the data allocation scheme. Examples of the 3 scenarios of the data allocation method are shown in Fig. 5. Fig. 5. Example of 3 scenarios of the data allocation In Fig. 5, (a) when the size of a feature subset is greater than"
7969,unknown,"than the available storage capacity of a slave node, this sub- set is allocated to limited multiple slaves that have similar physical locations. (b) When the size of a feature subset is equal to the available storage capacity of a slave node, the subset is allocated to the node. (c) When the size of a feature subset is smaller than the available storage capacity of a slave node, this node will acc"
7970,unknown,"subsets. In case (a), the data communication operations of the subsequent gain-ratio computing tasks occur among the slave nodes where the current feature subset is located. These data operations are local communications but not global communications. In cases (b) and (c), no data com- munication operations occur among different slave nodes in the subsequent gain-ratio computation process. The ste"
7971,unknown,"of the vertical data-partitioning and static data allocation of PRF are presented in Algorithm 4.1. In Algorithm 4.1, RDDo is split into (M −1) RDDFS objects via the vertical data-partitioning function ﬁrstly. Then, each RDDFS is allocated to slave nodes according to its volume and the available storage capacity of the slave nodes. To reuse the training dataset, each RDD object of the feature subs"
7972,unknown,feature subset is allocated and persisted to Spark cluster via a dataAllocation() function and a persist() function. Algorithm 4.1 Vertical data-partitioning and static data allocation of PRF Input: RDDo: an RDD object of the original training dataset S. Output: LFS : a list of the indexes of each feature subset’s RDD object and the allocated slave nodes. 1: for j = 0to (M −2) do 2: RDDFSj ←RDDo.m
7973,unknown,"2: RDDFSj ←RDDo.map 3: < i, yij, yi(M−1) >←RDDo.verticalPartition(j); 4: end map.collect() 5: slaves ←ﬁndAvailableSlaves().sortbyIP(); 6: if RDDFSj .size < slaves[0].availablesize then 7: dataAllocation(RDDFSj , slaves[0]); 8: slaves[0].availablesize ← slaves[0].availablesize - RDDFSj .size; 9: LFS ←< RDDFSj .id, slaves[0].nodeid >; 10: else 11: while RDDFSj ̸= null do 12: (RDD ′ FSj , RDD ′′ FSj "
7974,unknown,"slaves[i].availablesize)); 13: dataAllocation(RDD ′ FSj , slaves[i]); 14: RDD ′ FSj .persist(); 15: slaves[i].availablesize ← slaves[i].availablesize - RDD ′ FSj .size; 16: slavesids ←slaves[i].nodeid; 17: RDDFSj ←RDD ′′ FSj ; 18: i ←i + 1; 19: end while 20: LFS ←< RDDFSj .id, slavesids >; 21: end if 22: end for 23: return LFS . 4.2 Task-Parallel Optimization Each decision tree of PRF is built ind"
7975,unknown,"and each sub-node of a decision tree is also split indepen- dently. The structures of the PRF model and decision tree model make the computing tasks have natural parallelism. Based on the results of the data-parallel optimization, we propose a task-parallel optimization for PRF and implement it on Spark. A dual-parallel approach is carried out in the training process of PRF, and a task DAG is crea"
7976,unknown,"to the dual-parallel training process and the dependence of the RDD objects. Then, different task schedulers are invoked to perform the tasks in the DAG. 4.2.1 Parallel Training Process of PRF In our task-parallel optimization approach, a dual-parallel training approach is proposed in the training process of PRF on Spark. k decision trees of the PRF model are built in parallel at the ﬁrst level of"
7977,unknown,"(M−1) feature variables in each decision tree are calculated concurrently for tree node splitting at the second level of the training process. There are several computing tasks in the training process of each decision tree of PRF. According to the required data resources and the data communication cost, the computing tasks are divided into two types, gain-ratio computing tasks and node-splitting t"
7978,unknown,"Deﬁnition 3. Gain-ratio-computing task ( TGR) is a task set that is employed to compute the gain ratio of a feature 8 variable from the corresponding feature subset, which in- cludes a series of calculations for each feature variable, i.e., the entropy, the self-split information, the information gain, and the gain ratio. The results of TGR tasks are submitted to the corresponding subsequent node-"
7979,unknown,"Deﬁnition 4. Node-splitting task (TNS ) is a task set that is employed to collect the results of the relevant TGR tasks and split the decision tree nodes, which includes a series of calculations for each tree node, such as determining the best splitting variable holds the highest gain ratio value and splitting the tree node by the variable. After the tree node splitting, the results of TNS tasks a"
7980,unknown,slave to begin the next stage of the PRF’s training process. The steps of the parallel training process of the PRF model are presented in Algorithm 4.2. Algorithm 4.2 Parallel training process of the PRF model Input: k: the number of decision trees of the PRF model; TDSI : the DSI table of PRF; LFS : a list of the indexes of each feature subset’s RDD object and the allocated slave nodes. Output: P
7981,unknown,"Output: PRFtrained: the trained PRF model. 1: for i = 0 to (k −1) do 2: for j = 0 to (M −2) do 3: load feature subset RDDFSj ←loadData(LFS [i]); //TGR: 4: RDD(GR,best) ←sc.parallelize(RDDFSj ).map 5: load sampled data RDD(i,j) ←(TDSI [i], RDDFSj ); 6: calculate the gain ratio GR(i,j) ←GR(RDD(i,j)); 7: end map //TNS : 8: RDD(GR,best).collect().sorByKey(GR).top(1); 9: for each value y(j,v) in RDD(GR"
7982,unknown,"10: split tree node Nodej ←< y(j,v), V alue >; 11: append Nodej to Ti; 12: end for 13: end for 14: PRFtrained ←Ti; 15: end for 16: return PRFtrained. According to the parallel training process of PRF and the dependence of each RDD object, each job of the program of PRF’s training process is split into different stages, and a task DAG is constructed with the dependence of these job stages. Taking a"
7983,unknown,"stages. Taking a decision tree model of PRF as an example, a task DAG of the training process is presented in Fig. 6. There are several stages in the task DAG, which corre- spond to the levels of the decision tree model. In stage 1, after the dimension-reduction, (m−1) TGR tasks ( TGR1.0 ∼TGR1.(m−2)) are generated for the (m−1) input feature variables. These TGRs compute the gain ratio the corre- "
7984,unknown,"sponding feature variable, and submit their results to TNS1. TNS1 ﬁnds the best splitting variable and splits the ﬁrst tree node for the current decision tree model. Assuming that y0 is the best splitting variable at the current stage, and the value of y0 is in the range of {v01,v02,v03}. Hence, the ﬁrst tree node is constructed by y0, and 3 sub-nodes are split from the node, as shown in Fig. 6(b)"
7985,unknown,"splitting, the intermediate result of TNS1 are distributed to all slave nodes. The result includes information of the Fig. 6. Example of the task DAG for a decision tree of PRF splitting variable and the data index list of {v01,v02,v03}. In stage 2, because y0 is the splitting feature, there is no TGR task for FS0. The potential workload balance problem of this issue will be discussed in Section 4"
7986,unknown,"New TGR tasks are generated for all other feature subsets according to the result of TNS1. Due to the data index list of {v01,v02,v03}, there are no more than 3 TGR tasks for each feature subset. For example, tasks TGR2.11, TGR2.12, and TGR2.13 calculate the data of FS1 with the indexes corresponding to v01, v02, and v03, respectively. And the condition is similar in tasks for FS2 ∼FS(m−2). Then, "
7987,unknown,"results of tasks {TGR2.11, TGR2.21, TGR2.(m−2)1}are submit- ted to task TNS2.1 for the same sub-tree-node splitting. Tasks of other tree nodes and other stages are performed similarly. In such a way, a task DAG of the training process of each decision tree model is built. In addition, k DAGs are built respectively for the kdecision trees of the PRF model. 4.2.2 Task-Parallel Scheduling After the c"
7988,unknown,"4.2.2 Task-Parallel Scheduling After the construction of the task DAGs of all the decision trees, the tasks in these DAGs are submitted to the Spark task scheduler. There exist two types of computing tasks in the DAG, which have different resource requirements and parallelizables. To improve the performance of PRF efﬁciently and further minimize the data communication cost of tasks in the distribu"
7989,unknown,"different task-parallel schedulers to perform these tasks. In Spark, the TaskSchedulerListener module mon- itors the submitted jobs, splits the job into differ- ent stages and tasks, and submits these tasks to the TaskScheduler module. The TaskScheduler module re- ceives the tasks and allocates and executes them us- ing the appropriate executors. According to the differ- ent allocations, the TaskS"
7990,unknown,"sub-modules, such as LocalScheduler, ClusterScheduler, and MessosScheduler. Meanwhile, each task holds 5 types of locality property value: PROCESS LOCAL, NODE LOCAL, NO PREF , PACK LOCAL, and ANY . We set the value of the locality properties of these two types of tasks and submit them into different task schedulers. We invoke LocalScheduler for TGR tasks and ClusterScheduler to perform TNS tasks. "
7991,unknown,"9 (1) LocalScheduler for TGR tasks. The LocalScheduler module is a thread pool of the local computer, all tasks submitted by DAGScheduler is executed in the thread pool, and the results will then be returned to DAGScheduler. We set the locality property value of each TGR as NODE LOCAL and submit it to a LocalScheduler module. In LocalScheduler, all TGR tasks of PRF are allocated to the slave nodes"
7992,unknown,"corresponding feature subsets are located. These tasks are independent of each other, and there is no synchronization restraint among them. If a feature subset is allocated to multiple slave nodes, the corresponding TGR tasks of each decision tree are allocated to these nodes. And there exist local data communication operations of the tasks among these nodes. If one or more feature subsets are all"
7993,unknown,"to one slave node, the corresponding TGR tasks are posted to the current node. And there is no data communication operation between the current node and the others in the subsequent computation process. (2) ClusterScheduler for TNS tasks. The ClusterScheduler module monitors the execution situation of the computing resources and tasks in the whole Spark cluster and allocates tasks to suitable work"
7994,unknown,"mentioned above, TNS tasks are used to collect the results of the corresponding TGR tasks and split the decision tree nodes. TNS tasks are independent of all feature subsets and can be scheduled and allocated in the whole Spark cluster. In addition, these TNS tasks rely on the results of the corresponding TGR tasks, therefore, there is a wait and synchronization restraint for these tasks. Therefor"
7995,unknown,invoke the ClusterScheduler to perform TNS tasks. We set the locality property value of each TNS as ANY and submit to a ClusterScheduler module. The task-parallel scheduling scheme for TNS tasks is described in Algorithm 4.3. A diagram of task-parallel scheduling for the tasks in the above DAG is shown in Fig. 7. Algorithm 4.3 Task-parallel scheduling for TNS tasks Input: TSNS : a task set of all 
7996,unknown,Output: ERTS : the execution results of TSNS . 1: create manager ←new TaskSetManager(TSNS ); 2: append to taskset manager activeTSQueue ←manager; 3: if hasReceivedTask == false then 4: create starvationTimer ← scheduleAtFixedRate(new TimerTask); 5: rank the priority of TS 2 ←activeTSQueue .FIFO(); 6: for each task Ti in TS 2 do 7: get available worker executora from workers; 8: ERTS ←executora.lau
7997,unknown,"8: ERTS ←executora.launchTask(Ti.taskid); 9: end for 10: end if 11: return ERTS . 4.3 Parallel Optimization Method Analysis We discuss our hybrid parallel optimization method from 5 aspects as follows. In comparison with Spark-MLRF and other parallel methods of RF, our hybrid parallel opti- mization approach of PRF achieves advantages in terms of performance, workload balance, and scalability. Fig"
7998,unknown,"Fig. 7. Task-parallel scheduling based on the DAG in Fig. 6 4.3.1 Computational Complexity Analysis As discussed in Section 3.4, the total computational com- plexity of the improved PRF algorithm with dimension- reduction is O(k(MN + mNlog N)). After the paralleliza- tion of the PRF algorithm on Spark, M features of train- ing dataset are calculated in parallel in the process of dimension-reductio"
7999,unknown,"Therefore, the theoretical computational complexity of the PRF algorithm is O(k(MN +mN log N) k×M ) ≈O(N(log N + 1)). 4.3.2 Data Volume Analysis Taking advantage of the data-multiplexing method, the training dataset is reused effectively. Assume that the vol- ume of the original dataset is (N ×M) and the RF model’s scale is k, the volumes of the sampled training dataset in the original RF and Spar"
8000,unknown,"In our PRF, the volume of the sampled training dataset is (N ×2 ×(M −1)) ≈(2NM). Moreover, the increase of the scale of PRF does not lead to changes in the data size and storage location. Therefore, compared with the sampling method of the original RF and Spark-MLRF, the data-parallel method of our PRF decreases the total volume of the training dataset for PRF. 4.3.3 Data Communication Analysis In"
8001,unknown,"4.3.3 Data Communication Analysis In PRF, there exist data communication operations in the process of data allocation and the training process. Assume that there are n slaves in a Spark cluster, and the data volume of the sampled training dataset is (2NM). In the process of data allocation, the average data communication cost is (2MN n ). In the process of the PRF model training, if a feature subs"
8002,unknown,"feature subset is allocated to several computer nodes, local data communication operations of the subsequent comput- ing tasks occur among these nodes. If one or more feature subsets are allocated to one computer node, there is no 10 data communication operation among different nodes in the subsequent computation process. Generally, there is a small amount of data communication cost for the interm"
8003,unknown,results in each stage of the decision tree’s training pro- cess. The vertical data-partitioning and static data allocation method mitigates the amount of data communication in the distributed environment and overcomes the performance bottleneck of the traditional parallel method. 4.3.4 Resource and Workload Balance Analysis From the view point of the entire training process of PRF in the whole Spa
8004,unknown,"in the whole Spark cluster, our hybrid parallel optimization approach achieves a better storage and workload balance than other algorithms. One reason is that because the dif- ferent volumes of feature subsets might lead to different workloads of the TGR tasks for each feature variable, we allocate the feature subsets to the Spark cluster according to its volume. A feature subset with a large volu"
8005,unknown,"allocated to multiple slave nodes. And the corresponding TGR tasks are scheduled among these nodes in parallel. A feature subsets with a small volume are allocated to one slave node. And the corresponding TGR tasks are scheduled on the current node. A second reason is that with the tree nodes’ splitting, the slave nodes where the split variables’ feature subsets are located will revert to an idle "
8006,unknown,"of the entire training process of PRF, proﬁt from the data- multiplexing method of PRF, each feature subset is shared and reused by all decision trees, and it might be split for different tree nodes in different trees. That is, although a feature subset is split and useless to a decision tree, it is still useful to other trees. Therefore, our PRF not only does not lead to the problem of waste of r"
8007,unknown,"imbalance, but also makes full use of the data resources and achieves an overall workload balance. 4.3.5 Algorithm Scalability Analysis We discuss the stability and scalability of our PRF algorithm from 3 perspectives. (1) The data-multiplexing method of PRF makes the training dataset be reused effectively. When the scale of PRF expands, namely, the number of decision trees increases, the data siz"
8008,unknown,"feature subsets need not change. It only results in an increase in computing tasks for new decision trees and a small amount of data communication cost for the intermediate results of these tasks. (2) When the Spark cluster’s scale expands, only a few feature subsets with a high storage load are migrated to the new computer nodes to achieve storage load and workload balance. (3) When the scale of "
8009,unknown,"the training dataset increases, it is only necessary to split feature subsets from the new data in the same vertical data-partitioning way, and append each new subset to the corresponding original one. Therefore, we can draw the conclusion that our PRF algorithm with the hybrid parallel optimization method achieves good stability and scalability. 5 E XPERIMENTS 5.1 Experiment Setup All the experim"
8010,unknown,"form, which is built of one master node and 100 slave nodes. Each node executes in Ubuntu 12.04.4 and has one Pentium (R) Dual-Core 3.20GHz CPU and 8GB memory. All nodes are connected by a high-speed Gigabit network and are conﬁgured with Hadoop 2.5.0 and Spark 1.1.0. The algorithm is implemented in Scala 2.10.4. Two groups of datasets with large scale and high dimensionality are used in the exper"
8011,unknown,"in the experiments. One is from the UCI machine learning repository [33], as shown in Table 3. Another is from a actual medical project, as shown in Table 4. TABLE 3 Datasets from the UCI machine learning repository Datasets Instances Features Classes Data Size Data Size (Original) (Maximum) URL Reputation (URL) 2396130 3231961 5 2.1GB 1.0TB You Tube Video Games (Games) 120000 1000000 14 25.1GB 2."
8012,unknown,"Bag of Words (Words) 8000000 100000 24 15.8GB 1.3TB Gas sensor arrays (Gas) 1800000 1950000 15 50.2GB 2.0TB TABLE 4 Datasets from a medical project Datasets Instances Features Classes Data size Data size (Original) (Maximum) Patient 279877 25652 18 3.8GB 1.5TB Outpatient 3657789 47562 9 10.6GB 1.0TB Medicine 7502058 52460 12 20.4GB 2.0TB Cancer 3568000 46532 21 5.8GB 2.0TB In Table 3 and Table 4, "
8013,unknown,"the original size of the data from the UCI and the project, and Datasize(Maximum) refers to the peak size of data sampled by all of the comparison algorithms. In the Spark platform, the training data not be loaded into the memory as a whole. Spark can be used to process datasets that are greater than the total cluster memory ca- pacity. RDD objects in a single executor process are accessed by an i"
8014,unknown,"by an iteration, and the data are buffered or thrown away after the processing. The cost of memory is very small when there is no requirement of caching the results of the RDD objects. In this case, the results of the iterations are retained in a memory pool by the cache manager. When the data in the memory are not applicable, they will be saved on disk. In this case, part of the data can be kept "
8015,unknown,"the rest is stored in the disk. Therefore, the training data with the peak size of 2.0TB can be executed on Spark. 5.2 Classiﬁcation Accuracy We evaluate the classiﬁcation accuracy of PRF by compari- son with RF, DRF, and Spark-MLRF. 5.2.1 Classiﬁcation Accuracy for Different Tree Scales To illustrate the classiﬁcation accuracy of PRF, experiments are performed for the RF, DRF [18], Spark-MLRF, an"
8016,unknown,algorithms. The datasets are outlined in Table 3 and Table 4. Each case involves different scales of the decision tree. The experimental results are presented in Fig. 8. 11 Fig. 8. Average classiﬁcation accuracy for different tree scales Fig. 8 shows that the average classiﬁcation accuracies of all of the comparative algorithms are not high when the number of decision trees is equal to 10. As the 
8017,unknown,"decision trees increases, the average classiﬁcation accuracies of these algorithms increase gradually and have a tendency toward a convergence. The classiﬁcation accuracy of PRF is higher than that of RF by 8.9%, on average, and 10.6% higher in the best case when the number of decision trees is equal to 1500. It is higher than that of DRF by 6.1%, on average, and 7.3% higher in the best case when "
8018,unknown,"trees is equal to 1300. The classiﬁcation accuracy of PRF is higher than that of Spark-MLRF by 4.6% on average, and 5.8% in the best case when the number of decision trees is equal to 1500. Therefore, compared with RF, DRF, and Spark-MLRF, PRF improves the classiﬁcation accuracy signiﬁcantly. 5.2.2 Classiﬁcation Accuracy for Different Data Sizes Experiments are performed to compare the classiﬁcati"
8019,unknown,"accuracy of PRF with the RF, DRF, and Spark-MLRF al- gorithms. Datasets from the project described in Table 4 are used in the experiments. The experimental results are presented in Fig. 9. Fig. 9. Average classiﬁcation accuracy for different data sizes The classiﬁcation accuracies of PRF in all of the cases are greater than that of RF, DRF, and Spark-MLRF obviously for each scale of data. The clas"
8020,unknown,"greater than that of DRF by 8.6%, on average, and 10.7% higher in the best case when the number of samples is equal to 3,000,000. The classiﬁcation accuracy of PRF is greater than that of Spark-MLRF by 8.1%, on average, and 11.3% higher in the best case when the number of samples is equal to 3,000,000. For Spark-MLRF, because of the method of sampling for each partition of the dataset, as the size"
8021,unknown,"of the dataset increases, the ratio of the random selection of the dataset increases, and the accuracy of Spark-MLRF decreases inevitably. Therefore, compared with RF, DRF, and Spark-MLRF, PRF improves the classiﬁcation accuracy signiﬁcantly for different scales of datasets. 5.2.3 OOB Error Rate for Different Tree Scales We observe the classiﬁcation error rate of PRF under dif- ferent conditions. "
8022,unknown,"ferent conditions. In each condition, the Patient dataset is chosen, and two scales (500 and 1000) of decision trees are constructed. The experimental results are presented in Fig. 10 and Table 5. Fig. 10. OOB error rates of PRF for different tree scales When the number of decision trees of PRF increases, the OOB error rate in each case declines gradually and tends to a convergence condition. The "
8023,unknown,"is 0.138 when the number of decision trees is equal to 500, and it is 0.089 when the number of decision trees is equal to 1000. TABLE 5 OOB error rates of PRF for different tree scales Tree=500 Tree=1000 Rate OOB Class1 Class2 OOB Class1 Class2 max 0.207 0.270 0.354 0.151 0.132 0.318 min 0.113 0.051 0.092 0.067 0.010 0.121 mean 0.138 0.094 0.225 0.089 0.056 0.175 5.3 Performance Evaluation Various"
8024,unknown,"5.3 Performance Evaluation Various experiments are constructed to evaluate the perfor- mance of PRF by comparison with the RF and Spark-MLRF algorithms in terms of the execution time, speedup, data volume, and data communication cost. 5.3.1 Average Execution Time for Different Datasets Experiments are performed to compare the performance of PRF with that of RF and Spark-MLRF. Four groups of traini"
8025,unknown,"training datasets are used in the experiments, such as URL, Games, Outpatient, and Patient. In these experiments, the number of decision trees in each algorithm is both 500, and the number of Spark slaves is 10. The experimental results are presented in Fig. 11. 12 Fig. 11. Average execution time of the algorithms for different datasets When the data size is small (e.g., less than 1.0GB), the exec"
8026,unknown,"execution times of PRF and Spark-MLRF are higher than that of RF. The reason is that there is a ﬁxed time required to submit the algorithms to the Spark cluster and conﬁgure the programs. When the data size is greater than 1.0GB, the average execution times of PRF and Spark-MLRF are less than that of RF in the four cases. For example, in the Outpatient case, when the data size grows from 1.0 to 50"
8027,unknown,"500.0GB, the average execution time of RF increases from 19.9 to 517.8 seconds, while that of Spark-MLRF increases from 24.8 to 186.2 seconds, and that of PRF increases from 23.5 to 101.3 seconds. Hence, our PRF algorithm achieves a faster processing speed than RF and Spark-MLRF. When the data size increases, the beneﬁt is more noticeable. Taking advantage of the hybrid parallel optimization, PRF "
8028,unknown,"signiﬁcant strengths over Spark-MLRF and RF in terms of performance. 5.3.2 Average Execution Time for Different Cluster Scales In this section, the performance of PRF on the Spark plat- form for different scales of slave nodes is considered. The number of slave nodes is gradually increased from 10 to 100, and the experiment results are presented in Fig. 12. Fig. 12. Average execution time of PRF f"
8029,unknown,"In Fig. 12, because of the different data sizes and contents of the training data, the execution times of PRF in each case are different. When the number of slave nodes increases from 10 to 50, the average execution times of PRF in all cases obviously decrease. For example, the average execution time of PRF decreases from 405.4 to 182.6 seconds in the Gas case and from 174.8 to 78.3 seconds in the"
8030,unknown,"By comparison, the average execution times of PRF in the other cases decrease less obviously when the number of slave nodes increases from 50 to 100. For example, the average execution time of PRF decreases from 182.4 to 76.0 seconds in the Gas case and from 78.3 to 33.0 seconds in the Medicine case. This is because when the number of the Spark slaves greater than that of training dataset’s featur"
8031,unknown,"variables, each feature subset might be allocated to multiple slaves. In such a case, there are more data communication operations among these slaves than before, which leads to more execution time of PRF. 5.3.3 Speedup of PRF in Different Environments Experiments in a stand-alone environment and a Spark cloud platform are performed to evaluate the speedup of PRF. Because of the different volume o"
8032,unknown,"the execution times of PRF are not in the same range in different cases. To observe the comparison of the execution time intuitively, a normalization of execution time is taken. Let T(i,sa) be the execution time of PRF for dataset Si in the stand-alone environment, and ﬁrst normalized to 1. The execution time of PRF on Spark is normalized as described in Eq. (11): T ′ i = { T(i,sa) T(i,sa) = 1 Sta"
8033,unknown,"T(i,Spark) T(i,sa) Spark. (11) The speedup of PRF on Spark for Si is deﬁned in Eq. (12): Speedup(i,Spark) = T ′ (i,Spark) T ′ (i,sa) . (12) The results of the comparative analyses are presented in Fig. 13. Taking beneﬁts of the parallel algorithm and cloud environment, the speedup of PRF on Spark tends to increase in each experiment with the number of slave nodes. When the number of slave nodes is"
8034,unknown,"of PRF in all cases is in the range of 60.0 - 87.3, which is less than the theoretical value (100). Because there exists data communication time in a distributed environment and a ﬁxed time for the application submission and conﬁguration, it is understandable that the whole speedup of PRF is less than the theoretical value. Due to the different data volumes and contents, the speedup of PRF in each"
8035,unknown,"When the number of slave nodes is less than 50, the speedup in each case shows a rapid growth trend. For instance, compared with the stand-alone environment, the speedup factor of Gas is 65.5 when the number of slave nodes is equal to 50, and the speedup factor of Patient is 61.5. However, the speedup in each case shows a slow growth trend when the number of slave nodes is greater than 50. This is"
8036,unknown,"13 Fig. 13. Speedup of PRF in different environments scheduling, and data communication operations required for PRF. 5.3.4 Data Volume Analysis for Different RF Scales We analyze the volume of the training data in PRF against RF and Spark-MLRF. Taking theGamescase as an example, the volumes of the training data in the different RF scales are shown in Fig. 14. Fig. 14. Size of training dataset for "
8037,unknown,"In Fig. 14, due to the use of the same horizontal sampling method, the training data volumes of RF and Spark-MLRF both show a linear increasing trend with the increasing of the RF model scale. Contrary, in PRF, the total volume of all training feature subsets is 2 times the size of the orig- inal training dataset. Making use of the data-multiplexing approach of PRF, the training dataset is effecti"
8038,unknown,"When the number of decision trees is larger than 2, despite the expansion of RF scale, the volume of the training data will not increases any further. 5.3.5 Data Communication Cost Analysis Experiments are performed for different scales of the Spark cluster to compare the Data Communication Cost ( CDC) of PRF with that of Spark-MLRF. The suffer-write size of slave nodes in the Spark cluster is mon"
8039,unknown,"Fig. 15. Data communication costs of PRF and Spark-MLRF the algorithms. Taking the Patient case as an example, the results of the comparison of CDC are presented in Fig. 15. From Fig. 15, it is clear that the CDC of PRF are less than that of Spark-MLRF in all cases, and the distinction is larger with increasing number of slave nodes. Although Spark- MLRF also uses the data-parallel method, the hor"
8040,unknown,"partitioning method for training data makes the computing tasks have to frequent access data across different slaves. As the number of slaves increases from 5 to 50, the CDC of Spark-MLRF increases from 350.0MB to 2180.0MB. Different from Spark-MLRF, in PRF, the vertical data-partitioning and allocation method and the task scheduling method make the most of the computing tasks ( TGR) access data f"
8041,unknown,"local slave, reducing the amount of data transmission in the distributed environment. As the number of slaves increases from 5 to 50, the CDC of PRF increases from 50.0MB to 320.0MB, which is much lower than that of Spark-MLRF. Therefore, PRF minimizes the CDC of RF in a distributed environment. The expansion of the cluster’s scale does not lead to an obviously increase inCDC. In conclusion, our P"
8042,unknown,"achieves a superiority and notable advantages over Spark- MLRF in terms of stability and scalability. 6 C ONCLUSIONS In this paper, a parallel random forest algorithm has been proposed for big data. The accuracy of the PRF algorithm is optimized through dimension-reduction and the weighted vote approach. Then, a hybrid parallel approach of PRF combining data-parallel and task-parallel optimization"
8043,unknown,"performed and implemented on Apache Spark. Taking advantage of the data-parallel optimization, the training dataset is reused and the volume of data is reduced signif- icantly. Beneﬁtting from the task-parallel optimization, the data transmission cost is effectively reduced and the per- formance of the algorithm is obviously improved. Experi- mental results indicate the superiority and notable str"
8044,unknown,"of PRF over the other algorithms in terms of classiﬁcation accuracy, performance, and scalability. For future work, we will focus on the incremental parallel random forest algo- rithm for data streams in cloud environment, and improve the data allocation and task scheduling mechanism for the algorithm on a distributed and parallel environment. 14 ACKNOWLEDGMENT The research was partially funded by"
8045,unknown,"National Natural Science Foundation of China (Grant Nos. 61133005, 61432005), the National Natural Science Founda- tion of China (Grant Nos. 61370095, 61472124, 61202109, 61472126,61672221), the National Research Foundation of Qatar (NPRP , Grant Nos. 8-519-1-108), and the Natural Science Foundation of Hunan Province of China (Grant Nos. 2015JJ4100, 2016JJ4002). REFERENCES [1] X. Wu, X. Zhu, and G"
8046,unknown,"data,” Knowledge and Data Engineering, IEEE Transactions on, vol. 26, no. 1, pp. 97–107, January 2014. [2] L. Kuang, F. Hao, and Y. L.T., “A tensor-based approach for big data representation and dimensionality reduction,” Emerging Topics in Computing, IEEE Transactions on, vol. 2, no. 3, pp. 280–291, April 2014. [3] A. Andrzejak, F. Langner, and S. Zabala, “Interpretable models from distributed da"
8047,unknown,"sion trees,” in Computational Intelligence and Data Mining (CIDM), 2013 IEEE Symposium on. IEEE, 2013, pp. 1–9. [4] P . K. Ray, S. R. Mohanty, N. Kishor, and J. P . S. Catalao, “Optimal feature and decision tree-based classiﬁcation of power quality disturbances in distributed generation systems,” Sustainable Energy, IEEE Transactions on, vol. 5, no. 1, pp. 200–208, January 2014. [5] Apache, “Hadoo"
8048,unknown,"[5] Apache, “Hadoop,” Website, June 2016, http://hadoop. apache.org. [6] S. del Rio, V . Lopez, J. M. Benitez, and F. Herrera, “On the use of mapreduce for imbalanced big data using random forest,” Information Sciences, vol. 285, pp. 112–137, Novem- ber 2014. [7] K. Singh, S. C. Guntuku, A. Thakur, and C. Hota, “Big data analytics framework for peer-to-peer botnet detection using random forests,” "
8049,unknown,"using random forests,” Information Sciences, vol. 278, pp. 488–497, September 2014. [8] Apache, “Spark,” Website, June 2016, http: //spark-project.org. [9] L. Breiman, “Random forests,” Machine Learning, vol. 45, no. 1, pp. 5–32, October 2001. [10] G. Wu and P . H. Huang, “A vectorization-optimization- method-based type-2 fuzzy neural network for noisy data classiﬁcation,” Fuzzy Systems, IEEE Tran"
8050,unknown,"classiﬁcation,” Fuzzy Systems, IEEE Transactions on, vol. 21, no. 1, pp. 1–15, February 2013. [11] H. Abdulsalam, D. B. Skillicorn, and P . Martin, “Classiﬁca- tion using streaming random forests,” Knowledge and Data Engineering, IEEE Transactions on, vol. 23, no. 1, pp. 22–36, January 2011. [12] C. Lindner, P . A. Bromiley, M. C. Ionita, and T. F. Cootes, “Robust and accurate shape model matching"
8051,unknown,"dom forest regression-voting,” Pattern Analysis and Ma- chine Intelligence, IEEE Transactions on, vol. 25, no. 3, pp. 1–14, December 2014. [13] X. Yun, G. Wu, G. Zhang, K. Li, , and S. Wang, “Fastraq: A fast approach to range-aggregate queries in big data envi- ronments,” Cloud Computing, IEEE Transactions on, vol. 3, no. 2, pp. 206–218, April 2015. [14] M. Xu, H. Chen, and P . K. Varshney, “Dimen"
8052,unknown,"reduction for registration of high-dimensional data sets,” Image Processing, IEEE Transactions on, vol. 22, no. 8, pp. 3041–3049, August 2013. [15] Q. Tao, D. Chu, and J. Wang, “Recursive support vector machines for dimensionality reduction,” Neural Networks, IEEE Transactions on, vol. 19, no. 1, pp. 189–193, January 2008. [16] Y. Lin, T. Liu, and C. Fuh, “Multiple kernel learning for dimensionali"
8053,unknown,"dimensionality reduction,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 6, pp. 1147– 1160, June 2011. [17] C. Strobl, A. Boulesteix, T. Kneib, and T. Augustin, “Con- ditional variable importance for random forests,” BMC Bioinformatics, vol. 9, no. 14, pp. 1–11, 2007. [18] S. Bernard, S. Adam, and L. Heutte, “Dynamic random forests,” Pattern Recognition Letters, vol"
8054,unknown,"forests,” Pattern Recognition Letters, vol. 33, no. 12, pp. 1580–1586, September 2012. [19] T. M. Khoshgoftaar, J. V . Hulse, and A. Napolitano, “Com- paring boosting and bagging techniques with noisy and imbalanced data,” Systems, Man and Cybernetics, IEEE Transactions on, vol. 41, no. 3, pp. 552–568, May 2011. [20] G. Yu, N. A. Goussies, J. Yuan, and Z. Liu, “Fast action detection via discrimina"
8055,unknown,"k subvolume search,” Multimedia, IEEE Transactions on, vol. 13, no. 3, pp. 507–517, June 2011. [21] G. Biau, “Analysis of a random forests model,” Journal of Machine Learning Research, vol. 13, no. 1, pp. 1063–1095, January 2012. [22] J. D. Basilico, M. A. Munson, T. G. Kolda, K. R. Dixon, and W. P . Kegelmeyer, “Comet: A recipe for learning and using large ensembles on massive data,” in IEEE Inte"
8056,unknown,"Conference on Data Mining, October 2011, pp. 41–50. [23] K. M. Svore and C. J. Burges, “Distributed stochastic aware random forests efﬁcient data mining for big data,” in Big Data (BigData Congress), 2013 IEEE International Congress on. Cambridge University Press, 2013, pp. 425–426. [24] B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo, “Planet: Massively parallel learning of tree ensembles wit"
8057,unknown,"duce,” Proceedings of the Vldb Endowment, vol. 2, no. 2, pp. 1426–1437, August 2009. [25] S. Tyree, K. Q. Weinberger, and K. Agrawal, “Parallel boosted regression trees for web search ranking,” in In- ternational Conference on World Wide Web, March 2011, pp. 387–396. [26] D. Warneke and O. Kao, “Exploiting dynamic resource allocation for efﬁcient parallel data processing in the cloud,” Parallel an"
8058,unknown,"on, vol. 22, no. 6, pp. 985–997, June 2011. [27] L. Mashayekhy, M. M. Nejad, D. Grosu, Q. Zhang, and W. Shi, “Energy-aware scheduling of mapreduce jobs for big data applications,” Parallel and Distributed Systems, IEEE Transactions on, vol. 26, no. 3, pp. 1–10, March 2015. [28] L. D. Briceno, H. J. Siegel, A. A. Maciejewski, M. Oltikar, and J. Brateman, “Heuristics for robust resource allocation o"
8059,unknown,"of satellite weather data processing on a heterogeneous parallel system,” Parallel and Distributed Systems, IEEE Transactions on, vol. 22, no. 11, pp. 1780–1787, February 2011. [29] F. Zhang, J. Cao, W. Tan, S. Khan, K. Li, and A. Zomaya, “Evolutionary scheduling of dynamic multitasking work- loads for big-data analytics in elastic cloud,” Emerging Topics in Computing, IEEE Transactions on, vol. 2"
8060,unknown,"338–351, August 2014. [30] K. Li, X. Tang, B. Veeravalli, and K. Li, “Scheduling precedence constrained stochastic tasks on heterogeneous cluster systems,” Parallel and Distributed Systems, IEEE Transactions on, vol. 64, no. 1, pp. 191–204, January 2015. [31] Y. Xu, K. Li, J. Hu, and K. Li, “A genetic algorithm for task scheduling on heterogeneous computing systems using multiple priority queues,”"
8061,unknown,"multiple priority queues,” Information Sciences, vol. 270, no. 6, pp. 255–287, June 2014. [32] A. Spark, “Spark mllib - random forest,” Web- site, June 2016, http://spark.apache.org/docs/latest/ mllib-ensembles.html. [33] U. of California, “Uci machine learning repository,” Web- site, June 2016, http://archive.ics.uci.edu/ml/datasets. 15 Jianguo Chen received the Ph.D. degree in Col- lege of Compu"
8062,unknown,"lege of Computer Science and Electronic Engi- neering at Hunan University, China. He was a visiting Ph.D. student at the University of Illinois at Chicago from 2017 to 2018. He is currently a postdoctoral in University of Toronto and Hu- nan University. His major research areas include parallel computing, cloud computing, machine learning, data mining, bioinformatics and big data. He has published"
8063,unknown,"data. He has published research articles in inter- national conference and journals of data-mining algorithms and parallel computing, such as IEEE TPDS , IEEE/ACM TCBB, and Information Sciences. Kenli Li received the Ph.D. degree in computer science from Huazhong University of Science and Technology, China, in 2003. He was a vis- iting scholar at University of Illinois at Urbana- Champaign from 20"
8064,unknown,"Champaign from 2004 to 2005. He is currently a full professor of computer science and technol- ogy at Hunan University and director of National Supercomputing Center in Changsha. His major research areas include parallel computing, high- performance computing, grid and cloud comput- ing. He has published more than 180 research papers in international conferences and journals, such as IEEE-TC, IEEE"
8065,unknown,"IEEE-TPDS, IEEE-TSP, JPDC, ICPP, CCGrid. He is an outstanding member of CCF . He is a senior member of the IEEE and serves on the editorial board of IEEE Transactions on Computers. Zhuo Tang received the Ph.D. in computer sci- ence from Huazhong University of Science and Technology, China, in 2008. He is currently an associate professor of Computer Science and Technology at Hunan University. His r"
8066,unknown,"Technology at Hunan University. His research interests include security model, parallel algo- rithms, and resources scheduling for distributed computing systems, grid and cloud computing. He is a member of ACM and CCF . Kashif Bilal received his PhD from North Dakota State University USA. He is currently a post-doctoral researcher at Qatar University, Qatar. His research interests include cloud co"
8067,unknown,"puting, energy efﬁcient high speed networks, and robustness.Kashif is awarded CoE Student Researcher of the year 2014 based on his re- search contributions during his doctoral studies at North Dakota State University. Shui Yu is currently a Senior Lecturer of School of Information Technology, Deakin University. He is a member of Deakin University Academic Board (2015-2016), a Senior Member of IEEE"
8068,unknown,"and a member of AAAS, the vice chair of Tech- nical Subcommittee on Big Data Processing, Analytics, and Networking of IEEE Communica- tion Society. He is currently serving the editorial boards of IEEE TPDS, IEEE CST, IEEE Access. Chuliang Weng is a principal researcher at Huawei Shannon Lab. He received his Ph.D. from Shanghai Jiao Tong University in 2004, and from 2011 to 2012, he was a visiting "
8069,unknown,"with the Department of Computer Science at Columbia University in the City of New Y ork. His research interests include parallel and dis- tributed systems, operating systems and virtual- ization, and storage systems. He is a member of the IEEE, ACM and CCF . Keqin Li is a SUNY Distinguished Professor of computer science in the State University of New Y ork. His current research interests include p"
8070,unknown,"parallel computing and high-performance com- puting, distributed computing, energy-efﬁcient computing and communication, heterogeneous computing systems, cloud computing, big data computing, CPU-GPU hybrid and cooperative computing, multi-core computing, storage and ﬁle systems, wireless communication networks, sensor networks, peer-to-peer ﬁle sharing sys- tems, mobile computing, service computin"
8071,unknown,"cyber-physical systems. He has published over 590 journal articles, book chapters, and refereed conference papers, and has received sev- eral best paper awards. He is currently serving or has served on the ed- itorial boards of IEEE Transactions on Parallel and Distributed Systems, IEEE Transactions on Computers, IEEE Transactions on Cloud Comput- ing, IEEE Transactions on Services Computing, and "
8072,unknown,"on Sustainable Computing. He is an IEEE Fellow. RESEARCH ARTICLE The effect of sustainable business practices on profitability. Accounting for strategic disclosure Massimiliano Cerciello | Francesco Busato | Simone Taddeo Department of Economic & Legal Studies, University of Naples Parthenope, Naples, Italy Correspondence Simone Taddeo, Department of Economic & Legal Studies, University of Naples,"
8073,unknown,"Generale Parisi 13, 80132 Napoli, Italy. Email: simone.taddeo001@studenti. uniparthenope.it Abstract This work tackles from an empirical perspective the widely debated relationship between sustainability in business practices and profitability, focusing on a sample of listed European firms. To measure the extent of sustainable practices at the firm level, the Comprehensive Environmental, Social, a"
8074,unknown,"level, the Comprehensive Environmental, Social, and Governance (ESG) score is pro- posed. The indicator, computed using the Mazziotta-Pareto method, combines quali- tative ratings on adherence to ESG standards with quantitative observations on the extent of data disclosure. Firms failing to pursue full disclosure are penalized. Focus- ing on the constituents of the Euro Stoxx 300 index, a dynamic "
8075,unknown,"ing on the constituents of the Euro Stoxx 300 index, a dynamic panel model is imple- mented, where profitability is explained by the indicator. The results show that sustainability in business practices reduces profitability. These findings are in line with a strand of literature that highlights the role of strategic disclosure of ESG infor- mation on part of firms. Strategic disclosure occurs as "
8076,unknown,"mation on part of firms. Strategic disclosure occurs as a combination of greenwashing and social washing, with firms overstating the extent of their positive behaviors. The integration of sustainable practices within successful business models thus remains a relevant societal problem. The current EU policy framework is discussed in line with our findings. KEYWORDS dynamic panel estimation, environ"
8077,unknown,"sustainable development 1 | INTRODUCTION In the early 1990s, societal concerns over environmental and social sustainability, arising from the demand side of the market, sparked a shift in business paradigms across developed countries (Kotsantonis et al., 2016). Issues like greenhouse gas emissions, energy consump- tion, worker rights, workforce diversity and gender equity gradually gained momentum"
8078,unknown,"gained momentum in both the academic debate and the management discourse. Corporate policies tackling these dimensions were collec- tively labeled Environmental, Social and Governance (ESG) practices. Some pioneering large firms were the first to try to incorporate ESG practices into their business models, devising various forms of self- regulation schemes (Vitell & Hidalgo, 2006). Over the course"
8079,unknown,"2006). Over the course of the following 20 years, ESG practices have caught off extensively across industries and countries (Agudelo et al.,2019). Favorable reception on part of both the civil society and institutional actors has fueled their diffusion further. Despite the generalized praise enjoyed by ESG prac- tices worldwide, significant cross-firm heterogeneity is still reported in adherence t"
8080,unknown,"adherence to ESG practices, and a significant share of laggards exists (Gerard, 2019). Received: 7 March 2022 Revised: 5 September 2022 Accepted: 21 September 2022 DOI: 10.1002/csr.2389 This is an open access article under the terms of theCreative Commons Attribution-NonCommercial-NoDerivsLicense, which permits use and distribution in any medium, provided the original work is properly cited, the u"
8081,unknown,© 2022 The Authors.Corporate Social Responsibility and Environmental Managementpublished by ERP Environment and John Wiley & Sons Ltd. 802 Corp Soc Responsib Environ Manag.2023;30:802–819.wileyonlinelibrary.com/journal/csr
8082,unknown,"The presence of firms that fail to comply to ESG standards is per se an element that casts doubt on the economic effects of ESG prac- tices. From an economic perspective indeed, it is unclear whether and how ESG practices should affect profitability. Scholars largely disagree on the existence of an ESG premium (Avramov et al., 2021; Gillan et al.,2021; Pollard et al.,2018). One key problem in this"
8083,unknown,"tains to the all-but-straightforward measurement of ESG performance, which leaves room for several alternative approaches and yields con- trasting results (Friede et al., 2015; Gillan et al., 2021; Orlitzky et al.,2003). The empirical literature vastly resorts to ESG ratings, pro- vided by institutional agencies and generally considered as good prox- ies. Ratings alone however may be misleading, s"
8084,unknown,"in part based on information disclosed by the firms themselves, which may face an incentive to strategically avoid full disclosure and provide only information on some areas where they perform best (McBrayer, 2018; Tamimi & Sebastianelli,2017). Information on the environmental dimension of ESG practices in particular is less often available in comparative terms, which raises concerns on the driver"
8085,unknown,"of disclosure. Based on the above, this work attempts to address a long- standing question on the relationship between ESG practices and profitability (Wanger et al., 2002; Horváthová, 2010). To do so, we propose a novel composite indicator that combines ESG ratings with disclosure scores. Strategic disclosure on part of firms, aiming to over- state the actual extent of ESG practices, is thus pena"
8086,unknown,"with the idea that providing full ESG information is per se a positive signal (Minutolo et al., 2019). Ultimately, we aim to capture the real extent of ESG practices and to assess their impact on profitability. The originality of this study is twofold. First, to the best of our knowl- edge, this is the first time that a metric takes into account information on both the quality and quantity of info"
8087,unknown,"proposed. Using the Mazziotta-Pareto method, our comprehensive ESG score combines Refinitiv data on ESG ratings with Bloomberg data on ESG disclosure. The indicator introduces a nonlinear penalty for firms that fail to disclose full information, consistent with the idea that those that tell less have more to hide. Second, while much of the literature uses either US data or wide international sampl"
8088,unknown,"on the constituents of Euro Stoxx 300, meaning we focus on the Eurozone alone. Our results show that the actual effect of ESG prac- tices on economic performance is negative. Nonetheless, we argue that the long-run impact of ESG practices is not only crucial in terms of well-being, but also potentially beneficial for economic perfor- mance in a future world that is bound to be structurally differe"
8089,unknown,"the present. The rest of this work is organized as follows. Section 2 provides a review of the current debate in the economic literature and outlines the fundamental research questions of this work. Section3 describes the empirical strategy adopted, with a focus on the novel indicator proposed. Section 4 introduces the dataset available, providing a rationale for choice of covariates. Section5 sho"
8090,unknown,"results of the analysis. Section6 discusses the results obtained in light of the previous literature. Section 7 provides some final consider- ations and concluding remarks. 2 | LITERATURE REVIEW The publication of the report entitled“Who Cares Wins” by the UN Global Compact laid the foundations of ESG dimensions of economic activities, emphasizing the importance of ethical finance (Busch et al., 2"
8091,unknown,"et al., 2021; Eccles et al., 2020). The environmental pillar refers to issues such as climate change, biodiversity, air, and water quality. Thus, it measures firm commitment to energy efficiency, emission reduction, and effective use of natural resources. The social pillar instead covers firm commitment to social rights, equal opportunities, and gender equity, aiming for equitable working conditio"
8092,unknown,"inclusion, with a focus on the community in which the firm operates. Finally, the governance pillar relates to the firm's commitment to transparency in accounting and renumeration, to a balanced distribu- tion of responsibilities among shareholders (including minority share- holders) and to avoidance of illegal practices. While the earliest studies on the ESG dimensions of corporate activities dat"
8093,unknown,"activities date back to the 1970 (Friede et al., 2015), in recent years ESG practices have received much more attention from managers, investors and policymakers (Li et al.,2021), since they provide addi- tional information with respect to financial indicators, thus constitut- ing a valuable asset in the decision-making process (Busch et al., 2016). From the consumer's point of view, the products "
8094,unknown,"plied by firms featuring a strong ESG drive are more attractive due to their higher sustainability content. For investors, the stocks of firms focusing on ESG practices are less risky ceteris paribus (Minutolo et al., 2019). In the policymaker's perspective, ESG information sheds light on the extent to which economic development is paired with environmental conservation and social cohesion. For th"
8095,unknown,"firms have become more and more prone to sharing data on ESG practices (Slager et al., 2012). Focusing on Standard & Poor 500 con- stituents for instance, the listed companies that published ESG reports went from 20% in 2011 to 85% in 2017 (Coppola,2018). As of 2016, more than 100 rating agencies provided ESG data, including Morgan Stanley Capital International (MSCI), Thomson Reuters/Refinitiv, a"
8096,unknown,"Bloomberg. In comparison to US investors, European investors have been shown to be more responsive to ESG data when making portfo- lio choices (Amel-Zadeh & Serafeim, 2018). Within the growing body of literature on ESG practices, several studies have tackled the relation between ESG practices and firm per- formance (Brooks & Oikonomou,2018; Friede et al.,2015). In particu- lar, scholars have wonde"
8097,unknown,"lar, scholars have wondered whether ESG practices constitute a source of competitive advantage or mainly a cost burden. Over the last four decades, contradictory results have emerged and the nature of the relationship remains overall ambiguous (Friede et al., 2015; Gillan et al.,2021; Margolis & Walsh,2003). Many authors find a posi- tive association between the adoption of sustainable practices a"
8098,unknown,"profitability. An early meta-analysis covering 52 pioneering studies over a period of 30 years by Orlitzky et al. ( 2003) shows a strong and positive association between firm commitment to social issues and financial performance. Another meta-analysis by Wu (2006) finds that firms benefit from being socially responsible in terms of profits. Eccles et al. (2014), analysing a sample period of 180 US"
8099,unknown,"et al. (2014), analysing a sample period of 180 US firms between CERCIELLO ET AL. 803 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by"
8100,unknown,"1993 and 2009, argue that sustainable firms outperform non- sustainable firms in terms of stock market and accounting performance. More recently, a meta-analysis by Friede et al. ( 2015), aggregating 3700 studies, suggests that the majority of firm-level works find a positive relation between ESG commitment and corpo- rate financial performance. De Lucia et al. ( 2020), using sample of 1038 public"
8101,unknown,"2020), using sample of 1038 public companies in Europe, find evidence in support of a posi- tive relationship between ESG practices and return on equity (ROE). Alareeni and Hamdan ( 2020), focusing on S&P 500 listed companies during from 2009 to 2018, report that ESG disclosure has a positive effect on ROE and ROA. While this strand of literature is vast, the lack of specific, standardized, and co"
8102,unknown,"of specific, standardized, and comparable data on ESG performance casts doubt on its findings. A more recent set of studies has looked at ESG indicators, focusing on banks and financial institutions. Buallay ( 2019a), using a sample of 235 banks during the 2007–2016 time- span, finds that ESG practices have an overall positive effect on ROA and ROE, but the Social and Environmental pillars taken i"
8103,unknown,"produce negative effects. Cornett et al. (2016), focusing on US com- mercial banks from 2003 to 2013, find that ROE is positively impacted by ESG practices. Wu and Shen (2013) obtain the same result looking at 162 financial companies during the period 2003–2009. These con- tributions argue that ESG practices improve performance by reducing financial risks, especially during crises (Broadstock et a"
8104,unknown,"et al.,2017). Several theories in the current literature try to explain the posi- tive association between ESG practices and firm performance. Resource-based view (RBV) states that a firm's resources, such as human capital, corporate governance, and sustainable business pro- cesses, represent the key to competitive advantage. In this view, ESG practices constitute strategic resources (Bird et al.,"
8105,unknown,"2007; Barney, 1991; Hull & Rothenberg,2008; Russo & Fouts,1997) that boost firm reputation (Aguilera et al., 2007; Kim et al., 2018;L i et al.,2019; Orlitzky et al.,2003) and enhance loyalty and trust among consumers (Greening & Turban,2000; Roberts & Dowling,2002). ESG practices are thus described as a key driver of superior long-run per- formances (Ruf et al., 2001). Stakeholder theory proposes "
8106,unknown,"tive interpretation. In this view, the firm faces a moral duty to maximize the value of all subjects that gravitate around it, including customers, employees, and regulators (Azmi et al., 2021). The firm should thus meet the demands of multiple stakeholders, in view of profit-maximization (Freeman, 2010; Jones, 1995; Steurer et al.,2005; Ali et al.,2017). Satisfying the interests of different acto"
8107,unknown,"related to the firm, helps create value for all stakeholders (and not only for shareholders), enhancing the success of the firm in the long run (Freeman, 2010). To this regard, Makni et al. (2009) show that leaving stakeholders unsatisfied may lead to negative consequences for firm performance. Overall, stakeholder theory emphasizes the firm's ability to convert social responsibilities into profit"
8108,unknown,"Kramer ( 2006) argue that ESG activities should be seen as an opportu- nity for competitive advantage, rather than a source of costs. Firms thus undertake sustainable initiatives both for positive firm image (Franceschelli et al., 2018; Santoro et al.,2019) and due to the pres- sure exerted by stakeholders (Sharma & Henriques,2005). Another strand of the economic literature maintains that ESG prac"
8109,unknown,"practices produce a negative impact on firm performance (Brammer et al., 2006; Branco & Rodrigues,2008; Duque-Grisales & Aguilera- Caracuel, 2021; Smith et al.,2007). An early contribution by Aupperle et al. (1985) highlights an inverse relation between the emphasis put on the ethical components of economic activities and profitability. Lee et al. (2009) finds a decline in equity value for firms w"
8110,unknown,"scores, arguing that ESG activities penalize financial performance. Reinhardt and Stavins ( 2010) find that involvement in sustainable ini- tiatives leads to competitive disadvantage. Along the same line, Devinney (2009) points out that ESG practices entail significant costs that outweigh the additional flows of revenues they generate. Preston and O'Bannon ( 1997) find that the costs of sustainabl"
8111,unknown,"reduce shareholder value and firm profitability. Buallay (2019b), focus- ing on 342 financial institutions between 2007 and 2016, find that ESG practices decrease profitability. Mohamed Buallay et al. ( 2021) find a negative relationship between sustainability reporting and financial performance in different areas of the globe over the 2008– 2017 period. DasGupta (2021), using data from 27 countri"
8112,unknown,"2010 to 2019, concludes that ESG practices hinder financial perfor- mance. Similarly, Qureshi et al. (2021) suggests that spending on environmental and social activities decreases the profitability. Finally, Buallay et al. ( 2020), examining 882 financial firms from developed and developing countries in the 11 years after the 2008 financial crisis, find a negative impact of ESG practices on ROA, R"
8113,unknown,"ROE, and Tobin's Q. This strand of the literature points out that ESG activities are costly, and firms engaged in ethical initiatives are b o u n dt ou n d e r p e r f o r mi nt h el o n gr u n( B a u e re ta l . , 2006; Cardebat & Sirven,2010; Di Giuli & Kostovetsky,2014). Several theoretical explanations have been proposed to justify these results. Contrary to the stakeholder view, Brown and Cay"
8114,unknown,"( 2006) for instance claim that attempting to satisfy all stakeholders is not beneficial, but even negative for firm performance. The trade-off view considers ESG activities as a supplementary cost, eroding share- holders value through an inefficient use of resources which impacts firm performance (Friedman, 1970). In general, the traditional neoclas- sical approach, sees ESG practices mostly as a"
8115,unknown,"costs (Derwall et al.,2005; Hassel et al.,2005; Palmer et al.,1995). According to Schuler and Cording (2006), managers who implement ESG activities are giving up alternative initiatives that may prove more profitable. In this view, apart from the explicit costs implied by ESG activities, they also come with a relevant opportunity cost. Sprinkle and Maines ( 2010) widen the cost argument, identifyi"
8116,unknown,"of costs associated with ESG activities: opportunity costs, sunk costs and recurrent costs, all which contribute to worsening economic per- formance. According to agency theory instead, managerial incentives to pursue personal interests represent a cost for shareholders (Brown et al., 2006; Krüger,2015). ESG practices constitute one of the chan- nels through which the managerial agency problem occ"
8117,unknown,"(Bénabou & Tirole, 2010; Buchanan et al., 2018; Masulis & Reza, 2015; Seifert et al.,2004). Barnea and Rubin (2010) state that agency costs arise when managers tend to engage in ESG activities to develop and strengthen individual benefits, such as personal 804 CERCIELLO ET AL."
8118,unknown,"develop and strengthen individual benefits, such as personal 804 CERCIELLO ET AL. 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the"
8119,unknown,"reputation, decreasing the focus on core managerial duties and responsibilities (Jensen, 2002). Other studies confirm that managers may fail maximize profits while pursuing ESG targets for their own interest (Jensen & Meckling,1976; Jiraporn & Chintrakarn,2013). While some studies find positive effects of ESG practices on firm performance and other find negative effects, another strand of the empi"
8120,unknown,"empirical literature finds no significant relationship between the two var- iables. Garcia and Orsato (2020) for instance obtain mixed outcomes. Observing samples from developing countries such as Brazil and South Africa, they report a negative relationship between ESG scores financial performance. The direction of the relationship changes when using a sample of firms operating in developed countr"
8121,unknown,"( 2019b) illustrates that ESG practices positively affects firm performance positively in the manufacturing sector, but negatively in the banking sec- tor. Shakil et al. (2019), examining 93 banks in emerging countries during the period 2015–2018, find that the Environmental and Social pillars produce a positive impact on ROE, while the Governance pillar has no effect. Nirino et al. (2021) fail to"
8122,unknown,"effect. Nirino et al. (2021) fail to confirm the mitigating effect of ESG practice on the association between corporate controversies and finan- cial performance. La Torre et al. (2021), using a panel of European banks listed in STOXX Europe 600 from 2008 to 2019, find no relationship between ESG practices and account-based performance. Several other studies indeed find nonsignificant relationship"
8123,unknown,"the puzzle (Chih et al., 2010; Gilley et al., 2000; Hsu et al., 2018; Humphrey et al.,2012; Surroca et al.,2010). The vast and growing liter- ature tackling the relationship between sustainable practices and profit- ability has taken advantage of a substantial increase in the availability of ESG data (Kotsantonis et al.,2016). The economic literature however raises some concerns with respect to th"
8124,unknown,"respect to the quality of ESG information. The lack of an internation- ally recognized ESG auditing authority and the presence of behavioral issues at the firm level may hinder the accuracy of ESG measures (Friede, 2019; Kotsantonis & Serafeim,2019; Yu et al.,2020). In par- ticular, the fact that not all firms pursue full disclosure of ESG infor- mation casts doubt on the actual incentives that dr"
8125,unknown,"decisions. The general consensus holds that increases in the amount of information disclosed do not necessarily imply a stronger commit- ment to ESG practices (Plumlee et al., 2015). According to Hopwood (2009), strategic disclosure may smoothen the reputation building process. In particular, narrative disclosure may shape public opinion by catering self-servingly biased information (Merkl-Davies "
8126,unknown,"Brennan, 2011). In this view, managerial boards may misrepresent the extend of actual ESG practices, disclosing favorable information that improves reputation for both the firm and managers themselves (Brennan & Guillamon-Saorin, 2009; Melloni et al.,2017). Given the voluntary nature of ESG disclosure, managers strategically select the data to disclose, consistent with their preferences, influenci"
8127,unknown,"market environment in which they compete (Clarkson et al., 2008). This literature refers to the “management obfuscation hypothesis,” first introduced by Li (2008), according to which managers have an incentive to lie and cloud the quality of information when ESG perfor- mance is poor, while they are willing to disclose and divulgate full information when the ESG performance is strong. In spite of "
8128,unknown,"In spite of the problems related to strategic disclosure, the vast majority of the empirical studies on the effect of ESG practices use ESG scores provided by rating agencies (Escrig-Olmedo et al., 2019; TABLE 1 Sectoral breakdown Industry Number Share Banking services 18 9.0% Chemicals 14 7.0% Automobiles & auto parts 11 5.5% Electric utilities & IPPs 11 5.5% Insurance 10 5.0% Machinery, tools, h"
8129,unknown,Telecommunications services 10 5.0% Construction & engineering 7 3.5% Pharmaceuticals 7 3.5% Food & drug retailing 6 3.0% Oil & gas 6 3.0% Professional & commercial services 6 3.0% Software & IT services 6 3.0% Aerospace & defense 5 2.5% Multiline utilities 5 2.5% Residential & commercial REITs 5 2.5% Textiles & apparel 5 2.5% Beverages 4 2.0% Investment banking & investment services 4 2.0% Media 
8130,unknown,Oil & gas related equipment and services 4 2.0% Construction materials 3 1.5% Healthcare equipment & supplies 3 1.5% Hotels & entertainment services 3 1.5% Metals & mining 3 1.5% Personal & household products & services 3 1.5% Semiconductors & semiconductor equipment 3 1.5% Transport infrastructure 3 1.5% Consumer goods conglomerates 2 1.0% Healthcare providers & services 2 1.0% Homebuilding & con
8131,unknown,Paper & forest products 2 1.0% Passenger transportation services 2 1.0% Specialty retailers 2 1.0% Biotechnology & medical research 1 0.5% Communications & networking 1 0.5% Food & tobacco 1 0.5% Freight & logistics services 1 0.5% Household goods 1 0.5% Investment holding companies 1 0.5% Natural gas utilities 1 0.5% Renewable energy 1 0.5% Water & related utilities 1 0.5% CERCIELLO ET AL. 805
8132,unknown,"Renewable energy 1 0.5% Water & related utilities 1 0.5% CERCIELLO ET AL. 805 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the app"
8133,unknown,"Howard-Grenville, 2021; Tarmuji et al., 2016). Friede et al. (2015), running a meta-analysis on over 2000 scientific articles, find signifi- cant differences with respect to the measures employed in assessing ESG practices. Pagano et al. (2018) discuss the current state of ESG indicators, identifying the pros and cons of nine different ESG data providers. Multidimensional or original measures have"
8134,unknown,"Rodríguez-Fernández et al. (2019) for instance integrate Thompson Reuters's ESG score, with the controversy indicator (C), a proxy for firm exposure to ESG risks reflected in global media, obtaining the so- called ESGC index. Fiaschi et al. (2020) instead propose an original index of corporate wrongdoing, measured as the number of contro- versies in which firm is involved. In the face of this plet"
8135,unknown,"options, Muñoz-Torres et al. (2019), as well as Li and Polychronopou- los (2020) claim that investors and academics should choose the data providers whose ratings align more closely with their personal views on ESG practices. Summing up, in the presence of all these alterna- tives in the measurement of ESG practices, it should not be surprising that no broad consensus exists on how ESG practices a"
8136,unknown,"that no broad consensus exists on how ESG practices affect profitability. 3 | DATA The dataset used in this work refers to the constituents of Euro Stoxx 300, which is a share index of Eurozone stocks designed by STOXX, an index provider owned by the Deutsche Börse Group. It contains constituents belonging to 11 Eurozone countries. In total, Euro Stoxx 300 contains over 290 firms. For the purpose "
8137,unknown,"were considered due to data availability. In particular, some firms lacked either anESG Score or an ESG Disclosure Score, or even both. Overall, our dataset contains yearly observations for 200 firms, span- ning from 2010 to 2019. The firms considered operate in several dif- ferent industries. The sectoral breakdown of the firms in the sample is shown in Table 1. The main statistical features of t"
8138,unknown,"shown in Table 1. The main statistical features of the firms considered are summed up in Table2. The definitions of the variables used is provided below:  Return on Equity (ROE)is a defined as the ratio between net income and shareholder equity. This measure financial performance allows to assess relative profitability (Al-Qudah, 2017; Hou et al.,2015). It captures financial health, and it is com"
8139,unknown,"dependent variable in the ESG literature (Buallay,2019c; Cornett et al., 2016; Esteban-Sanchez et al., 2017; Nizam et al., 2019; Shakil et al.,2019; Waddock & Graves,1997).  The Debt on Equityratio is calculated as total liabilities over share- holder equity. It proxies the firm's capital structure and represents its ability to finance growth through debt. This metric is widely employed in the fi"
8140,unknown,"employed in the financial literature to denote leverage (Hovakimian et al., 2001). A high Debt on Equity ratio implies that the firm may have a hard time servicing its debt through cash flow. As such, it is a monitoring tool for financial risk. From a theoretical perspective, the relation between Debt on Equity and profitability is ambiguous. By taking on debt, firms face higher costs, which decre"
8141,unknown,"decreases profitability. Obtaining credit however allows assets to grow. Moreover, increases in the value of the debt decrease the value of the equity, which in turn improves ROE (equity appears at the denominator). The empirical evidence is also mixed. Some authors support a negative relationship between the two variables (Abor, 2005; Cassar & Holmes, 2003; Graham, 2000;H a l l et al.,2004; Musce"
8142,unknown,"et al.,2004; Muscettola & Naccarato,2016; Petersen & Rajan,1994; Titman & Wessels,1988). Others find a weak positive relationship (Champion, 1999; Gill et al., 2011; Margaritis & Psillaki, 2010; Taub, 1975), while still others unfold no significant relationship (Muscettola, 2014, 2015; Muscettola & Naccarato, 2016; Tailab, 2014).  Firm sizeis computed as the natural logarithm of total assets. It "
8143,unknown,"resents the total volume of all business operations, or alternatively, the scale of the business activities turned out (Desai & Dharmapala, 2009). In most empirical studies, size is considered as a fundamental factor for profitability, since it represents a dimen- sional control (Dang et al.,2018; Vijh & Yang,2013). The relation- ship between firm size and firm profitability is highly debated (_Is"
8144,unknown,"et al., 2017; Kuncová et al., 2016; Liargovas & Skandalis, 2010; Nunes & Serrasquero,2008; Wu, 2006). Conflicting and ambigu- ous results have been reported. Some support a direct link between size and profitability (Nunes & Serrasquero, 2008; Papadogonas, 2007). Others find evidence in favor of an inversely relation (Becker-Blease et al., 2010; Goddard et al., 2005; Lee, 2009). Still others find "
8145,unknown,"Burson, 2007; Niresh & Thirunavukkarasu, 2014: Kartikasari & Merianti, 2016).  Revenues are often seen as a proxy of firm growth (Delmar et al., 2003; Fuertes-Callén & Cuellar-Fernández, 2019) and for TABLE 2 Descriptive statisticsObservations Mean Std. dev. Min Max ROE 2000 11.782 12.849 /C0 66.870 264.360 Com ESG score 2000 68.363 20.483 0 100.000 Debt on equity 2000 126.600 341.124 0 5785.000 "
8146,unknown,"Firm size 2000 6.581 0.990 0 7.574 Revenues 2000 9.159 1.342 4.263 12.440 Market value 1984 2.351 2.624 0.170 79.850 Abbreviations: ESG, environmental, social, and governance; ROE, return on equity. 806 CERCIELLO ET AL."
8147,unknown,"806 CERCIELLO ET AL. 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License"
8148,unknown,"long-run survival probability (Fuertes-Callén & Cuellar- Fernández, 2019; Gupta et al.,2013). The state of the art about the relationship between revenues and profitability is complex (Wright & Stigliani, 2013; Love & Roper,2015; Davidsson et al., 2006). Many studies find a positive association (Coad, 2007; Davidsson et al., 2009; Federico & Capelleras, 2015; Goddard et al., 2004; Mendelson, 2000)"
8149,unknown,"et al., 2004; Mendelson, 2000). The theoretical justifications for this result range from economies of scale (Davidsson et al.,2009), to first mover-advantage (Lechner & Gudmundsson, 2014; Lieberman & Montgomery, 1988; Suarez & Lanzolla, 2007), to experience curve effects (Katz & Shapiro,1985; Ritala,2012). Con- versely, other works find a negative relationship (Reid,1995; Jang and Park, 2011). St"
8150,unknown,"and Park, 2011). Still others, like Markman and Gartner (2002)d o not find any significant effect.  Market value represents the worth of a company on the market in terms of its book value (Ceccagnoli,2009; Lee & Makhija,2009). It is computed by the market to book ratio (M/B), that compares the market value, determined share price and number of outstanding shares, and book value, which equals net "
8151,unknown,"helps investors and market participants understand whether the stock price of a firm is in line with its book value. In other words, it is a measure of the valuation of a company's stock relating to its net assets. The relationship between market value and profitability is positive. Market value may be seen as an overall picture of investment decision, assets management and growth opportunities (F"
8152,unknown,"(Fajaria & Isnalita, 2018). Several researchers have examined how market to book ratio may be related to profitability. For example, Beaver and Ryan (2000) have explored the ability of book-to- market ratios (the inverse of market to book ratio) to predict profit- ability, finding a negative and significant effect. Further research has shown that the book-to-market ratio is inversely related to RO"
8153,unknown,"ROE (Penman, 1991, 1992, 1996). This result is in line with Zhang (2005) who claims that firms with high market-to-book ratios are more profitable. The seminal contribution by Fama and French ( 1995) confirms that“high book-to-market stocks are less profit- able than low-ones for four years before and at least five years after ranking dates” (Fama & French,1995). 4 | METHODS This section illustrat"
8154,unknown,"4 | METHODS This section illustrates the novel indicator proposed in this work in order to provide a comprehensive measure of ESG practices. More- over, it outlines the empirical strategy devised, aiming to assess the relationship between the indicator and firm profitability. 4.1 | A novel measure of ESG practices This work proposes the comprehensive ESG score, a composite indi- cator combining th"
8155,unknown,"cator combining the ESG score provided by Refinitiv with the ESG Disclosure Score provided by Bloomberg. It is computed using the Mazziotta-Pareto method (Mazziotta and Pareto, 2011; see the Appendix A for more details). This method features the peculiar char- acteristic of not allowing for perfect substitutability across pillars: very high scores in a certain pillar cannot compensate very low sco"
8156,unknown,"another, so units featuring a good degree of balance across dimen- sions receive relatively higher scores, while units characterized by unbalanced values across dimensions are penalized. TheESG Score measures ESG performance at the firm level based on verifiable reported data in the public domain (Refinitiv, 2021). This score is based on three main pillars (ESG). Firm scores span from A+ to D/C0 ,"
8157,unknown,"where A+ indicates an excellent ESG performance (identifying ESG Leaders) and D/C0 indicates a very poor ESG performance (identifying ESG Laggards). We convert these qualitative ratings into numerical scores, ranging from 0 to 100 according to the percentiles of the dis- tribution. Figure 1 shows the distribution of the qualitativeESG Score. The ESG Disclosure Scoremeasures the extent of ESG discl"
8158,unknown,"on part of firms (McBrayer, 2018). The score ranges from 0 to 100, where firms that disclose no information at all obtain 0 and firms that disclose full information obtain 100. This score is computed dif- ferently according to economic sectors. It is important to stress that the ESG Disclosure Scorecaptures the amount of ESG information pub- licly disclosed, but it means nothing in terms of ESG pr"
8159,unknown,"the wESG Score is obtained, it is finally rescaled, using a monotonic transformation 1 so as to make sure that it lies in the 0–100 interval. 4.2 | GMM-SYS In order to evaluate the impact of the Comprehensive ESG Score on firm profitability, we use the GMM-SYS version of the Arellano-Bond estimator. This empirical approach is especially useful in the presence of inertia in the dependent variable. "
8160,unknown,"FIGURE 1 Refinitiv ESG ratings. ESG, environmental, social, and governance. ESG, environmental, social, and governance.Source: Original elaborations based on data provided by Refinitiv (2021) [Colour figure can be viewed atwileyonlinelibrary.com] 1In particular,wESG is multiplied by a constant factor, that guarantees all values lie within the 0–100 range. CERCIELLO ET AL. 807"
8161,unknown,"0–100 range. CERCIELLO ET AL. 807 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License"
8162,unknown,"of profits would likely cause biased results if persistency characterizes the profit dynamics. In formal terms, the model may be outlined as follows: Y t ¼ ρYt/C0 1 þXtβ þεt, ð1Þ where Yt is an N /C2 1 vector representing firm profitability,Xt is an N/C2 K matrix of contemporaneous covariates that includes a first col- umn of ones and εt is a well-behaved conformable vector of error terms. The par"
8163,unknown,"terms. The parameter ρ represents the effect of the autoregressive component that captures time persistency in the dependent variable, while β is aK /C2 1 vector of marginal effects. The presence ofYt/C0 1 on the right-hand side of the equation generates a problem of endogene- ity, which may be sorted out by instrumentingY t/C0 1 with its further lags. The estimator obtained using this procedure i"
8164,unknown,"since it resorts to a system of equations, both in levels and in first dif- ferences, from which the instrumental variables are drawn. To avoid los- ing time periods when deeper lags are introduced, we replace the missing values with zeros (Agovino et al., 2019; Arellano & Bond,1991; Baltagi, 2013; Ferraro et al.,2019; Holtz-Eakin et al.,1988). Endogeneity may arise also from the columns of theX m"
8165,unknown,"the presence of endogenous regressors (other than the lagged depen- dent variable), the deeper lags of the regressors themselves may be used as instruments (again, both in levels and in first differences) to address the problem. In order to corroborate the choice of the instru- ment set, the Sargan test is implemented. Failure to reject the null indicates that instrument validity is questionable ("
8166,unknown,"2002). 5 | RESULTS The results of the dynamic panel model are shown in Table3. Besides the lagged dependent variable— that is by definition endogenous— some control covariates are likely to be in a simultaneous relationship with ROE. As a result, they are suitably instrumented. It is important to stress that both the Sargan and the Hansen test for overidentifica- tion fail to reject the null, corr"
8167,unknown,"tion fail to reject the null, corroborating the choice of the instrument set, which contains the deep lags of the dependent variable and of the endogenous covariates. The AR tests confirm the validity of the model specification selected. In particular, the residual features a first-order autoregressive process, as should be expected, while higher order autocorrelation is ruled out by non-significa"
8168,unknown,"The coefficient associated with the lagged dependent variable is positive and significant, revealing the presence of a relevant degree of inertia in profits. This result is not surprising, since persistency in profits is largely documented in the empirical literature (see e.g., Hirsch & Gschwandtner, 2013; Lawrence et al.,2018). The most relevant result of this analysis concerns the negative and s"
8169,unknown,coefficient associated with the comprehensive ESG score. This result indicates that increases in the commitment to ESG practices worsen firm performance overall. The channels through which this effect occurs are either increases in costs (trade-off theory) or diversion of managerial focus from core activities (agency view). The effect of the leverage ratio (Debt on Equity) is non-signifi- cant. Gi
8170,unknown,"cant. Given the theoretical ambiguity of the effect and the mixed results obtained by the empirical literature, this result is not surprising. Revenues increase profitability, which is intuitive from an economic point of view. The real effect of increases in revenues thus prevails. Firms size exhibits a negative and significant coefficient. Following the interpretation proposed by Fama & French, K"
8171,unknown,"the interpretation proposed by Fama & French, K. ( 1992), this effect is related to risk, in that large firms, whose stocks are less risky on aver- age, do not need pay a risk premium to their shareholders. Thus, they can afford to distribute fewer profits and reinvest a larger share of their economic results. Finally, as largely agreed upon in the empirical TABLE 3 Estimation results ROE Lagged R"
8172,unknown,Lagged ROEa 0.485 (0.028)*** Combined ESG score /C0 0.018 (0.009)** Debt on equity 0.000 (0.000) Revenuesa 0.381 (0.206)* Firm size a /C0 0.472 (0.182)*** Market valuea 1.140 (0.184)*** Time dummies Yes Country dummies Yes _cons 4.869 (1.719)*** F-test on time dummies 19.48 (0.000)*** F-test on country dummies 17.22 (0.069)* Sargan test 52.22 (0.214) Hansen test 57.08 (0.117) AR (1) /C0 2.84 (0.00
8173,unknown,"(0.117) AR (1) /C0 2.84 (0.004)*** AR (2) 0.76 (0.449) AR (3) /C0 1.37 (0.171) N 1785 Note: Standard errors are shown below coefficient estimates.p-values are shown below test statistics. Abbreviation: ESG, environmental, social, and governance. aEndogenous variable, suitably instrumented. *p < 0.1; **p < 0.05; ***p < 0.01. 808 CERCIELLO ET AL."
8174,unknown,"*p < 0.1; **p < 0.05; ***p < 0.01. 808 CERCIELLO ET AL. 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commo"
8175,unknown,"literature, market value has a positive and significant effect on profitability. One interesting question pertaining to the effect of ESG practices on profitability concerns the stability of the negative effect indicated by our results across industrial sectors.2 In order to tackle this prob- lem, we estimate again the regression indicated in Equation (1), includ- ing this time both industrial dum"
8176,unknown,"ing this time both industrial dummies and interactions between the combined ESG score and the industrial dummies. In other words, we allow the marginal effect of the combined ESG score to vary across industrial sectors. The estimates produced by this richer version of the model are shown in Table 4. While the coefficient estimates obtained in this case are very sim- ilar to those displayed in Tabl"
8177,unknown,"ilar to those displayed in Table3, confirming substantially the findings already discussed, it is interesting to notice that for 10 of the 43 sec- tors covered by our data, the marginal effect of the Combined ESG score varies significantly with respect to the average value of /C0 0.024. 3 This means that ESG practices generate different effects depending on the industrial sector considered. Table5"
8178,unknown,"marginal effects for the Combined ESG Score for the sectors featuring a significant coefficient associated to the interaction term. In all of these sectors, the marginal effect is either negative or non-significant. The most negative effect is observed in the sector of consumer goods conglomerates, followed by the sector of biotechnol- ogy and medical research. Due to technological constraints, in"
8179,unknown,"these sectors commitment to ESG is especially expensive. In other sectors, namely electric utilities & IPPs, food & tobacco, freight & logistics services, household goods and investment banking & invest- ment services, the marginal effect is not significantly different from zero. This result may either spur from a lower cost associated to com- mitment to ESG practices or from a higher sensitivity "
8180,unknown,"ESG investments. Overall, our results indicate that— with a few exceptions— the effect of ESG practices on firm performance is negative. This relation- ship is well documented in the recent literature (Barauskaite & Streimikiene, 2021; Chen et al.,2021). Other recent works, resorting to different data sources, reach the same conclusion. Lin et al. (2019), using a panel of Fortune magazine's 100 mo"
8181,unknown,"2016 and 2017, claim that commitment to sustainable practices does not necessarily lead to better financial performance. Specifically, they find a negative association between sustainable practices and three accounting financial measures, namely ROE, ROA, and ROI. Lys et al. ( 2015) argue that if ESG practices are mainly pushed by opportunistic purposes, expenditures related to ESG activities repr"
8182,unknown,"rather than an investment. Lastly, Hirigoyen and Poulain-Rehm (2015) confirm the managerial opportunism hypothesis, according to which sustainable practices have a negative impact on financial performance. Less recently, McWilliams and Siegel (2000) assert that firms engaging in sustainable practices may miss out major investments in other aspects, worsening the financial performance. With respect"
8183,unknown,"literature, our work adds a relevant element in the relationship, that is, the role of strategic disclosure of ESG data. While previous works focus only on the raw measure of ESG score, we use the comprehen- sive ESG score, in order to obtain an unbiased estimate of the effect of the ESG practices on profitability. 6 | DISCUSSIONS AND POLICY IMPLICATIONS The empirical results obtained raise broad "
8184,unknown,"ESG practices play in determining firm performance. While a sizeable strand of literature supports the existence of a positive relationship, we find a negative effect. Traditional explanations for this effect include the trade-off perspective and agency theory. A novel argu- ment that may be drawn from this work relates to the strategic behav- ior adopted by firms in terms of disclosure. High-ESG "
8185,unknown,"achieved by committing to ESG practices strongly while pursuing full disclosure, or by tackling onlysome dimensions of ESG practices while strategically disclosing partial information, exactly on the (few) virtu- ous behaviors that the firm adopts. Failing to take into account the potential bias induced by behavioral issues, much of the previous liter- ature is likely to have obtained inconsistent"
8186,unknown,"versely aims to capture the actual effect of ESG practices by introducing a penalty for the firms that fail to disclose full information. This penalty reduces significantly the score assigned to many of the firms that face high-ROE values, thus yielding a negative coefficient for the comprehensive ESG score. The strategic disclosure argument is corroborated by descriptive evidence in our sample. I"
8187,unknown,"evidence in our sample. In particular, Figure 2 shows that the firms that disclose more information (i.e., those in the Top 25% of the dis- closure score distribution) obtain on average significantly larger ESG ratings with respect to the firms that disclose less information (i.e., those in the Bottom 25% of the disclosure score distribution, labeled disclosure laggards). So, the firms that disclo"
8188,unknown,"those that have more to hide. Failing to disclose some information may thus be viewed as a negative signal. Some firms thus resort to strategic behaviors, disclosing partial and cloudy information without committing at full to ESG practices or changing corporate vision (Yu et al., 2020). In particular, strategic dis- closure takes the form ofGreenwashing and Social Washing. Corporate greenwashing "
8189,unknown,"greenwashing occurs when firms falsely claim eco-friendliness (Laufer, 2003). Greenwashing is considered as distortion factor (Seele & Gatti,2017), since it leads firms to overstate their commit- ment to the environment through communication channels (De Vries et al., 2015). Firms find greenwashing attractive since it allows to enhance their reputation, which in turn increases profits (Ferr/C19on-"
8190,unknown,"Vílchez et al., 2021; Kucharska & Kowalczyk, 2019; Lyon & Montgomery, 2013). In this view, greenwashing leverages on the growing societal concerns over environmental sustainability to extend the firm's customer base (Figge, 2005), capturing ethical and eco- conscious consumers, an emerging market segment characterized by the willing to pay a significant price premium for sustainable products 2We t"
8191,unknown,"2We thank our anonymous referee for raising this point. 3For the sake of readability, Table4 does not show the coefficients associated to the interactions if they are not significant. Nonetheless, we will be happy to provide readers with this information, should they be interested. CERCIELLO ET AL. 809"
8192,unknown,"this information, should they be interested. CERCIELLO ET AL. 809 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Crea"
8193,unknown,"(Agovino et al.,2018; Yang et al.,2020; Zhang & Xie,2022). External stakeholders and competitive pressures moreover may induce firms to adopt greenwashing practices in response to a widespread market standard (Testa et al., 2018). Social Washing instead refers to decep- tive marketing strategies that firms adopt in order to overstate the social value they create (Rizzi et al., 2020; Troje & Gluch,"
8194,unknown,"the case of greenwashing, firms provide misleading information to dis- tort the insights of their stakeholders (Balluchi et al.,2020), leveraging on communication tools that influence consumer perception and brand image (Zhang & Xie, 2022). Broadly speaking, increasing commitment to ESG practices requires not only attitudinal changes on part of private agents, but also engagement on part of superv"
8195,unknown,"authorities. Commitment to ESG practices in this view needs to be actively pursued by governmental bodies. Several policy tools have been proposed. Gatti et al. ( 2019) maintain that the introduction of sustainability regulations, in the form of either industry self-regulation or public provisions, may lower the divergence between the extent of sustainable practices and corporate communication. Ac"
8196,unknown,"et al. ( 2012), credible third-party certification may be the key to enhancing sustainability in business practices (Wang et al., 2018). Mio et al. (2020) highlight the relevance of mandatory regulations, as a way to induce large companies to provide vaster non-financial infor- mation, reducing information asymmetry and agency problems. The TABLE 4 Regression results with slopes varying across ind"
8197,unknown,ROE Lagged ROEa 0.459 (0.030) *** Combined ESG score /C0 0.024 (0.009)** Debt on equity 0.000 (0.956) Revenuesa 0.333 (0.208) Firm size a /C0 0.492 (0.190) *** Market valuea 1.148 (0.189) *** Time dummies Yes Country dummies Yes Industry dummies Yes ESG /C2 sector: Biotechnology & medical research /C0 0.105 (0.016) *** ESG /C2 sector: Communications & networking /C0 0.064 (0.011) *** ESG /C2 secto
8198,unknown,(0.011) *** ESG /C2 sector: Consumer goods conglomerates /C0 0.403 (0.045) *** ESG /C2 sector: Electric utilities & IPPs 0.036 (0.019)* ESG /C2 sector: Food & tobacco 0.011 (0.004) *** ESG /C2 sector: Freight & logistics services 0.031 (0.005) *** ESG /C2 sector: Healthcare equipment & supplies /C0 0.032 (0.016)* ESG /C2 sector: Household goods 0.029 (0.005) *** ESG /C2 sector: Investment banking 
8199,unknown,services 0.047 (0.021)** ESG /C2 sector: Water & related utilities /C0 0.035 (0.005) *** _cons 5.758 (1.915) *** F-test on time dummies 21.19 TABLE 4 (Continued) ROE (0.002) *** F-test on country dummies 18.28 (0.050)** F-test on industry dummies 99.02 (0.000) *** Sargan 44.8 (0.124) Hansen 42.13 (0.19) AR (1) /C0 2.81 (0.005) *** AR (2) 0.71 (0.477) AR (3) /C0 1.41 (0.159) N 1785 Note: Standard e
8200,unknown,"shown below test statistics. Abbreviations: ESG, environmental, social, and governance; ROE, return on equity. aEndogenous variable, suitably instrumented. *p < 0.1; **p < 0.05; ***p < 0.01. 810 CERCIELLO ET AL."
8201,unknown,"*p < 0.1; **p < 0.05; ***p < 0.01. 810 CERCIELLO ET AL. 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commo"
8202,unknown,"lack of disclosure alignment moreover is commonly cited as a major problem (Beare et al.,2014). Overall, transparent sustainability report- ing standards and clear rules on the amount of information disclosed are needed (Hussain et al., 2018; Lee et al.,2017). From a regulatory point of view, the EU legal framework on sus- tainability reporting is evolving. In 2014, Directive 2014/95/EU— also know"
8203,unknown,"known as the Non-Financial Reporting Directive (NFRD)— was intro- duced, to establish rules on the disclosure of non-financial informa- tion, supporting a responsible approach to business by companies employing more than 500 workers. The NFRD modified the earlier Directive 34/2013 and extended the reporting obligation on social and environmental issues to a wider audience of listed companies and e"
8204,unknown,"and entities of public interest. Specifically, the NFRD requires large companies to annually report information on ESG issues, providing better information for consumers and investors on firm sustainabil- ity. The NFRD took effect in all EU member States in 2018, when all 28 countries have translated the Directive into national law, and it is now up to companies to fulfill. However, the NFRD is no"
8205,unknown,"tory, thus leaving a large amount offlexibility in the implementation of its provisions. Furthermore, the NFRD imposes neither manda- tory reporting standards nor disclosure bounds, leaving room for strategic disclosure. In 2018, the European Commission outlined the EU Action Plan on Sustainable Finance (European Commission, 2020b), an ambitious package of sustainability measures, whose purpose is"
8206,unknown,"and promote awareness toward environmental and social issues, in line with the Paris Agreement of December 2015. The plan estab- lished three main objectives. First, to reorient investments toward sustainable business. Second, to manage the financial risks that arise from environmental and social issues. Third, to increase transparency and forward-looking behaviors in economic activities. One aspe"
8207,unknown,"envisaged by the EU Action Plan concerns the inclusion of ESG dimensions in the evaluation of economic activities. Additional sus- tainable measures are included in the European Green Deal signed in 2019, which aims to make the EU the first climate neutral continent by 2050. In April 2021, the European Commission published its proposal for the Corporate Sustainability Reporting Directive (CSRD), a"
8208,unknown,"replace the NFRD, by adjusting its reporting requirements. The CSRD proposal is a key element of the EU sustainable finance package, which includes a comprehensive set of measures to improve the flow of capital to sustainable businesses across the EU. The goal of the CSRD proposal is to improve sustainability reporting in the EU single market, leading firms to align to high-quality disclosure stan"
8209,unknown,"ruling out strategic behaviors. The CSRD proposal indeed dictates more detailed reporting requirements and requires firms to report information according to mandatory EU sustainability reporting stan- dards. The fundamental cornerstone that will improve sustainability reporting and support the upcoming the CSRD proposal is the EU Green Taxonomy (European Commission, 2020a). Introduced by EU Regula"
8210,unknown,"2020a). Introduced by EU Regulation 2020/852, the Green Taxonomy is to sets the technical criteria that define unambiguously eco-friendly firms, aiming to increase transparency in the EU single market. By January 2023, the criteria are expected to be delivered. It is crucial in this view that indi- vidual EU countries acknowledge the EU Green Taxonomy within their national frameworks as soon as po"
8211,unknown,"their national frameworks as soon as possible, thus adhering to com- mon reporting standards. Concerning social washing, a major role is to be played by compe- tition authorities. Marketing strategies based on emotional advertise- ment indeed may easily elude the regulations on deceptive TABLE 5 Marginal effect of the combined ESG score on profitability by industrial sector Sector Coefficient Stan"
8212,unknown,Sector Coefficient Standard error z score p-value Lower bound Upper bound Biotechnology & medical research /C0 0.12838 0.019058 /C0 6.74 0.000 /C0 0.16573 /C0 0.09103 Communications & networking /C0 0.08725 0.012984 /C0 6.72 0.000 /C0 0.1127 /C0 0.0618 Consumer goods conglomerates /C0 0.42678 0.047128 /C0 9.06 0.000 /C0 0.51915 /C0 0.33441 Electric utilities & IPPs 0.012558 0.018049 0.7 0.487 /C0 
8213,unknown,Food & tobacco /C0 0.01227 0.008863 /C0 1.38 0.166 /C0 0.02964 0.0051 Freight & logistics services 0.007519 0.009236 0.81 0.416 /C0 0.01058 0.025621 Healthcare equipment & supplies /C0 0.05575 0.015051 /C0 3.7 0.000 /C0 0.08525 /C0 0.02625 Household goods 0.004899 0.010271 0.48 0.633 /C0 0.01523 0.02503 Investment banking & investment services 0.023069 0.020565 1.12 0.262 /C0 0.01724 0.063375 Wate
8214,unknown,"Water & related utilities /C0 0.05884 0.010692 /C0 5.5 0.000 /C0 0.07979 /C0 0.03788 FIGURE 2 Average ESG score in top-disclosers (top 25%) and disclosure laggards (bottom 25%). ESG, environmental, social, and governance. Source: Original Elaborations on Refinitiv and Bloomberg data [Colour figure can be viewed at wileyonlinelibrary.com] CERCIELLO ET AL. 811"
8215,unknown,"data [Colour figure can be viewed at wileyonlinelibrary.com] CERCIELLO ET AL. 811 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the"
8216,unknown,"advertising. As many consumers lack the instruments to identify false claims on allegedly responsible practices (Mo et al.,2018), regulatory bodies are required to step in (Rizzi et al.,2020). Similar to the regula- tions on functional advertisement, limits need to be imposed on the claims made by firms to attract consumers. Overall, much room remains for public intervention. Several issues howeve"
8217,unknown,"solved. ESG practices are non-obvious to implement and represent a significant cost, which not all firms might be able to face (McWilliams et al., 2006). Some firms gain benefits from being socially responsible, while for others ESG practices represent “costs with no offsetting benefits”. Many of the firms that are strongly committed to ESG prac- tices are actually successful for reasons other tha"
8218,unknown,"tices are actually successful for reasons other than their commitment (Cornell & Damodaran, 2020). Based on the above, it should not be surprising that ESG practices worsen firm performance on average. This is not however a sufficient reason to stop investing on them. The short-run effect of ESG prac- tices turns out to be negative, but it is very possible that in the long run, following a structu"
8219,unknown,"run, following a structural transformation of business activities, tech- nological endowments, and regulatory schemes, they will yield an eco- nomic return. The short-run cost increase induced by ESG practices, calls for public intervention. Governments should provide incentive plans for firms adopting sustainable long-term goals, so as to help them bear the costs of ESG practices. Additionalhard "
8220,unknown,"include the imposition of sustainability-related taxes, such as carbon taxes. 7 | CONCLUSIONS This work aims to establish the relationship between ESG practices and profitability using a sample of listed EU firms belonging to the Eurozone. In order to discriminate firms featuring an actual ESG com- mitment from those inflating their commitment through strategic dis- closure of ESG information, we "
8221,unknown,"closure of ESG information, we introduce the Comprehensive ESG Score, a composite indicator that penalizes incomplete disclosure. In line with a recent strand of literature, our results indicate that ESG practices worsen profitability on average. Thus, we find evidence in support of stakeholder theory, according to which sustainable prac- tices imply a substantial increase in costs. This point sho"
8222,unknown,"surprising, given the nature of ESG practices. This study paves the way for a strand of future works. First, more accurate implications may be drawn by analyzing each pillar of ESG practices separately. Additional research may be conducted in the future in order to test the relationships between the individual pillars and firm profitability, possibly looking for cross-pillar interactions. The avai"
8223,unknown,"availability of disclosure data disaggregated by pillar may also allow to separate, within the phenomenon of strategic disclosure, the extent of greenwashing from that of social washing. Moreover, this study calls for further empirical works extending the indicator proposed out- side the Eurozone. Data covering other developed and/or emerging markets may be the key to draw deeper and more solid co"
8224,unknown,on the long-stand problem that surrounds the link between sustain- ability and profitability. ACKNOWLEDGMENT Open Access Funding provided by Universita degli Studi di Napoli Parthenope within the CRUI-CARE Agreement. ORCID Massimiliano Cerciello https://orcid.org/0000-0002-4767-5529 Francesco Busato https://orcid.org/0000-0001-8813-0341 Simone Taddeo https://orcid.org/0000-0002-1656-1588 REFERENCE
8225,unknown,"REFERENCES Abor, J. (2005). The effect of capital structure on profitability: An empirical analysis of listed firms in Ghana.Journal of Risk Finance, 6(5), 438–446. Agovino, M., Cerciello, M., & Gatto, A. (2018). Policy efficiency in the field of food sustainability. The adjusted food agriculture and nutrition index. Journal of Environmental Management, 218, 220–233. Agovino, M., Garofalo, A., & C"
8226,unknown,"Agovino, M., Garofalo, A., & Cerciello, M. (2019). Do local institutions affect labour market participation? The Italian case.The BE Journal of Economic Analysis & Policy, 19(2), 1–21. Agudelo, M. A. L., J/C19ohannsd/C19ottir, L., & Davídsd/C19ottir, B. (2019). A literature review of the history and evolution of corporate social responsibility. International Journal of Corporate Social Responsibil"
8227,unknown,"International Journal of Corporate Social Responsibility, 4(1), 1–23. Aguilera, R. V., Rupp, D. E., Williams, C.A., & Ganapathi, J. (2007). Putting the S back in corporate social responsibility: A multilevel theory of social change in organizations.Academy of Management Review, 32(3), 836–863. Alareeni, B. A., & Hamdan, A. (2020). ESG impact on performance of US S&P 500-listed firms. Corporate Gov"
8228,unknown,"Business in Society, 20(7), 1409–1428. Ali, W., Frynas, J. G., & Mahmood, Z. (2017). Determinants of corporate social responsibility (CSR) disclosure in developed and developing countries: A literature review.Corporate Social Responsibility and Envi- ronmental Management, 24(4), 273–294. Al-Qudah, A. A. (2017). The relationship between capital structure and financial performance in the companies l"
8229,unknown,"exchange: Evidences from United Arab Emirates.Review of European Studies, 9(2), 1–9. Amato, L. H., & Burson, T. E. (2007). The effects of firm size on profit rates in the financial services.Journal of Economics and Economic Education Research, 8(1), 67. Amel-Zadeh, A., & Serafeim, G. (2018). Why and how investors use ESG information: Evidence from a global survey.Financial Analysts Journal, 74(3),"
8230,unknown,"74(3), 87–103. Arellano, M., & Bond, S. (1991). Some tests of specification for panel data: Monte Carlo evidence and an application to employment equations. The Review of Economic Studies, 58(2), 277–297. Aupperle, K. E., Carroll, A. B., & Hatfield, J. D. (1985). An empirical exami- nation of the relationship between corporate social responsibility and profitability. Academy of Management Journal,"
8231,unknown,"Avramov, D., Cheng, S., Lioui, A., & Tarelli, A. (2021). Sustainable investing with ESG rating uncertainty. Journal of Financial Economics, 145(2), 642–664. Azmi, W., Hassan, M. K., Houston, R., & Karim, M. S. (2021). ESG activities and banking performance: Internationalevidence from emerging economies. Journal of International Financial Markets, Institutions and Money, 70,1 –18. Balluchi, F., Laz"
8232,unknown,"Balluchi, F., Lazzini, A., & Torelli, R. (2020). CSR and greenwashing: A mat- ter of perception in the search of legitimacy. In M. Del Baldo, J. Dillard, M. G. Baldarelli, & M. Ciambotti (Eds.),Accounting, accountability and society. CSR, Sustainability, Ethics & Governance. Springer. https://doi. org/10.1007/978-3-030-41142-8_8 Baltagi, B. H. (2013). Panel data forecasting.Handbook of Economic Fo"
8233,unknown,"Baltagi, B. H. (2013). Panel data forecasting.Handbook of Economic Fore- casting, 2, 995–1024. Barauskaite, G., & Streimikiene, D. (2021). Corporate social responsibility and financial performance of companies: The puzzle of concepts, 812 CERCIELLO ET AL."
8234,unknown,"and financial performance of companies: The puzzle of concepts, 812 CERCIELLO ET AL. 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by "
8235,unknown,"definitions and assessment methods. Corporate Social Responsibility and Environmental Management, 28(1), 278–287. Barnea, A., & Rubin, A. (2010). Corporate social responsibility as a conflict between shareholders.Journal of Business Ethics, 97(1), 71–86. Barney, J. (1991). Firm resources and sustained competitive advantage. Journal of Management, 17(1), 99–120. Bauer, R., Otten, R., & Rad, A. T. ("
8236,unknown,"Bauer, R., Otten, R., & Rad, A. T. (2006). Ethical investing in Australia: Is there a financial penalty?Pacific-Basin Finance Journal, 14(1), 33–48. Beare, D., Buslovich, R., & Searcy, C. (2014). Linkages between corporate sustainability reporting and public policy.Corporate Social Responsibil- ity and Environmental Management, 21(6), 336–350. Beaver, W. H., & Ryan, S. G. (2000). Biases and lags i"
8237,unknown,"effects on the ability of the book-to-market ratio to predict book return on equity.Journal of Accounting Research, 38(1), 127–148. Becker-Blease, J. R., Kaen, F. R., Etebari, A., & Baumann, H. (2010). Employees, firm size and profitability in US manufacturing industries. Investment Management and Financial Innovations, 7(2), 7–23. Bénabou, R., & Tirole, J. (2010). Individual and corporate social "
8238,unknown,"ity. Economica, 77(305), 1–19. Bird, R., Hall, A. D., Momentè, F., & Reggiani, F. (2007). What corporate social responsibility activities are valued by the market? Journal of Business Ethics, 76(2), 189–206. Bowsher, C. G. (2002). On testing overidentifying restrictions in dynamic panel data models.Economics Letters, 77(2), 211–220. Brammer, S., Brooks, C., & Pavelin, S. (2006). Corporate social p"
8239,unknown,"and stock returns: UK evidence from disaggregate measures.Financial Management, 35(3), 97–116. Branco, M. C., & Rodrigues, L. L. (2008). Social responsibility disclosure: A study of proxies for the public visibility of Portuguese banks.The British Accounting Review, 40(2), 161–181. Brennan, E., & Guillamon-Saorin, A. P. (2009). Methodological insights: Impression management: Developing and illustr"
8240,unknown,"ysis for narrative disclosures – A methodological note. Accounting, Auditing & Accountability Journal, 22(5), 789–832. Broadstock, D. C., Chan, K., Cheng, L. T., & Wang, X. (2021). The role of ESG performance during times of financial crisis: Evidence from COVID-19 in China.Finance Research Letters, 38, 101716. Brooks, C., & Oikonomou, I. (2018). The effects of environmental, social and governance"
8241,unknown,"and governance disclosures and performance on firm value: A review of the literature in accounting and finance. The British Accounting Review, 50(1), 1–15. Brown, L. D., & Caylor, M. L. (2006). Corporate governance and firm valua- tion. Journal of Accounting and Public Policy, 25(4), 409–434. Brown, W. O., Helland, E., & Smith, J. K. (2006). Corporate philanthropic practices. Journal of Corporate "
8242,unknown,"practices. Journal of Corporate Finance, 12(5), 855–877. Buallay, A. (2019a). Is sustainability reporting (ESG) associated with perfor- mance? Evidence from the European banking sector.Management of Environmental Quality: An International Journal, 30(1), 98–115. Buallay, A. (2019b). Between cost and value: Investigating the effects of sustainability reporting on a firm's performance. Journal of Ap"
8243,unknown,"Accounting Research, 20(4), 481–496. Buallay, A. (2019c). Sustainability reporting and firm's performance: Com- parative study between manufacturing and banking sectors.Interna- tional Journal of Productivity and Performance Management , 69(3), 431–445. Buallay, A., Fadel, S. M., Alajmi, J., & Saudagaran, S. (2020). Sustainability reporting and bank performance after financial crisis: Evidence fro"
8244,unknown,"developed and developing countries.Competitiveness Review: An Inter- national Business Journal, 31(4), 747–770. Buchanan, B., Cao, C. X., & Chen, C. (2018). Corporate social responsibility, firm value, and influential institutional ownership.Journal of Corporate Finance, 52,7 3–95. Busch, T., Bauer, R., & Orlitzky, M. (2016). Sustainable development and financial markets: Old paths and new avenues"
8245,unknown,"financial markets: Old paths and new avenues. Business & Society, 55(3), 303–329. Busch, T., Bruce-Clark, P., Derwall, J., Eccles, R., Hebb, T., Hoepner, A., Klein, C., Krueger, P., Paetzold, F., Scholtens, B., & Weber, O. (2021). Impact investments: A call for (re) orientation.SN Business & Econom- ics, 1(2), 1–13. Cardebat, J. M., & Sirven, N. (2010). What corporate social responsibility reporti"
8246,unknown,"reporting adds to financial return?Journal of Economics and Interna- tional Finance, 2(2), 20–27. Cassar, G., & Holmes, S. (2003). Capital structure and financing of SMEs: Australian evidence.Accounting & Finance, 43(2), 123–147. Ceccagnoli, M. (2009). Appropriability, preemption, and firm performance. Strategic Management Journal, 30(1), 81–98. Champion, D. (1999). Finance: The joy of leverage. H"
8247,unknown,"Champion, D. (1999). Finance: The joy of leverage. Harvard Business Review, 77(4), 19–22. Chen, L., Yuan, T., Cebula, R. J., Shuangjin, W., & Foley, M. (2021). Fulfill- ment of ESG responsibilities and firm performance: A zero-sum game or mutually beneficial.Sustainability, 13(19), 10954. Chih, H. L., Chih, H. H., & Chen, T. Y. (2010). On the determinants of cor- porate social responsibility: Inte"
8248,unknown,"industry. Journal of Business Ethics, 93(1), 115–135. Clarkson, P., Li, Y., Richardson, G., & Vasvari, F. (2008). Revisiting the rela- tion between environmental performance and environmental disclo- sure: An empirical analysis.Accounting, Organizations and Society, 33, 303–327. Coad, A. (2007). Testing the principle of‘growth of the fitter’: The rela- tionship between profits and firm growth.Stru"
8249,unknown,"tionship between profits and firm growth.Structural Change and Eco- nomic Dynamics, 18(3), 370–386. Coppola L. (2018):“Eighty one percent of the s&p 500 index companies published corporate sustainability reports in 2017”. Flash Report, Gov- ernance & Accountability Institute, May 2018. Cornell, B., & Damodaran, A. (2020). Valuing ESG: Doing good or sounding good? The Journal of Impact and ESG Inve"
8250,unknown,"Cornett, M. M., Erhemjamts, O., & Tehranian, H. (2016). Greed or good deeds: An examination of the relation between corporate social responsibility and the financial performance of US commercial banks around the financial crisis.Journal of Banking & Finance, 70, 137–159. Dang, C., Li, Z. F., & Yang, C. (2018). Measuring firm size in empirical cor- porate finance.Journal of Banking & Finance, 86, 1"
8251,unknown,"86, 159–176. DasGupta, R. (2021). Financial performance shortfall, ESG controversies, and ESG performance: Evidence from firms around the world.Finance Research Letters, 46, 102487. Davidsson, P., Delmar, F., & Wiklund, J. (2006). Entrepreneurship as growth; growth as entrepreneurship.Entrepreneurship and the Growth of Firms, 1(2), 21–38. Davidsson, P., Steffens, P., & Fitzsimmons, J. (2009). Grow"
8252,unknown,"growing from profits: Putting the horse in front of the cart?Journal of Business Venturing, 24(4), 388–406. De Muro, P., Mazziotta, M., & Pareto, A. (2011). Composite indices of development and poverty: An application to MDGs.Social indicators research, 104(1), 1–18. De Lucia, C., Pazienza, P., & Bartlett, M. (2020). Does good ESG lead to better financial performances by firms? Machine learning an"
8253,unknown,"models of public enterprises in Europe.Sustainability, 12(13), 5317. De Vries, G., Terwel, B. W., Ellemers, N., & Daamen, D. D. (2015). Sustain- ability or profitability? How communicated motives for environmental policy affect public perceptions of corporate greenwashing.Corporate Social Responsibility and Environmental Management, 22(3), 142–154. Delmar, F., Davidsson, P., & Gartner, W. B. (2003"
8254,unknown,"growth firm.Journal of Business Venturing, 18(2), 189–216. Derwall, J., Guenster, N., Bauer, R., & Koedijk, K. (2005). The eco- efficiency premium puzzle.Financial Analysts Journal, 61(2), 51–63. Desai, M. A., & Dharmapala, D. (2009). Corporate tax avoidance and firm value. The Review of Economics and Statistics, 91(3), 537–546. Devinney, T. M. (2009). Is the socially responsible corporation a myt"
8255,unknown,"good, the bad, and the ugly of corporate social responsibility.Academy of Management Perspectives, 23(2), 44–56. CERCIELLO ET AL. 813 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Libra"
8256,unknown,"Di Giuli, A., & Kostovetsky, L. (2014). Are red or blue companies more likely to go green? Politics and corporate social responsibility.Journal of Financial Economics, 111(1), 158–180. Duque-Grisales, E., & Aguilera-Caracuel, J. (2021). Environmental, social and governance (ESG) scores and financial performance of multilati- nas: Moderating effects of geographic international diversification and f"
8257,unknown,"financial slack.Journal of Business Ethics, 168(2), 315–334. Eccles, R. G., Ioannou, I., & Serafeim, G. (2014). The impact of corporate sustainability on organizational processes and performance.Manage- ment Science, 60(11), 2835–2857. Eccles, R. G., Lee, L. E., & Stroehle, J. C. (2020). The social origins of ESG: An analysis of Innovest and KLD.Organization & Environment, 33(4), 575–596. Escrig-O"
8258,unknown,"575–596. Escrig-Olmedo, E., Fernández-Izquierdo, M.´A., Ferrero-Ferrero, I., Rivera- Lirio, J. M., & Muñoz-Torres, M. J. (2019). Rating the raters: Evaluating how ESG rating agencies integrate sustainability principles.Sustainabil- ity, 11(3), 915. Esteban-Sanchez, P., de la Cuesta-Gonzalez, M., & Paredes-Gazquez, J. D. (2017). Corporate social performance and its relation with corporate financial"
8259,unknown,"financial performance: International evidence in the banking industry. Journal of Cleaner Production, 162, 1102–1110. European Commission (2020a). EU taxonomy for sustainable activities. https://ec.europa.eu/info/business-economy-euro/banking-and- finance/sustainable-finance/eu-taxonomy-sustainable-activities_en. European Commission (2020b). Commission action plan on financing sus- tainable growth"
8260,unknown,"tainable growth. https://ec.europa.eu/info/publications/sustainable- finance-renewed-strategy_en. Fajaria, A. Z., & Isnalita, N. I. D. N. (2018). The effect of profitability, liquid- ity, leverage and firm growth of firm value with its dividend policy as a moderating variable. International Journal of Managerial Studies and Research (IJMSR), 6(10), 55–69. Fama, E. F., & French, K. (1992). The cros"
8261,unknown,"returns. Journal of Finance, 47(2), 427–465. Fama, E. F., & French, K. R. (1995). Size and book-to-market factors in earnings and returns.The Journal of Finance, 50(1), 131–155. Federico, J. S., & Capelleras, J. L. (2015). The heterogeneous dynamics between growth and profits: The case of young firms.Small Business Economics, 44(2), 231–253. Ferraro, A., Cerciello, M., Agovino, M., & Garofalo, A. "
8262,unknown,"of cultural consumption in reducing social exclusion: Empirical evi- dence from Italy in a spatial framework. Economia Politica , 36(1), 139–166. Ferr/C19on-Vílchez, V., Valero-Gil, J., & Suárez-Perales, I. (2021). How does greenwashing influence managers' decision-making? An experimental approach under stakeholder view.Corporate Social Responsibility and Environmental Management, 28(2), 860–880. "
8263,unknown,"Fiaschi, D., Giuliani, E., Nieri, F., & Salvati, N. (2020). How bad is your com- pany? Measuring corporate wrongdoing beyond the magic of ESG metrics. Business Horizons, 63(3), 287–299. Figge, F. (2005). Value-based environmental management. From envi- ronmental shareholder value to environmental option value. Corpo- rate Social Responsibility and Environmental Management , 12(1), 19–30. Francesch"
8264,unknown,"19–30. Franceschelli, M. V., Santoro, G., & Candelo, E. (2018). Business model innovation for sustainability: A food start-up case study.British Food Journal, 120(10), 2483–2494. Freeman, R. E. (2010).Strategic management: A stakeholder approach.C a m - bridge University Press. Friede, G. (2019). Why don't we see more action? A metasynthesis of the investor impediments to integrate environmental, "
8265,unknown,"governance factors. Business Strategy and the Environment , 28(6), 1260–1282. Friede, G., Busch, T., & Bassen, A. (2015). ESG and financial performance: Aggregated evidence from more than 2000 empirical studies.Journal of Sustainable Finance & Investment, 5 (4), 210–233. Friedman, M. (1970). A Friedman doctrine: The social responsibility of business is to increase its profits. The New York Times M"
8266,unknown,"13(1970), 32–33. Fuertes-Callén, Y., & Cuellar-Fernández, B. (2019). Inter-relationship between firm growth and profitability in a context of economic crisis. Journal of Business Economics and Management, 20(1), 86–106. Garcia, A. S., & Orsato, R. J. (2020). Testing the institutional difference hypothesis: A study about environmental, social, governance, and financial performance. Business Strateg"
8267,unknown,"financial performance. Business Strategy and the Environment, 29(8), 3261–3272. Gatti, L., Seele, P., & Rademacher, L. (2019). Grey zone in greenwash out. A review of greenwashing research and implications for the voluntary- mandatory transition of CSR.International Journal of Corporate Social Responsibility, 4(1), 1–15. Gerard, B. (2019). ESG and socially responsible investment: A critical review"
8268,unknown,"Gill, A., Biger, N., & Mathur, N. (2011). The effect of capital structure on profitability: Evidence from the United States.International Journal of Management, 28(4 Pt 1), 3–15. Gillan, S. L., Koch, A., & Starks, L. T. (2021). Firms and social responsibility: A review of ESG and CSR research in corporate finance.Journal of Cor- porate Finance, 66, 101889. Gilley, K. M., Worrell, D. L., Davidson, "
8269,unknown,"Gilley, K. M., Worrell, D. L., Davidson, W. N., III, & El–Jelly, A. (2000). Cor- porate environmental initiatives and anticipated firm performance: The differential effects of process-driven versus product-driven green- ing initiatives.Journal of Management, 26(6), 1199–1216. Goddard, J., Molyneux, P., & Wilson, J. O. (2004). Dynamics of growth and profitability in banking. Journal of Money, Credi"
8270,unknown,"profitability in banking. Journal of Money, Credit and Banking , 36, 1069–1090. Goddard, J., Tavakoli, M., & Wilson, J. O. (2005). Determinants of profit- ability in European manufacturing and services: Evidence from a dynamic panel model.Applied Financial Economics, 15(18), 1269–1282. Graham, J. R. (2000). How big are the tax benefits of debt?The Journal of Finance, 55(5), 1901–1941. Greening, D."
8271,unknown,"Finance, 55(5), 1901–1941. Greening, D. W., & Turban, D. B. (2000). Corporate social performance as a competitive advantage in attracting a quality workforce.Business & Society , 39(3), 254–280. Gupta, P. D., Guha, S., & Krishnaswami, S. S. (2013). Firm growth and its determinants. Journal of Innovation and Entrepreneurship, 2(1), 1–14. Hall, G. C., Hutchinson, P. J., & Michaelas, N. (2004). Deter"
8272,unknown,"capital structures of European SMEs. Journal of Business Finance & Accounting, 31(5–6), 711–728. Hassel, L., Nilsson, H., & Nyquist, S. (2005). The value relevance of envi- ronmental performance.European Accounting Review, 14(1), 41–61. Hirigoyen, G., & Poulain-Rehm, T. (2015). Relationships between corporate social responsibility and financial performance: What is the causality? Journal of Busine"
8273,unknown,"Journal of Business & Management, 4(1), 18–43. Hirsch, S., & Gschwandtner, A. (2013). Profit persistence in the food indus- try: evidence from five, profit persistence in the food industry: Evi- dence from five European countries. Holtz-Eakin, D., Newey, W., & Rosen, H. S. (1988). Estimating vector auto- regressions with panel data. Econometrica.Journal of the econometric society, 56(6), 1371–1395"
8274,unknown,"society, 56(6), 1371–1395. Hopwood, A. G. (2009). Accounting and the environment. Accounting, Organizations and Society, 34(3–4), 433–439. Horváthová, E. (2010). Does environmental performance affect financial performance? A meta-analysis.Ecological Economics, 70(1), 52–59. Hou, K., Xue, C., & Zhang, L. (2015). Digesting anomalies: An investment approach. The Review of Financial Studies, 28(3), 65"
8275,unknown,"Hovakimian, A., Opler, T., & Titman, S. (2001). The debt-equity choice. Journal of Financial and Quantitative Analysis, 36(1), 1–24. Howard-Grenville, J. (2021, January 22). ESG impact is hard to measure - but it's not impossible. Harvard Business Review. Retreived from https://hbr.org/2021/01/esg-impact-is-hard-to-measure-but-its-not- impossible 814 CERCIELLO ET AL."
8276,unknown,"https://hbr.org/2021/01/esg-impact-is-hard-to-measure-but-its-not- impossible 814 CERCIELLO ET AL. 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles ar"
8277,unknown,"Hsu, J., Liu, X., Shen, K., Viswanathan, V., & Zhao, Y. (2018). Outperfor- mance through investing in ESG in need.The Journal of Index Investing, 9(2), 18–26. Hull, C. E., & Rothenberg, S. (2008). Firm performance: The interactions of corporate social performance with innovation and industry differentia- tion. Strategic Management Journal, 29(7), 781–789. Humphrey, J. E., Lee, D. D., & Shen, Y. (2"
8278,unknown,"able? Journal of Corporate Finance, 18(3), 626–639. Hussain, N., Rigoni, U., & Cavezzali, E. (2018). Does it pay to be sustain- able? Looking inside the black box of the relationship between sustain- ability performance and financial performance. Corporate Social Responsibility and Environmental Management, 25(6), 1198–1211. _Isık, Ö., Aydın Ünal, E., & Ünal, Y. (2017). The effect of firm size on "
8279,unknown,"ability: Evidence from Turkish manufacturing sector.Journal of Busi- ness Economics and Finance, 6(4), 301–308. Jang S., & Park, K. (2011). Hospitality finance research during recent two decades: subjects, methodologies, and citations.International Journal of Contemporary Hospitality Management, 23(4), 479–497. Jensen, M. C. (2002). Value maximization, stakeholder theory and the cor- porate object"
8280,unknown,"porate objective function.Business Ethics Quarterly, 12(2), 235–256. Jensen, M. C., & Meckling, W. H. (1976). Theory of the firm: Managerial behavior, agency costs and ownership structure.Journal of Financial Economics, 3(4), 305–360. Jiraporn, P., & Chintrakarn, P. (2013). How do powerful CEOs view corpo- rate social responsibility (CSR)? An empirical note.Economics Letters, 119(3), 344–347. Jone"
8281,unknown,"119(3), 344–347. Jones, T. M. (1995). Instrumental stakeholder theory: A synthesis of ethics and economics.The Academy of Management Review, 20, 404–437. Kartikasari, D., & Merianti, M. (2016). The effect of leverage and firm size to profitability of public manufacturing companies in Indonesia.Inter- national Journal of Economics and Financial Issues, 6(2), 409–413. Katz, M. L., & Shapiro, C. (198"
8282,unknown,"compatibility. The American Economic Review , 75(3), 424–440. Kim, K. H., Kim, M., & Qian, C. (2018). Effects of corporate social responsi- bility on corporate financial performance: A competitive-action per- spective. Journal of Management, 44(3), 1097–1118. Kotsantonis, S., Pinney, C., & Serafeim, G. (2016). ESG integration in investment management: Myths and realities.Journal of Applied Corpo- "
8283,unknown,"investment management: Myths and realities.Journal of Applied Corpo- rate Finance, 28(2), 10–16. Kotsantonis, S., & Serafeim, G. (2019). Four things no one will tell you about ESG data.Journal of Applied Corporate Finance, 31(2), 50–58. Krüger, P. (2015). Corporate goodness and shareholder wealth.Journal of Financial Economics, 115(2), 304–329. Kucharska, W., & Kowalczyk, R. (2019). How to achieve"
8284,unknown,"Employee's point of view on company's culture and CSR practice.Cor- porate Social Responsibility and Environmental Management , 26(2), 453–467. Kuncová, M., Hedija, V., & Fiala, R. (2016). Firm size as a determinant of firm performance: The case of swine raising.Agris on-line Papers in Eco- nomics and Informatics, 8(3), 77–89. La Torre, M., Leo, S., & Panetta, I. C. (2021). Banks and environmental"
8285,unknown,"social and governance drivers: Follow the market or the authorities? Corporate Social Responsibility and Environmental Management, 12(16), 6387–1634. Laufer, W. S. (2003). Social accountability and corporate greenwashing. Journal of Business Ethics, 43(3), 253–261. Lawrence, A., Sloan, R., & Sun, E. (2018). Why are losses less persistent than profits? Curtailments vs. conservatism. Management Scie"
8286,unknown,"64(2), 673–694. Lechner, C., & Gudmundsson, S. V. (2014). Entrepreneurial orientation, firm strategy and small firm performance.International Small Business Journal, 32(1), 36–60. Lee, D. D., Faff, R. W., & Langfield-Smith, K. (2009). Revisiting the vexing question: Does superior corporate social performance lead to improved financial performance? Australian Journal of Management, 34(1), 21–49. Le"
8287,unknown,"34(1), 21–49. Lee, J. (2009). Does size matter in firm performance? Evidence from US public firms. International Journal of the Economics of Business, 16(2), 189–203. Lee, S. H., & Makhija, M. (2009). Flexibility in internationalization: Is it valu- able during an economic crisis?Strategic Management Journal, 30(5), 537–555. Lee, S. M., Noh, Y., Choi, D., & Rha, J. S. (2017). Environmental policy "
8288,unknown,"formances for sustainable development: From the perspective of ISO 14001 certification. Corporate Social Responsibility and Environmental Management, 24(2), 108–120. Li, F. (2008). Annual report readability, current earnings, and earnings per- sistence. Journal of Accounting and Economics, 45(2–3), 221–247. Li, F., & Polychronopoulos, A. (2020, January 20). What a difference an ESG ratings provide"
8289,unknown,"ESG ratings provider makes. Research Affiliates Publication https:// www.researchaffiliates.com/publications/articles/what-a-difference- an-esg-ratings-provider-makes Li, J., Haider, Z. A., Jin, X., & Yuan, W. (2019). Corporate controversy, social responsibility and market performance: International evidence.Journal of International Financial Markets, Institutions and Money, 60,1 –18. Li, T. T., W"
8290,unknown,"Li, T. T., Wang, K., Sueyoshi, T., & Wang, D. D. (2021). ESG: Research pro- gress and future prospects.Sustainability, 13(21), 11663. Liargovas, P. G., & Skandalis, K. S. (2010). Factors affecting firms' perfor- mance: The case of Greece.Global Business and Management Research: An International Journal, 2(2), 184–197. Lieberman, M. B., & Montgomery, D. B. (1988). First-mover advantages. Strategic "
8291,unknown,"Strategic Management Journal, 9(S1), 41–58. Lin, W. L., Law, S. H., Ho, J. A., & Sambasivan, M. (2019). The causality direction of the corporate social responsibility–corporate financial per- formance nexus: Application of panel vector autoregression approach. The North American Journal of Economics and Finance, 48, 401–418. Lins, K. V., Servaes, H., & Tamayo, A. (2017). Social capital, trust, and"
8292,unknown,"performance: The value of corporate social responsibility during the financial crisis.The Journal of Finance, 72(4), 1785–1824. Love, J. H., & Roper, S. (2015). SME innovation, exporting and growth: A review of existing evidence.International Small Business Journal, 33(1), 28–48. Lyon, T. P., & Montgomery, A. W. (2013). Tweetjacked: The impact of social media on corporate.Journal of Business Ethic"
8293,unknown,"Lys, T., Naughton, J. P., & Wang, C. (2015). Signaling through corporate accountability reporting. Journal of Accounting and Economics, 60(1), 56–72. Makni, R., Francoeur, C., & Bellavance, F. (2009). Causality between corpo- rate social performance and financial performance: Evidence from Canadian firms.Journal of Business Ethics, 89(3), 409–422. Margaritis, D., & Psillaki, M. (2010). Capital str"
8294,unknown,"and firm performance.Journal of Banking & Finance, 34(3), 621–632. Margolis, J. D., & Walsh, J. P. (2003). Misery loves companies: Rethinking social initiatives by business. Administrative Science Quarterly, 48(2), 268–305. Markman, G. D., & Gartner, W. B. (2002). Is extraordinary growth profit- able? A study of Inc. 500 high –growth companies. Entrepreneurship Theory and Practice, 27(1), 65–75. M"
8295,unknown,"Masulis, R. W., & Reza, S. W. (2015). Agency problems of corporate philan- thropy. The Review of Financial Studies, 28(2), 592–636. Mazziotta, M., & Pareto, A. (2011). Un indice sintetico non compensativo per la misura della dotazione infrastrutturale: Un'applicazione in ambito sanitario.Rivista di statistica ufficiale, 1(2011), 63–79. Mazziotta, M., & Pareto, A. (2013). A non-compensatory composi"
8296,unknown,"for measuring well-being over time. Cogito.Multidisciplinary Research Journal, 5(4), 93–104. Mazziotta, M., & Pareto, A. (2016). On a generalized non-compensatory composite index for measuring socio-economic phenomena. Social Indicators Research, 127(3), 983–1003. CERCIELLO ET AL. 815"
8297,unknown,"Indicators Research, 127(3), 983–1003. CERCIELLO ET AL. 815 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative C"
8298,unknown,"McBrayer, G. A. (2018). Does persistence explain ESG disclosure deci- sions? Corporate Social Responsibility and Environmental Management, 25(6), 1074–1086. McWilliams, A., & Siegel, D. (2000). Corporate social responsibility and financial performance: Correlation or misspecification?Strategic Man- agement Journal, 21(5), 603–609. McWilliams, A., Siegel, D. S., & Wright, P. M. (2006). Corporate so"
8299,unknown,"responsibility: Strategic implications. Journal of Management Studies, 43(1), 1–18. Melloni, G., Caglio, A., & Perego, P. (2017). Saying more with less? Disclo- sure conciseness, completeness and balance in integrated reports. Journal of Accounting and Public Policy, 36(3), 220–238. Mendelson, H. (2000). Organizational architecture and success in the information technology industry. Management Sci"
8300,unknown,"513–529. Merkl-Davies, D. M., & Brennan, N. M. (2011). A conceptual framework of impression management: New insights from psychology, sociology and critical perspectives.Accounting and Business Research, 41(5), 415–437. Minutolo, M. C., Kristjanpoller, W. D., & Stakeley, J. (2019). Exploring envi- ronmental, social, and governance disclosure effects on the S&P 500 financial performance. Business S"
8301,unknown,"500 financial performance. Business Strategy and the Environment , 28(6), 1083–1095. Mio, C., Fasan, M., Marcon, C., & Panfilo, S. (2020). The predictive ability of legitimacy and agency theory after the implementation of the EU directive on non-financial information. Corporate Social Responsibility and Environmental Management, 27(6), 2465–2476. Mo, Z., Liu, M. T., & Liu, Y. (2018). Effects of fu"
8302,unknown,"on self and others.Psychology & Marketing, 35(5), 368–382. Mohamed Buallay, A., Al Marri, M., Nasrallah, N., Hamdan, A., Barone, E., & Zureigat, Q. (2021). Sustainability reporting in banking and financial services sector: A regional analysis.Journal of Sustainable Finance & Investment, 1,1 –26. Muñoz-Torres, M. J., Fernández-Izquierdo, M. ´A., Rivera-Lirio, J. M., & Escrig-Olmedo, E. (2019). Can "
8303,unknown,"Escrig-Olmedo, E. (2019). Can environmental, social, and governance rating agencies favor business models that promote a more sustainable development? Corporate Social Responsibility and Environmental Man- agement, 26(2), 439–452. Muscettola, M. (2014). Cash conversion cycle and firm's profitability: An empirical analysis on a sample of 4,226 manufacturing SMEs of Italy. International Journal of B"
8304,unknown,"International Journal of Business and Management, 9(5), 25. Muscettola, M. (2015). Predictive ability of accounting ratio for bank- ruptcy. Journal of Applied Finance and Banking, 5(1), 13. Muscettola, M., & Naccarato, F. (2016). The casual relationship between debt and profitability: The case of Italy.Athens Journal of Business and Economics, 2(1), 17–32. Niresh, A., & Thirunavukkarasu, V. (2014)"
8305,unknown,"Economics, 2(1), 17–32. Niresh, A., & Thirunavukkarasu, V. (2014). Firm size and profitability: A study of listed manufacturing firms in Sri Lanka.International Journal of Business and Management, 9(4), 57–64. Nirino, N., Santoro, G., Miglietta, N., & Quaglia, R. (2021). Corporate con- troversies and company's financial performance: Exploring the moder- ating role of ESG practices. Technological F"
8306,unknown,"ating role of ESG practices. Technological Forecasting and Social Change, 162, 120341. Nizam, E., Ng, A., Dewandaru, G., Nagayev, R., & Nkoba, M. A. (2019). The impact of social and environmental sustainability on financial perfor- mance: A global analysis of the banking sector.Journal of Multinational Financial Management, 49,3 5–53. Nunes, P. M., & Serrasquero, Z. S. (2008). Performance and size"
8307,unknown,"Evidence from Portuguese SMEs Small Business Economics , 31(2), 195–217. Orlitzky, M., Schmidt, F. L., & Rynes, S. L. (2003). Corporate social and environmental responsibility: A meta-analysis. Organization Studies, 24(3), 403–441. Pagano, M. S., Sinclair, G., & Yang, T. (2018). Understanding ESG ratings and ESG indexes. In S. Boubaker, D. Cumming, & D. K. Nguyen (Eds.), Research handbook of finan"
8308,unknown,"Research handbook of finance and sustainability . Edward Elgar Publishing. Palmer, K., Oates, W. E., & Portney, P. R. (1995). Tightening environmental standards: The benefit-cost or the no-cost paradigm?Journal of Eco- nomic Perspectives, 9(4), 119–132. Papadogonas, T. A. (2007). The financial performance of large and small firms: Evidence from Greece.International Journal of Financial Services Ma"
8309,unknown,"Management, 2(1–2), 14–20. Penman, S. H. (1991). An evaluation of accounting rate-of-return.Journal of Accounting, Auditing & Finance, 6(2), 233–255. Penman, S. H. (1992). Return to fundamentals.Journal of Accounting, Audit- ing & Finance, 7(4), 465–483. Penman, S. H. (1996). The articulation of price-earnings ratios and market- to-book ratios and the evaluation of growth. Journal of Accounting Re"
8310,unknown,", 34(2), 235–259. Petersen, M. A., & Rajan, R. G. (1994). The benefits of lending relation- ships: Evidence from small business data.The Journal of Finance, 49(1), 3–37. Plumlee, M., Brown, D., Hayes, R. M., & Marshall, R. S. (2015). Voluntary environmental disclosure quality and firm value: Further evidence. Journal of Accounting and Public Policy, 34(4), 336–361. Pollard, J. L., Sherwood, M. W.,"
8311,unknown,"Pollard, J. L., Sherwood, M. W., & Klobus, R. G. (2018). Establishing ESG as risk premia.Journal of Investment Management, 16(1), 32–43. Porter, M. E., & Kramer, M. R. (2006). Strategy and society: The link between competitive advantage and corporate social responsibility. Harvard Business Review, 84(12), 78–92. Preston, L. E., & O'bannon, D. P. (1997). The corporate social-financial per- formance"
8312,unknown,"formance relationship: A typology and analysis. Business & Society, 36(4), 419–429. Qi, G., Zeng, S., Li, X., & Tam, C. (2012). Role of internalization process in defining the relationship between ISO 14001 certification and corpo- rate environmental performance. Corporate Social Responsibility and Environmental Management, 19(3), 129–140. Qureshi, M. A., Akbar, M., Akbar, A., & Poulova, P. (2021)"
8313,unknown,"endeavors assist firms in achieving superior financial performance? A case of 100 best corporate citizens.SAGE Open, 11(2), 1–18. Refinitiv. (2021). Retrieved from https://www.refinitiv.com/en/financial- data/company-data/esg-research-data#find-out-more. Reid, G. C. (1995). Early life-cycle behaviour of micro-firms in Scotland. Small Business Economics, 7(2), 89–95. Reinhardt, F. L., & Stavins, R."
8314,unknown,"Reinhardt, F. L., & Stavins, R. N. (2010). Corporate social responsibility, business strategy, and the environment. Oxford Review of Economic Policy, 26(2), 164–181. Ritala, P. (2012). Coopetition strategy–when is it successful? Empirical evi- dence on innovation and market performance.British Journal of Man- agement, 23(3), 307–324. Rizzi, F., Gusmerotti, N., & Frey, M. (2020). How to meet reuse "
8315,unknown,"ration for reuse targets? Shape advertising strategies but be aware of “social washing”. Waste Management, 101, 291–300. Roberts, P. W., & Dowling, G. R. (2002). Corporate reputation and sus- tained superior financial performance: Reputation and persistent profitability. Strategic Management Journal , 23(12), 1077–1093. Rodríguez-Fernández, M., Sánchez-Teba, E. M., L /C19opez-Toro, A. A., & Borreg"
8316,unknown,"Borrego-Domínguez, S. (2019). Influence of ESGC indicators on finan- cial performance of listed travel and leisure companies.Sustainability, 11(19), 5529. R u f ,B .M . ,M u r a l i d h a r ,K . ,B r o w n ,R .M . ,J a n n e y ,J .J . ,&P a u l ,K .( 2 0 0 1 ) . An empirical investigation of the relationship between change in corporate social performance and financial performance: A stake- holder "
8317,unknown,"holder theory perspective. Journal of Business Ethics , 32(2), 143–156. Russo, M. V., & Fouts, P. A. (1997). A resource-based perspective on cor- porate environmental performance and profitability.Academy of Man- agement Journal, 40(3), 534–559. 816 CERCIELLO ET AL."
8318,unknown,"agement Journal, 40(3), 534–559. 816 CERCIELLO ET AL. 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons"
8319,unknown,"Santoro, G., Bresciani, S., Bertoldi, B., & Liu, Y. (2019). Cause-related mar- keting, brand loyalty and corporat e social responsibility: A cross- country analysis of Italian and Japanese consumers. International Marketing Review, 37(4), 773–791. Schuler, D. A., & Cording, M. (2006). A corporate social performance– corporate financial performance behavioral model for consumers. Academy of Managem"
8320,unknown,"Academy of Management Review, 31(3), 540–558. Seele, P., & Gatti, L. (2017). Greenwashing revisited: In search of a typol- ogy and accusation-based definition incorporating legitimacy strate- gies. Business Strategy and the Environment, 26(2), 239–252. Seifert, B., Morris, S. A., & Bartkus, B. R. (2004). Having, giving, and get- ting: Slack resources, corporate philanthropy, and firm financial per"
8321,unknown,"mance. Business & Society, 43(2), 135–161. Shakil, M. H., Mahmood, N., Tasnia, M., & Munim, Z. H. (2019). Do envi- ronmental, social and governance performance affect the financial per- formance of banks? A cross-country study of emerging market banks. Management of Environmental Quality: An International Journal, 30(6), 1331–1344. Sharma, S., & Henriques, I. (2005). Stakeholder influences on sust"
8322,unknown,"practices in the Canadian forest products industry.Strategic Manage- ment Journal, 26(2), 159–180. Slager, R., Gond, J. P., & Moon, J. (2012). Standardization as institutional work: The regulatory power of a responsible investment standard. Organization Studies, 33(5–6), 763–790. Smith, M., Yahya, K., & Amiruddin, A. M. (2007). Environmental disclosure and performance reporting in Malaysia. Asian "
8323,unknown,"and performance reporting in Malaysia. Asian Review of Accounting, 15(2), 185–199. Sprinkle, G. B., & Maines, L. A. (2010). The benefits and costs of corporate social responsibility.Business Horizons, 53(5), 445–453. Steurer, R., Langer, M. E., Konrad, A., & Martinuzzi, A. (2005). Corpora- tions, stakeholders and sustainable development I: A theoretical explo- ration of business–society relations."
8324,unknown,"ration of business–society relations. Journal of Business Ethics, 61(3), 263–281. Suarez, F. F., & Lanzolla, G. (2007). The role of environmental dynamics in building a first mover advantage theory. Academy of Management Review, 32(2), 377–392. Surroca, J., Trib/C19o, J. A., & Waddock, S. (2010). Corporate responsibility and financial performance: The role of intangible resources. Strategic Man- a"
8325,unknown,"Strategic Man- agement Journal, 31(5), 463–490. Tailab, M. (2014). The effect of capital structure on profitability of energy American firms. International Journal of Business and Management Invention, 3(12), 54–61. Tamimi, N., & Sebastianelli, R. (2017). Transparency among S&P 500 com- panies: An analysis of ESG disclosure scores.Management Decision, 55(8), 1660–1680. Tarmuji, I., Maelah, R., & T"
8326,unknown,"55(8), 1660–1680. Tarmuji, I., Maelah, R., & Tarmuji, N. H. (2016). The impact of environmen- tal, social and governance practices (ESG) on economic performance: Evidence from ESG score.International Journal of Trade, Economics and Finance, 7(3), 67–74. Taub, A. J. (1975). Determinants of the firm's capital structure.Review of Economics and Statistics, 57(4), 410–417. Testa, F., Boiral, O., & Iral"
8327,unknown,"Testa, F., Boiral, O., & Iraldo, F. (2018). Internalization of environmental practices and institutional complexity: Can stakeholders pressures encourage greenwashing?Journal of Business Ethics, 147(2), 287–307. Titman, S., & Wessels, R. (1988). The determinants of capital structure choice. The Journal of Finance, 43(1), 1–19. Troje, D., & Gluch, P. (2020). Beyond policies and social washing: How "
8328,unknown,"social procurement unfolds in practice.Sustainability, 12(12), 4956. Vijh, A. M., & Yang, K. (2013). Are small firms less vulnerable to overpriced stock offers?Journal of Financial Economics, 110(1), 61–86. Vitell, S. J., & Hidalgo, E. R. (2006). The impact of corporate ethical values and enforcement of ethical codes on the perceived importance of ethics in business: A comparison of US and Spanish"
8329,unknown,"of Business Ethics, 64(1), 31–43. Waddock, S. A., & Graves, S. B. (1997). The corporate social performance– financial performance link. Strategic Management Journal , 18(4), 303–319. Wagner, M., Van Phu, N., Azomahou, T., & Wehrmeyer, W. (2002). The relationship between the environmental and economic performance of firms: An empirical analysis of the European paper industry.Corporate Social Respon"
8330,unknown,"Social Responsibility and Environmental Management, 9(3), 133–146. Wang, Z., Hsieh, T. S., & Sarkis, J. (2018). CSR performance and the read- ability of CSR reports: Too good to be true? Corporate Social Responsi- bility and Environmental Management, 25(1), 66–79. Wright, M., & Stigliani, I. (2013). Entrepreneurship and growth.Interna- tional Small Business Journal, 31(1), 3–22. Wu, M. (2006). Cor"
8331,unknown,"Wu, M. (2006). Corporate social performance, corporate financial perfor- mance, and firm size: A meta-analysis.Journal of American Academy of Business, 8(1), 163–171. Wu, M. W., & Shen, C. H. (2013). Corporate social responsibility in the banking industry: Motives and financial performance.Journal of Bank- ing & Finance, 37(9), 3529–3547. Yang, Z., Nguyen, T. T. H., Nguyen, H. N., Nguyen, T. T. N."
8332,unknown,"(2020). Greenwashing behaviours: Causes, taxonomy and conse- quences based on a systematic literature review.Journal of Business Economics and Management, 21(5), 1486–1507. Yu, E. P. Y., Van Luu, B., & Chen, C. H. (2020). Greenwashing in environ- mental, social and governance disclosures. Research in International Business and Finance, 52, 101192. Zhang, D., & Xie, Y. (2022). Customer environmenta"
8333,unknown,"margin: Evidence from manufacturing firms.Journal of Economics and Business, 120, 106057. Zhang, L. (2005). The value premium. The Journal of Finance , 60(1), 67–103. How to cite this article:Cerciello, M., Busato, F., & Taddeo, S. (2023). The effect of sustainable business practices on profitability. Accounting for strategic disclosure.Corporate Social Responsibility and Environmental Management,"
8334,unknown,"802–819. https://doi.org/10.1002/csr.2389 APPENDIX A This section provides further methodological information on the Mazziotta-Pareto procedure, employed to compute the comprehen- sive ESG score. A.1. | The Mazziotta-Pareto index The Mazziotta–Pareto index (MPI) is a composite indicator that does not allow for full substitutability among components. In other words, the MPI prevents high values in "
8335,unknown,"low values in another (Agovino et al., 2018; Mazziotta & Pareto, 2016). It is based on a nonlinear function that assigns a pen- alty to units featuring unbalances across components (De Muro et al., 2011; Mazziotta & Pareto,2011, 2013, 2016). Rather than providing information aimed at creating a ranking of the units under CERCIELLO ET AL. 817"
8336,unknown,"information aimed at creating a ranking of the units under CERCIELLO ET AL. 817 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the a"
8337,unknown,"investigation, the MPI provides information on theintensity the phe- nomenon of interest. Moreover, it is independent of any benchmark or ideal unit. These characteristics make the method particularly suit- able when it comes to measuring the extent of ESG practices. Although the MPI allows for the inclusion of a large number of ele- mentary variables, further aggregated into pillars, in this work"
8338,unknown,"only two variables, that is, the ESG score and the ESG disclosure score. Each variable may be viewed as a separate pillar. Variable standardization is the first methodological step for the calculation of the comprehensive ESG score. The variables are rescaled around a mean of 100, so that at least 89% of the observa- tions range between 70 and 130. In order to standardize, vertical (i.e., cross-fi"
8339,unknown,"i is computed as: Mj ¼ 1 n Xn i¼1 xij Sj ¼ ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1 n Xn i¼1 xij /C0 Mj /C0/C1 2 , vuut ðA1Þ where X is ann /C2 m matrix, whose elements xijfg are the elementary values. The n rows represent firms, whereas them columns are the dimensions of the indicator (in our case two). Consider now matrix Z, whose elements z ij /C8/C9 are defined as: zij ¼ 10 /C3 xij /C0 Mj Sj /C6 "
8340,unknown,"Sj /C6 100: ðA2Þ The sign in Equation (A2) depends on the economic interpretation of the indicator. In our case, both dimensions increase the compre- hensive ESG score, so they get positive signs. Horizontal variability (i.e., cross-pillar or within firm variability) is defined as: M j ¼ 1 m Xn i¼1 zij Sj ¼ ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1 m Xn i¼1 zij /C0 Mi /C0/C1 2 : vuut ðA3Þ The variatio"
8341,unknown,"Mi. The MPI for uniti is then computed as: MPIi ¼ Mi ∓ Si cvi: ðA4Þ In summary, the MPI adjusts the mean of each variable by adding or subtracting an amount proportional to the mean square error. This amount is known as thepenalty, since it penalizes units (i.e., firms in our case) featuring large differences in the standardized variables. The larger the gap between the ESG score and the ESG discl"
8342,unknown,"for one firm, the larger the penalty. Similar to Equation ( A2), the sign in Equation (A4) depends on the interpretation of the phenomenon. A.2. | Robustness of the Mazziotta-Pareto procedure When measuring multidimensional phenomena, different aggregation methods may be used. To ensure the robustness of the comprehen- sive ESG score, we compare the results obtained under several different approac"
8343,unknown,"provided below: 1. Method of the rankings The values of the variables xij /C8/C9 are transformed into ranking values tij /C8/C9 , representing the relative positions of each observation in the overall ranking of units. This transformation removes the mea- surement unit and yields an integer, regardless of the original elemen- tary value. Ranking i ¼ 1 p Xp j¼1 tij: ðA5Þ 2. Method of the relative i"
8344,unknown,"The values of the variablesxij /C8/C9 are rescaled in the (0, 1) interval. The relative indices tij /C8/C9 are defined as: tij ¼ xij /C0 min xij /C0/C1 max xij /C0/C1 /C0 min xij /C0/C1 : ðA6Þ This transformation removes the measurement unit. The final index is computed as RIi ¼ 1 p Xp j¼1 tij: ðA7Þ 3. Wroclaw's taxonomic method An ideal value is identified for each variable— usually based on the "
8345,unknown,"unit featuring the highest value in the sample. The elementary values xij /C8/C9 are normalized by subtracting the mean and dividing by the stan- dard deviation, so as to obtain the transformed valuedtij /C8/C9 . tij ¼ xij /C0 xj σj : ðA8Þ Then the Euclidean distance between the transformed values and the ideal unit is computed Di ¼ ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Xp j¼1 tij /C0 max tij"
8346,unknown,"Xp j¼1 tij /C0 max tij /C8/C9/C0/C1 2 vuut , ðA9Þ Wroclavi ¼ Di D0 , D0 ¼ D0 þ2σ0, ðA10Þ where D0 is the mean distance andσ0 is the mean square error. The resulting index yields non-negative values, representing the adjusted distance from the ideal unit. The observations are ranked accordingly. 818 CERCIELLO ET AL."
8347,unknown,"distance from the ideal unit. The observations are ranked accordingly. 818 CERCIELLO ET AL. 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are gover"
8348,unknown,4. Method of the standardized values The elementary values xij /C8/C9 are standardized by subtracting the mean and dividing by the standard deviation. tij ¼ xij /C0 x σj : ðA11Þ Then the index is then computed as SVi ¼ 1 p Xp j¼1 tij: ðA12Þ We implement all the methods above to compute the compre- hensive ESG score. The correlation coefficients obtained when com- paring the results are shown in Ta
8349,unknown,"paring the results are shown in TablesA1 and A2, that refer to the first and last year of our panel. Similar results emerge for all other years. TABLE A1 Correlation matrix (2010) Rankings Standard values Relative indices Mazziotta-Pareto Wroclaw Rankings 1 Standard values 0.9773 1 Relative indices 0.9773 1 1 Mazziotta-Pareto 0.9781 0.9992 0.9993 1 Wroclaw 0.9700 0.9963 0.9960 0.9925 1 TABLE A2 Co"
8350,unknown,Rankings Standard values Relative indices Mazziotta-Pareto Wroclaw Rankings 1 Standard values 0.9280 1 Relative indices 0.9275 0.9988 1 Mazziotta-Pareto 0.9286 0.9991 0.9983 1 Wroclaw 0.9188 0.9941 0.9896 0.9899 1 CERCIELLO ET AL. 819
8351,unknown,"Mazziotta-Pareto 0.9286 0.9991 0.9983 1 Wroclaw 0.9188 0.9941 0.9896 0.9899 1 CERCIELLO ET AL. 819 15353966, 2023, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/csr.2389 by Durham University - University, Wiley Online Library on [24/02/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles ar"
8352,unknown,"RESEARCH ARTICLE Open Access Accuracy of random-forest-based imputation of missing data in the presence of non-normality, non-linearity, and interaction Shangzhi Hong and Henry S. Lynn * Abstract Background: Missing data are common in statistical analyses, and imputation methods based on random forests (RF) are becoming popular for handling missing data especially in biomedical research. Unlike st"
8353,unknown,"approaches, RF-based imputation methods do not assume normality or require specification of parametric models. However, it is still inconclusive how they perform for non-normally distributed data or when there are non-linear relationships or interactions. Methods: To examine the effects of these three factors, a variety of datasets were simulated with outcome- dependent missing at random (MAR) cov"
8354,unknown,"missForest and CALIBERrfimpute were evaluated in comparison with predictive mean matching (PMM). Results: Both missForest and CALIBERrfimpute have high predictive accuracy but missForest can produce severely biased regression coefficient estimates and downward biased confidence interval coverages, especially for highly skewed variables in nonlinear models. CALIBERrfimpute typically outperforms mis"
8355,unknown,"regression coefficients, although its biases are still substantial and can be worse than PMM for logistic regression relationships with interaction. Conclusions: RF-based imputation, in particular missForest, should not be indiscriminately recommended as a panacea for imputing missing data, especially when data are highly skewed and/or outcome-dependent MAR. A correct analysis requires a careful c"
8356,unknown,"variables in the data. Keywords: Missing data imputation, Imputation accuracy, Random forest Background Missing data are common in clinical and public health studies, and imputation methods based on machine learning algorithms, especially those based on random forest (RF) are gaining acceptance [ 1]. In the original article, the RF-based missing data imputation R package missForest is described as"
8357,unknown,signed for mixed continuous and/or categorical data in the presence of complex interactions and non-linearity without requiring to specify the distributions of the vari- ables [ 2]. Another R package CALIBERrfimpute allowed for multiple imputation by sampling from conditional distributions constructed using RF [ 3]. © The Author(s). 2020 Open Access This article is licensed under a Creative Common
8358,unknown,"which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise i"
8359,unknown,"licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver ( http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in t"
8360,unknown,"data made available in this article, unless otherwise stated in a credit line to the data. * Correspondence: hslynn@shmu.edu.cn Department of Biostatistics, Key Laboratory on Public Health Safety of the Ministry of Education, School of Public Health, Fudan University, Shanghai, China Hong and Lynn BMC Medical Research Methodology (2020) 20:199 https://doi.org/10.1186/s12874-020-01080-1 Several stu"
8361,unknown,"Several studies have reported that missForest performed favorably compared to standard imputation methods, and missForest has been used as a benchmark for non-parametric imputation methods [ 4, 5]. In a com- parison study done by Waljee et al. [ 6], missForest was found to consistently produce the lowest imputation error compared with other imputation methods, includ- ing k-nearest neighbors ( k-N"
8362,unknown,"[7], when data were missing completely at random (MCAR). Tang and Ishwaran also recommended mis- sForest when variables have high inter-correlations [ 5]. Yet Shah et al. [ 3] reported that missForest produced substantially biased estimates for variables missing at random (MAR) and poor coverage of confidence inter- vals compared with CALIBERrfimpute. Solaro et al. [ 8] demonstrated that the relat"
8363,unknown,"est varied with the MCAR data patterns and did not show a clear advantage. Overall, the imputation accuracy and applicability of missForest is still unclear. Moreover, the differences between CALIBERrfimpute and missFor- est imputation on statistical analyses warrant further investigation. This study evaluates the imputation accuracy of missForest and CALIBERrfimpute in the presence of non-normall"
8364,unknown,"linearity when data are MAR [ 9]. The examination was done through a series of simulation experiments and a case study based on clinical data for patients with hepatocellular carcinoma (HCC). Methods Imputation methods MissForest The missForest algorithm can be summarized as follows: (1) Initialization. For a variable containing missing values, the missing values will be replaced with its mean (fo"
8365,unknown,"(for continuous variables) or its most frequent class (for categorical variables). (2) Imputation. The imputation process is done se- quentially for the variables in the data in ascending (or descending, if appointed) order of missing observations for each variable. The variable under imputation is used as the response for building the RF model. The observa- tions in the dataset are divided into t"
8366,unknown,"to whether the variable is observed or missing in the ori- ginal dataset. The observed observations are used as the training set, and the missing observations are used as the prediction set. The missing part of the variable under imputation is replaced by predictions from RF models [ 10, 11]. (3) Stop. When all the variables with missing data have been imputed then one imputation iteration is com-"
8367,unknown,"pleted. The imputation process is iterated until the rela- tive sum of squared differences (or proportion of falsely classified entries for categorical variables) between the current and the previous imputation results increases, and missForest outputs the previous imputation as the final result [ 2]. A maximum number of iterations user setting with a default value of 10 is also installed in the p"
8368,unknown,"process to limit the computational time to a reasonable level. MICE-based imputation Both CALIBERrfimpute and predictive mean matching (PMM) operate under the framework of multivariate imputation using chained equations (MICE), with different MICE imputation methods differing in the process they use to impute missing values. The typical process of MICE-based imputation can be summarized as: (1) In"
8369,unknown,"summarized as: (1) Initialization. For a variable containing missing values, the missing values will be replaced with random samples from observed values of that variable. (2) Imputation. The imputation process is done sequentially for the variables according to their original order in the dataset. The variable under imputation is used as the response for model building. The observa- tions in the "
8370,unknown,"to whether the variable is observed or missing in the original dataset. The observed observations are used as the training set, and the missing observations are used as the prediction set. The missing part of the variable under imputation is replaced by the imputed values gen- erated from the user-specified MICE-based imputation method. (3) Stop. When all the variables with missing data have been "
8371,unknown,"completed. The imputation p rocess is iterated until the maximum number of iterations (default value of 5) is reached, and the final result is the last imputation. The PMM method is a semi-parametric imputation method recommended as the default imputation method by the “mice” R package [ 7] to serve as a “baseline” method for comparison. For each variable, PMM calcu- lates the predicted regression"
8372,unknown,and missing observations. It then fills in a missing value by randomly selecting one from the non-missing obser- vations whose predicted values are closest to the pre- dicted value for the missing observation. The purpose of the regression in PMM is to construct a metric for matching observations with missing values to similar observations with observed values that can be used for imputation. CALI
8373,unknown,"influences of imputation using the conditional distri- bution of the RF prediction errors. To form condi- tional distributions using RF, for each variable Hong and Lynn BMC Medical Research Methodology (2020) 20:199 Page 2 of 12 C A L I B E R r f i m p u t ea s s u m e dn o r m a l i t yf o rt h eR Fp r e - diction errors and used the out-of-bag mean square error as the estimator of their variance"
8374,unknown,"missing values are imputed by random draws from in- dependent normal distributions centered on the con- ditional means predicted by RF. It should be noted that to focus on the comparison with the prediction of imputed values in missForest, the additional boot- strapping of observed data in CALIBERrfimpute to account for sampling variation in multiple imputation was not used. In this study, five it"
8375,unknown,"imputation within the MICE framework as small iteration numbers were recommended [ 7]. Also, for the RF-based imputation methods, the number of trees built was set to ten as suggested by Shah et al. [ 3] for less biased results. Simulation study Data simulation and statistical analyses were carried out using R (R Core Team, Vienna, Austria) [ 12], and four sequential stages were involved: 1. Data "
8376,unknown,"based on each of the predefined scenarios for each of the distributions. 2. Amputation: the complete datasets were made incomplete based on specified rules to generate missing data values. 3. Imputation: the missing values of the simulated incomplete datasets were filled-in by different imputation methods, resulting in imputed complete datasets. 4. Analysis: statistical analyses were performed on "
8377,unknown,"both the original complete datasets and the corresponding imputed datasets, and comparisons of the different imputation methods were made. Data generation A total of 1000 datasets of 500 observations (where the dependent variable Y was continuous) or 1000 observa- tions (where Y was binary) were generated for each of the distributions in four scenarios as binary dependent variables usually provide"
8378,unknown,"parameters than do continuous dependent variables. The four scenarios comprised of 2 × 2 combinations where Y was either continuous or binary, and whether Y has a quadratic relationship with the independent variable X or its relationship with X was modified by a normally distributed variable Z. In each scenario, X was generated from one of the following eight distributions (Fig. 1): (1) Normal(4, "
8379,unknown,"(2) Uniform(0, 8), (3) Lognormal(0, 0.25), Lognormal(0, 0.625), (4) Gamma(1, 1), Gamma(2, 0.5), (5) [N(1, 1), N(6, 3)], [N(1, 1), N(6, 10)] (1:1 normal mixtures). The specific relationships among the variables in each scenario were as follow: (1) a linear regression with quadratic term: Y = β0 + β1X1 + β2X2 + ε =2+2 X + X2 + ε, ε~Normal(0,1.) (2) a logistic regression with quadratic term: Y /C24 B"
8380,unknown,"Y /C24 Binomial 1 ; πðÞ logit πðÞ ¼ β0 þ β1X1 þ β2X2 ¼ − 1:2 þ 0:1X þ 0:05X2 (3) a linear regression with interaction term: Y = β0 + β1X1 + β2X2 + β3X3 =2+ X + XZ + Z + ε, ε~Normal(0, 1), Z /C24 Normal 4 ; 2ðÞ (4) a logistic regression with interaction term: Y /C24 Binomial 1 ; πðÞ logit πðÞ ¼ β0 þ β1X1 þ β2X2 þ β3X3 ¼ − 2 þ 0:5X − 0:0625XZ þ 0:25Z Z /C24 Normal 4 ; 2ðÞ In scenario 2, we set β0 = "
8381,unknown,"of Y =1 i s π = 0.5 when X = 4, and β1 =2 β2 = 0.1, corre- sponding to a log odds ratio of 1 for X = 5 versus X =3 (plus or minus one standard deviation (SD) when X~Normal(4, 1)). The observations with Y = 1 can vary in proportions across the different distributions for X.I n scenario 4, with β0 = − 2, the probability of Y =1 i s π = 0.5 when X =4 ; β1 = 0.5, β2 = − 0.625 and β3 = 0.25, cor- respo"
8382,unknown,"responding to a log odds ratio of 0.5 for X = 5 versus X = 3 with Z = 4, and a log odds ratio of 0.5 for X =5 , Z = 6 versus X =3 , Z =2 . Amputation Generally, missing data problems can be classified into three categories [ 13]. When data are MCAR, the prob- ability of being missing is the same for all cases. When data are MAR, the probability of being missing is only related to the observed data"
8383,unknown,"holds, then data are missing not at random (MNAR). While MCAR is simple to consider, most of the missing data methods are designed to address the MAR assump- tion. In this study, we primarily report situations when data are MAR and refer the reader to supplemental re- sults when data are MCAR. During amputation missing values were introduced into the simulated complete datasets using the amputatio"
8384,unknown,"R by Schouten et al. [ 14] Specifically, MCAR was intro- duced by setting the probability of X being missing to Hong and Lynn BMC Medical Research Methodology (2020) 20:199 Page 3 of 12 25% across observations. MAR was introduced by setting the probability of X being missing to 25% according to a standard right-tailed logistic function in Y, thus the probability of X being missing is higher for ob"
8385,unknown,"with higher values of Y. Missing values were only gener- ated for X (i.e., Y and Z were kept intact), but its corre- sponding quadratic or interaction term would also be missing whenever X was missing. Imputation For each amputated dataset, the missing values were im- puted by three different imputation methods: PMM, and two RF-based imputation methods, missForest and CALIBERrfimpute as described "
8386,unknown,"CALIBERrfimpute as described above. Analysis Linear or logistic regression was performed on each imputed dataset by regressing Y on the other variables in the dataset without knowledge of the presence of quadratic or interaction terms. The three imputation methods were first compared in terms of their accuracy of the imputed values using (i) the normalized root mean squared error (NRMSE) [15], ﬃﬃﬃ"
8387,unknown,"mean Xtrue − Ximp /C0/C1 2/C16/C17 var XtrueðÞ vuut ; where Xtrue and Ximp are the true and imputed data matrix, respectively, and the mean and variance are computed only over the missing values; and (ii) relative bias for the mean of the imputed variable: mean Vimp /C0/C1 mean VtrueðÞ − 1; where V is either one of the imputed variables ( X, X2,o r Fig. 1 Distributions used for covariate X.( a) sy"
8388,unknown,"(d) bimodal distributions (mixture of two normal distributions). The panels display the kernel densities based on 1 million observations randomly sampled from each distribution. ½Nðμ1; σ2 1Þ; Nðμ1; σ2 1Þ/C138 represents a homogeneous mixture of 50% Normal ðμ1; σ2 1Þ and 50% Normal ðμ1; σ2 1Þ. For figures with boxplots, the top and bottom 0.025 percentiles were truncated to avoid extreme values in "
8389,unknown,"the boxplots Hong and Lynn BMC Medical Research Methodology (2020) 20:199 Page 4 of 12 XZ) Vtrue is the original vector of true values, Vimp is the data vector after imputation, and the mean was computed over all the data values. Second, the three imputation methods were compared in terms of their accuracy in estimating the coefficients in the linear and logistic regression models using (i) the re"
8390,unknown,"(i) the relative bias of the coefficient estimate, ð^βp − βpÞ=βp, p= 1 or 2 for imputed variables; and (ii) the coverage of their 95% confidence intervals. Third, the three imputation methods were compared in terms of their accuracy in prediction using Lin ’s con- cordance correlation coefficient (CCC) [ 16] for agree- ment with the prediction using the true model. The predictions were made from r"
8391,unknown,mated from imputed datasets and the corresponding true model using 1000 new datasets of 500 observations (for continuous outcome) or 1000 observations (for bin- ary outcome) generated as described in the Data Gener- ation section. Lin ’s coefficient increases towards one as the predicted values from models estimated using im- puted data have better agreement with the predictions from the true mode
8392,unknown,"from the true model. An imputation method with superior performance can be generally characterized by smaller NRMSE, relative bias of imputation mean closer to zero, relative bias of regression coefficient estimate closer to zero, coverage closer to nominal coverage probability, and a CCC nearer towards one. Results Accuracy of imputed variables NRMSE value Overall, missForest had the smallest NRM"
8393,unknown,"0.39, 0.88, 0.26, 0.79, for scenarios 1 to 4) compared to CALIBERrfimpute (mean = 0.35, 0.98, 0.34, 0.92) and PMM (mean = 0.64, 1.06, 0.48, 1.06) uniformly across all eight distributions except for scenario 1 where CALI- BERrfimpute outperformed missForest (Fig. S 1). Bias of variable estimates When estimating the mean of X across the eight distri- butions (Fig. 2), missForest on average gave rela"
8394,unknown,"biases of 2.0, 1.3, 1.7, 1.4%, compared to 1.4, 2.5, 2.3, 1.7% in CALIBERrfimpute, 3.2, 1.4, 2.7, 5.3% in PMM for scenarios 1 through 4, respectively. (To be concise, we report in the text the mean of the absolute values of the mean relative bias for each distribution when sum- marizing the relative bias across the eight distributions.) MissForest had the smallest bias except for scenario 1. When "
8395,unknown,"When estimating the mean of X2 or XZ across the eight distributions, missForest gave relative biases of 8.3, 8.9, 2.1, 3.4% compared to 6.3, 2.3, 3.4, 1.4% in CALIBERr- fimpute, 11.6, 4.8, 2.6, 5.9% in PMM for scenarios 1 through 4, respectively. CALIBERrfimpute outperformed missForest except for scenario 3, where missForest had smaller bias. MissForest can produce more biased results for non-norm"
8396,unknown,"for non-normal data. Accuracy of regression estimates Bias of regression coefficient estimates In contrast to estimating the characteristics of the im- puted variables, the ability of the imputation methods to estimate regression coefficients was much poorer (Fig. 3), especially for logistic regressions. When estimating the regression coefficient of X across the eight distributions, missForest on "
8397,unknown,"653.3, 58.6, 70.4%, compared to 104.3, 335.8, 32.9, 31.3% in CALIBERrfimpute, 64.2, 371.0, 80.4, 7.7% in PMM for scenarios 1 through 4, respectively. PMM out-performed RF-based methods in both scenarios 1 and 4. When esti- mating the mean of X2 or XZ, missForest gave relative biases of 78.7, 252.7, 23.9, 138.1%, compared to 59.9, 94.6, 18.8, 82.3% in CALIBERrfimpute, 61.1, 101.0, 30.0, 47.0% in PM"
8398,unknown,"PMM outperformed RF-based methods in scenario 4. For RF-based imputation, the bias of the estimated means and the bias of their corresponding estimated regression coefficients can be strongly correlated. Like- wise, as in the case for estimating means, missForest can produce more biased results for non-normal data. Coverage of 95% confidence intervals The coverages of the confidence intervals were"
8399,unknown,"formly poor for all three imputation methods. When es- timating the regression coefficient of X across the eight distributions (Fig. 4), missForest gave coverages of 17.0, 30.7, 32.7, 51.9%, compared to 18.3, 35.2, 51.5, 57.3% in CALIBERrfimpute, 47.8, 28.5, 27.0, 79.0% in PMM, and 95.1, 95.1, 95.3, 94.7% in the original non-missing data, for scenarios 1 through 4, respectively. When estimating th"
8400,unknown,"the regression coefficient of X2 or XZ, missForest gave coverages of 12.9, 30.1, 13.2, 49.5%, compared to 14.2, 61.4, 27.4, 40.5% in CALIBERrfimpute, 12.8, 33.9, 18.5, 71.9% in PMM, and 95.4, 95.1, 95.1, 94.8% in the original non-missing data, for scenarios 1 through 4, respectively. Accuracy of regression model predictions When predicting new data (Fig. 5), missForest on aver- age gave CCCs of 0."
8401,unknown,"0.95, 0.88, 0.99, 0.91 in CALIBERrfimpute, and 0.94, 0.81, 0.98, 0.84 in PMM, for scenarios 1 through 4, re- spectively. CALIBERrfimpute imputation had the highest prediction accuracy, but for logistic regression the agree- ment is poorer due to biased coefficient estimates. Hong and Lynn BMC Medical Research Methodology (2020) 20:199 Page 5 of 12 Case study To explore the imputation performance o"
8402,unknown,"mentioned methods in real datasets, a longitudinal study on patients with HCC was examined [ 17]. It should be noted that the analysis presented here was to investigate the imputation methods, and is not intended as a defini- tive analysis of the data. Two baseline covariates, serum alpha-fetoprotein (AFP) level (IU/ml) and diameter of main tumor (mm), were considered important prognos- tic factor"
8403,unknown,"tic factors for HCC [ 18, 19], and thus were standardized and used to predict survival at 12 months after baseline using logistic regression. The complete dataset consist- ing of 1897 patients were amputed like what was done in the Amputation section. except here both covariates Fig. 2 Relative bias of the estimated mean of imputed variables for MAR data Hong and Lynn BMC Medical Research Methodol"
8404,unknown,"were MCAR, or MAR according to the outcome. About 25% of the observations were missing, and at most one covariate value was missing for a single observation. Table 1 shows that more biased estimates of regression coefficients can be observed in missForest for MAR data and even for MCAR data. These results also confirm the previous simulation results that RF-based imputation does not outperform PMM"
8405,unknown,"does not outperform PMM in the presence of a logistic regression relationship. Discussion MissForest has been reported to “successfully handle missing values, particularly in datasets including differ- ent types of variables ” [2] (p.112). Waljee et al. further Fig. 3 Relative bias of the estimated regression coefficient of imputed variables for MAR data Hong and Lynn BMC Medical Research Methodol"
8406,unknown,"concluded that it “is a highly accurate method of imput- ation for missing laboratory data and outperforms other common imputation techniques ” [6] (p.1). These two studies focused on MCAR data, and while other re- searchers have examined missForest in the presence of MAR [ 4] and even MNAR data [ 5], all of these research judged missForest in terms of its predictive accuracy. However, an imputati"
8407,unknown,"missing values by minimizing prediction error can be problematic since it does not try to recover the joint dis- tribution of the data and thus can result in biased parameter estimates. Therefore, other measures of Fig. 4 Coverage of 95% confidence intervals (with binomial proportion confidence intervals) of the estimated regression coefficients of imputed variables for MAR data Hong and Lynn BMC "
8408,unknown,"Hong and Lynn BMC Medical Research Methodology (2020) 20:199 Page 8 of 12 imputation accuracy; i.e., bias of imputation mean and regression coefficient estimate, and coverage would be more important and relevant when considering all the variables together. The simulations showed that missForest typically had the lowest NRMSE and smaller bias when estimating the mean of the imputed variables compar"
8409,unknown,"ever, missForest often under-estimated the standard de- viation of the imputed variables, suggesting that subsequent significance testing would result in elevated false positive error rates, and this is because missForest simply used the conditional mean for imputation. Yet, Fig. 5 Lin’s concordance correlation coefficient (CCC) from predictions using models estimated from imputed MAR data Table 1"
8410,unknown,"Table 1 Parameter estimates of the regression from the case study Method Intercept Regression coefficient for AFP Regression coefficient for tumor diameter Estimate (95% CI) p-value Estimate (95% CI) p-value Estimate (95% CI) p-value Original data 0.54 (0.43, 0.64) < 0.001 −0.20 ( − 0.31, − 0.10) < 0.001 − 0.65 ( − 0.76,-0.54) < 0.001 MAR data PMM 0.55 (0.45, 0.65) < 0.001 − 0.19 ( − 0.29, − 0.09)"
8411,unknown,"PMM 0.55 (0.45, 0.65) < 0.001 − 0.19 ( − 0.29, − 0.09) < 0.001 −0.66 ( − 0.77, − 0.56) < 0.001 missForest 0.54 (0.44, 0.64) < 0.001 −0.17 ( − 0.28, − 0.07) 0.002 −0.74 ( − 0.85, − 0.62) < 0.001 CALIBERrfimpute 0.54 (0.44, 0.64) < 0.001 −0.19 ( − 0.29, − 0.09) < 0.001 −0.66 ( − 0.77, − 0.56) < 0.001 MCAR data PMM 0.54 (0.44, 0.64) < 0.001 −0.20 ( − 0.31, − 0.10) < 0.001 −0.66 ( − 0.77, − 0.56) < 0."
8412,unknown,"PMM 0.54 (0.44, 0.64) < 0.001 −0.20 ( − 0.31, − 0.10) < 0.001 −0.66 ( − 0.77, − 0.56) < 0.001 missForest 0.54 (0.44, 0.64) < 0.001 −0.21 ( − 0.32, − 0.10) < 0.001 −0.72 ( − 0.84, − 0.61) < 0.001 CALIBERrfimpute 0.55 (0.45, 0.65) < 0.001 −0.22 ( − 0.33, − 0.12) < 0.001 −0.68 ( − 0.79, − 0.58) < 0.001 *Results were obtained from logistic regression CI confidence interval, MCAR missing completely at "
8413,unknown,"CI confidence interval, MCAR missing completely at random, MAR missing at random Hong and Lynn BMC Medical Research Methodology (2020) 20:199 Page 9 of 12 the more critical issue is that good predictive accuracy of imputed values does not necessarily imply better ac- curacy in estimating the relationships involving the im- puted variables. For MAR data, all the imputation approaches were considera"
8414,unknown,"regression coefficients. MissForest was shown to pro- duce many-fold more biased regression coefficient esti- mates than PMM, and its inferior performance often appeared when the imputed variable was highly skewed in nonlinear models or when there was an interaction term. (This is consistent with the biased Cox regression coefficient estimates in missForest reported by Shah et al. [ 3].) Under suc"
8415,unknown,"can often outperform missForest, but its bias was still substantial. Due to single imputation and biased regres- sion coefficient estimates, coverages of confidence inter- vals were far from nominal coverage. This was also the case even for MCAR data (refer to Supplementary Infor- mation for results). For MCAR data, RF-based imput- ation showed higher accuracy than PMM for mean and regression coef"
8416,unknown,"regression coefficient estimates except for estimating regression coefficients in scenario 4. The conditional mean prediction from missForest is a weighted average of the imputed variable ’s observed values, and when data are outcome-dependent MAR this can result in information loss near both ends of the vari- able range and severely biased regression estimates, especially when variables are highl"
8417,unknown,"linearity is present (Fig. 6). While CALIBERrfimpute can overcome this issue to a certain extent by taking the pre- diction errors of RF into consideration and drawing im- putations from the conditional distributions, it cannot fundamentally recover the information loss due to the sparseness and high leverage data points. Such trunca- tion in the variable range is also apparent for PMM, although t"
8418,unknown,"although to a lesser degree. The simulations only considered (generalized) linear models so favorable imputation results can be achieved by PMM in certain scenarios as PMM is essentially linear-model-based, but for data with complex Fig. 6 Scatter plot of Y versus X from imputation results of a randomly selected dataset when X~Gamma(1, 1) in scenario 1 for MAR data. Dashed"
8419,unknown,"line is for the true model, solid line is for the estimated model using imputed data. Only imputed observations were shown for direct comparison between imputation methods Hong and Lynn BMC Medical Research Methodology (2020) 20:199 Page 10 of 12 relationships PMM may not have such advantages over RF-based methods. Since missForest was originally de- signed to be a single imputation method, this s"
8420,unknown,"single imputation for CALIBERrfimpute and PMM to fa- cilitate comparison. Therefore, all three imputation methods did not account for the uncertainty from missing data in the analysis. This can be problematic especially when the proportion of missingness is high and may lead to inflated false positive error rates. In practice, multiple imputation should be recommended for considering the uncertain"
8421,unknown,"uncertainty induced by missing data. The results showed that CALIBERrfimpute can often outperform missForest, indicating that sampling from a conditional distribution can help in the estimation of regression coefficients and variable means even with only single imputation. How- ever, it should be noted that its out-of-bag mean squared error can be influenced by extreme values in the variable under"
8422,unknown,"under imputation, leading to biased results even when data are MCAR. Therefore, the detection and handling of extreme values when using CALIBERrfimpute should be considered for less biased results. Eight common distributions and four simple regression models were used to generate 32 simulation settings for examining the accuracy of the imputation methods. The chosen distributions provided a variet"
8423,unknown,"symmetric, heavy tailed, and skewed distributions, and the regression models considered both non-linearity and interaction coupled with either a continuous or binary outcome. These settings are intended to depict the mixed data types that missForest was believed to handle well. The performance of the imputation methods was weaker for logistic regression models because the binary out- comes provide"
8424,unknown,"parameters than the continuous outcomes, and the estimation of the log odds ratios was also more sensitive to inaccuracies in the imputed variables. The simulations generated a realistic proportion of miss- ing data, and increasing the proportion of missing data to 50% did not materially alter the main findings. A straight- forward MAR data pattern was introduced where the prob- ability of X being"
8425,unknown,"observed outcome values. Although MNAR data cannot be ruled out in practice, we focused on the MAR mechanism to highlight how RF-based imputation algorithms can be problematic even though multiple imputation was designed to address MAR data. Another limitation is that Y was as- s u m e dt ob ef r e eo fm i s s i n gv a l u e ss i n c ew ew a n t e dt ou s e a relatively simple MAR mechanism to exa"
8426,unknown,"ent distributions of X can impact the imputation of its missing values. MissForest also performed poorly when estimating regression relationships where the MAR mech- anism was additionally dependent on other covariates [ 3], suggesting that outcome-de pendent MAR is a sufficient reason for its deficiency. The number of trees built for RF models was set to 10 to limit bias and compared to PMM opera"
8427,unknown,"their default parameters. The iteration number for MICE-based imputation was set to five as recommended and for a faster computational speed. But in practice the influence of tuning parameters on imputation accuracy and computational burden still requires additional study, and parameter searching may be necessary for complex datasets to attain better imputation accuracy. For ex- ample, it is uncle"
8428,unknown,"missingness may impact missForest ’s stopping criterion, and users should monitor whether missForest stopped because it reached the maximum number of iterations as this may indicate a slow or problematic convergence. For all three imputation methods, an inclusive imput- ation model that included the outcome was used. In sit- uations where the imputation model fails to account for variables that im"
8429,unknown,"does not accurately capture the relationship of the vari- ables, non-parametric imputation methods may fare better than parametric imputation methods and further study is warranted. Conclusions The results of the simulation experiments and case study present evidence that although RF-based imputation can have good predictive accuracy, it may also lead to se- verely biased inference when the impute"
8430,unknown,used in subsequent regression analyses. Missforest has the characteristic that its predicted value does not ex- tend beyond the range of the imputed variable ’s ob- served values but this can lead to underperformance with outcome-dependent MAR and highly skewed data. CALIBERrfimpute can slightly alleviate this problem by sampling from the RF predicted value ’s conditional dis- tribution but regres
8431,unknown,"bias and poor confidence interval coverage. Therefore, RF-based imputation and especially missForest should not be indiscriminately used as a panacea for imputing missing data. Once again it proves that a correct analysis requires a careful critique of the missing data mechan- ism and the inter-relationships between the variables in the data. Supplementary information Supplementary information acc"
8432,unknown,Supplementary information accompanies this paper at https://doi.org/10. 1186/s12874-020-01080-1. Additional file 1: Appendix S1. Distributions and functions used for simulation studies. Appendix S2. Additional results for MAR data. Appendix S3. Results for MCAR data. Additional file 2: Figure S1. NRMSE value for MAR data. Additional file 3: Figure S2. Standard deviation of imputed variables for MA
8433,unknown,for MAR data. Hong and Lynn BMC Medical Research Methodology (2020) 20:199 Page 11 of 12 Additional file 4: Figure S3. Width of 95% confidence intervals of the estimated regression coefficients of imputed variables for MAR data. Additional file 5: Figure S4. NRMSE value for MCAR data. Additional file 6: Figure S5. Relative bias of the estimated mean of imputed variables for MCAR data. Additional f
8434,unknown,Additional file 7: Figure S6. Standard deviation of imputed variables for MCAR data. Additional file 8: Figure S7. Relative bias of the estimated regression coefficient of imputed variables for MCAR data. Additional file 9: Figure S8. Width of 95% confidence intervals of the estimated regression coefficients of imputed variables for MCAR data. Additional file 10: Figure S9. Coverage of 95% confide
8435,unknown,(with binomial proportion confidence intervals) of the estimated regression coefficients of imputed variables for MCAR data. Additional file 11: Figure S10. Lin’s concordance correlation coefficient (CCC) from predictions using models estimated from imputed MCAR data. Abbreviations CCC: Concordance correlation coefficient; HCC: Hepatocellular carcinoma; kNN: K-nearest neighbors; MAR: Missing at ra
8436,unknown,completely at random; MICE: Multivariate imputation using chained equations; MNAR: Missing not at random; NRMSE: Normalized root mean squared error; PMM: Predictive mean matching; RF: Random forests; SD: Standard deviation Acknowledgments We would like to thank Yuqi Sun and Hanying Li for kindly providing suggestions about refining simulation studies in this research. Authors’ contributions All au
8437,unknown,"All authors made substantial contributions to the direction of the study, the design of the simulation studies, the interpretation of the results, and the writing of the manuscript. The first author (SH) carried out the simulations and data analysis. Drafts of the manuscript were provided by the first author (SH) and were co-developed by the other author (HL). All authors have read and approved th"
8438,unknown,"Funding No funding was received for the conduct of this research. Availability of data and materials Further details of the simulation studies are provided in the supplementary information, and software packages used in this study are publicly available on the Comprehensive R Archive Network (CRAN), https://cran.r-project.org. The dataset supporting the conclusions of this article is available in "
8439,unknown,"Dryad Digital Repository, https://datadryad.org (doi:https://doi.org/10.5061/ dryad.pd44k8r). Ethics approval and consent to participate Ethics approval was not required for the conduct of this research. Consent for publication Not applicable. Competing interests Authors declare that they have no competing interests. Received: 23 April 2020 Accepted: 15 July 2020 References 1. Van Buuren S. Flexib"
8440,unknown,"References 1. Van Buuren S. Flexible imputation of missing data: chapman and hall/CRC; 2018. 2. Stekhoven DJ, Buhlmann P. MissForest--non-parametric missing value imputation for mixed-type data. Bioinformatics. 2012;28(1):112 – 8. 3. Shah AD, Bartlett JW, Carpenter J, Nicholas O, Hemingway H. Comparison of random forest and parametric imputation models for imputing missing data using MICE: a CALIB"
8441,unknown,"data using MICE: a CALIBER study. Am J Epidemiol. 2014;179(6):764 – 74. 4. Ramosaj B, Pauly M. Predicting missing values: A comparative study on non- parametric approaches for imputation. Comput Stat. 2019;34(4):1741 – 1764. 5. Tang F, Ishwaran H. Random Forest missing data algorithms. Stat Analysis Data Mining. 2017;10(6):363 – 77. 6. Waljee AK, Mukherjee A, Singal AG, Zhang Y, Warren J, Balis U,"
8442,unknown,"Zhu J, Higgins PD. Comparison of imputation methods for missing laboratory data in medicine. BMJ Open. 2013;3(8):e002847. 7. Van Buuren S, Groothuis-Oudshoorn K. Mice: Multivariate Imputation by Chained Equations in R. J Stat Softw. 2011;45(3):1 – 67. 8. Solaro N, Barbiero A, Manzi G, Ferrari PA. A simulation comparison of imputation methods for quantitative data in the presence of multiple data p"
8443,unknown,"patterns. J Stat Comput Simul. 2018;88(18):3588 – 619. 9. Bhaskaran K, Smeeth L. What is the difference between missing completely at random and missing at random? Int J Epidemiol. 2014;43(4):1336 – 9. 10. Liaw A, Wiener M. Classification and regression by randomForest. R news. 2002;2(3):18– 22. 11. Breiman L. Random forests. Mach Learn. 2001;45(1):5 – 32. 12. R Core Team. R: A language and enviro"
8444,unknown,"12. R Core Team. R: A language and environment for statistical computing. 2020. 13. Rubin DB. Inference and missing data. Biometrika. 1976;63(3):581 – 92. 14. Schouten RM, Lugtig P, Vink G. Generating missing values for simulation purposes: a multivariate amputation procedure. J Stat Comput Simul. 2018; 88(15):2909– 30. 15. Oba S, Sato MA, Takemasa I, Monden M, Matsubara K, Ishii S. A Bayesian mis"
8445,unknown,"missing value estimation method for gene expression profile data. Bioinformatics. 2003;19(16):2088 – 96. 16. Lin LI-K. A Concordance Correlation Coefficient to Evaluate Reproducibility. Biometrics. 1989;45(1):255 – 268. 17. Shen L, Zeng Q, Guo P, Huang J, Li C, Pan T, Chang B, Wu N, Yang L, Chen Q, et al. Dynamically prognosticating patients with hepatocellular carcinoma through survival paths map"
8446,unknown,"through survival paths mapping based on time-series data. Nat Commun. 2018;9(1):2230. 18. Yamamoto K, Imamura H, Matsuyama Y, Kume Y, Ikeda H, Norman GL, Shums Z, Aoki T, Hasegawa K, Beck Y, et al. AFP, AFP-L3, DCP, and GP73 as markers for monitoring treatment response and recurrence and as surrogate markers of clinicopathological variables of HCC. J Gastroenterol. 2010;45(12):1272– 82. 19. Cerny "
8447,unknown,"19. Cerny M, Bergeron C, Billiard JS, Murphy-Lavallee J, Olivie D, Berube J, Fan B, Castel H, Turcotte S, Perreault P, et al. LI-RADS for MR imaging diagnosis of hepatocellular carcinoma: performance of major and ancillary features. Radiology. 2018;288(1):118 – 28. Publisher’sN o t e Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliatio"
8448,unknown,"Hong and Lynn BMC Medical Research Methodology (2020) 20:199 Page 12 of 12 Department of Mathematical Sciences Project IV - (MATH4072) Equity Fund Profitability and Sustainability Modelling Using Multiple-Response Regression Author: Raul Unnithan Supervisor: Ric Crossman May 2, 2025 Acknowledgement I would like to sincerely thank Ric for his support and guidance, both as my dissertation supervisor"
8449,unknown,and throughout my time at Durham as my academic advisor. His advice and encouragement have been instrumental in guiding this project and in completing my academics at university. Declaration This piece of work is a result of my own work and I have complied with the Department’s guidance on multiple submission and on the use of AI tools. Material from the work of others not involved in the project 
8450,unknown,tools have been declared. Contents 1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.1.1 Multiple-Response Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.1.2 Equity F
8451,unknown,1.1.2 Equity Funds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Equity Fund Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2.2 ROE and Sustainability Score . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2.3 Motivation 
8452,unknown,1.2.3 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Small-Scale Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.4 Report Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.5 Report Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
8453,unknown,2 Exploratory Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.1 Initial Data Collection and Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2 Variable Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3 Exploratory Data Analysis . . . . . . . . . . . . . . .
8454,unknown,2.3 Exploratory Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3.1 Underlying Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3.2 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3.3 Imputing Missing Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3.4 Multicollin
8455,unknown,2.3.4 Multicollinearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3.5 Multivariate Normality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.4 Response Intercorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8456,unknown,3.1 Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.1.1 Single-Response Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.1.2 Multiple-Response Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . 11 3.1.3 Stepwise Selection Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.1.4 Analysis o
8457,unknown,3.1.4 Analysis of Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.1.5 Multiple Analysis of Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.2 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.2.1 Model Evaluation and Performance . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.2.2 Standard
8458,unknown,3.2.2 Standard Multiple Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . 17 3.2.3 Code Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.2.4 Small-Scale Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.2.5 Applying Stepwise Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.2.6 Extending and Eva
8459,unknown,3.2.6 Extending and Evaluating Stepwise Selection . . . . . . . . . . . . . . . . . . . 20 4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.1 Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 1 CONTENTS 4.1.1 Ridge Regression . . . . . . . . . . . 
8460,unknown,4.1.1 Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.1.2 Lasso Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.1.3 Shrinkage Methods for MRR . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.1.4 Reduced Rank Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.1.5 Multivariate Regress
8461,unknown,4.1.5 Multivariate Regression with Covariance Estimation . . . . . . . . . . . . . . . 26 4.2 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 4.2.1 Small-Scale Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 4.2.2 Code Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.2.3 Results . . 
8462,unknown,4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5 Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 5.1 Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 5.1.1 Classification and Regression Trees . . 
8463,unknown,5.1.1 Classification and Regression Trees . . . . . . . . . . . . . . . . . . . . . . . . . 35 5.1.2 Bagging for Classification and Regression Trees . . . . . . . . . . . . . . . . . . 36 5.1.3 Random Forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.1.4 Multivariate Regression Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.1.5 Multivariate Rand
8464,unknown,5.1.5 Multivariate Random Forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 5.1.6 Covariance Regression with Random Forests . . . . . . . . . . . . . . . . . . . 39 5.2 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 5.2.1 Small-Scale Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 5.2.2 Code Explanati
8465,unknown,5.2.2 Code Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 6 XG Boost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 6.1 Theory . . . . . . . . . . . . . . . . . . . 
8466,unknown,6.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 6.1.2 Iterative Steps of XGBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 6.1.3 Cholesky Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 6.2 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 6.2.1 Small-S
8467,unknown,6.2.1 Small-Scale Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 6.2.2 Code Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 6.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8468,unknown,7.1 Summary of Findings and Model Comparisons . . . . . . . . . . . . . . . . . . . . . . 54 7.1.1 Summary of Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 7.1.2 Model Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 7.2 Challenges and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 7.3 Report Overview a
8469,unknown,7.3 Report Overview and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 A Appendices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 A.1 Additional Data Visualisations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 A.2 Extra Chapter Insights . . . . . . . . . 
8470,unknown,A.2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 A.2.2 Exploratory Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 A.2.3 Multiple-Response Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . 63 A.2.4 Lasso and Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 A.2.5 Reduced Rank Ridge
8471,unknown,A.2.5 Reduced Rank Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . 65 A.2.6 Multivariate Regression with Covariance Estimation . . . . . . . . . . . . . . . 67 A.2.7 Cholesky-Gaussian XGBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 2 Chapter 1 Introduction 1.1 Background 1.1.1 Multiple-Response Regression Single-response regression models one response agains
8472,unknown,"be extended to multiple predictors, but what if more than one response needs to be modelled? One approach is to run several independent single-response models. However, this approach can lead to suboptimal predictions as it ignores that when responses are correlated, the off-diagonal elements of the response-covariance matrix are non-zero. Multiple-response regression (MRR) addresses this issue, t"
8473,unknown,"thereby improving predictions. However, selecting the most relevant predictors remains a challenge, and each MRR model does this differently. This report has examined and applied different MRR models to an equity fund dataset to determine which of these fits it the best. 1.1.2 Equity Funds An equity fund is a pooled investment that puts money predominantly in stocks listed on major exchanges.1 The"
8474,unknown,"returns, but they also come with risks associated with stock market volatility and losses. 1 Equity funds can be categorised according to factors such as their investment style and portfolio focus, where a portfolio is the collection of stocks held by the fund, which is how they are represented in this data.1 1.2 Equity Fund Dataset 1.2.1 Overview The equity fund dataset analysed in this report co"
8475,unknown,"with varying equity fund characteristics, including, for example, the type of equity. Modelling equity fund data is important as it can indicate which funds are worth investing in and which are not. Traditional financial models prioritise profitability above all else. 2 However, this report extends this by modelling two responses, with moderate correlation. These responses are return on equity (RO"
8476,unknown,"as otherwise it would be more appropriate to model each response separately. 1.2.2 ROE and Sustainability Score ROE is a measure of an equity fund’s profitability and how efficiently it generates those profits. 3 It is calculated by dividing net income by shareholders’ equity.3 ROE is typically expressed as a percentage, so this value is multiplied by a hundred. In the context of an equity fund, n"
8477,unknown,"from sales minus the cost of goods sold as it would be for a company. 4 Instead, an equity fund’s net income comes primarily from earnings such as interest, after subtracting operating expenses like 3 CHAPTER 1. INTRODUCTION distribution costs.4 The higher the ROE, the more efficiently an equity fund is generating income from its stocks it has invested in. 5 The sustainability score measures an eq"
8478,unknown,"The sustainability score measures an equity fund’s sustainability performance and how effectively it mitigates environmental, social, and governance (ESG) risks across its portfolio. Each equity fund’s sustainability score is its Morningstar Sustainability Rating, which directly measures these ESG fac- tors. The Morningstar Sustainability Rating is calculated through a multi-step process. First, a"
8479,unknown,"only qualifies for a rating if at least 67% of its assets have an assigned ESG score. Each company within the fund’s portfolio is then scored, typically on a scale from 1 to 100. The fund’s portfolio score is adjusted by subtracting points for any ESG-related negative events, such as environmental incidents. Finally, the equity fund’s overall sustainability score is then compiled as a weighted ave"
8480,unknown,"over the past 12 months, placing more emphasis on recent performance. 6 Lower sustainability scores indicate better ESG performance. 1.2.3 Motivation Modelling specifically ROE and sustainability scores in particular together is important as it gives a more long-term view of each equity fund’s risk versus return potential. Sustainability has become more relevant as investors move to prioritise sus"
8481,unknown,"growing concern over climate change and global warming, which have significant long-term social and economic implications. In a 2021 letter to other CEOs, Larry Fink, the CEO of BlackRock, wrote that 81% of a globally representative selection of BlackRock’s sustainable indexes outperformed their parent benchmarks. 7 This finding emphasises the importance of considering sustainability metrics along"
8482,unknown,"alongside profitability measures when evaluating financial assets, including equity funds. 1.3 Small-Scale Dataset Every MRR model in this report generated predictions for the two responses. These predictions were evaluated using the average normalised root mean square error, which is the root mean square error divided by the standard deviation averaged across each response (see Subsection 3.2.1 f"
8483,unknown,"Each MRR model was also applied to a small-scale dataset with Exam Scores to demonstrate explicitly how these models make predictions: X1: Hours Studied X2: Time Spent on Papers Y1: Math Scores Y2: Science Scores 5 2 78 80 7 3 85 79 8 4 88 88 3 1 65 70 10 5 92 74 Table 1.1: Study Time vs. Exam Scores The calculations involving this dataset used precise values throughout, but, where written, the in"
8484,unknown,"The calculations involving this dataset used precise values throughout, but, where written, the in- termediary and final results were rounded to 2 decimal places for brevity - unless explicitly stated otherwise. The Exam Scores dataset was used for this purpose because its values are straightforward, and the responses have a moderate correlation of 0.526 (3dp), indicating that it requires MRR (see"
8485,unknown,"Subsection A.2.1 for the full calculation). 4 CHAPTER 1. INTRODUCTION 1.4 Report Terminology In this report, certain common terminology was used. First, the identity matrix was denoted through- out as I with dimensions subscripted where necessary. Second, there was the correlation and covari- ance between the responses, which, as stated, MRR considers unlike single-response regression. This consid"
8486,unknown,"consideration is essential for every MRR model as it is what qualifies them as multiple-response. The correlation between the responses, referred to as the response intercorrelation, was defined as a matrix, R, using the formula: R = D−1ΣYD−1, where Cov(Y) = ΣY denotes the covariance matrix of the response variables, or the response covariance matrix, and D is a diagonal matrix of the standard dev"
8487,unknown,"Rij = (ΣY)ij σiσj , where ( ΣY)ij is the covariance between response variables i and j, and σi, σj are their respective standard deviations.8 This relation ultimately means that if an MRR model considers the off-diagonal elements in the response covariance matrix, it considers the response intercorrelation and vice versa. 1.5 Report Outline The rest of this report is broken down into chapters cont"
8488,unknown,"The rest of this report is broken down into chapters containing the exploratory data analysis, the MRR models and a conclusion comparing every model’s performance. Chapter 2 covers an Exploratory Data Analysis of the equity fund dataset. The master dataset includes equity and bond funds with a lot of missing data. This chapter focuses on cleaning and refining the dataset, defining the predictor va"
8489,unknown,and understanding the relationship between the two responses: ROE and sustainability score. Chapter 3 introduces Multiple-Response Linear Regression. It delves into its underlying theory and explains how it can be combined with multiple analysis of variance and stepwise selection to select appropriate predictors. These stepwise selection methods are evaluated on the equity fund dataset. Chapter 4 
8490,unknown,"Chapter 4 progresses to shrinkage-based regression methods. It looks at more advanced methods appropriate for MRR, including Multivariate Regression with Covariance Estimation and Reduced Rank Ridge Regression. These methods are again evaluated, and their performance is assessed to determine whether they offer improvements over the MRLR models. Chapter 5 introduces non-linear modelling through Ran"
8491,unknown,"Chapter 5 introduces non-linear modelling through Random Forests. It extends them to MRR using Covariance Regression with Random Forests, and then applies the resulting model to the dataset. Chapter 6 continues the non-linear modelling approach using XGBoost. It also outlines Cholesky Decomposition, which enables XGBoost to be used in MRR. The resulting Cholesky Decomposition XGBoost model is then"
8492,unknown,"XGBoost model is then evaluated on the equity fund dataset. Finally, Chapter 7 gives an overall comparison of the performance of each modelling approach applied. It identifies the most effective method for predicting ROE and sustainability scores based on the findings, discusses challenges and limitations throughout the report, and looks into potential directions for future work. 5 Chapter 2 Explo"
8493,unknown,"directions for future work. 5 Chapter 2 Exploratory Data Analysis This chapter prepares the equity fund dataset for modelling. It begins by describing the master dataset and the initial preprocessing steps taken to remove irrelevant variables. The key predictor variables in the partially cleaned dataset are then explained. Exploratory data analysis follows, in- cluding an assessment of the distrib"
8494,unknown,"cluding an assessment of the distributions of the responses and the correlation structure among the predictors. Imputation is subsequently applied to complete the dataset, facilitating multicollinearity testing. Finally, this chapter examines the intercorrelation between the responses, tests the assumption of multivariate normality, and explores the relationship between the responses in greater de"
8495,unknown,"2.1 Initial Data Collection and Preprocessing The dataset was obtained from Kaggle and simplified for this analysis. This report focuses on analysing equity funds, but the master dataset contains a mixture of equity and bond funds. The first step was to filter out the bond funds and keep the equities. This filtering was done using Excel’s filter features. A text filter was used to select the relev"
8496,unknown,"Next, there was an issue with the columns because there were 130 dependent variables in this intermediary dataset. However, because many variables were for bonds only, these were removed. Some variables, such as a fund’s isin number, were irrelevant for analysis, further reducing the number of dependent variables to 29. Because it was critical to be able to look into how chosen predictors would af"
8497,unknown,"Because it was critical to be able to look into how chosen predictors would affect the responses, the aim was to have 10-20 predictors in the model as this is a manageable amount. Also, a lot of the remaining variables served the same purpose, which would lead to multicollinearity. For example, management fees was removed because it is a significant component of ongoing cost. 2.2 Variable Explanat"
8498,unknown,"2.2 Variable Explanation category gives the type of each equity fund. The cleaned equity fund dataset contained 91 unique equity types, consisting of country equities, such as ""Italy Equity"", and sector equities, such as ""Sector Equity Precious Metals"". Sector equity funds invest in firms in a specific sector or indus- try, such as technology, regardless of a company’s country of operation. Howeve"
8499,unknown,"invest in firms with extensive operations in a specific country, regardless of the industry or sector. A rating is given by analysts assigned to the equity fund, and this takes values {1, 2, 3, 4, 5 }. In Category 1, funds have excellent performance and high returns relative to their risk of investment. Category 2 includes funds with a solid performance and risk versus return performance, though n"
8500,unknown,"as good as Category 1. Category 3 includes funds that are fairly valued and perform in line with market averages, and investors are advised to hold their positions. Category 4 includes funds that underperform their peers. Lastly, Category 5 represents funds that underperform over the long term and are higher risk, meaning they should be withdrawn from. An equity’s risk rating is a metric of its ma"
8501,unknown,"An equity’s risk rating is a metric of its market volatility. 9 risk rating takes the same values as rating but measures the risk of investing in an equity fund. Category 1 funds are less volatile and more stable, and thus are suitable for conservative investors. Category 2 funds are moderately volatile but still reasonably safe. Category 3 funds have medium volatility, balancing risk and potentia"
8502,unknown,"6 CHAPTER 2. EXPLORATORY DATA ANALYSIS returns. Category 4 funds are more volatile, with a greater possibility of returns as well as greater exposure to market volatility. Lastly, Category 5 funds are subject to the highest risk and volatility. rating, and risk rating are ordinal variables as they exhibit an order by definition. category is a categorical variable because it represents different no"
8503,unknown,"and sectors, which do not have a natural order. 10 The rest of the variables are continuous. equity style score is the difference between two investment strategies: growth and value stocks.11 Growth stocks are expected to bring in a lot of capital due to strong growth in the underlying com- pany.12 Value stocks are either underrated or ignored by the market, and they could gain value eventually.12"
8504,unknown,"eventually.12 Therefore, a lower equity style score means the stock is more of a value stock, and a higher equity style score indicates more of a growth stock. equity size score measures the performance of said equity relative to the company’s value.13 price prospective earnings gives the ratio between a company’s share price and earnings per share. price cash flow ratio is a ratio that compares a"
8505,unknown,"compares a company’s market value to the amount of money it generates during a select period. 13 A dividend is the money companies might decide to pay you if you own their shares, and the dividend yield is the dividend paid as a percentage of the share price. The dividend yield factor is the dividend yield, except it is used to compare the different equities, considering variations in dividend yie"
8506,unknown,yield performance. historical earnings growth measures how a stock’s earnings per share have grown in the last 5 years. sales growth is the increase in sales of a company’s products/ services over time. asset cash is the proportion of a fund’s assets held in cash. holdings n stock is the number of different stock holdings each equity fund holds. A stock holding is the number of other companies whi
8507,unknown,"running of a business. 14 fund size is the sum of all the assets in the fund. 2.3 Exploratory Data Analysis Now that the dataset was cleaned and the variables specified and defined, the next step was to perform data analysis in R and find underlying relationships between the variables. To fully represent the dataset, it is best to use actual values rather than simulate any missing values. So, this"
8508,unknown,analysis was done with only rows with no missing values. 2.3.1 Underlying Distributions QQ plots indicate if a variable is normally distributed. These are shown below for the responses: −4 −2 0 2 4 −10 0 10 20 30 40 QQ Plot for roe Theoretical Quantiles Sample Quantiles −3 −2 −1 0 1 2 3 10 20 30 40 50 60 QQ Plot for Sustainability_Score Theoretical Quantiles Sample Quantiles Figure 2.1: QQ Plots f
8509,unknown,"Theoretical Quantiles Sample Quantiles Figure 2.1: QQ Plots for ROE and Sustainability Scores From the QQ Plots above, there is strong evidence that ROE and sustainability score are normally distributed because the points on the QQ plot approximate a straight line. 7 CHAPTER 2. EXPLORATORY DATA ANALYSIS 2.3.2 Correlation It was also essential to examine the correlation among the predictors to dete"
8510,unknown,"be removed. High correlation implies multicollinearity, so this was checked before proceeding. After creating the correlation matrix for the predictors, there were two high correlation values. One high cor- relation, 0.8099, was observed between price prospective earnings and price cash flow ratio. Based on the definitions of these variables, the latter was judged to align more closely with profit"
8511,unknown,"and sustainability and was therefore retained. Similarly, a high negative correlation of −0.8029 was identified between dividend yield factor and equity style score. Here, dividend yield factor was considered a more relevant factor. After these adjustments, 12 predictors remained. 2.3.3 Imputing Missing Values Although high correlation implies multicollinearity, the opposite is not necessarily tru"
8512,unknown,"ticollinearity is not just about pairwise relationships. Multicollinearity arises from predictor linear dependence. For instance, if Variable A is a linear combination of VariablesB and C, say A = 2B +C, multicollinearity can occur even if there is no high pairwise correlation. 15 Therefore, methods such as the Variance Inflation Factor (VIF) are used to detect multicollinearity beyond correlation"
8513,unknown,"the Variance Inflation Factor (VIF) are used to detect multicollinearity beyond correlation. 16 The dataset should also be complete before carrying out a VIF analysis. The first step to ensuring completeness involves analysing how the NA values are spread and how often they occur in each row. It was found that 90% of the rows had two NA entries or less, and because the dataset is large, only these"
8514,unknown,"these rows were used in further analysis. There were three types of missing data here corresponding to the numerical, categorical and ordinal variables that needed to be imputed. Random forest imputation was used to fill in the missing categorical and ordinal data for several reasons. Although it is computationally more expensive than using methods such as the mode to fill in missing data, it deal"
8515,unknown,"this relationship between the ordinal variables, rating and risk rating. It gave an association of -0.0728, indicating there was sufficient evidence to suggest a lack of association between them. Random forest imputation also works for categorical and ordinal data through surrogate splits, and outliers influence them less as they use an ensemble of decision trees. Random forest imputations also do"
8516,unknown,"methods such as linear model imputation, which requires underlying distribution normality. 17 The random forest model was trained on rows with no NA values and was set to make sure it was predicting categorical and ordinal values rather than numerical ones. Ultimately, the model learned the relationships between the target variable it was imputing and the other columns in the dataset. Linear model"
8517,unknown,"Linear model imputation was used to fill in missing numerical features because it uses the rela- tionships between variables rather than relying on simpler approaches such as mean imputation. This method assumes that each response variable is drawn from an underlying normal distribution, and this is supported by the Q-Q plots shown in Figure 2.1. 18 In the linear model, the ordinal predictors, whi"
8518,unknown,"In the linear model, the ordinal predictors, which have integer values from 1 to 5, were used as numerical predictors as they have roughly equal spacing among levels. 19 However, the categorical predictor, category, was frequency encoded to prevent it from imposing a hierarchy in the model. 2.3.4 Multicollinearity VIF analysis was employed using a multiple-response linear regression model of the r"
8519,unknown,"the features (see Section 3.1.2 for more on this), where category was frequency encoded. Following this, a dummy response was added for VIF calculation, and the VIF values were derived from this new model. If any predictors had VIF values greater than 10, they would have been removed due to multicollinearity. Here, however, all VIF values were below 5 (see Figure A.1 for confirmation of this). The"
8520,unknown,"Therefore, none of the predictors exhibited multicollinearity and hence, they were all retained. 8 CHAPTER 2. EXPLORATORY DATA ANALYSIS Finally, this left a cleaned equity fund dataset with 1248 rows, each with individual equity fund information. In addition, there were 14 columns, with 2 response variables and 12 predictors. 2.3.5 Multivariate Normality Many of the MRR models in this report relie"
8521,unknown,"Many of the MRR models in this report relied on the assumption of response multivariate normality. This was evaluated using Mardia’s test, which assesses multivariate skewness and kurtosis. The null and alternative hypotheses of Mardia’s test are that the responses are and are not multivariate normally distributed, respectively. Therefore, if either the kurtosis or skewness statistic deviates sign"
8522,unknown,"from their expected values under multivariate normality, then the null hypothesis is rejected. 8 Mardia’s test returned p-values of 0.0732 for skewness and 0.0862 for kurtosis, meaning that in both cases, we failed to reject the null hypothesis of multivariate normality at a 5% significance level. Therefore, there was sufficient evidence to suggest a multivariate normal assumption for the response"
8523,unknown,"(see Subsection A.2.2 for more on how these p-values were derived). However, the relatively low p-values suggested that the assumption may not hold perfectly, and this mild deviation from normality helped explain differences in performance between models that require distributional assumptions and those that do not. 2.4 Response Intercorrelation As stated in the introduction, considering response "
8524,unknown,"As stated in the introduction, considering response intercorrelation is what separates MRR from single-response regression. If the dataset consisted of response variables which shared no correlation, one model could be built for each response, and MRR would not be needed. Before proceeding with each MRR model, it was worth briefly examining the relationship between the two response variables: ROE "
8525,unknown,"the two response variables: ROE and sustainability score. Figure 2.2 shows a scatter plot of these responses, with a fitted regression line included to highlight their correlation: −10 0 10 20 30 40 10 20 30 40 50 ROE vs Sustainability Score Return on Equity (ROE) Sustainability Score Figure 2.2: Scatter Plot of Return on Equity vs. Sustainability Score The regression line above shows a slight neg"
8526,unknown,"and sustainability score. Using the cor function in R gives a response correlation of -0.313 (to 3sf). This correlation means that, generally, as ROE increases, the sustainability score decreases. Here were the full empirical covariance and correlation matrices between the responses (to 3sf): R = 1.00 −0.313 −0.313 1 .00 ! , ΣY = 62.0 −8.79 −8.79 12 .7 ! . The negative correlation and covariance m"
8527,unknown,"! . The negative correlation and covariance might be surprising given current trends toward a more sustainable climate. However, a lower sustainability score means an equity fund is a more sustainable option, so this does follow the expected trend. 9 Chapter 3 Linear Regression This chapter introduces the methodology of the first MRR model, multiple-response linear regression, by starting with sin"
8528,unknown,"by starting with single-response linear regression. Stepwise selection methods are then introduced, followed by an explanation of how they can be extended to consider multiple responses, using the multiple analysis of variance, to determine which predictors are included in the multiple-response models. The accompanying code will then be explained, and the models in this chapter are evaluated using"
8529,unknown,"3.1 Theory 3.1.1 Single-Response Linear Regression Before delving into multiple-response linear regression, let us start with single-response linear regres- sion (SRLR). SRLR models the relationship between one response variable and a set of predictor variables. The general form of this model, with the dimensions subscripted, is: Y(n×1) = X(n×p)β(p×1) + ϵ(n×1), (3.1) where n is the number of obser"
8530,unknown,"where n is the number of observations, p the number of predictors (including the column of ones), Y the response variable vector, X the design matrix, β the coefficient vector and ϵ is the error variable vector. Including a column of ones inX adds an intercept term to the model. This is the expected response when all predictors are zero. When modelling any data, the aim is to minimise the residual"
8531,unknown,"When modelling any data, the aim is to minimise the residuals. A residual is the difference between predicted and actual values, and reducing this ensures the model is as accurate as possible. This accuracy is achieved by finding the most optimal coefficient vector, β. This optimal coefficient vector, also known as the ordinary least squares estimator (OLS), ˆβ, is found by minimising the sum of s"
8532,unknown,"of squared residuals. It can also be defined using X and Y as follows: ˆβOLS = (X⊤X)−1X⊤Y. The model’s accuracy can then be evaluated using metrics such as the R2-value, which measures the proportion of variance in the dependent variable explained by the independent variables. SRLR comes with a few underlying assumptions, which relate to the properties of its error terms. They are linearity, homos"
8533,unknown,"means there is a linear relationship between the dependent variables and the independent variables: E(Yi|Xi) = β0 + β1xi. The assumption of normality refers to the distribution of residuals, which are assumed to follow a normal distribution with mean zero and constant variance, σ2. Homoscedasticity means that an even spread of residual values is expected, and the assumption that the model oper-"
8534,unknown,"means that an even spread of residual values is expected, and the assumption that the model oper- ates well for all values of the predictor variable holds. Finally, independence implies that the error covariance matrix is diagonal, and this is given by: Cov(ϵ) = σ2In, 10 CHAPTER 3. LINEAR REGRESSION where σ2 is the true error variance. Since the true error variance, σ2, is unknown, it is typically"
8535,unknown,"Since the true error variance, σ2, is unknown, it is typically estimated from the residuals, ˆϵ = Y − Xˆβ, where ˆβ is the OLS estimator. An unbiased estimator of σ2 is given by: ˆσ2 = 1 n − pˆϵ⊤ˆϵ, (3.2) where n is the number of observations and p is the number of predictors. Therefore, because σ2 can be estimated, the error covariance matrix, Cov( ϵ), can be too. If this method were extended to "
8536,unknown,"If this method were extended to model m responses using m independent single-response regression models, it would ignore the covariance between responses, leading to sub-optimal predictions when responses are correlated (see Equation 3.5 for a visual of this). However, SRLR can be extended to the multiple-response setting while incorporating this structure. This extended approach is known as multi"
8537,unknown,"3.1.2 Multiple-Response Linear Regression Multiple-response linear regression (MRLR) is the most basic model in MRR. Its general form is: Y(n×m) = X(n×p) B(p×m) + E(n×m), (3.3) with: E(ei) = 0 and Cov( ei, ek) = σikIn i, k= 1, 2, . . . , m, where n is the number of observations,m is the number of responses, and p is the number of predictors, including the column of ones, Y the response variable ma"
8538,unknown,"matrix, and E is the error variable matrix. Including a column of ones in X allows the model to estimate an intercept for each response variable. These intercepts appear as the first row of the coefficient matrix, B. Like with SRLR, this represents the expected value of each response when all predictors are zero. The first key difference between MRLR and SRLR lies in the structures of the response"
8539,unknown,"and errors. These are now matrices rather than vectors, reflecting that the responses are modelled together. As an example, suppose n = 4, p = 3, and m = 2, then the regression equation becomes: Y(4×2) = X(4×3) B(3×2) + E(4×2). This expands to:   y11 y12 y21 y22 y31 y32 y41 y42   (4×2) =   1 x11 x12 1 x21 x22 1 x31 x32 1 x41 x42   (4×3) ·   b01 b02 b11 b12 b21 b22   (3×2)"
8540,unknown,"·   b01 b02 b11 b12 b21 b22   (3×2) +   e11 e12 e21 e22 e31 e32 e41 e42   (4×2) . Each row in Y corresponds to a single observation (i.e. a data point), and each column represents one of the m response variables. The coefficient matrix B contains a separate column of regression coefficients for each response, and the error matrix E captures the errors associated with each response "
8541,unknown,"for every observation. The second key difference between SRLR and MRLR lies in the structure of the error terms. MRLR allows for correlation between errors across different responses within the same observation. 11 CHAPTER 3. LINEAR REGRESSION This error covariance matrix captures this correlation: ΣE = {σik}, i, k = 1, 2, . . . , m, where σik denotes the covariance between the error terms ei and "
8542,unknown,"in the same observation. 20 The sample error covariance matrix can be estimated using a formula analogous to the unbiased estimator of σ2 in SRLR (from Equation 3.2): ˆΣE = 1 n − pE⊤E, where n is the number of observations and p is the number of predictors. For illustration, consider again the case where n = 4, p = 3 and m = 2: ˆΣE = 1 n − pE⊤E = "" e11 e21 e31 e41 e12 e22 e32 e42 #   e11 e12 "
8543,unknown,"e21 e22 e31 e32 e41 e42   = "" P4 i=1 e2 i1 P4 i=1 ei1ei2P4 i=1 ei1ei2 P4 i=1 e2 i2 # . (3.4) If we used 2 independent single-response models, the off-diagonal elements of ˆΣE would be 0. In this case, ˆΣE instead becomes: ˆΣE = ""P4 i=1 e2 i1 0 0 P4 i=1 e2 i2 # , (3.5) which may lead to suboptimal predictions in the presence of correlated responses. This error covariance structure is important"
8544,unknown,"This error covariance structure is important as it qualifies MRLR as an MRR model. It does this because it is used to compute the sample response covariance matrix, ˆΣY, as follows: ˆΣY = Cov(Y) = Cov(XB + E) = Cov(XB) + Cov(E) = B⊤Cov(X)B + Cov(E) = B⊤ ˆΣXB + ˆΣE, (3.6) where the linearity property of covariance justifies the first expansion (see Section A.2.3 for the sim- plification of Cov(XB) "
8545,unknown,"plification of Cov(XB) to B⊤Cov(X)B). Equation 3.6 shows that error interdependence remains, even after accounting for the predictors, and it is captured directly by the off-diagonal elements of ˆΣE. Since ˆΣE appears explicitly in the expression for the response covariance matrix ˆΣY, modelling this structure is essential to capturing the covariance of the responses for MRLR. Therefore, a non-zer"
8546,unknown,"Therefore, a non-zero value of the off-diagonal element of ΣE, σik, implies that responses i and k tend to vary together, suggesting interdependence between them. MRLR comes with a few underlying assumptions, which can be extended from SRLR. Linearity comes from the formula for MRLR, which, when expanded, is: Y1 = b01 + b11x1 + ··· + bp1xp + e1, Y2 = b02 + b12x1 + ··· + bp2xp + e2, ... Ym = b0m + "
8547,unknown,"These rows are also independent, another MRLR property from SRLR. 12 CHAPTER 3. LINEAR REGRESSION Normality also holds here, but it differs slightly. If we look at using several independent single- response models, each error term is independent and identically normally distributed, with ϵi ∼ N(0, I). However, in MRLR, the error terms, E, share the same distribution. They are assumed to have a mul"
8548,unknown,"have a multivariate normal distribution with constant variance: E ∼ Nm(0, ΣE), where ΣE is the covariance matrix of errors. The error terms associated with different responses may be correlated. 20 However, despite this consideration of the correlation between errors, we continue to assume that observations (i.e. rows of Y) are uncorrelated. 20 To formalise this, let the vector of responses for th"
8549,unknown,"ith observation be denoted by: Y(i) = XB(i) + E(i), i = 1, 2, . . . , m, with Cov(E(i)) = σiiIn as before, but the errors associated with different responses within the same observation may be correlated.20 Each response variable,i, is modelled independently, but all responses share the same set of predictors. The predictor matrix X must also have full column rank to ensure that the least squares "
8550,unknown,"The predictor matrix X must also have full column rank to ensure that the least squares estimator of B is well-defined. Full rank ensures that the least squares estimator, ˆB(i), exists and is unique for each response. Under the assumptions of MRLR, where responses are estimated independently, OLS, which derives ˆB(i), minimises the residual sum of squares (RSS) separately for each response in MRL"
8551,unknown,"MRLR as shown here: RSS(i) = ∥Y(i) − XB(i)∥2 2.20 This logic for the OLS estimate is just like for SRLR applied to each response variable separately: ˆB(i) = (X⊤X)−1X⊤Y(i). Collecting these univariate least squares estimates gives: ˆB = h ˆB(1) ˆB(2) ··· ˆB(m) i = (X⊤X)−1X⊤ h Y(1) Y(2) ··· Y(m) i , or equivalently: ˆBOLS = (X⊤X)−1X⊤Y. Note the similarity of this formula to the OLS estimator in SRL"
8552,unknown,"ˆBOLS = (X⊤X)−1X⊤Y. Note the similarity of this formula to the OLS estimator in SRLR. Using the least squares estimate, ˆBOLS, the matrices of the predicted values and residuals can be made.20 The predicted values are calculated as: ˆY = XˆB = X(X⊤X)−1X⊤Y. And the residuals are given by: ˆE = Y − ˆY = [I − X(X⊤X)−1X⊤]Y. The predicted values and residuals can be used to evaluate MRLR’s performance,"
8553,unknown,"The predicted values and residuals can be used to evaluate MRLR’s performance, which is what was done on the equity fund dataset (see Section 3.2.1 for more on how these criteria were used). 3.1.3 Stepwise Selection Methods Now that MRLR has been established and its qualification as an MRR model clarified, the next step is to determine which predictors to include. Selecting the most significant pr"
8554,unknown,"building an interpretable model with a good fit. Since there are multiple ways to evaluate predictor importance, a strategy is also required to choose the best model resulting from the different selections of predictors. This report used stepwise selection methods for this purpose. Stepwise selection consists of forward, backward and bidirectional approaches. These stepwise selection 13 CHAPTER 3."
8555,unknown,"13 CHAPTER 3. LINEAR REGRESSION methods were used because they are more computationally efficient than best subset selection, as they consider a smaller set of models. Forward stepwise selection starts with a model having no predictors and adds the predictor that produces the largest improvement in model fit. Model fit is usually determined using a selection criterion, for example, the Akaike Info"
8556,unknown,"or adjusted R2. At each step, the variable that contributes most significantly to this criterion is added. Backward stepwise selection begins with the full model as the initial model, and successively re- moves the predictor that makes the smallest contribution to the model according to the same criterion. Bi-directional stepwise selection incorporates both forward and backward stepwise selection,"
8557,unknown,"Bi-directional stepwise selection incorporates both forward and backward stepwise selection, with inclusion or exclusion of predictors at every step based on their impact on the model fit as a whole. When using all of these selection methods in single-response regression, the “best” model is chosen for every number of predictors, and then the single best model is picked. This “best” criterion is g"
8558,unknown,"to the model with the smallest residual sum of squares, or largest R2. The single “best” model is then picked using a cross-validated prediction error, Cp (or the AIC), BIC, or the adjusted R2 value. However, in MRR, the response covariance matrix must be taken into account when selecting predictors. As a result, the selection criteria must also adapt to reflect this response structure. Multiple a"
8559,unknown,"analysis of variance allows for evaluating predictors under this consideration. Note: the response structure refers to the joint distributional characteristics of the responses. This includes the variances, covariances, non-linear dependencies, and how they conditionally relate to different predictor combinations. 21 3.1.4 Analysis of Variance ANOVA primarily analyses the differences among group m"
8560,unknown,"ANOVA primarily analyses the differences among group means and their associated variances. It helps determine whether there is a significant difference between the means of different groups by comparing the variance within groups to the variance between groups. It can also be used to evaluate the predictor contribution to response variation. It does this by partitioning total response variance, or"
8561,unknown,of squares of regression (SSR) and the sum of squares of error (SSE). The SSR is the variation in responses accounted for by the p predictors. The SSE is the variation in responses not accounted for by the predictors. The partitioning means that these terms are all related by the equation: SST = SSR + SSE. This decomposition is crucial in evaluating how much the predictors explain response variati
8562,unknown,"ANOVA results are displayed as a table which generates an F statistic, F = MSR MSE , where MSR = SSR p−1 and MSE = SSE n−p, where n is the number of observations. This generates a probability value: P[Fp−1,n−p > F], where a low p-value (e.g. p <0.05) means the predictors significantly impact response variation. Sequential ANOVA is a type of ANOVA which decomposes the regression component of an ANO"
8563,unknown,"Sequential ANOVA is a type of ANOVA which decomposes the regression component of an ANOVA table by considering a sequence of m nested models. This approach allows an assessment of how the unexplained variation in the response changes as predictors are added to the model one at a time. Here is the algorithm for Sequential ANOVA: First consider a sequence of m nested models M1 ⊂ M2 ⊂ ··· ⊂Mm, where "
8564,unknown,"First consider a sequence of m nested models M1 ⊂ M2 ⊂ ··· ⊂Mm, where each model Mj contains all predictors from Mj−1 plus one additional predictor. Each model Mj is represented as: Mj : E[Y | X] = b1 + b2X2 + ··· + bpj Xpj , and the next model adds an extra predictor: 14 CHAPTER 3. LINEAR REGRESSION Mj+1 : E[Y | X] = b1 + b2X2 + ··· + bpj Xpj + bpj+1 Xpj+1 . At each step, a hypothesis test is per"
8565,unknown,"improves model fit. Sequential ANOVA was useful in this report as it can be extended to the multivariate setting to evaluate which predictors are most appropriate for inclusion in MRLR. 3.1.5 Multiple Analysis of Variance Multivariate Analysis of Variance (MANOVA) extends ANOVA to multiple responses. It comes with the assumption that the responses need to have a multivariate normal distribution, l"
8566,unknown,"is confirmed in Section 2.3.5 for the equity fund dataset). 22 MANOVA tests if the means of several responses differ across predictors. It also assesses the effects of continuous predictors on multiple responses simultaneously, while accounting for response intercorrelation, rather than fitting separate ANOVAs for each response variable. 23 It assesses this effect using variance-covariance matrice"
8567,unknown,"effect using variance-covariance matrices instead of scalar variance values. The total response variation, or the total sum of squares and cross products (SSCP) matrix, T, is partitioned into 2 matrices, analogous to ANOVA. The first is the explained SSCP matrix, H, representing the response variation explained by the predictor variable(s). The second is the unexplained SSCP matrix, W, representin"
8568,unknown,"By their set-up, W and H relate through the total SSCP matrix: T = H + W.24 MANOVA is suitable for MRR because the total SSCP matrix,T, is an unscaled version of the sample response covariance matrix, i.e.: T = (n − 1)ˆΣY, where n is the number of observations and ˆΣY = 1 n−1 (Y − ¯Y)⊤(Y − ¯Y). This relation means MANOVA accounts for response intercorrelation by directly considering re- sponse cov"
8569,unknown,"sponse covariances, which is ignored when using several independent univariate ANOVAs. Before proceeding with these values, let us look at how they are computed. From single-response ANOVA, H, W and T are extensions of SSE, SSR and SST, respectively. The unexplained SSCP matrix W is calculated as follows: W = (Y − ˆY)⊤(Y − ˆY), where Y is the n × m matrix of observed response values, and ˆY is the"
8570,unknown,"where Y is the n × m matrix of observed response values, and ˆY is the n × m matrix of predicted response values from the model. ˆY is computed using ˆY = XˆB, where ˆB is the MRLR OLS estimator. The explained SSCP matrix H is calculated as follows: H = ( ˆY − ¯Y)⊤( ˆY − ¯Y), where ¯Y(n×1) is the overall mean response vector given by ¯Y = 1 n Pn i=1 Yi. Lastly, the total SSCP matrix T is calculate"
8571,unknown,"n Pn i=1 Yi. Lastly, the total SSCP matrix T is calculated as follows: T = (Y − ¯Y)⊤(Y − ¯Y). To test significance, MANOVA has multiple tests it can use. This report uses the Wilks’ Lambda (Λ) test, which measures the proportion of variance not explained by the independent variables: Λ = |W| |T| , 15 CHAPTER 3. LINEAR REGRESSION where |W| and |T| are the determinants of the unexplained and total S"
8572,unknown,"where |W| and |T| are the determinants of the unexplained and total SSCP matrices, respectively. 24 A small Λ (close to 0) indicates the predictors explain a large proportion of response variation, and a large Λ (close to 1) indicates the predictors explain a small proportion of response variation. The Wilks’ Lambda value is useful because it can be transformed into an F-statistic: F = (1 − Λ)/m Λ"
8573,unknown,"F = (1 − Λ)/m Λ/(N − m − 1) where m is the number of dependent variables and N is the total sample size. The null hypothesis in MANOVA is that all group means for the dependent variables are equal. So, a significant F-test (p <0.05) suggests that at least one of the dependent variables significantly differs across groups. 23. However, this report uses the difference in Wilks’ Lambda values as spec"
8574,unknown,"However, this report uses the difference in Wilks’ Lambda values as specified by Sequential MANOVA. Sequential MANOVA extends sequential ANOVA to cover multiple responses. It works the same as sequential ANOVA, but at each step, the Wilks’ Lambda value (Λ j) is computed for each model: Λ1 ≥ Λ2 ≥ ··· ≥Λm, where the difference: Λ j − Λj+1 is the additional variance explained by the newly added predi"
8575,unknown,"significant drop in Λ indicates that the new predictor significantly improves the model. 23 Another advantage of using Wilks’ Lambda is that it prevents the inflation of Type I errors. If multiple response variables are analysed separately using ANOVA, the probability of finding at least one false positive increases. Since MANOVA jointly examines multiple responses, it controls for the increased r"
8576,unknown,"provides a more accurate assessment of significant predictors. 3.2 Application 3.2.1 Model Evaluation and Performance Multiple models were developed and evaluated throughout this report, and their performance was assessed using the average normalised root mean square error (ANRMSE). While the root mean square error (RMSE) measures model error for each response variable sep- arately, it retains the"
8577,unknown,"RMSE (NRMSE) scales the error by the standard deviation of each response, returning a unitless and comparable measure of model performance. Since two response variables were considered, the NRMSE was computed for each and then averaged to obtain a single summary statistic, the ANRMSE. Here is the formula for the ANRMSE: ANRMSE = 1 m mX j=1   q 1 n Pn i=1(yij − ˆyij)2 sj  , (3.7) where m is the"
8578,unknown,"where m is the number of responses, n is the number of observations, yij is the observed value for response j, observation i, ˆyij is the predicted value, and sj is the sample standard deviation of the jth response given by: sj = vuut 1 n − 1 nX i=1 (yij − ¯yj)2. (3.8) The ANRMSE gives the proportion of natural variability in the responses not accounted for by the model. An excellent model has a 0"
8579,unknown,"model. An excellent model has a 0-0.5 ANRMSE, a good model has a 0.5-0.6 ANRMSE, a satisfactory model has a 0.6-0.7 ANRMSE, and an unsatisfactory model has an ANRMSE greater than 0 .7.25 16 CHAPTER 3. LINEAR REGRESSION 3.2.2 Standard Multiple Linear Regression Before proceeding with the analysis, it was verified that the equity fund dataset followed the necessary assumptions from the Sequential MA"
8580,unknown,"assumptions from the Sequential MANOVA Stepwise Selection and MRLR. Most of the assumptions from MRLR and MANOVA were all catered for in the exploratory data analysis (see Chapter 2) through methods such as Mardia’s Test for Skewness and Kurtosis. However, the assumption of a full rank predictor matrix, X, had yet to be checked. The full rank was confirmed using QR-decomposition, which showed that"
8581,unknown,"which showed that the rank of X equalled the number of predictors (see Subsection 2.3.4 for more on the dimensions of the dataset). The first model evaluated on the equity fund dataset was an MRLR using all of the available predictors. The residuals from the model were used to calculate the ANRMSE, which was 0.7606. This high value indicated that this model did not perform well on the equity fund "
8582,unknown,"sense because the model was not built to understand the underlying complexity of the equity fund dataset, so it underfitted the data. Stepwise selection will consider this complexity more, and it was adapted for MRR using Sequential MANOVA. 3.2.3 Code Explanation All of these selection methods had to be manually coded because the stepAIC command, which performs stepwise selection in R, only applie"
8583,unknown,"performs stepwise selection in R, only applies to single-response regression models. The selection process begins with different initial models depending on the chosen method. In for- ward selection, the model starts with no predictors. However, in backward and bidirectional selection, it starts with all available predictors included. At each iteration, the algorithm evaluates the effect of adding"
8584,unknown,"using the functions add predictors and remove predictors, respectively. The criterion for inclusion or exclusion depends on the change in Wilks’ Lambda (Λ): ∆Λ = Λ current −Λnew, where Λcurrent is the Wilks’ Lambda from the current model, and Λ new is it after including or excluding a given predictor. The predictor associated with the largest reduction in Wilks’ Lambda is selected or removed, depe"
8585,unknown,"depending on the chosen selection method. If no predictor reduces Wilks’ Lambda, so ∆Λ < 0 for all candidates, the model remains unchanged for that step. A key consideration in this procedure that has not been discussed thus far was the risk of overfitting. Just as in single-response regression, where the residual sum of squares always decreases with the inclusion of more variables, Wilks’ Lambda "
8586,unknown,"are added. If used alone, this could result in models that overfit the data. To address this, the algorithm includes an additional evaluation step using the ANRMSE, which is computed via k-fold cross-validation (CV) using thecalculate nrmse function. Cross-validation helps avoid overfitting by rotating the test set through the entire dataset. This ensures every observation is used for training and"
8587,unknown,"A predictor is only accepted into, or removed from, the model if its inclusion or exclusion reduces the Wilks’ Lambda and ANRMSE. This ensures that model selection favours variables that improve performance on unseen data. The new model formed is then stored in a list named candidate models. The process continues iteratively. In bidirectional selection, the algorithm stops if two consecutive itera"
8588,unknown,"iterations fail to improve the model. In forward or backward selection, it stops once all available predictors have been evaluated without improving Wilks’ Lambda and ANRMSE. The final model is then selected as the one with the lowest ANRMSE from the candidate models list. If two models have a similar performance, the simpler model is preferred due to its interpretability. For brevity, this overal"
8589,unknown,"For brevity, this overall procedure will be referred to as Sequential MANOVA Stepwise Selection for the remainder of this paper. The method is first illustrated using a simpler dataset of Study Time versus Exam Scores before being applied to the full equity fund dataset. 17 CHAPTER 3. LINEAR REGRESSION 3.2.4 Small-Scale Example Let us take a small-scale dataset evaluating correlated Mathematics an"
8590,unknown,that may influence them: X1: Hours Studied X2: Time Spent on Papers Y1: Math Scores Y2: Science Scores 5 2 78 80 7 3 85 79 8 4 88 88 3 1 65 70 10 5 92 74 Table 3.1: Study Time vs. Exam Scores Let us now walk through an example of Sequential MANOVA Forward Stepwise Selection. The first step is evaluating each predictor one at a time to see which predictor gives the largest Wilks’ lambda difference.
8591,unknown,"The first step involves computing the total, explained and subsequently unexplained SSCP matri- ces. This starts by first computing the means to get ¯Y = [ ¯Y1, ¯Y2]⊤: ¯Y1 = 78 + 85 + 88 + 65 + 92 5 = 81.6, ¯Y2 = 80 + 79 + 88 + 70 + 74 5 = 78.2. Next, compute the deviations from the mean as follows: Y1,i − ¯Y1 Y2,i − ¯Y2 −3.6 1.8 3.4 0.8 6.4 9.8 −16.6 −8.2 10.4 −4.2 The total SSCP matrix, T, is th"
8592,unknown,"T = "" P i(Y1,i − ¯Y1)2 P i(Y1,i − ¯Y1)(Y2,i − ¯Y2)P i(Y1,i − ¯Y1)(Y2,i − ¯Y2) P i(Y2,i − ¯Y2)2 # , where i goes from 1 to 5, which is the number of observations. Now, compute each term: X i (Y1,i − ¯Y1)2 = 449.2, X i (Y2,i − ¯Y2)2 = 184.8, X i (Y1,i − ¯Y1)(Y2,i − ¯Y2) = 634. Thus, T = "" 449.2 634 634 184 .8 # . The next step is to compute the unexplained SSCP Matrix,W, which uses the prediction fr"
8593,unknown,"# . The next step is to compute the unexplained SSCP Matrix,W, which uses the prediction from MRLR: ˆY = XˆB. Here, we want to compute the OLS estimator for MRR, ˆB, which is an identical process to that in single-response, i.e., by simply using ˆB = (X⊤X)−1X⊤Y. In our case: Y = "" 78 85 88 65 92 80 79 88 70 74 #⊤ and X = "" 1 1 1 1 1 5 7 8 3 10 #⊤ . Remember, only X1 is being used here because a mo"
8594,unknown,"18 CHAPTER 3. LINEAR REGRESSION Plugging these values into the formula for the OLS estimator, ˆB = (X⊤X)−1X⊤Y, gives: ˆB = "" 56.47 72 .23 3.81 0 .90 # . Therefore, the predicted values are: ˆY1,i = 56.47 + 3.81X1,i and ˆY2,i = 72.23 + 0.90X1,i. Now, we can compute W = (Y − ˆY)⊤(Y − ˆY): W = "" 25.73 50 .86 50.86 160 .93 # . Next, we compute the Wilks’ Lambda value using the determinants of these ma"
8595,unknown,"Λ = |W| |T| = 1553.08 60090.2 = 0.0258, This value is very low, meaning the model including X1 explains a large proportion of the variation in the responses, which is ideal. Next, the difference between this and the previous Wilks’ Lambda value is computed. The previous case was when we had the column of ones in our design matrix, X. This means the model only estimated the mean of each response, s"
8596,unknown,"Y. Hence, the predicted values were the column means. As a result, the explained SSCP matrix, H, is 0. Therefore, the unexplained SSCP matrix, W, equals the total SSCP matrix, T, and Λ = 1. Computing the differences gives: 1-0.0258 = 0.9742. This is compared to the Wilks’ Lambda Difference from only using X2, and the predictor with the largest difference is added as a candidate model for one predi"
8597,unknown,"the lowest ANRMSE is returned. To calculate the ANRMSE, say for the model with only the X1 predictor, start by computing the predictions. For MRLR, the predictions are computed using ˆY = XˆBOLS where X and ˆBOLS have been set out previously for this model: ˆY = XˆB =   1 5 1 7 1 8 1 3 1 10   "" 56.47 72 .23 3.81 0 .90 # =   75.52 76 .73 83.14 78 .53 86.95 79 .43 67.90 74 .93 9"
8598,unknown,"94.57 81 .23   . From here, the residuals can be computed as: Y − ˆY =   78 80 85 79 88 88 65 70 92 74   −   75.52 76 .73 83.14 78 .53 86.95 79 .43 67.90 74 .93 94.57 81 .23   =   2.49 3 .25 1.88 0 .44 1.07 8 .53 −2.89 −4.95 −2.55 −7.27   This gives the standard deviation of each response as: (10.60, 6.80) after plugging this into Equation 3.8 with "
8599,unknown,"with n = 5 and m = 2. Finally, plugging all these values into Equation 3.7 gives an NRMSE of each response variable as (to 3dp): "" 0.214 0.835 # . Therefore, the overall ANRMSE is 0.524 (3dp) for the model using only X1 as a predictor. 19 CHAPTER 3. LINEAR REGRESSION 3.2.5 Applying Stepwise Selection Sequential MANOVA Stepwise Selection was then applied to the equity fund dataset with ROE and sust"
8600,unknown,"sustainability score as the responses, and category frequency encoded. The data was also standard- ised. This is not strictly necessary here, but it improves stability. Both forward and bi-directional Stepwise Selection led to the same model with all the predictors kept. This model was the sum of all the predictors in the dataset and therefore, had the same AN- RMSE. However, backward elimination "
8601,unknown,"RMSE. However, backward elimination returned a different model than the other selection methods. The difference was that it did not include risk rating. This gave a higher ANRMSE than the pre- vious model with 0.7924. It makes sense for the selection methods to potentially produce a different model because each selection method has different starting points and directions, which can lead to other "
8602,unknown,"other paths through the model and, ultimately, a different set of selected predictors. 3.2.6 Extending and Evaluating Stepwise Selection Although there were already a large number of predictors in this dataset, extending the model to consider non-linear and interaction terms improved its performance. To include these, all that was done was to make a new function which generated the new predictors."
8603,unknown,"done was to make a new function which generated the new predictors. When generating the non-linear terms, the model improved as shown by the ANRMSE reducing to0.6893. Using the interaction terms further improved the model, and the ANRMSE was reduced to 0.5613, a noteworthy improvement. One counterargument to introducing these terms is that these models overfit the data, but the implementation of C"
8604,unknown,"can be complex. However, all this data was already available. Table 3.2 provides an overview of all the selection models evaluated in this chapter. Model ANRMSE Full Model - MRLR using all the Predictors 0.7606 Forward Stepwise Selection - Sum of all the Predictors 0.7606 Backward Stepwise Selection - One predictor removed 0.7924 Bidirectional Stepwise Selection - Sum of the Predictors 0.7606 Bidi"
8605,unknown,"Bidirectional Stepwise Selection with Interaction Terms 0.5613 Bidirectional Stepwise Selection with Non-Linear Terms 0.6893 Table 3.2: Selection Methods and their ANRMSE Values See Figure A.2 for a graphical illustration of Model 2: Forward Stepwise Selection in action. Stepwise selection comes with multiple benefits. Firstly, it is easy to implement. Although manual code had to be made in this c"
8606,unknown,"code had to be made in this case, it proved to be easy to test on the dataset. Stepwise selection reduces the work of model building to the click of a button and often yields simple results. 26 However, they come with a few drawbacks. Firstly, although CV can be employed, they are prone to over-fitting because they can find idiosyncrasies in the population that do not exist.26 Also, stepwise"
8607,unknown,"selection produces models that are highly sensitive to slight variations in the data, causing inconsistent results. Another issue with this method is inflated effect sizes, so even when the model identifies valid predictors, it overestimates their effect sizes. 26 This means that the impact of significant predictors on the response variables, ROE and sustainability score, will be inflated. Shrinka"
8608,unknown,"Shrinkage Methods are the next type of regression model evaluated in this report. None of the stepwise selection drawbacks apply to shrinkage methods. They address them by penalising the model coefficients, which prevents over-fitting and reduces sensitivity to outliers. 20 Chapter 4 Shrinkage Methods This chapter delves into Shrinkage Methods, focusing on Multivariate Regression with Covariance E"
8609,unknown,"Estimation and Reduced Rank Ridge Regression, which extend Lasso and Ridge Regression. First, it introduces the theory behind Lasso and Ridge Regression and then extends them to multiple responses. Next, it delves into the theory of Multivariate Regression with Covariance Estimation and Reduced Rank Ridge Regression, and finally, applies these concepts to the equity fund dataset. 4.1 Theory Stepwi"
8610,unknown,"Stepwise selection is a discrete model search method, where the best model is selected based on information criteria. The next type of model this report will consider is shrinkage methods, which are a continuous model search method. In this method, the entire model is trained, but the estimated regression coefficients are reduced in magnitude and, in some cases, even set to zero. This is achieved "
8611,unknown,"by minimising a loss function that combines model fit with a penalty on coefficient size. 4.1.1 Ridge Regression Ridge regression shrinks estimated regression coefficients towards zero by minimising the following penalised loss function: L(β) = ∥Y − Xβ∥2 2 + λ∥β∥2 2, or equivalently: ˆβ Ridge = arg min β  ∥Y − Xβ∥2 2 + λ∥β∥2 2 . (4.1) where || · ||2 is the ℓ2-norm, λ ≥ 0 is a regularisation param"
8612,unknown,"SRLR (see Equation 3.1). The tuning parameter is crucial as it controls the strength of the penalty, balancing the trade-off between model fit and coefficient magnitude. Therefore, as λ increases, the penalty term gets larger, so to minimise the entire loss function, the regression coefficients get smaller. Ridge regression minimises the cost function, and this can be shown by differentiating the "
8613,unknown,"Ridge regression minimises the cost function, and this can be shown by differentiating the cost function with respect to β and setting it equal to zero. This gives the ridge regression closed-form coefficient estimator as: ˆβ Ridge =  X⊤X + λIp −1 X⊤Y. (4.2) See Subsection A.2.4 for the full derivation of the closed-form estimator. Ridge regression is particularly useful when the predictors exhi"
8614,unknown,"Ridge regression is particularly useful when the predictors exhibit high multicollinearity or when the number of predictors, p, is close to or exceeds the number of observations, n. Note: the latter is also known as high-dimensionality. In such cases, the matrixX⊤X becomes singular, making the OLS estimator unstable or undefined. The ridge penalty term λ∥β∥2 2 regularises the inversion by ensuring"
8615,unknown,"2 regularises the inversion by ensuring thatX⊤X+λI is always invertible, leading to more stable coefficient estimates. This regularisation reduces the variance of the model at the expense of a small increase in bias, which generally improves model prediction accuracy when there is predictor multicollinearity or high-dimensionality in the data. 21 CHAPTER 4. SHRINKAGE METHODS Ridge regression estim"
8616,unknown,"on the sum of squared coefficients: ∥β∥2 2 = Pp j=1 β2 j ≤ c, where p is the number of predictors and c is a positive constant. This constraint defines an ℓ2-ball, which in the two-dimensional case ( p = 2) corresponds to a filled circle and provides a geometric interpretation of the ridge penalty. See the right-hand graphic in Figure 4.1 for this interpretation: Figure 4.1: Left: the Lasso constr"
8617,unknown,"Figure 4.1: Left: the Lasso constraint |β1| + |β2| ≤c, Right: the Ridge constraint β2 1 + β2 2 ≤ c. In higher dimensions, so as p increases, the ℓ2-norm forms a hypersphere, which is a higher-dimensional generalisation of the circle. As the constraint region is circular, the ridge regression solution typically lies within the interior of the ball rather than on the axes. As a result, although ridg"
8618,unknown,"zero, it does not set any of them exactly to zero, even those corresponding to irrelevant predictors. This inability to create sparse models, i.e. a model in which some coefficients are exactly zero, is a key limitation of ridge regression, as it cannot perform feature selection. Ideally, a simpler model would exclude the uninformative variables entirely. Lasso regression does this. 4.1.2 Lasso Re"
8619,unknown,"4.1.2 Lasso Regression Lasso regression removes irrelevant features by shrinking their coefficients exactly to zero. Instead of penalising the squared magnitude of the coefficients like ridge, lasso penalises their absolute values. It shares the same aim of minimising a loss function as ridge, but with a different penalty term: L(β) = ∥Y − Xβ∥2 2 + λ∥β∥1, (4.3) or equivalently: ˆβ Lasso = arg min "
8620,unknown,"L(β) = ∥Y − Xβ∥2 2 + λ∥β∥1, (4.3) or equivalently: ˆβ Lasso = arg min β  ∥Y − Xβ∥2 2 + λ∥β∥1 , where || · ||1 is the ℓ1-norm and the other variables follow from Equations 3.1 and 4.1. Unlike ridge regression, lasso has no closed-form solution because the ℓ1-norm penalty, ∥β∥1, is not differentiable at zero. This makes the optimisation problem non-smooth and means lasso requires iterative algorith"
8621,unknown,"iterative algorithms, such as coordinate descent, to solve it. Coordinate descent for the lasso regression, from Hastie et. al (2009), proceeds as follows: From SRLR, β ∈ Rp×1 and now suppose the p predictors and response are standardised to have mean zero and variance one. First, initialise all the βj = 0. At each iteration, cycle over j = 1, . . . , p(the rows, or entries, of the coefficient vec"
8622,unknown,"each coefficient by: 1. First, computing the partial residual, rj, as: rj = Y − X−jβ−j, 22 CHAPTER 4. SHRINKAGE METHODS where X−j denotes the design matrix,X, with thej-th column removed, andβ−j is the coefficient vector, β, with the j-th entry removed. 2. Next, compute the simple least squares coefficient of these residuals on the jth predictor, β∗ j : β∗ j = x⊤ j rj, where xj is the j-th column "
8623,unknown,"3. Finally, update βj via soft-thresholding: βj ← sign(β∗ j )   |β∗ j | −λ  + , where (z)+ = max(0, z) is the positive part function. 27 This process repeats until convergence, which is defined as the point when the maximum change in the coefficient vector between successive iterations falls below a chosen threshold ϵLasso > 0; that is, when: ∥β(k) − β(k−1)∥∞ < ϵLasso, where β(k) denotes the coef"
8624,unknown,"value norm, where ∥x∥∞ = max1≤i≤n |xi|. Like ridge regression, lasso introduces bias in exchange for reduced variance, which can improve prediction accuracy. Lasso is particularly useful in high dimensions, as it is scalable and can perform automatic feature selection by shrinking some coefficients exactly to zero. Both ridge and lasso can also mitigate multicollinearity. However, for the equity f"
8625,unknown,"were largely addressed during preprocessing (see Chapter 2). As with ridge regression, this optimisation problem can be viewed as least squares subject to a different constraint, the ℓ1-norm: ∥β∥1 = Pp j=1 |βj| ≤c. Geometrically, in the two-dimensional case, when there are two predictors, this constraint defines a diamond-shaped region in the parameter space. The corners of this diamond lie on the"
8626,unknown,"that the solution lies exactly on one of them. This is why lasso can shrink some coefficients exactly to zero. See the left-hand graphic in Figure 4.1 for an illustration of this constraint. In higher dimensions, the ℓ1-norm forms a cross-polytope. This is a higher-dimensional generali- sation of the diamond, which promotes sparsity by encouraging solutions closer to the axes. These regularisation"
8627,unknown,"These regularisation methods, developed for single-response models, can also be extended to MRR. 4.1.3 Shrinkage Methods for MRR Ridge regression extends to MRR by adding a Frobenius norm penalty to the coefficient matrix, ˆBOLS, from MRLR: ˆBRidge = arg min B {∥Y − XB∥2 F + λ∥B∥2 F }, (4.4) where λ ≥ 0 is the regularisation parameter, ∥B∥2 F = Pq j=1 Pm i=1 b2 ij denotes the Frobenius norm penalt"
8628,unknown,"penalty, and the matrices X, Y and B are defined as in Equation 3.3. 28 In single-response ridge regression, the penalty is given by the squared ℓ2-norm of the coefficient vector. See Equation 4.1. However, since the standard ℓ2-norm applies only to vectors, it cannot directly penalise all the entries in the coefficient matrix. The Frobenius norm replaces it because it extends the ℓ2-norm to matri"
8629,unknown,"extends the ℓ2-norm to matrices, summing the squared entries across all rows and columns in B. The closed-form solution for ˆBRidge in multiple-response ridge regression is: ˆBRidge = (X⊤X + λIp)−1X⊤Y, (4.5) 23 CHAPTER 4. SHRINKAGE METHODS which follows from single-response ridge regression (see Equation 4.2). Similarly, Lasso Regression can also be extended to MRR using the ℓ1-norm penalty: ˆBLas"
8630,unknown,"ˆBLasso = arg min B {∥Y − XB∥2 F + λ∥B∥1}. (4.6) As in single-response lasso regression, the ℓ1-penalty shrinks some elements of B to exactly zero, enabling variable selection in the regression model. Multivariate Lasso also uses coordinate descent to solve its optimisation problem. However, this slightly changes because now there is a coefficient matrix to estimate rather than a coefficient vecto"
8631,unknown,"(see Subsection A.2.4 for this algorithm). Although Ridge and Lasso are MRR models as they build upon MRLR, they only consider the response covariance matrix through the joint estimation of the coefficient matrix. However, there are extensions of them that further exploit the multivariate response structure. Two such approaches are reduced rank ridge regression and multivariate regression with cov"
8632,unknown,"4.1.4 Reduced Rank Ridge Regression In MRLR, predictions are generated using Y = XˆBOLS where ˆBOLS = (X⊤X)−1X⊤Y. Reduced rank ridge regression (RRRR) builds upon this by modifying the estimation of the coef- ficient matrix using multiple-response ridge regression (see Equation 4.4), and a rank constraint. It aims to solve the following optimisation problem: ˆB(λ, r) = arg min B:rank(B)≤r ∥Y − XB∥"
8633,unknown,"F + λ∥B∥2 F , (4.7) where r ≤ min{p, m} controls the rank of B and λ ≥ 0 is the ridge parameter. Note: m is the number of responses and p is the number of predictors. Although the unconstrained ridge solution ˆBRidge is typically full rank, which is confirmed on the equity fund dataset via QR-decomposition (see Subsection 3.2.2), a rank constraint remains useful. It encourages dimensionality reduc"
8634,unknown,"It encourages dimensionality reduction and reflects the assumption that the responses are caused by a small number of latent variables. 28 This results in a model that not only considers the underlying latent structure of the data, but is also interpretable. Before proceeding, assume that X and Y have been standardised, meaning each column has a mean of zero (mean-centring) and unit variance. As a"
8635,unknown,"mean of zero (mean-centring) and unit variance. As a result, the number of predictors, p, excludes the intercept term, and the column of ones is omitted from the design matrix. This assumption will be further justified later in this section. The first step involves transforming Equation 4.7 into a standard reduced rank regression problem on an augmented dataset, which incorporates the ridge penalt"
8636,unknown,"on an augmented dataset, which incorporates the ridge penalty. 28 This augmentation involves adding artificial rows to the original predictor and response matrices. The augmented least squares problem is equivalent to applying the closed-form ridge estimator from Equation 4.5, when no rank constraint is applied. Augmentation is used instead in RRRR because it allows the RRRR problem to be solved m"
8637,unknown,"it allows the RRRR problem to be solved more efficiently using Singular Value Decomposition (SVD), rather than solving the regularised optimisation problem directly. Augmentation is also performed separately for each fixed λ, as the optimal value is not known in advance and is selected via CV. Here is augmentation written mathematically: X∗ (n+p)×p = X√ λI ! , Y∗ (n+p)×m = Y 0 ! . 24 CHAPTER 4. SH"
8638,unknown,"X∗ (n+p)×p = X√ λI ! , Y∗ (n+p)×m = Y 0 ! . 24 CHAPTER 4. SHRINKAGE METHODS The augmented data leads to the following rank-constrained optimisation problem: ˆB(λ, r) = arg min rank(B)≤r ∥Y∗ − X∗B∥2 F , where B can be estimated using OLS. See Subsection A.2.5 for the full derivation of this. This problem can be simplified further due to the orthogonality of the OLS estimate. This orthog- onal proje"
8639,unknown,"onal projection property means the squared error loss function can be decomposed into 2 parts: ∥Y∗ − X∗B∥2 F = ∥Y∗ − ˆY∗ R∥2 F + ∥ ˆY∗ R − X∗B∥2 F , where ˆY∗ R = X∗ ˆB∗ R denotes the ridge regression estimate. 28 The first term of this equation does not involve B so the objective function can be further simplified into: ˆB(λ, r) = arg min rank(B)≤r || ˆY∗ R − X∗B||2 F . (4.8) The next step involv"
8640,unknown,"R This starts by taking its SVD: ˆY∗ R = τX i=1 diuiv⊤ i , where the di’s denote the singular values and ui ∈ Rn×1, vi ∈ Rm×1 denote the left and right singular vectors of ˆY∗ R respectively.28 This step is important because RRRR further captures response intercorrelation through vi. This can be proven by how the sample response covariance, ˆΣY, is defined: ˆΣY = 1 n − 1(Y − ¯Y)⊤(Y − ¯Y) = 1 n − 1"
8641,unknown,"ˆΣY = 1 n − 1(Y − ¯Y)⊤(Y − ¯Y) = 1 n − 1Y⊤Y. The last equality holds because Y has been mean-centred (see Subsection A.2.5 for how ˆΣY and Y⊤Y would relate without mean centring). This is important because the SVD of Y in matrix form is: Y = UDV⊤. Now, compute Y⊤Y using its SVD form: Y⊤Y = VD⊤U⊤UDV⊤ = VD2V⊤, where the final equality is produced because U is orthonormal, meaning U⊤U = In, and D = D"
8642,unknown,"where the final equality is produced because U is orthonormal, meaning U⊤U = In, and D = D⊤.29 This means V clearly diagonalises Y⊤Y. Therefore, it directly represents the principal directions of response covariance. Thus, the columns of V, vi, capture response intercorrelation. Then, the Eckhart-Young Theorem is used to provide the best rank r approximation to ˆY∗ R in the Frobenius norm as follo"
8643,unknown,"R in the Frobenius norm as follows: ˆY∗ r = rX i=1 diuiv⊤ i .28 The next step to RRRR is to define the projection matrix, Pr, which projects the predictions onto an r-dimensional subspace: Pr = Pr i=1 viv⊤ i . Let ˆB(λ, r) = ˆB∗ RPr and clearly, rank( ˆB(λ, r)) ≤ r, as rank(Pr) = r.28 Substituting this into the fitted values, X∗ ˆB(λ, r), we obtain predictions corresponding to the rank- r solution"
8644,unknown,"and thereby to the original problem in Equation 4.7: X∗ ˆB(λ, r) = X∗ ˆB∗ RPr = τX i=1 diuiv⊤ i !  rX j=1 vjv⊤ j   = rX i=1 diuiv⊤ i = ˆY∗ r.28 25 CHAPTER 4. SHRINKAGE METHODS See Subsection A.2.5 for the third simplification. This has shown that the proposed solution ˆB(λ, r) = ˆB∗ RPr is the minimiser of Equation 4.8 and hence the optimisation problem. 28 Writing it explicitly in terms of X,"
8645,unknown,"ˆB(λ, r) = ˆB∗ RPr = (X∗⊤X∗)−1X∗⊤YPr = (X⊤X + λI)−1X⊤YPr.28 Therefore: ˆY(λ, r) = XˆB(λ, r) = XˆB∗ RPr = X   X⊤X + λI −1 X⊤YPr = XˆBRidgePr. From this estimate, the role of the ridge closed-form solution becomes clear: the full-rank ridge estimator ˆB∗ R is first computed, and then its fitted values ˆYλ = X∗ ˆB∗ R are projected onto a lower- dimensional subspace. The rank- r constraint then arise"
8646,unknown,"values from the original m-dimensional response space to an r-dimensional subspace spanned by the top r right singular vectors. While RRRR further accounts for response covariance by applying SVD to a low-rank approxi- mation of the fitted response matrix, an alternative approach is to model the inverse error covariance matrix, which captures conditional dependencies between responses directly. Th"
8647,unknown,"tivariate Regression with Covariance Estimation, which jointly estimates both the coefficient and inverse error covariance matrices. 4.1.5 Multivariate Regression with Covariance Estimation Multivariate Regression with Covariance Estimation (MRCE) builds upon MRLR by jointly estimating the regression coefficient matrix B and the inverse error covariance matrix Ω = Σ−1 E . Before solving this probl"
8648,unknown,"Starting with the familiar MRLR model: Y(n×m) = X(n×p)B(p×m) + E(n×m), where p no longer includes the column of ones. Each row yi ∈ R1×m of Y follows the conditional distribution: yi|xi ∼ Nm(xiB, ΣE) from the multivariate normality of the errors. Therefore, the probability density function for the ith row, i.e. a single observation, is: p(yi | xi) = 1 (2π)m/2|ΣE|1/2 exp  −1 2(yi − xiB)⊤Σ−1 E (yi "
8649,unknown,"p(yi | xi) = 1 (2π)m/2|ΣE|1/2 exp  −1 2(yi − xiB)⊤Σ−1 E (yi − xiB)  . (4.9) Equation 4.9 implies that, in MRLR, the negative log-likelihood function of ( B, Ω) can be expressed up to a constant as: g(B, Ω) = tr 1 n(Y − XB)⊤(Y − XB)Ω  − log |Ω|.30 See Subsection A.2.6 for the full derivation. The MRCE estimation for B extends this by adding 2 penalties to the negative log-likelihood function, g"
8650,unknown,"( ˆB, ˆΩ) = arg min B,Ω   g(B, Ω) + λ1 X j′̸=j |ωj′j| + λ2 pX j=1 qX k=1 |bjk|   , (4.10) where ωj′j are the elements of Ω, bjk are the elements of B, λ1 controls sparsity in Ω and λ2 for B.30 26 CHAPTER 4. SHRINKAGE METHODS Equation 4.10 can be solved in 2 ways: by solving it for ˆB with Ω fixed at a chosen Ω0 and by solving for ˆΩ with B fixed at a chosen B0: 1. This is the formula for ˆB,"
8651,unknown,"from the objective function: ˆB(Ω0) = arg min B   tr 1 n(Y − XB)⊤(Y − XB)Ω0  + λ2 X j,k |bjk|   . (4.11) 2. This is the formula for Ω, fixing B as B0, therefore the B terms in Equation 4.10 can be removed from the objective function: ˆΩ(B0) = arg min Ω   tr h ˆΣRΩ i − log |Ω| + λ1 X j′̸=j |ωj′j|   . (4.12) where ˆΣR = 1 n(Y − XB0)⊤(Y − XB0). Before delving further into this problem, "
8652,unknown,"Before delving further into this problem, let us discuss how this model qualifies as MRR and, consequently, how it accounts for response intercorrelation. Since MRCE estimates both B and Ω simultaneously, the coefficient matrix is optimised consid- ering response covariance. This reasoning follows from the explanation for why MRLR qualifies as an MRR (see Equation 3.6 for this). The error covarian"
8653,unknown,"The error covariance matrix, ΣE, describes marginal dependencies between response variables, whereas the inverse error covariance matrix, Ω = Σ−1 E , describes conditional dependencies. 31 This means that if Ωij = 0, then responses Yi and Yj are conditionally independent given all other responses and, if Ω ij ̸= 0, then responses Yi and Yj have a dependency not accounted for by the predictors. 31 "
8654,unknown,"Thus, MRCE does not assume independent residuals, but instead models how responses are related even after accounting for predictors. By contrast, MRLR assumes that ΣE is unstructured but fixed during estimation, and does not account for conditional error dependencies when estimating B. Equations 4.11 and 4.12 are each solved using a distinct approach. Equation 4.11 is solved by the following algor"
8655,unknown,"following algorithm. Let us call it Algorithm 1. This algorithm comes from Friedman et al. (2007): Given Ω and an initial value ˆB(0), let S = X⊤X and H = X⊤YΩ, the algorithm goes as follows: 1. For the t-th iteration, set ˆB(t) ← ˆB(t−1). Visit all entries of ˆB(m) in some sequence, and for each entry (r, c), update ˆb(t) rc by minimising the objective function with respect to that single entry, "
8656,unknown,"keeping all other coefficients fixed: ˆb(t) rc ← sign  ˆb(t) rc + hrc − urc srrωcc  ˆb(t) rc + hrc − urc srrωcc − nλ2 srrωcc  + , where urc = Pp j=1 Pq k=1 ˆb(t) jk srjωkc and (a)+ means taking max(a, 0). 2. If P j,k ˆb(t) jk − ˆb(t−1) jk < ϵRidge P j,k ˆbRidge jk , then stop, otherwise go to the prior step. Here: ˆBRidge is the ridge closed-form estimator of the coefficient matrix (see Equati"
8657,unknown,"is the ridge closed-form estimator of the coefficient matrix (see Equation 4.5 for this estimator). For the full derivation of Algorithm 1, see Subsection A.2.6. Equation 4.12 is solved using the graphical lasso (see Subsection A.2.6 for further justification). It is solved using this algorithm due to its speed. 30 Graphical lasso (glasso) is widely used for estimating sparse inverse covariance ma"
8658,unknown,"and Friedman et al. (2008), is as follows: 27 CHAPTER 4. SHRINKAGE METHODS 1. First, define ˆΩ (t+1) 0 = ( S + λ1Im)−1, where ˆΩ (t+1) 0 denotes the initial iteration of ˆΩ (t+1) and S = ˆΣR = 1 n(Y − XˆB(t+1))⊤(Y − XˆB(t+1)) is the empirical error covariance matrix computed at fixed ˆB(t+1). 2. For the i-th intermediate update of ˆΩ(t+1), ˆΩ(t+1) i , (this update comes from co-ordinate descent, w"
8659,unknown,"whose relevance will be explained later, on each column of ˆΩ(t+1) i ) and each column: j = 1, 2, . . . , m, solve for the j-th column of ˆΩ(t+1) i , while keeping the rest of ˆΩ(t+1) i fixed. Let us denote ˆΩ(t+1) i as Ω in this step for brevity. First, partition both Ω and S as: Ω = "" Ω−j,−j ω−j,j ωj,−j ωjj # , S = "" S−j,−j s−j,j sj,−j sjj # , where Ω−j,−j is Ω with the j-th row and column remov"
8660,unknown,"excluding the diagonal entry ωjj (and likewise logic for S). Note: Since Ω and S are symmetric, updating the j-th column determines the j-th row. Now, define: P = Ω1/2 −j,−j, b = Ω−1/2 −j,−js−j,j. Here, Ω1/2 −j,−j refers to the unique symmetric positive definite matrix such that: Ω1/2 −j,−j Ω1/2 −j,−j = Ω−j,−j, and likewise for Ω−1/2 −j,−j: Ω−1/2 −j,−j Ω−1/2 −j,−j = Ω−1 −j,−j. These matrices are o"
8661,unknown,"−j,−j: Ω−1/2 −j,−j Ω−1/2 −j,−j = Ω−1 −j,−j. These matrices are obtained via eigen-decomposition. Banerjee et al. (2007) showed that solving the problem for ω−j,j is equivalent to solving the following dual problem for β, where β ∈ Rm−1 is an intermediate vector: min β  ∥b − Pβ∥2 2 + λ1∥β∥1 . This is the lasso problem, where Pβ is the prediction and b is the pseudo-response. Therefore, it can be s"
8662,unknown,"it can be solved by coordinate descent (see Subsection 4.1.2). This gives a m − 1 vector solution ˆβ. Finally, fill in the corresponding column (and hence row) of Ω using ω−j,j = Ω−j,−j ˆβ. This update only adjusts the off-diagonal elements of Ω as these directly relate to the dependencies between responses. 3. Once all m columns (and hence rows) have been updated by coordinate descent, this gives"
8663,unknown,"matrix ˆΩ (t+1) i+1 for the next iteration. Step 2 is repeated until convergence between successive iterations, which is when for some k + 1-th iteration: ∥ˆΩ (t+1) k+1 − ˆΩ (t+1) k ∥∞ < ϵOmega, where ϵOmega is the convergence threshold for the inverse error covariance matrix. 32,33 Therefore, when this convergence occurs, ˆΩ(t+1) = ˆΩ (t+1) k+1 . Now, we can explicitly outline the full MRCE algor"
8664,unknown,"k+1 . Now, we can explicitly outline the full MRCE algorithm from Rothman et al. (2010) as follows: For fixed values of λ1 and λ2, initialise ˆB(0) = 0 and ˆΩ(0) = ˆΩ( ˆB(0)). 28 CHAPTER 4. SHRINKAGE METHODS 1. Compute ˆB(t+1) = ˆB( ˆΩ(t)) by solving Equation 4.11 using Algorithm 1. 2. Compute ˆΩ(t+1) = ˆΩ( ˆB(t+1)) by solving Equation 4.12 using the glasso algorithm. 3. If P j,k ˆb(t+1) jk − ˆb(t"
8665,unknown,"jk − ˆb(t) jk < ϵRidge P j,k ˆbRidge jk , then stop; otherwise, go back to Step 1. Therefore, when updating Ω only the dependencies between responses are updated which means ˆB is updated considering this. MRCE performance also depends on the regularisation parameters, which are selected via CV: (λ∗ 1, λ∗ 2) = arg min λ1,λ2 KX k=1 ∥Y(k) − X(k)B(−k) λ1,λ2 ∥2 F , where Y(k) is the matrix of response"
8666,unknown,"of observations in the kth fold, and B(−k) λ1,λ2 is the estimated regression coefficient matrix computed with observations outside the kth fold, with tuning parameters λ1 and λ2.30 The next section will talk about how these 2 methods were evaluated on the equity fund dataset. 4.2 Application Although standardisation involves both mean-centring and scaling to unit variance for X and Y, only mean-ce"
8667,unknown,mean-centring is shown in these calculations for brevity. 4.2.1 Small-Scale Example Let us now apply both of these shrinkage methods to a small dataset before proceeding to the equity fund dataset. We will use the familiar Mathematics and Science scores data: X1: Hours Studied X2: Time Spent on Papers Y1: Math Scores Y2: Science Scores 5 2 78 80 7 3 85 79 8 4 88 88 3 1 65 70 10 5 92 74 Table 4.1: 
8668,unknown,"8 4 88 88 3 1 65 70 10 5 92 74 Table 4.1: Study Time vs. Exam Scores Reduced Rank Ridge Regression This example will show a rank-1 reduction of RRRR using λ = 0.1 as the shrinkage parameter. Based on our prior justification of RRRR as an MRR model, mean-centring is essential to prevent biasing from the mean structure. The initial predictor matrix, X, and initial response matrix, Y, are: X = "" 5 7 "
8669,unknown,"X = "" 5 7 8 3 10 2 3 4 1 5 #⊤ , Y = "" 78 85 88 65 92 80 79 88 70 74 #⊤ . After mean-centering, i.e. ˜X = X − ¯X and ˜Y = Y − ¯Y, the predictor and response matrices become: ˜X = "" −1.6 0 .4 1 .4 −3.6 3 .4 −1 0 1 −2 2 #⊤ , ˜Y = "" −3.6 3 .4 6 .4 −16.6 10 .4 1.8 0 .8 9 .8 −8.2 −4.2 #⊤ . 29 CHAPTER 4. SHRINKAGE METHODS We apply RRRR with λ = 0.01 by first incorporating ridge regularisation, which we d"
8670,unknown,"X∗ = "" ˜X√ λI # , Y∗ = ""˜Y 0 # . Since λ = 0.01, we compute √ λ = √ 0.01 = 0.1, so the augmented matrices are: X∗ = "" −1.6 0 .4 1 .4 −3.6 3 .4 0 .1 0 −1 0 1 −2 2 0 0 .1 #⊤ , Y∗ = "" −3.6 3 .4 6 .4 −16.6 10 .4 0 0 1.8 0 .8 9 .8 −8.2 −4.2 0 0 #⊤ . The Ridge penalty is incorporated into X∗ through √ λI. The augmented system behaves like a standard regression problem, meaning we solve for B using the s"
8671,unknown,"this as ˆB∗ R as a reminder that we are incorporating ridge regression. Using the OLS estimate formula: ˆB∗ R = (X∗⊤X∗)−1X∗⊤Y∗. After subbing in X∗ and Y∗, this gives us the augmented coefficient matrix, ˆB∗ R: ˆB∗ R = "" 7.40 −2.28 −6.18 5 .47 # . Thus, the ridge predictions are: ˆY∗ R = X∗ ˆB∗ R = "" −5.67 2 .96 4 .19 −14.29 12 .81 0 .74 −0.62 −1.82 −0.91 2 .28 −2.73 3 .19 −0.23 0 .55 #⊤ . The nex"
8672,unknown,"R = UDV⊤. This gives (to 3sf): D = "" 21.209 0 0 2 .294 # , V = "" 0.975 −0.223 0.223 0 .975 # and U = "" −0.280 0 .127 0 .216 −0.686 0 .622 0 .0316 −0.0227 −0.224 −0.675 0 .562 0 .227 0 .111 −0.169 0 .293 #⊤ . For full derivation of D, U and V see Subsection A.2.5. Since D1 = 21.21 ≫ D2 = 2.29, the best rank ( r=1) approximation is taken on the first column of D. Therefore, ˆY∗ 1 ≈ d1u1v⊤ 1 , where "
8673,unknown,"D. Therefore, ˆY∗ 1 ≈ d1u1v⊤ 1 , where (to 3sf): d1 = 21.209, v1 = h 0.975 0 .223 i⊤ and u1 = h −0.280 0 .127 0 .216 −0.686 0 .622 0 .0316 −0.0227 i⊤ . However, this is just an intermediary step as we still need to rescale the coefficient matrix. This is done through the projection matrix, Pr: Pr = v1v⊤ 1 = "" 0.95 0 .22 0.22 0 .05 # . Finally, the RRRR estimator is: ˆBr = ˆB∗ RPr = "" 6.54 1 .49 −4"
8674,unknown,"ˆBr = ˆB∗ RPr = "" 6.54 1 .49 −4.68 −1.07 # . 30 CHAPTER 4. SHRINKAGE METHODS Now, we can do the final predicted values, which are ˆY = ˜X∗ ˆBr, giving: ˆY = "" −5.78 2 .62 4 .47 −14.18 12 .87 −1.32 0 .60 1 .02 −3.24 2 .94 #⊤ . Remember, these predictions are on the mean-centred Y. We can remove this mean-centring, but that does not affect its model fit evaluation using the ANRMSE as these predictio"
8675,unknown,"to the actual mean-centred Y. Multivariate Regression with Covariance Estimation Now, we perform an MRCE iteration on Table 4.1 with λ1 = λ2 = 0.1. Remember we want to mean centre X and Y, so let us use ˜X and ˜Y from the RRRR calculation: ˜X = "" −1.6 0 .4 1 .4 −3.6 3 .4 −1 0 1 −2 2 #⊤ , ˜Y = "" −3.6 3 .4 6 .4 −16.6 10 .4 1.8 0 .8 9 .8 −8.2 −4.2 #⊤ . From here on, X = ˜X and Y = ˜Y. This is to make"
8676,unknown,"#⊤ . From here on, X = ˜X and Y = ˜Y. This is to make notation easier. Let us first compute ˆBRidge because that is used in determining when Algorithm 1 stops: ˆBRidge = (X⊤X + λ2I)−1X⊤Y = "" 5.07 −0.77 −2.19 2 .89 # . For Algorithm 1, initialise ˆB(0) = 0 and ˆΩ(0) = ˆΩ( ˆB(0)). To compute ˆΩ(0), first do the error, E(0): E(0) = Y − XˆB(0) = "" −3.6 3 .4 6 .4 −16.6 10 .4 1.8 0 .8 9 .8 −8.2 −4.2 #⊤ "
8677,unknown,"#⊤ . Now, compute the residual covariance matrix Σ(0) E : Σ(0) E = 1 nE(0)⊤ E(0) = "" 89.84 30 .28 30.28 36 .96 # . Next, add this to a regularisation parameter, λ2 = 0 .1, and invert. The regularisation parameter avoids dividing by zero when inverting ΣE(0) for the precision matrix (to 3dp): Ω(0) = (Σ(0) E + λ2Im)−1 = "" 0.015 −0.013 −0.013 0 .037 # . (4.13) Now compute ˆB(1) = ˆB( ˆΩ(0)) by solvin"
8678,unknown,"Now compute ˆB(1) = ˆB( ˆΩ(0)) by solving Equation 4.11 using Algorithm 1. The manual calculation of this will be shown on one element of B(1): ˆb(1) 11 . First compute H and S: S = X⊤X = "" 29.2 17 17 10 # , H = X⊤YΩ(0) = "" 1.37 −0.41 0.78 −0.21 # . Now set ˆB(1) = ˆB(0) (this is to store the update) and update ˆb(1) 11 as follows: ˆb(1) 11 ← sign  ˆb(1) 11 + h11 − u11 s11ω11  ˆb(1) 11 + h11 − "
8679,unknown,"s11ω11  ˆb(1) 11 + h11 − u11 s11ω11 − nλ2 s11ω11  + , where u11 = P2 j=1 P2 k=1 ˆb(1) jk s1jωk1. After subbing in and computing, this gives: ˆb11 = 1.95. Doing 31 CHAPTER 4. SHRINKAGE METHODS this on every element, we get: ˆB(1) = "" 1.95 0 0 0 # . The next step of Algorithm 1 involves checking for convergence of ˆB relative to ˆBRidge. Here, if: X j,k ˆb(1) jk − ˆb(0) jk < ϵRidge X j,k ˆbRidge "
8680,unknown,"X j,k ˆb(1) jk − ˆb(0) jk < ϵRidge X j,k ˆbRidge jk , then stop; otherwise, go to the prior step. The left-hand side simplifies to: X j,k ˆb(1) jk − ˆb(0) jk = X j,k |ˆb(1) jk | = 1.95. And the right-hand side to: ϵRidge P j,k ˆbRidge jk = 10.92ϵRidge. We want 1.95 < 10.92ϵRidge, which means that convergence occurs for ϵRidge ≳ 0.18. Let us choose ϵRidge = 0 .2 so we can move to the next phase of "
8681,unknown,"ϵRidge = 0 .2 so we can move to the next phase of MRCE, estimating Ω using the graphical lasso algorithm: ˆΩ(1) = ˆΩ(ˆB(1)). Firstly, let us set a new initial Ω, using this updated ˆB, ˆB(1): S = ˆΣR = 1 n(Y − XˆB(1))⊤(Y − XˆB(1)) = "" 25.24 19 .97 19.97 36 .96 # . When inverting, again use a regularisation parameter but now it is λ1 = 0.1, giving (to 2sf): ˆΩ(1) 0 = (S + λ1I2)−1 = "" 0.069 −0.037 −"
8682,unknown,"−0.037 0 .047 # . Now, we move on to coordinate descent. Let us denote ˆΩ(1) 0 as Ω for brevity. First, partition Ω and S as: Ω = "" Ω−1,−1 ω−1,1 ω1,−1 ω11 # = "" 0.047 −0.037 −0.037 0 .069 # , S = "" S−1,−1 s−1,1 s1,−1 s11 # = "" 36.96 19 .97 19.97 0 .069 # . Then, define: P = Ω1/2 −1,−1 = 0.0471/2 ≈ 0.217, b = Ω−1/2 −1,−1s−1,1 = 0.0471/2 × 19.97 ≈ 92.2. Note: eigen-decomposition is not needed here a"
8683,unknown,"Now, we solve the following dual problem: min β  ∥b − Pβ∥2 2 + λ1∥β∥1 . Now we can start coordinate descent. First, initialise β1 as 0. 1. First, compute the partial residual, r1, as: r1 = b − P−1β−1 = b = 92.2, where the last simplification occurs since there is only one coordinate, no other β−1 or P−1 exists, so the partial residual is simply r1 = b. 2. Next, compute the simple least squares co"
8684,unknown,"1: β∗ 1 = p⊤ 1 r1 = 0.217 × 92.2 = 19.97. 32 CHAPTER 4. SHRINKAGE METHODS 3. Finally, update β1 via soft-thresholding: β1 = sign(β∗ 1) (|β∗ 1| −0.1)+ = sign(19.97) (|19.97∗| −0.1)+ = 19.87. Note: β ∈ R(m−1) and m = 2. Therefore, there is only one predictor in β. This process repeats until convergence; that is, when: ∥β(k) − β(k−1)∥∞ < ϵLasso →||β(1) 1 − β(0) 1 ||∞ < ϵLasso as k = 1 =||19.87 − 0||∞"
8685,unknown,"1 − β(0) 1 ||∞ < ϵLasso as k = 1 =||19.87 − 0||∞ < ϵLasso ϵLasso > 19.87 Note: ϵLasso is specified because it is a different threshold than the outer loop threshold used later in this calculation. Let us choose ϵLasso as 20, so we can compute the update step ω−j,j: ω−j,j = ω−1,1 = Ω−1,−1 ˆβ = 0.047 × 19.97 = 0.93. Repeating this process for the 2nd column gives: ω−2,2 = 1.36. Therefore: ˆΩ(1) 1 = "
8686,unknown,""" 0.069 1 .36 0.93 0 .047 # We have completed all columns, so now we need to check that: ∥ˆΩ (t+1) k+1 − ˆΩ (t+1) k ∥∞ < ϵOmega → ∥ˆΩ (1) 1 − ˆΩ (1) 0 ∥∞ < ϵOmega as k, t= 0 → || "" 0.069 1 .36 0.93 0 .047 # − "" 0.069 −0.037 −0.037 0 .047 # ||∞ = 1.397 < ϵOmega. Let us choose ϵOmega = 1 .5 here. Therefore, we have convergence, meaning ˆΩ(1) 1 = ˆΩ(1). This completes one full MRCE iteration: initial"
8687,unknown,"completes one full MRCE iteration: initialising B(0) and ˆΩ(0) and then updating to B(1) and ˆΩ(1) after solving the penalised optimisation problem. This process continues iteratively until convergence. The final coefficient, ˆB(n), is then used to compute the predictions: ˆY = XˆB(n). This can again have its centring removed, but this is not needed when evaluating using the ANRMSE. 4.2.2 Code Exp"
8688,unknown,"4.2.2 Code Explanation The data was pre-processed as in Subsection 3.2.5, but standardisation was used due to necessity here. The MRCE model was applied using the MRCE package, which jointly estimates the regression coefficient matrix and the inverse error covariance matrix of the responses. The optimal values of the regularisation parameters λ1 and λ2 were selected using 5-fold CV, implemented in"
8689,unknown,"the regularisation parameters λ1 and λ2 were selected using 5-fold CV, implemented in the fit mrce function. A grid of candidate values was evaluated by training and validating the model across CV folds, and the parameter pair, ( λ1, λ2), which minimised the cross-validated ANRMSE, was used for final model fitting. Test-set predictions were then computed using the fitted coefficient matrix. RRRR w"
8690,unknown,"RRRR was evaluated using thefit rrridge function. In this function, a ridge-penalised coefficient estimate was first computed, followed by SVD to retain only the top singular components, reducing the rank. The optimal rank was then selected via CV within the training set. The resulting coefficient matrix was then used to generate test-set predictions, which were evaluated via the ANRMSE. Next, add"
8691,unknown,"Next, additional feature engineering was applied to try to improve the performance of these meth- ods. A degree-two polynomial expansion was applied using poly, and second-order interaction terms were generated with model.matrix. Finally, all models were compared based on their ANRMSE. 33 CHAPTER 4. SHRINKAGE METHODS 4.2.3 Results These models were applied to the dataset to evaluate how well the p"
8692,unknown,These models were applied to the dataset to evaluate how well the predictors model the responses: ROE and sustainability score. Table 4.2 shows the results of all the models evaluated in this section: Model ANRMSE Model 1: MRCE 0.7612 Model 2: RRRR 0.7510 Model 3: MRCE including Polynomial Terms 0.6781 Model 4: RRRR including Polynomial Terms 0.7276 Model 5: MRCE including Interaction Terms 0.6777
8693,unknown,"Model 5: MRCE including Interaction Terms 0.6777 Model 6: RRRR including Interaction Terms 0.7231 Table 4.2: Comparison of ANRMSE values across shrinkage methods The baseline models show that RRRR had the smallest ANRMSE, followed by MRCE. This means that the reduced-rank constraint in RRRR enabled it to better capture the covariance structure between responses, particularly where there were moder"
8694,unknown,"responses, particularly where there were moderate intercorrelations among them. MRCE, by contrast, involves an additional step of covariance estimation. This introduces bias where response intercorre- lations are moderate, as they were here (the correlation between the ROE and sustainability score in this dataset was 0.3171). MRCE generally performs better with stronger response intercorrelations."
8695,unknown,"The inclusion of polynomial and interaction feature transformations improved performance in both models. MRCE benefited most from polynomial features, with a slight decrease in ANRMSE. RRRR also benefited, but to a lesser extent, suggesting that the inclusion of polynomial terms introduced unnecessary complexity not well used under a low-rank constraint. Adding interaction terms improved performan"
8696,unknown,"Adding interaction terms improved performance slightly for both models. MRCE, including in- teraction terms, had the lowest overall ANRMSE. This suggests that MRCE’s covariance estimation benefits more from structured interactions between predictors, whereas RRRR is impacted more by the added multicollinearity from such terms. These results indicate that the performance of each model depends on th"
8697,unknown,"These results indicate that the performance of each model depends on the type of introduced features. While MRCE gave the smallest ANRMSE, RRRR showed a more stable performance across different features. Thus, RRRR could be considered the more stable shrinkage method for this dataset. The models in this chapter were also surprising because they performed as well as MRLR in Chapter 3 on the equity "
8698,unknown,"Chapter 3 on the equity fund dataset (see Table 3.2 for the evaluation of the Chapter 3 model). This was unexpected given that MRCE and RRRR are designed to handle multiple responses with more considerations, particularly in the presence of multicollinearity or high-dimensional predictors, meaning they should generally be more accurate in modelling. However, the predictors used here were not highl"
8699,unknown,"However, the predictors used here were not highly correlated after the pre-processing steps de- scribed in Chapter 2, and the dataset is not high-dimensional ( p = 14, n = 1269). As such, the benefits of shrinkage and dimensionality reduction were less pronounced. In fact, standard OLS out- performed both MRCE and RRRR, likely because it preserved the relationship between the predictors and respon"
8700,unknown,"and responses, without the added bias introduced by regularisation. Although shrinkage methods like MRCE and RRRR are helpful in reducing overfitting and handling high-dimensional data, their linearity-dependent nature can limit their expressiveness. Here, as seen, there will be a loss of some interaction and patterns. Random Forests mitigate these drawbacks by using an ensemble of decision trees "
8701,unknown,"need for feature engineering. The application of an ensemble also reduces overfitting, which further supports the ability of Random Forests to handle complex datasets. 34 Chapter 5 Random Forests This chapter delves into Random Forests. It starts with classification and regression trees, from which we can create an ensemble of decision trees, otherwise known as random forests, which improve pre- d"
8702,unknown,"dictions. The forests are then extended to MRR, and they are adjusted to form Covariance Regression Random Forests, which are tested on the equity fund dataset. 5.1 Theory The previous modelling approaches used in this project, MRLR (see Chapter 3), MRCE and RRRR (see Chapter 4), rely on global parametric assumptions. These include properties such as multivariate normality of the responses and lin"
8703,unknown,"normality of the responses and linear relationships between predictors and responses. Tree-based models, however, are non-parametric, meaning they require no prior assumptions about the data. These models work by using a set of splitting rules to partition the feature space into smaller regions with similar response values. 5.1.1 Classification and Regression Trees Random forests are tree-based mo"
8704,unknown,"Random forests are tree-based models built upon classification and regression trees (CARTs) as a building block. A CART partitions the training data into groups with similar response values, and then predicts the same category for all data within a given subgroup. The CART starts with a root node containing all the objects, and is then divided into “leaf” nodes by recursive binary splitting.34 Thi"
8705,unknown,"that occur only within specific regions of the predictor space, including non-linear relationships and interactions that other models may miss. Each leaf node relates to a hyper-rectangle in the feature space, Rj, j = {1, . . . , J}. In other words, the feature space defined by X1, X2, . . . , Xp is split into J distinct regions which do not overlap, as shown below: Figure 5.1: Incorrect vs. Corre"
8706,unknown,"Figure 5.1: Incorrect vs. Correct Partitioning To grow the tree, CARTs use a greedy fitting algorithm, which means it selects the best split at each step by evaluating all possible features and thresholds. 35 CHAPTER 5. RANDOM FORESTS Note: the threshold is the value used to split the data on a feature. This decision is made locally and recursively, without considering future splits. The algorithm"
8707,unknown,"the split that minimises the RSS: JX j=1 X i:xi∈Rj   yi − ˆyRj 2 , where ˆyRj is the mean response for the training observations within the j-th rectangle. A regression tree is used as the responses in the equity fund dataset are continuous. A regression tree assigns a value to each feature space, Rj. This means that the model predicts the output (i.e. which feature space) based on the average re"
8708,unknown,"which feature space) based on the average response values for all observations in that subgroup. The greedy fitting algorithm provides good predictions for the training data but may overfit said data, resulting in poor performance on the test dataset. However, pruning can mitigate this issue. Pruning involves removing branches that contribute little to the tree’s prediction accuracy to control the"
8709,unknown,"the bias–variance trade-off. This is either based on a validation set or a complexity penalty. A common approach to pruning is the weakest link pruning algorithm. This algorithm introduces a non-negative tuning parameter for regression trees, α, which minimises the following cost-complexity objective: |T|X j=1 X i:xi∈Rj (yi − ˆyRj )2 + α|T|, where |T| is the number of terminal nodes (or the size) "
8710,unknown,"5.1.2 Bagging for Classification and Regression Trees Although CARTs are easy to interpret and are similar to standard decision-making processes, the trees generally have high variance. This means that small sample changes lead to significant changes in the fit, and they tend to have poor predictive accuracy. Bootstrap aggregating, also known as bagging, improves the stability and accuracy of CART"
8711,unknown,"improves the stability and accuracy of CART algorithms by averaging models. Bagging helps reduce variance and avoid over-fitting, but the model becomes more challenging to interpret. For regression trees, all the predictions are averaged to obtain: ˆfbag(x) = 1 B BX b=1 ˆfb(x), where B is the total number of bootstrap samples (i.e. the number of trees trained), and ˆfb(x) is the prediction from th"
8712,unknown,"prediction from the b-th tree. Bagging for CART addresses the overfitting issue in two ways: it can grow large trees with minimal (or no) pruning and rely on the averaging effect of bagging to reduce variance, or alternatively, it can prune each tree individually. Bagging is advantageous when there is a large amount of data, as is the case here for the equity fund dataset. This is because the empi"
8713,unknown,"distribution. Additionally, it is the best option when the aim is to minimise the variance of a predictor. However, bagging also brings some disadvantages. Firstly, interpretability is lost as the final estimate is not a tree. Secondly, it is an ensemble prediction, i.e. multiple trees are combined to make the final prediction. 35 Also, bagged trees are inherently correlated. This is problematic b"
8714,unknown,"more correlated the random variables are, the smaller the variance reduction of their average, which can undermine one of the key advantages of bagging. 36 CHAPTER 5. RANDOM FORESTS 5.1.3 Random Forests Random Forests make bagged CART models more independent, improving variance reduction in their ensemble predictions. They do this by resampling observations and restricting the model to random subs"
8715,unknown,"are more diverse and hence influential. 36,37 The Random Forests algorithm is used in the following way: a bootstrap resample ( y∗ i , x∗ i )n i=1 is taken of the training data, like for bagging. Then, the tree is built and each time a split in a tree is considered, m predictors are randomly selected out of the full set ofp predictors as split candidates, and the best split within those m predicto"
8716,unknown,"the prediction of all the regression forest trees. Random forests are a great option, but they are not interpretable when bagging and using random subspaces. Thus, quantifying the importance of each variable can be difficult. There are two popular approaches which quantify this importance. One approach is to run a loop over each tree in the forest to determine the significance of each feature vari"
8717,unknown,"the improvement in the chosen loss criterion, such as accuracy or the Gini index, is calculated for each split. Each improvement is then summed for every tree in the forest, indicating the significance of xj. Another approach is to use out-of-bag (OOB) error estimates, which can be computed via bagging to determine the significance of each feature xj, where j = 1, . . . , p.36 For each tree, the O"
8718,unknown,"(i.e. the observations that were not included in the bootstrap sample used to train that tree) are used to evaluate predictive accuracy. To assess the importance of xj, the entries in the j-th column of the OOB feature matrix, denoted xoob, are randomly permuted, breaking the association between xj and the rest of the variables. The modified matrix, xoob∗, is then passed through the tree and the r"
8719,unknown,"change in predictive accuracy is computed. This decrease in accuracy caused by the permutation is averaged across all trees, providing a measure of the importance of feature xj. Random forests inherit the advantages of bagging and trees. Thus, tuning is rarely needed. How- ever, they are challenging to implement and have issues with extrapolation. Note: tuning for random forests involves adjusting"
8720,unknown,"Note: tuning for random forests involves adjusting hyperparameters, such as the number of trees or features considered, at each split to optimise predictions. 5.1.4 Multivariate Regression Trees CARTs are limited to single-response regression, making them unsuitable for modelling relationships between multiple correlated responses. Multivariate Regression Trees (MRTs) generalise CARTs by modelling"
8721,unknown,"modelling all responses simultaneously and hence accounting for multiple correlated responses. 38 Like CARTs, MRTs also build decision trees by recursively splitting the data into binary parti- tions along predictor variable thresholds, in order to minimise variability across the response matrix. Ultimately, each split aims to cluster observations that share similar patterns across the responses. "
8722,unknown,"MRTs can also be grown until each leaf contains a single observation, but this leads to overfitting. Therefore, trees are pruned using CV, typically selecting the smallest tree within one standard error of the minimum cross-validated error. 39 This is known as the 1-standard error rule. However, unlike CARTs, MRTs reveal how relationships between predictors and responses vary across different regi"
8723,unknown,"returns a vector of predicted responses for each input. MRTs also adapt the impurity criterion from CARTs to account for all response variables simul- taneously, evaluating splits by minimising the total variation across the multivariate response. 39 The impurity measure in MRTs minimises this total variance, and it is the total sum of squares of the 37 CHAPTER 5. RANDOM FORESTS responses around t"
8724,unknown,"responses around the multivariate mean at each node: impurity = nX i=1 mX j=1 (yij − ¯yj)2 , where yij represents the j-th response for the i-th sample, and ¯yj is the mean response at the node. 34 Because impurity minimisation is performed over the joint space, MRTs naturally cluster correlated responses, and thus, preserving their multivariate response structure. However, MRTs do not explicitly "
8725,unknown,"model changes in the covariance structure across predictor values. MRTs are also highly interpretable, with each node representing a split on a predictor variable, and each leaf corresponding to a distinct cluster. This makes it straightforward to visualise local interactions and the influence of predictors. However, different visualisation tools are needed for MRTs to interpret their results. For"
8726,unknown,"of each response within that node, helping to visualise the multivariate outcomes and understand the impact of splits on all responses jointly. 34 In Figure 5.2, the left panel visualises the MRT setup: a response matrix, with two continuous response variables Y1 and Y2, is regressed on a set of predictors X1, X2, X3, shown as distinct columns in the predictor matrix. The right-hand panel displays"
8727,unknown,"in the predictor matrix. The right-hand panel displays the fitted MRT, where each internal node represents a binary split on one of the predictors. Each terminal node contains a bar plot showing the mean values of Y1 and Y2 for the observations within that group. This helps assess how each split affects the joint distribution of responses. Figure 5.2: MRT Splitting. 38 Note: if a terminal node con"
8728,unknown,"Note: if a terminal node contains fewer bars than the number of responses, this indicates that one or more response variables have a mean of zero within that group. In other words, bars are only displayed for response variables whose node-wise means are non-zero. Although MRTs improve on CARTs in MRR, they come with some drawbacks. One such drawback is the inherent instability of tree-structured p"
8729,unknown,"optimisation.34 Multivariate random forests can be used instead, to resolve this issue. 5.1.5 Multivariate Random Forests Multivariate Random Forests (MRFs), like univariate random forests, extend MRTs by constructing an ensemble of them through bootstrap and random predictor subsampling. 40 The only change from the random forest algorithm in Subsection 5.1.3 not covered by MRTs is that the bootst"
8730,unknown,"i , x∗ i )n i=1 as there are multiple responses to consider. However, while MRFs are a great extension of MRTs, they exhibit an important limitation in MRR. Although their loss function can be adjusted to implicitly consider response correlations, MRFs do not explicitly force each split to lead to a meaningful change in the response covariance structure. 38 CHAPTER 5. RANDOM FORESTS As a result, w"
8731,unknown,"As a result, when responses are highly correlated, MRFs fail to consider the correlations optimally when building trees, potentially leading to suboptimal splits. 40 Covariance Regression with Random Forests, however, forces each split to lead to a meaningful change in the response covariance structure. 5.1.6 Covariance Regression with Random Forests Covariance Regression with Random Forests (CRRF"
8732,unknown,"Covariance Regression with Random Forests (CRRFs) extend MRFs by incorporating a splitting criterion, which maximises differences in sample covariance estimates between child nodes. Note: A “child” refers to the rows selected when a decision tree splits at a node. Unlike MRFs, which focus solely on predicting the conditional mean, CRRFs explicitly account for response intercorrelation. They extend"
8733,unknown,"matrix of a multivariate response, given a set of predictors. 41 While CRRFs are designed to estimate covariance structures, they inherently compute the sample response mean at each terminal node. These mean values can be direct response estimates, allowing CRRFs to generate predictions. Let Σyi be the true conditional covariance matrix of yi (the responses), based on predictors xi, and let ΣX be "
8734,unknown,"and let ΣX be the collection of all conditional covariance matrices for n observations: ΣX = {Σyi : i = 1, . . . , n}.41 Also, let ˆΣyi be the estimated conditional covariance matrix of yi based on predictors xi, and ˆΣX be the collection of estimated conditional covariance matrices for n observations.41 First, a random forest is trained with the set of predictors X to find subgroups of observatio"
8735,unknown,"First, a random forest is trained with the set of predictors X to find subgroups of observations with similar covariance matrices of Y, using decision trees to uncover the data structures. These trees are built with a splitting criterion that will be defined later. 41 The tree-growing process follows the CART algorithm, which aims to obtain subgroups of observations with distinct covariance matric"
8736,unknown,"CART algorithm, which aims to obtain subgroups of observations with distinct covariance matrices.42 Hence, a customised splitting rule at each node is used to increase the difference in covariance matrices between two child nodes in the tree. 43,44,45,46 Before defining this splitting rule, let us first set up some notation. ˆΣ L is the sample response covariance matrix estimate, so ˆΣyi of the le"
8737,unknown,"ˆΣ L = 1 nL − 1 X i∈tL (yi − ¯YL)(yi − ¯YL)⊤, where tL is the set of indices (or the positions within the original data) of the observations in the left node, nL is the left node size, and: ¯YL = 1 nL X i∈tL yi.41 The estimate of the sample response covariance matrix for the right node, ˆΣ R , is calculated similarly, where tR is the set of indices of observations in the right node and nR is its s"
8738,unknown,"The splitting criterion of CRRF is: √nLnR × d(ˆΣ L , ˆΣ R ), (5.1) where d(ˆΣ L , ˆΣ R ) is the Euclidean distance between the upper triangular part of the two matrices, and is computed as follows: d(D, E) = vuut qX i=1 qX j=i (Dij − Eij)2, where Dq×q and Eq×q are symmetric matrices. 41 The best split among those possible is the one that maximises Equation 5.1. This split is what facilitates CRRF "
8739,unknown,"a response covariance-based splitting criterion. 39 CHAPTER 5. RANDOM FORESTS The final covariance matrices are estimated using random forests. For a new observation, nearest neighbour observations are used to estimate the response mean and final covariance matrix, ensuring both the predicted value and its uncertainty are captured. 41 This set of observations is called the Bag of Observations for "
8740,unknown,"of Observations for Prediction (BOP). For a new observation x∗, the set of nearest neighbour observations is formed with the oob obser- vations.47,48 The BOPoob for a new observation is: BOPoob(x∗) = B[ b=1 Ob(x∗), where B is the number of trees and Ob(x∗) is the set of oob observations in the same terminal node as x∗ in the bth tree, and each tree is built with a selected random sub-sample. 41 Af"
8741,unknown,"After training the random forest with the new split for a new observation x∗, we form BOPoob(x∗), which is key for response prediction. 41 The response mean and covariance matrix are then estimated using the sample mean and covariance matrix of the observations in BOPoob(x∗), respectively: ˆY∗ = 1 |BOPoob(x∗)| X i∈BOPoob(x∗) yi, ˆΣ∗ Y = 1 |BOPoob(x∗)| −1 X i∈BOPoob(x∗) (yi − ˆY∗)(yi − ˆY∗)⊤, where"
8742,unknown,"Y represents the estimated conditional covariance matrix of the responses given x∗. 5.2 Application 5.2.1 Small-Scale Example We will work with this familiar dataset again: X1: Hours Studied X2: Time Spent on Papers Y1: Math Scores Y2: Science Scores 5 2 78 80 7 3 85 79 8 4 88 88 3 1 65 70 10 5 92 74 Table 5.1: Study Time vs. Exam Scores Each tree is trained on a bootstrap sample, which is obtaine"
8743,unknown,"Each tree is trained on a bootstrap sample, which is obtained by randomly drawing observations with replacements from the original dataset. For the provided dataset, a possible bootstrap sample could be, for example, rows 1,1,2,4 and 5. Here, you can see that row 1 has been repeated, and row 3 has been left out. Another possible bootstrap sample is just getting each row once, so rows 1,2,3,4 and 5"
8744,unknown,"This corresponds to: X =   (5, 2, 78, 80) (7, 3, 85, 79) (8, 4, 88, 88) (3, 1, 65, 70) (10, 5, 92, 74)   . 40 CHAPTER 5. RANDOM FORESTS After constructing a bootstrap sample, recursive partitioning is performed by selecting the split that maximises the scaled Euclidean distance between the covariance matrices of the left and right child nodes. The Euclidean distance is computed as:"
8745,unknown,"d(ΣL, ΣR) = vuut qX i=1 qX j=i (ΣL ij − ΣR ij)2, (5.2) where ΣL and ΣR are the empirical covariance matrices of the left and right nodes, respectively. In reality, the CRRF will evaluate all possible splits and compute Equation 5.1 but for simplicity, let us assume the split that maximises this equation is X1 < 7. It is important to understand how Equation 5.1 is computed for any split. First, let"
8746,unknown,"splitting the data into 2 child nodes: left and right. The left child node contains the observations and responses of: XL = "" 5 2 78 80 3 1 65 70 # , and the right child node contains: XR =   7 3 85 79 8 4 88 88 10 5 92 74  . For each node, the mean response is computed as: ¯YL = 1 2 X YL = h 71.5 75 i , ¯YR = 1 3 X YR = h 88.33 80 .33 i . The covariance matrices are then estimated using: ΣL"
8747,unknown,"ΣL = 1 nL − 1 X i∈L (Yi − ¯YL)(Yi − ¯YL)⊤, ΣR = 1 nR − 1 X i∈R (Yi − ¯YR)(Yi − ¯YR)⊤. The computed covariance matrices for the left and right nodes are: ΣL = "" 142.25 105 .25 105.25 97 .25 # , ΣR = "" 41.67 −10.67 −10.67 21 # . Applying the Euclidean distance formula to these matrices gives: d(ΣL, ΣR) = p (142.25 − 41.67)2 + 2(105.25 − (−10.67))2 + (97.25 − 21)2 = 206.89. Finally, the scaled Euclid"
8748,unknown,"Finally, the scaled Euclidean distance is: √nLnR × d(ˆΣ L , ˆΣ R ) = 206.89 √ 2 × 3 = 506.77 Since CRRF selects the split that maximises this distance, we can see the split at X1 = 7 does lead to a large Euclidean distance between the response sample covariance matrices of each child node. When predicting, each new observation is assigned to a terminal node based on its feature values. The predict"
8749,unknown,"The prediction is computed using the BOP within the same node. For a new test point x∗ = (6, 3), which falls into the left node, the relevant BOP observations here are: BOPoob(x∗) = {(78, 80), (65, 70)}. 41 CHAPTER 5. RANDOM FORESTS The predicted mean response is then given by: ˆY∗ = 1 |BOPoob(x∗)| X i∈BOPoob(x∗) Yi = "" 71.5 75 # . This predicted mean response will then be used to compute the ANRM"
8750,unknown,"The covariance structure of the predictions can then be estimated as follows: ˆΣ∗ Y = 1 |BOPoob(x∗)| −1 X i∈BOPoob(x∗) (Yi − ˆY∗)(Yi − ˆY∗)⊤. Here, |BOPoob(x∗)| = 2. Therefore, ˆΣ∗ Y is: ˆΣ∗ Y = "" 142.25 105 .25 105.25 97 .25 # . In this case, the prediction ˆY∗ and its covariance matrix ˆΣ∗ Y correspond to ¯YL and ΣL respectively, because only one tree has been made. For CRRF, many trees are form"
8751,unknown,"because only one tree has been made. For CRRF, many trees are formed from multiple bootstraps because each bootstrap leads to different splits being determined to maximise the Euclidean distance. As a result, the output ˆY∗ will be adjusted when more and more trees are added. 5.2.2 Code Explanation No changes were made to the dataset beyond the exploratory data analysis presented in Chapter 2, as"
8752,unknown,"No changes were made to the dataset beyond the exploratory data analysis presented in Chapter 2, as CRRF is a non-parametric model. Therefore, it does not require preprocessing steps such as frequency encoding (unlike MRLR, RRRR and MRCE). CRRFs come with a pre-set package in R called CovReg, which enables mean extraction, but there were still some necessary considerations to be made. To prevent o"
8753,unknown,"To prevent over-fitting, explicit stopping criteria were implemented in the tree growth process. A node was set as terminal if either the depth limit of the tree was reached, depth = 0, or the number of samples in the node fell below the threshold, n <5. In these cases, the node retained the mean and covariance of the responses, rather than splitting further. This procedure stopped the tree from b"
8754,unknown,"becoming too large and avoided overfitting. Furthermore, to evaluate the performance of CRRF, a 5-fold CV was implemented using the ANRMSE. The data was randomly split into 5 folds of equal size, where each fold was used as a validation set once, and the remaining 4 folds formed the training set. This ensured that every observation was used in training and validation. At the end of the CV, the fin"
8755,unknown,"test folds was used to measure CRRF fit. Parallelisation was also implemented within the CRRF because it uses multi-threading to speed up training by running multiple trees concurrently rather than sequentially. Parallel computing makes programs and processes run faster as more CPUs are used. 49 5.2.3 Results The CRRF model achieved an ANRMSE of 0.5238, the lowest value thus far. This value was es"
8756,unknown,"The CRRF model achieved an ANRMSE of 0.5238, the lowest value thus far. This value was especially surprising given that it did not even consider interaction and polynomial terms. These terms were not explicitly tested as RFs already capture non-linear relationships and interactions through their splits. The CRRF model was also analysed in relation to how it determined predictor feature importance "
8757,unknown,"in modelling the 2 responses, ROE and sustainability score. Note: since MRLR, MRCE, and RRRR return coefficients, they already provide direct insight into 42 CHAPTER 5. RANDOM FORESTS predictor-response relationships. In contrast, CRRFs and XGBoost, which will be covered in the next chapter, are non-linear ensemble methods that do not return interpretable coefficients, so feature importance metric"
8758,unknown,"importance metrics were used to support their interpretation. However, the primary focus of this report remains on evaluating model prediction accuracy. Figure 5.3: CRRF Feature Importance For ROE, the CRRF model gave the highest feature importance toprice cash flow ratio, dividend yield factor and historical earnings growth. Therefore, these features led to the most signifi- cant ROE-related cova"
8759,unknown,"cant ROE-related covariance changes. In contrast, risk rating and asset cash had minimal influ- ence, implying they did not meaningfully alter ROE-related covariances. For the sustainability score, price cash flow ratio again had the highest feature importance, reinforcing its role in separating subgroups with differing response covariances. equity size score became more relevant here, which impli"
8760,unknown,"more than for ROE. historical earnings growth and dividend yield factor remained relevant (from ROE) but with a smaller effect compared to price cash flow ratio. sales growth remained as important for sustainability score as it was for ROE, andrisk rating still had minimal importance. Overall, the consistent importance of price cash flow ratio highlights its strong role in dictating which equity f"
8761,unknown,"which equity funds should be invested in long-term, based on the CRRF. Meanwhile, features with minimal covariance impact across both responses, like risk rating and asset cash, should not be prioritised at all. Although CRRF has modelled this dataset well, they have some drawbacks. First, they are compu- tationally intensive because they need to calculate covariance matrices at each split. Second"
8762,unknown,"CV mitigates this somewhat, they are prone to over-fitting with irrelevant features. XGBoost can deal with these issues because it uses approximate tree learning and has built-in regularisation. 43 Chapter 6 XG Boost This chapter investigates Extreme Gradient Boosting by first explaining its principles in the single- response case, including how it improves upon traditional gradient boosting. This"
8763,unknown,"response case, including how it improves upon traditional gradient boosting. This method is then extended to the multivariate case using the Cholesky Decomposition to decorrelate the responses prior to fitting. The resulting model is then evaluated on the equity fund dataset to assess its performance. 6.1 Theory Now, we move on to Extreme Gradient Boosting, another non-parametric method, which dif"
8764,unknown,"Now, we move on to Extreme Gradient Boosting, another non-parametric method, which differs from Random Forests as it builds trees sequentially rather than independently. While Random Forests rely on bagging to reduce variance, XGBoost fits each new tree to the negative gradients of the loss function with respect to the current model’s predictions. This iterative process forms an ensemble, refining"
8765,unknown,"refining predictions at every step. 6.1.1 Introduction Gradient tree boosting builds a model by sequentially adding regression trees to minimise a predefined loss function. Each new tree is trained on the negative gradient of the current model, enabling the ensemble to iteratively refine its predictions. 50 Formally, for a dataset with n observations and m features, we write: D = {(xi, yi)}, xi ∈ "
8766,unknown,"The prediction for each input, xi, is the sum of K regression trees: ˆyi = ϕ(xi) = KX k=1 fk(xi), f k ∈ F, (6.1) where each fk is an individual regression tree in the ensemble, and F is the space of all possible regression trees. 50 One immediate point of concern is that the estimate will keep increasing as the number of regression trees increases. However, each function fk is not an independent p"
8767,unknown,"term trained to minimise the loss function with respect to the ensemble’s current prediction. For a general loss function ℓ(yi, ˆyi), this correction corresponds to the negative gradient, gi, of the loss with respect to the current prediction: fk(xi) ≈ −∂ℓ(yi, ˆyi) ∂ˆyi ˆyi=ˆy(t−1) i = −gi, where the superscript (t−1) denotes the prediction from the previous iteration of the boosting process, i.e."
8768,unknown,"i.e., the ensemble after t − 1 trees have been added. 44 CHAPTER 6. XG BOOST In the case of the squared error loss function: ℓ(yi, ˆyi) = 1 2(yi − ˆyi)2, (6.2) the negative gradient simplifies to the residual yi − ˆy(t−1) i . The space of all possible regression trees can be formally written as: F =  f(x) = wq(x) q : Rm → {1, . . . , T}, w∈ RT , (6.3) where T is the number of leaves in the tree, "
8769,unknown,"leaf index, and w ∈ RT is a vector of leaf weights.50 Each weight wj corresponds to a real-valued score assigned to leaf j. Therefore, the leaf weights are the prediction scores returned by the tree. When an input xi is passed through the tree, q maps it to a specific leaf q(xi), and the tree returns the corresponding score wq(xi) as its prediction. Unlike classification trees, which assign a disc"
8770,unknown,"continuous values, often referred to as prediction scores, at each leaf. These values are learned during training and represent the model’s prediction for any inputs that fall into their corresponding leaves. Building on Equations 6.1 and 6.3, each regression tree has its own structure, q(k), and set of leaf scores, w(k).50 Hence, the final model prediction is the sum of these scores across all tr"
8771,unknown,"ˆyi = KX k=1 w(k) q(k)(xi). Extreme Gradient Boosting, otherwise known as XGBoost, improves gradient boosting by including regularisation explicitly in the loss function to address over-fitting and improve model predictions. Its objective function is: L(ϕ) = nX i=1 l(yi, ˆyi) + TX t=1 Ω(ft), (6.4) where l represents the loss function, and Ω(ft) is a regularisation term.50 The loss function in XGBo"
8772,unknown,"regression can take various forms, such as the log loss. For this report, however, the unscaled squared error loss was used due to its simplicity in gradient derivation (see Equation 6.2). Note: the omission of the 1 2 has no impact on model performance, as the constant does not affect the direction of the gradient or the speed of convergence. XGBoost uses regularisation to help control model comp"
8773,unknown,"Ω(ft) = γT + 1 2λ TX j=1 w2 j , where T is the number of leaves in the tree, wj are the leaf weights, γ is the pruning parameter which penalises additional leaves, and λ controls the regularisation of leaf weights. 50 The pruning parameter can be selected automatically via CV or hyper-parameter tuning. However, it is ultimately set by the user. See Equation 6.5 for more on this parameter. These we"
8774,unknown,"These weights are what XGBoost aims to determine, as they comprise the prediction values as- signed to each leaf. XGBoost computes them using the Approximate Greedy Split-Finding Algorithm (see Subsection 6.1.2 for more on this). XGBoost also incorporates shrinkage and column subsampling to reduce overfitting further. Shrink- age scales the leaf weights of newly added trees by a factor of η ∈ (0, "
8775,unknown,"This reduces the influence of each individual tree, giving subsequent trees more room to improve the model incrementally.50 45 CHAPTER 6. XG BOOST Column subsampling, in which only a random subset of features is considered when constructing each tree, not only acts as a form of regularisation but also improves parallel algorithm computation. 50 Note: this algorithm was implemented in the CRRF (see"
8776,unknown,"Note: this algorithm was implemented in the CRRF (see Subsection 5.2.2). XGBoost optimises the objective function using a second-order Taylor expansion. With constant terms omitted, the loss function at iteration t is approximated as: L(t) ≈ nX i=1  gift(xi) + 1 2hift(xi)2  + Ω(ft), where gi = ∂ˆy(t−1) i l(yi, ˆy(t−1) i ) and hi = ∂2 ˆy(t−1) i l(yi, ˆy(t−1) i ) are the first and second-order gra"
8777,unknown,"i l(yi, ˆy(t−1) i ) are the first and second-order gradients of the loss function, respectively. 50 To construct decision trees, XGBoost evaluates potential binary splits using the Greedy Algorithm for Split Finding. This algorithm involves maximising a gain function: Gain = 1 2  G2 L HL + λ + G2 R HR + λ − (GL + GR)2 HL + HR + λ  − γ. 50 (6.5) Each candidate split divides the training data into"
8778,unknown,"Each candidate split divides the training data into two subsets based on a feature threshold. The instances satisfying the condition (e.g. xj ≤ s) are assigned to the left node, while those that do not are assigned to the right node. Here, GL and HL are the sums of first and second-order gradients for the left child ( L) node, GR and HR are the corresponding sums for the right child ( R) node, λ i"
8779,unknown,"γ is the pruning parameter. 50 Note: A “child” refers to the rows selected when a decision tree splits at a node. Equation 6.5 means that a node will only be split if the gain in the objective function exceeds the pruning parameter, γ. Higher values of γ encourage shallower trees by penalising complexity. In simplified form, the Gain can be written as: Gain = 1 2 [SL + SR − SP ] − γ, where: SL = G"
8780,unknown,"2 [SL + SR − SP ] − γ, where: SL = G2 L HL + λ, S R = G2 R HR + λ, S P = (GL + GR)2 HL + HR + λ. Note: Each component within the Gain’s bracket is called the (similarity) score and SP is the score for the parent node of the new node being derived. 6.1.2 Iterative Steps of XGBoost As outlined in Chen and Guestrin (2016), the exact Greedy Algorithm occurs by iterating over each feature dimension k, "
8781,unknown,"feature dimension k, ranging from 1 to m, where m is the total number of predictors. For each feature, potential split points are evaluated by sorting the training instances based on their corresponding feature values. During this process, the cumulative sums of gradients and Hessians for the left child node, denoted GL and HL, are initialised to zero and updated at each step as: GL ← GL + gj, H L"
8782,unknown,"GL ← GL + gj, H L ← HL + hj. The corresponding values for the right child are computed as GR = G −GL and HR = H −HL, where G and H are the total sums over the parent node. The quality of each split is evaluated using the gain, which measures the reduction in loss. At each 46 CHAPTER 6. XG BOOST step, the algorithm compares the gain of the current split to the best one found so far: Gainnew = SL + "
8783,unknown,"Gainnew = SL + SR − SP , and retains the maximum. Note: the pruning parameter γ is only applied after split selection, not during the gain evaluation. After evaluating all candidate splits, the algorithm selects the one with the highest gain. If no split yields a positive gain, the node remains a leaf. This approach ensures that each decision tree grows in a way that minimises residual error at ea"
8784,unknown,"grows in a way that minimises residual error at each step, leading to more accurate predictions. 50 The exact Greedy Algorithm is ideal for XGBoost, but due to its computational cost, an approx- imate greedy algorithm is used in practice. This method is similar to the prior one, but with some adjustments which are explained below, again from Chen and Guestrin (2016). The approximate greedy algorit"
8785,unknown,"The approximate greedy algorithm begins by iterating over each feature dimension k = 1, . . . , m, where m is the total number of features. For each feature k, a set of candidate splits Sk = {sk1, sk2, . . . , skl} is proposed by dividing the feature values into percentiles. This ensures the split proposals are distributed evenly across the feature range, capturing key points that are likely to le"
8786,unknown,"be generated globally or locally: in the global setting, split points are selected once per tree and reused at each node, while in the local setting, new split points are computed independently for each node. After generating the candidate splits, the algorithm evaluates their quality. For each proposed split sk,v in feature k, the gradients and Hessians of instances falling between two consecutiv"
8787,unknown,"points are accumulated. The gradients and Hessians are then computed as: Gkv ←= P j∈{j|sk,v≥xjk>sk,v−1} gj Hkv ←= P j∈{j|sk,v≥xjk>sk,v−1} hj where Gkv denotes the sum of gradients, and Hkv the sum of Hessians over the interval between sk,v−1 and sk,v. This process approximates the exact split-finding procedure by evaluating only a limited subset of potential split points. Once the gradients and He"
8788,unknown,"Once the gradients and Hessians have been computed for all proposed splits, the algorithm selects the split that maximises the gain. This follows the same logic as the exact greedy method but limits evaluation to the proposed split points, significantly reducing computational cost. 50 The same procedure described earlier is then applied, but restricted to the proposed splits. While this approximat"
8789,unknown,"to large datasets. After identifying the optimal split, the corresponding leaf weight is computed as: wj = − Gj Hj + λ, where Gj is the sum of gradients for all instances in the leaf, Hj is the sum of Hessians for all instances in the leaf, and λ is the regularisation parameter. 50 Each instance’s prediction is updated as follows: ˆy(t) = ˆy(t−1) + ηwj, where ˆy(t−1) represents the previous predic"
8790,unknown,"η is the learning rate - which controls the step size of updates. 50 47 CHAPTER 6. XG BOOST 6.1.3 Cholesky Decomposition Although a prior distribution is not required to apply Cholesky decomposition, assuming a multi- variate normal distribution (MVN), which can be done on the equity fund dataset, helps satisfy its requirements, which are a positive definite symmetric matrix. The probability densi"
8791,unknown,"MVN distribution is given by: f(Y|µY, ΣY) = 1p (2π)D|ΣY| exp  −1 2(Y − µY)⊤Σ−1 Y (Y − µY)  , where µY ∈ RD represents a vector of conditional means, ΣY is the positive definite symmetric D ×D response covariance matrix, D is the number of responses, and | · |denotes the determinant. 51 In the bivariate case (D = 2), the local response covariance matrix for instance i, denoted Σiy, is: Σiy = "" σ2"
8792,unknown,"Σiy = "" σ2 i,1(x) ρi(x)σi,1(x)σi,2(x) ρi(x)σi,1(x)σi,2(x) σ2 i,2(x) # , where, for each i = 1 , . . . , N, ρi(x) is the conditional correlation between the two responses, the diagonal entries represent the variances, and the off-diagonal entries represent the covariances. 51 To ensure positive definiteness of the response covariance matrix, ΣY, a common and efficient approach is to use the Cholesk"
8793,unknown,"approach is to use the Cholesky decomposition, which factorises the matrix as follows: ΣY = LL⊤, where L ∈ RD×D is a lower triangular matrix. 51 This decomposition guarantees positive definiteness as long as all diagonal elements, ℓii, of L are strictly positive. The D(D − 1)/2 off-diagonal elements, ℓij (for j < i), may take any value in R. The Cholesky decomposition uses the following algorithm "
8794,unknown,"1. Initialise L as an n × n zero matrix. 2. For each row i (from 1 to n): • Compute the diagonal element Lii: Lii = vuutAii − i−1X k=1 L2 ik • For each column j (below diagonal, j > i): Lji = Aji − Pi−1 k=1 LjkLik Lii 3. Return L. Cholesky decomposition can be used to remove intercorrelation between response variables by apply- ing the transformation L−1Y⊤, which produces decorrelated responses. 5"
8795,unknown,"ing the transformation L−1Y⊤, which produces decorrelated responses. 52 This decorrelation allows XGBoost to be applied independently to each transformed response. After prediction, the original correlated response space is recovered by reapplying L, which reintroduces the covariance structure. Note: There does exist an experimental version of XGBoost, Multi-Output XGBoost, which natively supports"
8796,unknown,"supports MRR. However, its methodology still needs to be fleshed out. For more details on this implementation, see the official XGBoost multi-output regression demo here. 48 CHAPTER 6. XG BOOST 6.2 Application 6.2.1 Small-Scale Example We will work with the familiar dataset, evaluating Mathematics and Science Scores: X1: Hours Studied X2: Time Spent on Papers Y1: Math Scores Y2: Science Scores 5 2"
8797,unknown,"7 3 85 79 8 4 88 88 3 1 65 70 10 5 92 74 Table 6.1: Study Time vs. Exam Scores We have: X = "" 5 7 8 3 10 2 3 4 1 5 #⊤ , Y = "" 78 85 88 65 92 80 79 88 70 74 #⊤ , ¯Y = h 81.6 78 .2 i⊤ . First,calculate the sample response covariance matrix, ˆΣY: ˆΣY = 1 n − 1(Y − ¯Y)⊤(Y − ¯Y) = "" 112.30 37 .85 37.85 46 .20 # , This matrix is symmetric and positive definite because its (1,2) and (2,1) entries are the"
8798,unknown,"has positive eigenvalues. Therefore, Cholesky decomposition can be applied. Next comes the Cholesky Decomposition of the response covariance matrix. This subsection is for clarity, so for ease of computation, let us show Cholesky Decomposition on a “nicer” matrix: A = "" 4 2 2 3 # . We aim to find a lower triangular matrix L such that: A = LL⊤, L = "" L11 0 L21 L22 # . The first step is to compute L"
8799,unknown,"Lii = vuutAii − i−1X k=1 L2 ik ⇒ L11 = p A11 = √ 4 = 2 ⇒ L = "" 2 0 L21 L22 # . Next, compute L21. For off-diagonal elements: Lji = Aji − Pi−1 k=1 LjkLik Lii ⇒ L21 = A21 L11 = 2 2 = 1 ⇒ L = "" 2 0 1 L22 # . 49 CHAPTER 6. XG BOOST Finally, compute L22, going back to our diagonal element formula: Lii = vuutAii − i−1X k=1 L2 ik ⇒ L22 = q A22 − L2 21 = √ 3 − 1 = √ 2 ⇒ L = "" 2 0 1 √ 2 # . See Subsection "
8800,unknown,"Applying this algorithm to ΣY gives: ΣY = LL⊤ = "" 10.60 0 3.57 5 .78 #"" 10.60 3 .57 0 5 .78 # . Now, we can decorrelate the responses using L−1: ˜Y = L−1Y⊤ = "" 7.36 8 .02 8 .30 6 .13 8 .68 9.29 8 .71 10 .09 8 .32 7 .43 #⊤ . Here, the off-diagonal elements of the covariance matrix of the transformed responses are zero, and it leaves the identity matrix, I2: Cov( ˜Y) ≈ "" 1 0 0 1 # . Now that the res"
8801,unknown,"response. Let us apply XGBoost, with a squared error loss function, to response ˜Y1 (the first column of the decorrelated responses), Mathematics Scores against the predictors X1 and X2. XGBoost starts by initialising predictions for all samples. The initial prediction is typically 0.5, but let us start here instead by taking the mean of ˜Y1: 7.36 + 8.02 + 8.30 + 6.13 + 8.68 5 = 38.49 5 = 7.70. (6"
8802,unknown,"Thus, the initial prediction for all samples is: ˆY (0) 1 = h 7.70 7 .70 7 .70 7 .70 7 .70 i⊤ . Next, the residuals are calculated as the difference between the actual values ( ˜Y1) and the initial predictions ( ˆY (0) 1 ): r1 = ˜Y1 − ˆY (0) 1 = h 7.36 8 .02 8 .30 6 .13 8 .68 i⊤ − h 7.70 7 .70 7 .70 7 .70 7 .70 i⊤ = h −0.34 0 .32 0 .60 −1.57 0 .98 i⊤ . The residuals are the errors in the initial p"
8803,unknown,"The residuals are taken because they are half the gradients when using the squared error loss function. The Gradients, gi, and Hessians, hi, are needed because they are critical in evaluating the Score and computing the Gain. The gradients are calculated as: gi = ∂L ∂ ˆYi = ∂ ∂ ˆYi nX i=1 ( ˆYi − Yi)2 = 2(Yi − ˆYi) = 2ri, where the summation vanishes because we are looking at each ith element when"
8804,unknown,"50 CHAPTER 6. XG BOOST From the residuals, the gradients are: g = h −0.68 0 .64 1 .21 −3.13 1 .96 i⊤ . For the squared error loss, the second-order gradient (or Hessian) is: hi = ∂2L ∂ ˆY 2 i = 2. Since we are using the squared error loss function, the Hessian for all observations is: h = h 2 2 2 2 2 i⊤ Now, we train a small decision tree to predict the gradients. This starts with the parent node,"
8805,unknown,"Now, we train a small decision tree to predict the gradients. This starts with the parent node, P, which contains all the data. Each tree starts with a single leaf, and all residuals go to that leaf. 53 All that needs to be done here is to compute the score of the leaf using the exact residuals: SP = (PgP )2 PhP + λ = 0. where gP , hP are the Gradients and Hessians of the parent node, respectively"
8806,unknown,"It might be surprising that SP = 0. However, this makes sense because, as a result of this loss function, the gradients are proportional to the residuals, which always sum to zero. Note: When the gradients within a node have opposite signs, they tend to cancel each other out, reducing the numerator’s contribution in the leaf score formula. This results in a relatively small score. In contrast, whe"
8807,unknown,"score. In contrast, when the gradients are similar in direction and magnitude, the summed gradient is large, producing a higher score. 53 XGBoost will then evaluate splits after computing the corresponding scores by comparing the Gain for each split in X1 and X2 in this dataset, and finding the split which maximises said Gain. Maximising the Gain means there is a better separation of gradients int"
8808,unknown,"Iterating through all possible split points is computationally expensive. To address this, XGBoost uses a weighted quantile sketch to approximate the best split points (see Subsection A.2.7 for details). This significantly speeds up training by reducing the number of candidate splits. Now, let us show how the gain is computed for an example split: X1 < 7, which splits the data into 2 sets of predi"
8809,unknown,into 2 sets of predictors and corresponding transformed responses: X1: Hours Studied X2: Time Spent on Papers ˜Y1: Mathematics Scores r1: Residuals 5 2 7.36 -0.34 3 1 6.13 -1.57 Table 6.2: Study Time vs. Mathematics Scores for X1 < 7 X1: Hours Studied X2: Time Spent on Papers ˜Y1: Mathematics Scores r1: Residuals 7 3 8.02 0.32 8 4 8.30 0.60 10 5 8.68 0.98 Table 6.3: Study Time vs. Mathematics Scor
8810,unknown,"7 3 8.02 0.32 8 4 8.30 0.60 10 5 8.68 0.98 Table 6.3: Study Time vs. Mathematics Scores for X1 ≥ 7 Science Scores, ˆY2, have been removed as we are just modelling ˜Y1 here, and the initial residuals are placed for clarity when calculating the gradients and Hessians. 51 CHAPTER 6. XG BOOST Let us call Table 6.2 the “left” ( L) node and Table 6.3 the “right” ( R) node. From each node, the cumulative"
8811,unknown,"GL = 2X i=1 2(ri)L = −3.81, H L = 2X i=1 2 = 4, G R = 2X i=1 2(ri)R = 3.81, H R = 3X i=1 2 = 6. This is used to calculate the similarity scores for the left and right nodes with λ = 0.1: SL = G2 L HL + λ = 3.54, S R = G2 R HR + λ = 2.38. Now, compute the Gain for X1 < 7 in comparison with the parent node: Gain = SL + SR − SP = 5.93. For simplicity, assume the tree split of X1 < 7 gives the optimal"
8812,unknown,"predictions. The leaf weight differs by the child node that each row falls in, and is calculated as: wL = − GL HL + λ = 0.93, w R = − GR HR + λ = −0.62 The predictions are then adjusted using the leaf weight with learning rateη, which we set as 0.3. Here, the initial prediction was the mean, from Equation 6.6. So, the final transformed predictions are: ˆY (1) 1,L = ˆY (0) 1 + η × wL = 7.98, ˆY (1)"
8813,unknown,"ˆY (1) 1,L = ˆY (0) 1 + η × wL = 7.98, ˆY (1) 1,R = ˆY (0) 1 + η × wR = 7.51. These need to be converted back to the original response space with the original correlation structure, using: LˆY (1) 1,L and LˆY (1) 1,R. This gives the following original predictions: Left Node : ˆY (1) 1 = 84.55, Right Node : ˆY (1) 1 = 79.61. Therefore, when new observations of the form ( X∗ 1 , X∗ 2 ) pass through "
8814,unknown,"Y ∗ 1 = 84.55 if X∗ 1 < 7 and Y ∗ 1 = 79.61 if X∗ 1 ≥ 7. As more and more splits are evaluated and nodes added to the tree, different observations will produce different responses. 6.2.2 Code Explanation No changes were made to the dataset beyond the exploratory data analysis presented in Chapter 2, as XGBoost, like CRRF, is a non-parametric model. Therefore, it does not require preprocessing step"
8815,unknown,"such as frequency encoding. The primary implementation detail (which has not been covered thus far) involved usingk-fold CV. The dataset was partitioned into five folds using KFold, with shuffling enabled to ensure randomness in CV. For each fold, the empirical response covariance matrix of the target variables was computed using numpy.cov to capture interdependencies between responses. Cholesky d"
8816,unknown,"Cholesky decomposition was then applied via numpy.linalg.cholesky, producing a lower trian- gular matrix L. Its inverse, L−1, was computed using numpy.linalg.inv to decorrelate the response variables prior to model training. Each decorrelated response variable was then modelled indepen- dently using an XGBoost regression model that was implemented with XGBRegressor. The models were trained on the "
8817,unknown,"were trained on the transformed responses usingmodel.fit(X train, Y train), and predictions were generated on the validation set usingmodel.predict(X val). As the predictions were produced in the transformed space, they were mapped back to the original scale by multiplying by the Cholesky fac- tor via numpy.dot. This step ensured that the predicted responses retained their original correlation str"
8818,unknown,"structure. Finally, the adjusted predictions were used to compute the ANRMSE across all folds. 52 CHAPTER 6. XG BOOST 6.2.3 Results XGBoost achieved an ANRMSE of 0.4267, indicating an excellent fit. This was the best performance among all the models tested and highlights XGBoost’s effectiveness in modelling complex datasets. Figure 6.1: XGBoost Feature Importance The most important feature in expl"
8819,unknown,"The most important feature in explaining the ROE was the price cash flow ratio. This means that, following decorrelation, this ratio had the greatest impact in terms of accounting for variation in the response. However, due to the Cholesky transformation, some of its influence may have been partially transferred to the sustainability score when mapping back to the original response space. Other no"
8820,unknown,"Other notable contributors included the equity size score and the dividend yield factor, re- flecting their substantial role in predicting the decorrelated response. On the other hand, variables such as asset cash and fund size had relatively low feature importance, indicating that these features provided little additional information once response dependencies had been taken into account. The mos"
8821,unknown,"The most impactful feature for the sustainability score was sales growth. Unlike ROE, where a dominant predictor emerged, sustainability appeared to be influenced by a few predictors (such as holdings n stock and historical earnings growth), as indicated by its more even distribution of importance across features. The price cash flow ratio again ranked highly, similar to its impor- tance in predic"
8822,unknown,"correlation, which was separated using Cholesky decomposition. Overall, the consistent importance of price cash flow ratio highlights its strong role in dictating which equity funds should be invested in long-term, based on Cholesky-Decomposed XGBoost (this further supports the suggestion from CRRF - see Subsection 5.2.3). Meanwhile, features with minimal impact, like fund size and asset cash, sho"
8823,unknown,"impact, like fund size and asset cash, should not be prioritised. Ultimately, XGBoost proved highly effective on the equity fund dataset, but it does bring some limitations. One notable drawback is that, even though feature importance plots can be generated to ease this, interpreting XGBoost can still be challenging due to its inherent complexity. 54 53 Chapter 7 Conclusion 7.1 Summary of Findings"
8824,unknown,53 Chapter 7 Conclusion 7.1 Summary of Findings and Model Comparisons 7.1.1 Summary of Findings Table 7.1 provides the key figures generated in this report from the MRR models: Table 7.1: Model Fit Comparison on Equity Fund Dataset Model ANRMSE Model Fit Chapter 3: Linear Regression Model 1: MRLR with all predictors 0.7606 Unsatisfactory Model 2: Forward Selection 0.7606 Unsatisfactory Model 3: Ba
8825,unknown,Model 4: Bidirectional Selection 0.7606 Unsatisfactory Model 5: Bidirectional Selection with Interaction Terms 0.5613 Good Model 6: Bidirectional Selection with Non-Linear Terms 0.6893 Satisfactory Chapter 4: Shrinkage Methods Model 7: MRCE 0.7612 Unsatisfactory Model 8: RRRR 0.7510 Unsatisfactory Model 9: MRCE including Polynomial Terms 0.6781 Satisfactory Model 10: RRRR including Polynomial Term
8826,unknown,Model 11: MRCE including Interaction Terms 0.6777 Satisfactory Model 12: RRRR including Interaction Terms 0.7231 Unsatisfactory Chapter 5: Random Forests Model 13: Covariance Regression with Random Forests 0.5238 Good Chapter 6: XGBoost Model 14: Cholesky-Decomposition XGBoost 0.4267 Excellent The “Model Fit” column was defined based on the ANRMSE criterion set out in Subsection 3.2.1. 54 CHAPTER 
8827,unknown,"54 CHAPTER 7. CONCLUSION 7.1.2 Model Comparisons The MRLR model using all predictors performed poorly, and all the stepwise selection methods (for- wards, backwards and bi-directional) failed to improve performance significantly. Backward selection performed the worst, suggesting that eliminating variables here led to worse model performance. However, the introduction of interaction terms, alongsi"
8828,unknown,"huge improvement, which resulted in a “Good” model fit. Similarly, the addition of non-linear terms, also selected through bi-directional stepwise selection, improved upon the MRLR model (with just the standard predictors), and achieved a “Satisfactory” fit. The shrinkage methods aimed to improve regression stability by using penalisation techniques. However, the MRCE and RRRR models, with the reg"
8829,unknown,"However, the MRCE and RRRR models, with the regular features, performed similarly to their coun- terparts in standard MRLR, again returning an “Unsatisfactory” fit. Introducing polynomial terms in MRCE improved performance, but for RRRR, this was less ef- fective. While adding interaction terms improved MRCE, but did not improve RRRR as much. The performance of shrinkage methods was also noteworth"
8830,unknown,"The performance of shrinkage methods was also noteworthy. This is because, despite being specif- ically designed to handle multiple responses, particularly under multicollinearity or high-dimensional settings, both MRCE and RRRR performed as poorly as, or even worse than, the MRLR models. This can be attributed to the pre-processing of the dataset: after pre-processing (see Chapter 2), the predict"
8831,unknown,"predictors were not highly correlated, and the data was not high-dimensional ( p = 14, n= 1269). In this case, the advantages that regularisation and rank reduction bring are less impactful. Tree-based models significantly improved upon linear and shrinkage-based methods. The CRRF model was significantly better than previous models and was evaluated as a “Good” fit model. Mean- while, the XGBoost "
8832,unknown,"while, the XGBoost gave further improvements, being the only “Excellent” model. The strong performance of XGBoost and CRRF, particularly in comparison to the limited perfor- mance of the models in Chapters 3 and 4 with only the given features, indicates that the equity fund responses, sustainability score and ROE, are driven more by conditional and non-linear interactions among predictors. This no"
8833,unknown,among predictors. This notion is further supported by the improved performance of every model in Chapters 3 and 4 with interaction or non-linear terms introduced. Mardia’s test also supports this through its borderline p-values of 0.0732 for skewness and 0.0862 for kurtosis (see Subsection 2.3.5 for further details). While these values are not low enough to reject the null hypothesis of multivaria
8834,unknown,"normality at standard significance levels, they do indicate a potential deviation from normality. This means that the response structure is unlikely to be adequately captured by linear models with additive assumptions (like MRLR, MRCE and RRRR), thereby justifying the use of more flexible models (like Cholesky-Decomposed XGBoost and CRRFs), that can consider extra predictor effects. Overall, the f"
8835,unknown,"Overall, the findings suggest that the most effective modelling approaches are those that accom- modate flexible, non-linear and interaction effects, as opposed to approaches based on rigid parametric or distributional assumptions. 7.2 Challenges and Limitations This report faced a few different challenges. The first of which was finding an appropriate dataset to evaluate these models. This was a "
8836,unknown,"to evaluate these models. This was a challenge because it involved finding a dataset that satisfied all the assumptions underpinning each MRR model. In particular, priority was given to datasets with independent observations, multivariate normality between responses, full rank in the predictor matrix, and a moderate degree of correlation between response variables. These criteria ensured that the "
8837,unknown,"the models could be appropriately applied and compared. In retrospect, several limitations of the equity fund dataset itself became clearer through the modelling process. Although the dataset had a reasonably large number of observations, the number of predictors, p = 14, limited the potential benefits of the shrinkage methods used. 55 CHAPTER 7. CONCLUSION Imputing missing data presented another "
8838,unknown,"Imputing missing data presented another challenge. In this report, linear model imputation was used for numerical variables and random forests for the categorical ones. While these approaches are more sophisticated than simple imputation methods such as taking the mean or mode, they assume that data are missing at random, that is, the missingness depends only on observed values. This assumption ma"
8839,unknown,"may not hold, particularly if some variables are missing not at random and influenced by unobserved or sensitive information. In retrospect, a formal assessment of the missingness mechanism should have been carried out. For example, Hotelling’s multivariatet-test or absolute standardised mean differences could have been used to test if the distributions of observed data differed between missing an"
8840,unknown,"addition, more advanced imputation techniques, such as multiple imputation via chained equations, could have been used to deal with the missing data. Another limitation in this report was the relatively low degree of feature engineering. For example, frequency encoding was used for the equity category feature, which treated each fund category as a separate level. However, many of these categories "
8841,unknown,"by region or (e.g., Sweden, Switzerland) sector (e.g., Consumer Goods). Grouping them on this basis might have improved performance in every model. While some models incorporated interaction terms and polynomial features, a more systematic feature engineering method, such as using principal component analysis, could have improved model performance or further uncovered complex patterns. With more t"
8842,unknown,"context of the dataset, this could have been explored further. Finally, one of the most significant limitations was the interpretability of more complex models, particularly those in later chapters. While highly accurate, models such as XGBoost are often regarded as “black boxes”, as their internal decision structures are not easily interpretable.55 However, this lack of interpretability does not "
8843,unknown,"inform decision-making, even if the model’s internal mechanisms are not clear. 7.3 Report Overview and Future Work This report began with MRLR, extending SRLR to cover multiple responses. Upon examining MRLR, it was combined with stepwise selection and sequential MANOVA to identify relevant predictors. The analysis then progressed to shrinkage-based methods, namely MRCE and RRRR, both of which int"
8844,unknown,"analysis then progressed to shrinkage-based methods, namely MRCE and RRRR, both of which intro- duced regularisation to improve model stability. Following this, tree-based approaches were explored through CRRFs, which aimed to maximise Euclidean distance between nodes during splits. Finally, XGBoost, which is natively single-response, was adapted to MRR via the Cholesky-Gaussian decom- position. T"
8845,unknown,"XGBoost models on each response. While this report has covered several MRR models on a single dataset, there are a few areas for future work. One area could be to apply these models across a lot of different datasets to understand how performance varies under different conditions, such as varying levels of multicollinearity, response correlation, and sample size. This could help to clarify further"
8846,unknown,"each model and why certain models perform better in specific scenarios. Another area for future work could involve using more MRR models on this dataset. For example, neural networks can be extended to MRR, and hence evaluated on this dataset, using Multi-Output Gaussian Processes. Ultimately, this could improve both predictions and insight into the underlying structure of the equity fund data. 56"
8847,unknown,"structure of the equity fund data. 56 Bibliography 1. James Chen. Investing in equity funds: a beginner’s guide. Investopedia, April 2024. Reviewed by Gordon Scott, Fact checked by Ariel Courage. 2. B.R. Kumar. Sustainable finance and investment: The intersection of profitability and environ- mental impact. Library Progress (International), 44:16477–16485, 10 2024. 3. Jason Fernando. Return on equ"
8848,unknown,"3. Jason Fernando. Return on equity (roe) calculation and what it means. Investopedia. URL: https://www. investopedia.com/terms/r/returnonequity. asp (date of access: 14.12.2023) , 2023. 4. Will Kenton. Net income (NI): Definition, uses, and formula, 2024. Investopedia. Updated 25 June 2024. Reviewed by Khadija Khartit and fact-checked by Kirsten Rohrs Schmitt. 5. Mitchell Franklin, Patty Graybeal"
8849,unknown,"nancial Accounting. OpenStax, 2019. 6. James Chen. Morningstar sustainability rating: Definition and how it works, 2023. Investopedia. Updated 30 November 2023. Reviewed by Gordon Scott. 7. Larry Fink. Larry fink’s 2021 letter to ceos. https://www.blackrock.com/corporate/about-us/ sustainability-resilience-research, January 2021. Chairman and CEO of BlackRock. 8. Kanti V Mardia, John T Kent, and C"
8850,unknown,"2024. 9. Stockopedia. risk rating, 2024. Accessed: 27 November 2024. 10. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statis- tical Learning with Applications in R . Springer, 2 edition, 2021. 11. AMG Funds LLC. Equity style analysis: Growth vs. value, 2023. Accessed: 27 November 2024. 12. Adam Hayes. Growth stock: What it is, examples, vs. value stock, De"
8851,unknown,"12. Adam Hayes. Growth stock: What it is, examples, vs. value stock, December 2023. Updated 11 December 2023. Reviewed by Chip Stapleton. Fact checked by Kirsten Rohrs Schmitt. Accessed: 27 November 2024. 13. Hitul Adatiya. Eda on european mf dataset, 2022. Version 2 of 4. Accessed: 27 November 2024. Licensed under Apache 2.0 open source. 14. Chris B. Murphy. Operating costs definition: Formula, t"
8852,unknown,"14. Chris B. Murphy. Operating costs definition: Formula, types, and real-world examples, 2024. Updated 28 June 2024. Reviewed by David Kindness. Fact checked by Amanda Jackson. Accessed: 27 November 2024. Part of the series: The Evolution of Accounting and its Terminology. 15. Michael H. Kutner, Christopher J. Nachtsheim, John Neter, and William Li. Applied Linear Statistical Models. McGraw-Hill/"
8853,unknown,"Statistical Models. McGraw-Hill/Irwin, 5 edition, 2005. 16. David A. Belsley, Edwin Kuh, and Roy E. Welsch. Regression Diagnostics: Identifying Influential Data and Sources of Collinearity . Wiley, 1980. 57 BIBLIOGRAPHY 17. Shangzhi Hong and Henry S Lynn. Accuracy of random-forest-based imputation of missing data in the presence of non-normality, non-linearity, and interaction.BMC Medical Research"
8854,unknown,"20:1–12, 2020. 18. C´ atia M. Salgado, Carlos Azevedo, Hugo Proen¸ ca, and Susana M. Vieira. Missing data. In Ross C. Mitchell, editor, Secondary Analysis of Electronic Health Records, pages 143–162. Springer, Cham, 2016. Open Access chapter distributed under the terms of the Creative Commons Attribution- NonCommercial 4.0 International License. 19. openfunds. Fund ratios and exposures, 2020. Acce"
8855,unknown,"19. openfunds. Fund ratios and exposures, 2020. Accessed: 2025-04-03. 20. Richard Johnson and Dean Wichern. Multivariate linear regression models: Section 7.7. In Applied Multivariate Statistical Analysis: Pearson New International Edition , pages 360–429. Pearson Education, Limited, 6th edition, 2013. Accessed: 27 November 2024. 21. Jos´ e C Pinheiro. Conditional versus marginal covariance repres"
8856,unknown,"models. Austrian Journal of Statistics , 35(1):31–44, 2006. 22. Gudmund R. Iversen. Multivariate analysis of variance and covariance (manova and mancova). In Alan Bryman Michael S. Lewis-Beck and Tim Futing Liao, editors, The SAGE Encyclopedia of Social Science Research Methods, volume 2, pages 702–703. SAGE Publications, Inc., Thousand Oaks, CA, 2004. 23. Newsom. Multivariate analysis of variance"
8857,unknown,"23. Newsom. Multivariate analysis of variance. Psy 522/622 Multiple Regression and Multivariate Quantitative Methods, 2024. Winter 2024 Lecture Notes. 24. STAT 505 Pennsylvania State University. Lesson 8: Multivariate analysis of variance (manova), 2024. 25. Daniel N Moriasi, Jeffrey G Arnold, Michael W Van Liew, Ronald L Bingner, R Daren Harmel, and Tamie L Veith. Model evaluation guidelines for "
8858,unknown,"and Tamie L Veith. Model evaluation guidelines for systematic quantification of accuracy in watershed simulations. Transactions of the ASABE , 50(3):885–900, 2007. 26. Kristin L. Sainani. Multivariate regression: The pitfalls of automated variable selection. PM&R, 5(9):791–794, 2013. 27. Trevor Hastie. Fast regularisation paths via coordinate descent. In The 14th ACM SIGKDD International Conferenc"
8859,unknown,"International Conference on Knowledge Discovery and Data Mining, Denver , volume 2009, 2008. 28. Ashin Mukherjee and Ji Zhu. Reduced rank ridge regression and its kernel extensions. Statistical Analysis and Data Mining: the ASA data science journal , 4(6):612–622, 2011. 29. Ian T Jolliffe. Principal Component Analysis for Special Types of Data . Springer, 2002. 30. Adam J Rothman, Elizaveta Levina"
8860,unknown,"30. Adam J Rothman, Elizaveta Levina, and Ji Zhu. Sparse multivariate regression with covariance estimation. Journal of Computational and Graphical Statistics , 19(4):947–962, 2010. 31. Jian Guo, Elizaveta Levina, George Michailidis, and Ji Zhu. Joint estimation of multiple graphical models. Biometrika, 98(1):1–15, 02 2011. 32. Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse "
8861,unknown,"with the graphical lasso. Biostatistics, 9(3):432–441, 2008. 33. Onureena Banerjee, Laurent El Ghaoui, and Alexandre d’Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. The Journal of Machine Learning Research, 9:485–516, 2008. 58 BIBLIOGRAPHY 34. Frederik Questier, Raf Put, Danny Coomans, Beata Walczak, and Yvan Vander Heyden. The us"
8862,unknown,"use of cart and multivariate regression trees for supervised and unsupervised feature selection. Chemometrics and Intelligent Laboratory Systems , 76(1):45–54, 2005. 35. Elsevier Inc. Data Science Process . Elsevier, Amsterdam, Netherlands, 2019. DOI: https://doi.org/10.1016/B978-0-12-814761-0.00002-2. 36. Leo Breiman. Statistics department, university of california, berkeley, ca 94720. 2001. Rand"
8863,unknown,"Forests, 2001. 37. Tin Kam Ho. The random subspace method for constructing decision forests. IEEE Transactions on Pattern Analysis and Machine Intelligence , 20(8):832–844, 1998. 38. Quebec Centre for Biodiversity Science. Qcbs r workshop series: Workshop 10: Advanced multi- variate analyses in r, 2023. Creative Commons Attribution-NonCommercial-ShareAlike 4.0 Inter- national Licenses all content."
8864,unknown,"39. G. De’ath. Multivariate regression trees: A new technique for modelling species-environment relationships. Ecological Society of America, 88:2783–2792, 2002. 40. Mark Segal and Yuanyuan Xiao. Multivariate random forests.WIREs Data Mining and Knowledge Discovery, 1(1):80–87, 2011. 41. Cansu Alakus, Denis Larocque, and Aur´ elie Labbe. Covariance regression with random forests. BMC bioinformatic"
8865,unknown,"BMC bioinformatics, 24(1):258, 2023. 42. L Breiman, JH Friedman, RA Olshen, and CJ Stone. Classification and regression trees. boca raton, florida: Chapman hall/crc, 1984. 43. Hoora Moradian, Denis Larocque, and Fran¸ cois Bellavance. L 1 l 1 splitting rules in survival forests. Lifetime data analysis , 23:671–691, 2017. 44. Sami Tabib and Denis Larocque. Non-parametric individual treatment effect"
8866,unknown,"vival data with random forests. Bioinformatics, 36(2):629–636, 2020. 45. Cansu Alaku¸ s, Denis Larocque, S´ ebastien Jacquemont, Fanny Barlaam, Charles-Olivier Martin, Kristian Agbogba, Sarah Lipp´ e, and Aur´ elie Labbe. Conditional canonical correlation estimation based on covariates with random forests. Bioinformatics, 37(17):2714–2721, 2021. 46. Susan Athey, Julie Tibshirani, and Stefan Wager."
8867,unknown,"2019. 47. Benjamin Lu and Johanna Hardin. A unified framework for random forest prediction error esti- mation. Journal of Machine Learning Research , 22(8):1–41, 2021. 48. Cansu Alakus, Denis Larocque, and Aurelie Labbe. Rfpredinterval: An r package for prediction intervals with random forests and boosted forests. ArXiv Preprint ArXiv:2106.08217, 2021. 49. N Azizah, LS Riza, and Y Wihardi. Impleme"
8868,unknown,"49. N Azizah, LS Riza, and Y Wihardi. Implementation of random forest algorithm with parallel com- puting in r. In Journal of Physics: Conference Series , volume 1280, page 022028. IOP Publishing, 2019. 50. Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 785–794,"
8869,unknown,"785–794, 2016. 59 BIBLIOGRAPHY 51. Alexander M¨ arz. Multi-target xgboostlss regression. ArXiv Preprint ArXiv:2210.06831, 2022. 52. DSG Pollock and Emi Mise. The cholesky decomposition of a toeplitz matrix and a wiener- kolmogorov filter for seasonal adjustment. Technical report, IDEAS, 2020. 53. StatQuest with Josh Starmer. XGBoost Part 1 (of 4): Regression, 2019. [Online; accessed 6- March-2025]"
8870,unknown,"54. XGBoosting. Xgboost advantages and disadvantages (pros vs cons), 2024. Accessed: 2025-04-11. 55. Jo Jackson. Multivariate techniques: Advantages and disadvantages. https://classroom. synonym.com/multivariate-techniques-advantages-and-disadvantages-123456.html , 2018. Updated 05 September, 2018. Accessed 06 December 2024. 56. Alexander Von Eye and G Anne Bogat. Testing the assumption of multiva"
8871,unknown,"chology Science, 46:243–258, 2004. 57. Han Liu, Mark Palatucci, and Jian Zhang. Blockwise coordinate descent procedures for the multi- task lasso, with applications to neural semantic basis discovery. In Proceedings of the 26th annual International Conference on Machine Learning , pages 649–656, 2009. 60 Chapter A Appendices A.1 Additional Data Visualisations Figure A.1: VIF Values post Dataset Cl"
8872,unknown,"Figure A.2: Forward Stepwise Selection with an ANRMSE Curve 61 APPENDIX A. APPENDICES A.2 Extra Chapter Insights A.2.1 Introduction Correlation Between Math Scores and Science Scores X1: Hours Studied X2: Time Spent on Papers Y1: Math Scores Y2: Science Scores 5 2 78 80 7 3 85 79 8 4 88 88 3 1 65 70 10 5 92 74 Table A.1: Study Time vs. Exam Scores Let each pair (xi, yi) represent Math (x) and Scie"
8873,unknown,"Let each pair (xi, yi) represent Math (x) and Science ( y) scores: (x1, y1) = (78, 80), (x2, y2) = (85, 79), (x3, y3) = (88, 88), (x4, y4) = (65, 70), (x5, y5) = (92, 74). Next, compute the means. Let ¯x be the mean of the Math scores, and ¯y be the mean of the Science scores: ¯x = 78 + 85 + 88 + 65 + 92 5 = 81.6, ¯y = 80 + 79 + 88 + 70 + 74 5 = 78.2. For each pair (xi, yi), compute   xi − ¯x  an"
8874,unknown,"xi − ¯x  and   yi − ¯y  . For clarity, the table below shows these values: xi yi xi − ¯x y i − ¯y 78 80 78 − 81.6 = −3.6 80 − 78.2 = 1.8 85 79 85 − 81.6 = 3.4 79 − 78.2 = 0.8 88 88 88 − 81.6 = 6.4 88 − 78.2 = 9.8 65 70 65 − 81.6 = −16.6 70 − 78.2 = −8.2 92 74 92 − 81.6 = 10.4 74 − 78.2 = −4.2 Next, multiply each pair of deviations and sum them: 5X i=1 (xi − ¯x)(yi − ¯y) = (−3.6)(1.8) + (3.4)(0.8"
8875,unknown,"(xi − ¯x)(yi − ¯y) = (−3.6)(1.8) + (3.4)(0.8) + (6.4)(9.8) + (−16.6)(−8.2) + (10.4)(−4.2) = 151.4. Next, compute the Sum of Squares for Each Variable: 5X i=1 (xi − ¯x)2 = (−3.6)2 + (3.4)2 + (6.4)2 + (−16.6)2 + (10.4)2 = 449.2, 5X i=1 (yi − ¯y)2 = (1.8)2 + (0.8)2 + (9.8)2 + (−8.2)2 + (−4.2)2 = 184.8. 62 APPENDIX A. APPENDICES Finally, apply the Pearson Correlation Formula. The Pearson correlation c"
8876,unknown,"r = P(xi − ¯x)(yi − ¯y)pP(xi − ¯x)2 P(yi − ¯y)2 = 151.4√449.2 × 184.8 ≈ 0.526. Thus, the correlation between the Math and Science scores in this dataset is approximately 0 .526, indicating a moderate positive relationship. A.2.2 Exploratory Data Analysis Mardia’s Test for Multivariate Normality Mardia’s test evaluates whether a set of multivariate data follows a multivariate normal distribution by"
8877,unknown,"by examining two key properties: multivariate skewness and multivariate kurtosis. 56 The multivariate skewness statistic is defined as: β1,d = 1 N2 NX i=1 NX j=1 h (xi − ¯x)⊤S−1(xj − ¯x) i3 , where xi is the observation vector for thei-th observation, ¯x is the sample mean vector, S is the sample covariance matrix, N is the number of observations and d is the number of response variables. 56 Under"
8878,unknown,"Under the null hypothesis of multivariate normality, the skewness statistic β1,d is asymptotically distributed as chi-squared with d(d+1)(d+2) 6 degrees of freedom. 56 The p-value is calculated as: p = P   χ2 df > β1,d  , which corresponds to the upper-tail probability of the chi-squared distribution. The multivariate kurtosis statistic is defined as: β2,d = 1 N NX i=1 h (xi − ¯x)⊤S−1(xi − ¯x) i2"
8879,unknown,"The multivariate kurtosis statistic is defined as: β2,d = 1 N NX i=1 h (xi − ¯x)⊤S−1(xi − ¯x) i2 , For a multivariate normal distribution, the expected value of β2,d is d(d + 2), and the variance is approximately: Var(β2,d) = 8d(d + 2) N .56 To assess deviation from normality, the kurtosis statistic is standardised: Z = β2,d − d(d + 2)p Var(β2,d) . The p-value is then computed from the standard no"
8880,unknown,"The p-value is then computed from the standard normal distribution as: p = 2 P(Z > |z|). This two-tailed test detects whether the kurtosis significantly deviates from the expected value under mul- tivariate normality. For the equity fund dataset, the sample covariance matrix is the response covariance matrix be- tween ROE and sustainability score. d = 2 because there are 2 responses and N = 1248, "
8881,unknown,"number of equity funds in the dataset. A.2.3 Multiple-Response Linear Regression Covariance Expansion Cov(XB) = E[(XB − E[XB])(XB − E[XB])⊤]. 63 APPENDIX A. APPENDICES Since expectation is linear: E[ XB] = E[X]B, the centered term becomes: (XB − E[X]B) = (X − E[X])B. Substitute this into the covariance definition: Cov(XB) = E h (X − E[X])B · (X − E[X])⊤B⊤ i . Factor out B because it is constant: C"
8882,unknown,"i . Factor out B because it is constant: Cov(XB) = B⊤E h (X − E[X])(X − E[X])⊤ i B. The middle term is just Cov( X), therefore: Cov( XB) = B⊤Cov(X)B. A.2.4 Lasso and Ridge Regression Closed-Form Coefficient Estimator Derivation We begin with the ridge regression loss function: L(β) = ∥Y − Xβ∥2 2 + λ∥β∥2 2, where X ∈ Rn×p, Y ∈ Rn×1, and β ∈ Rp×1. To find the closed-form solution, we differentiate L"
8883,unknown,"to zero giving: ∇βL(β) = −2X⊤(Y − Xβ) + 2λβ = 0, X⊤Y = X⊤Xβ + λβ, (X⊤X + λI)β = X⊤Y. Hence, the closed-form solution for ridge regression is: ˆβ Ridge = (X⊤X + λI)−1X⊤Y. As desired. Multivariate Lasso Coordinate Descent Algorithm This algorithm comes from Liu et al. (2009) and its inputs are the standardised predictors, X ∈ Rn×p, and standardised responses, Y ∈ Rn×m, and the regularisation paramet"
8884,unknown,"First, initialise B = 0 and carry out the following loop: 1. For each predictor j = 1, . . . , p: (a) Compute the partial residual matrix for predictor j: Rj = Y − X k̸=j X·kBk·, where X·k is the k-th column of X and Bk· is the k-th row of B (across all responses). (b) Compute the least squares update for row j (before thresholding): B∗ j· = X⊤ ·j Rj, 64 APPENDIX A. APPENDICES giving a 1 × m row v"
8885,unknown,"(c) Apply element-wise soft-thresholding to update Bj·: Bj· ← sign(B∗ j·) ◦   |B∗ j·| −λ  + , where (z)+ = max(0, z) is applied element-wise, and ◦ denotes element-wise multiplication. 2. Stop when: ∥B(k) − B(k−1)∥∞ < ϵLasso, where ϵLasso is the convergence threshold. 57 The output of this algorithm is the estimated coefficient matrix, ˆB. A.2.5 Reduced Rank Ridge Regression Minimisation Problem "
8886,unknown,"Minimisation Problem Simplification Starting with the penalised objective: ∥Y − XB∥2 F + λ∥B∥2 F . The above is the sum of two Frobenius norms. Therefore, this can be written as a single Frobenius norm of a block matrix as follows: = Y − XB − √ λB ! 2 F . Now note that: Y − XB − √ λB ! = Y 0 ! − X√ λI ! B. Therefore, the penalised problem can be written as an equivalent least squares problem on an"
8887,unknown,"mented dataset: ∥Y − XB∥2 F + λ∥B∥2 F = Y 0 ! − X√ λI ! B 2 F = ∥Y∗ − X∗B∥2 F . Hence, the original penalised objective can be rewritten as the following unconstrained least squares objective on an augmented dataset: ˆB(λ, r) = arg min rank(B)≤r ∥Y∗ − X∗B∥2 F . Mean Centring and the Response-Covariance Matrix As a reminder, in general: ˆΣY = 1 n − 1(Y − ¯Y)⊤(Y − ¯Y) The response matrix Y can be wr"
8888,unknown,"The response matrix Y can be written as the sum of its mean-centred component and the mean matrix: Y = (Y − ¯Y) + ¯Y. In this decomposition, Y − ¯Y is the mean-centred version of Y, while ¯Y is a matrix where each row is the mean response vector ¯y. Substituting the decomposition Y = (Y − ¯Y) + ¯Y into Y⊤Y, gives [(Y − ¯Y) + ¯Y]⊤[(Y − ¯Y) + ¯Y]. Expanding this product results in Y⊤Y = (Y − ¯Y)⊤(Y "
8889,unknown,"Y⊤Y = (Y − ¯Y)⊤(Y − ¯Y) + ¯Y⊤(Y − ¯Y) + (Y − ¯Y)⊤ ¯Y + ¯Y⊤ ¯Y. 65 APPENDIX A. APPENDICES The two cross terms ¯Y⊤(Y − ¯Y) and ( Y − ¯Y)⊤ ¯Y are equal to zero. This vanishing comes from the fact that the mean response matrix ¯Y is constant across all rows, meaning that the sum of all deviations from the mean is equal to zero. Since: nX i=1 (yi − ¯y) = 0, it follows that when multiplied by ¯Y, the re"
8890,unknown,"nX i=1 ¯y⊤(yi − ¯y) = ¯y⊤ nX i=1 (yi − ¯y) = 0.29 Since this holds for all response variables, it means that ¯Y⊤(Y − ¯Y) = 0 and ( Y − ¯Y)⊤ ¯Y = 0. Substituting these results back into the expansion simplifies the equation to: Y⊤Y = (Y − ¯Y)⊤(Y − ¯Y) + ¯Y⊤ ¯Y. The term ¯Y⊤ ¯Y can be further simplified. Since each row of ¯Y is equal to the mean response vector ¯y, and there are n such rows, the sum"
8891,unknown,"¯y, and there are n such rows, the sum simplifies as follows: ¯Y⊤ ¯Y = n¯y⊤¯y. This scaling by n occurs because the same mean vector is repeated for all observations. Substituting this result into the equation gives: Y⊤Y = (Y − ¯Y)⊤(Y − ¯Y) + n ¯Y⊤ ¯Y.29 This equation shows that Y⊤Y consists of the variance of the mean-centred data plus the contribution of the mean structure. In its final form, th"
8892,unknown,"Y⊤Y = (n − 1)ˆΣY + n ¯Y⊤ ¯Y, where ¯Y⊤ ¯Y captures the mean effects of Y. Projection Simplification Starting with: τX i=1 diuiv⊤ i !  rX j=1 vjv⊤ j   = τX i=1 diui  v⊤ i rX j=1 vjv⊤ j  . The inner expression simplifies using orthonormality: • If i ≤ r: v⊤ i rX j=1 vjv⊤ j = v⊤ i . • If i > r: v⊤ i rX j=1 vjv⊤ j = 0⊤. So the entire sum becomes: rX i=1 diuiv⊤ i . 66 APPENDIX A. APPENDICES Sin"
8893,unknown,"diuiv⊤ i . 66 APPENDIX A. APPENDICES Singular Value Decomposition Given the matrix ˆY∗ R: ˆY∗ R = "" −5.67 2 .96 4 .19 −14.29 12 .81 0 .74 −0.62 −1.82 −0.91 2 .28 −2.73 3 .19 −0.23 0 .55 #⊤ . We aim to compute its Singular Value Decomposition: ˆY∗ R = UDV⊤. First, compute the Gram matrix: ˆY∗ R ⊤ ˆY∗ R = "" 427.76 96 .576 96.58 27 .34 # . This symmetric matrix is used to compute the eigenvalues and "
8894,unknown,". This symmetric matrix is used to compute the eigenvalues and eigenvectors, which in turn provide the singular values. These singular values are the square roots of the eigenvalues of ˆY∗ R ⊤ ˆY∗ R. The eigenvalues of ˆY∗ R ⊤ ˆY∗ R are: λ1 = 449.84 and λ2 = 5.26. Hence: D = ""√ 449.84 0 0 √ 5.26 # = "" 21.21 0 0 2 .29 # . The eigenvectors of ˆY∗ R ⊤ ˆY∗ R are the matrix V (to 3sf): V = "" 0.975 −0.2"
8895,unknown,"V = "" 0.975 −0.223 0.223 0 .975 # . Now, we compute U using the formula: U = ˆY∗ RVS−1. Since V is an orthonormal matrix, we have V⊤ = V−1, giving (to 3sf): U = "" −0.280 0 .127 0 .216 −0.686 0 .622 0 .0316 −0.0227 −0.224 −0.675 0 .562 0 .227 0 .111 −0.169 0 .293 #⊤ . A.2.6 Multivariate Regression with Covariance Estimation Negative Log-Likelihood Derivation Starting with the familiar MRLR model: Y"
8896,unknown,"Y(n×m) = X(n×p)B(p×m) + E(n×m), where p no longer includes the column of ones. Each row yi ∈ R1×m of Y follows: yi|xi ∼ Nm(xiB, ΣE), The probability density function for a single observation is: p(yi | xi) = 1 (2π)m/2|ΣE|1/2 exp  −1 2(yi − xiB)⊤ΣE−1(yi − xiB)  . 67 APPENDIX A. APPENDICES Assuming independence across observations, the total log-likelihood becomes: log L(B, ΣE) = nX i=1 log p(yi |"
8897,unknown,"nX i=1 log p(yi | xi) = −nm 2 log(2π) − n 2 log |ΣE| −1 2 nX i=1 (yi − xiB)⊤ΣE−1(yi − xiB) (*) Subbing the residual matrix, E = Y − XB, into the quadratic form gives: nX i=1 (yi − xiB)⊤ΣE−1(yi − xiB) = tr  E⊤EΣE−1  = tr  (Y − XB)⊤(Y − XB)Ω  , where Ω = ΣE−1. Substituting back into (*), the log-likelihood becomes: log L(B, Ω) = −nm 2 log(2π) + n 2 log |Ω| −1 2 tr  (Y − XB)⊤(Y − XB)Ω  . Ignori"
8898,unknown,"2 log(2π), the negative log-likelihood (NLL) becomes: NLL(B, Ω) = 1 2 tr  (Y − XB)⊤(Y − XB)Ω  − n 2 log |Ω|. Finally, multiplying and dividing the trace term by n, and omitting the constant 1 2 , we obtain the simplified negative log-likelihood: g(B, Ω) = tr 1 n(Y − XB)⊤(Y − XB)Ω  − log |Ω|. This is the negative log-likelihood function (up to a constant) for the MRLR model. Proof of Algorithm "
8899,unknown,"Proof of Algorithm 1 This proof comes from Rothman et al. (2010) and is as follows: The objective function for Ω fixed at Ω0 (from Equation 4.11), can be written more simply as: f(B) = g(B, Ω0) + λ2 pX j=1 qX k=1 |bjk|, where: g(B, Ω0) = tr 1 n(Y − XB)⊤(Y − XB)Ω0  − log |Ω0|. First, solve for B with cyclical coordinate descent. Express the directional derivatives as: ∂f + ∂B = 2 nX⊤XBΩ − 2 nX⊤YΩ"
8900,unknown,"∂f − ∂B = −2 nX⊤XBΩ + 2 nX⊤YΩ − λ21(bij > 0) + λ21(bij ≤ 0), 68 APPENDIX A. APPENDICES where the indicator 1() is understood to be a matrix. Let S = X⊤X and H = X⊤YΩ, and define: urc = pX j=1 qX k=1 bjksrjωkc. Next, update a single parameter brc, using the directional derivatives: ∂f + ∂brc = urc − hrc + nλ21(bij ≥ 0) − nλ21(bij < 0), ∂f − ∂brc = −urc + hrc − nλ21(bij > 0) + nλ21(bij ≤ 0). Let b0 "
8901,unknown,"rc be the current iterate. The unpenalised univariate minimised, ˆb∗ rc, solves: ˆb∗ rcsrrωcc − b0 rcsrrωcc + urc − hrc = 0, implying: ˆb∗ rc = b0 rc + hrc − urc srrωcc . If ˆb∗ rc > 0, then we look leftward, and by convexity, the penalised minimiser is: ˆbrc = max  0,ˆb∗ rc − nλ2 srrωcc  . Similarly, if ˆb∗ rc < 0 then we look to the right, and by convexity, the penalised univariate minimiser i"
8902,unknown,"ˆbrc = min  0,ˆb∗ rc + nλ2 srrωcc  . Thus, ˆbrc = sign(ˆb∗ rc)  |ˆb∗ rc| −nλ2 srrωcc  + . Also, if ˆb∗ rc = 0, which has probability zero, then both the loss and penalty part of the objective function are minimised and the parameter stays at 0. We can write this solution compactly as: ˆbrc = sign  b0 rc + hrc − urc srrωcc  b0 rc + hrc − urc srrωcc − nλ2 srrωcc  + .30 As desired. Graphical "
8903,unknown,"srrωcc  b0 rc + hrc − urc srrωcc − nλ2 srrωcc  + .30 As desired. Graphical Lasso Form First, let us justify why glasso works using its setup and how MRCE corresponds. The glasso setup, from Friedman et al. (2008), begins as follows: you have an observed data matrix Z, where Z ∈ Rn×m. These vectors are assumed to be drawn from a multivariate Gaussian distribution N(0, Σ), where Σ is the m × m co"
8904,unknown,"is the m × m covariance matrix. The aim of glasso is to estimate the precision matrix, Ω = Σ−1. Glasso estimates Ω by solving the following optimisation problem: arg max Ω≻0 {log |Ω| −tr(SΩ) − λ∥Ω∥1}, (A.1) where λ > 0 is the regularisation parameter, ∥Ω∥1 = P j,k |ωjk| and S is the empirical covariance matrix, computed as S = 1 nZ⊤Z.32 69 APPENDIX A. APPENDICES MRCE uses the glasso setup to estim"
8905,unknown,"MRCE uses the glasso setup to estimate the inverse error covariance matrix, Ω = ΣE, for the errors in MRLR because the errors Ei ∼ N(0, ΣE) match the Multivariate Gaussian assumption of glasso. Its optimisation problem, for Ω, can also be rewritten in the form of Equation A.1. Here is Equation 4.12, the optimisation problem, for Ω, again: ˆΩ(B0) = arg min Ω   tr h ˆΣRΩ i − log |Ω| + λ1 X j′̸=j "
8906,unknown,"|ωj′j|   , where ˆΣR = 1 n(Y − XB0)⊤(Y − XB0). arg min Ω   tr( ˆΣR Ω) − log |Ω| + λ1 X i̸=j |[Ω]ij|    = arg max Ω≻0 n log |Ω| −tr( ˆΣR Ω) − λ1∥Ω∥1 o , which comes from the rule for argmin and argmax that: arg max x f(x) = arg min x [−f(x)] This gives the graphical-lasso form: ˆΩ(B0) = arg max Ω≻0 n log |Ω| −tr( ˆΣR Ω) − λ1∥Ω∥1 o , where ∥Ω∥1 = X i̸=j |Ωij|. This setup allows MRCE to acco"
8907,unknown,"A.2.7 Cholesky-Gaussian XGBoost Weighted Quantile Sketches In the approximate split-finding algorithm, as already stated, candidate split points are chosen based on percentiles of the feature values. The rank function rk(z) calculates the proportion of instances where feature k is less than z, weighted by second-order gradient statistics h.50 The algorithm aims to find split points sk1, sk2, . . ."
8908,unknown,"small, controlled by ϵ, which is an approximation factor. 50 This process ensures that the split points are well-distributed across the range of the feature values. The Hessian, hi, is used as a weight for each instance or row in the dataset, meaning that data points with larger Hessians carry more influence in determining the split. This weighting reflects the importance of the instance in minimi"
8909,unknown,"the importance of the instance in minimising loss. Higher-weighted instances show where the model struggles, guiding the algorithm to focus on these critical areas during split finding. Traditional quantile sketch algorithms work for datasets where all instances have equal weights. 50 However, no existing algorithm efficiently handles weighted data, which creates challenges in approxi- mating spli"
8910,unknown,"heuristic approaches have been used to address this issue, but these methods lack theoretical guar- antees.50 As a result, they may fail to identify the optimal split points, leading to suboptimal model performance. To overcome this limitation, XGBoost introduces a novel weighted quantile sketch algo- rithm that can handle weighted data with provable theoretical guarantees. 50 This algorithm ensur"
8911,unknown,"accurate split findings by accounting for the distribution of weights across the dataset. 70 APPENDIX A. APPENDICES Cholesky-Decomposition Verification Verification occurs by computing: LL⊤ = "" 2 0 1 √ 2 #"" 2 1 0 √ 2 # = "" (2)(2) + (0)(0) (2)(1) + (0)( √ 2) (1)(2) + ( √ 2)(0) (1)(1) + ( √ 2)( √ 2) # = "" 4 2 2 3 # = A. As desired. 71 AUSTRIAN JOURNAL OF STATISTICS V olume 35 (2006), Number 1, 31–44"
8912,unknown,"Conditional versus Marginal Covariance Representation for Linear and Nonlinear Models Jos´e C. Pinheiro Dept. of Biostatistics, Novartis Pharmaceuticals, East Hanover, USA Abstract: Grouped data, such as repeated measures and longitudinal data, are increasingly collected in different areas of application, as varied as clin- ical trials, epidemiological studies, and educational testing. It is often"
8913,unknown,"interest, for these data, to explore possible relationships between one or more response variables and available covariates. Because of the within-group cor- relation typically present with this type of data, special regression models that allow the joint estimation of mean and covariance parameters need to be used. Two main approaches have been proposed to represent the covariance structure of th"
8914,unknown,"structure of the data with these models: (i) via the use of random effects, the so-called conditional model and (ii) through direct representation of the co- variance structure of the responses, known as the marginal approach. Here we discuss and compare these two approaches in the context of linear and non-linear regression models with additive Gaussian errors, using a real data example to motiva"
8915,unknown,"example to motivate and illustrate the discussion. Zusammenfassung: Gruppierte Daten, wie wiederholte Messungen und Lon- gitudinaldaten, werden h ¨auﬁg in den verschiedensten Anwendungsgebieten, etwa bei klinischen und epidemiologischen Studien und in den Erziehungswis- senschaften erhoben. M¨ogliche Zusammenh¨ange zwischen einer oder mehreren Responsevariablen und vorhandenen Kovariaten sind oft "
8916,unknown,"suchung. Um die Korrelation innerhalb der Gruppe zu ber ¨ucksichtigen, sind Regressionsmodelle n ¨otig, die eine gemeinsame Sch ¨atzung von Mittelwert und Kovarianzparametern erlauben. Zwei Ans ¨atze wurden vorgeschlagen, um die Kovarianzstruktur der Daten zu ber ¨ucksichtigen: (i) durch die Ver- wendung von zuf ¨alligen Effekten, das sogenannte konditionale Modell und (ii) durch die direkte Repr "
8917,unknown,"(ii) durch die direkte Repr ¨asentation der Kovarianzstruktur der Responses, bekannt als das marginale Modell. Hier vergleichen und diskutieren wir beide Ans¨atze im Kontext von linearen und nicht-linearen Regressionsmodellen mit additiven Gaußschen Fehlern anhand eines realen Beispiels. Keywords: Correlated Data, Clustered Data, Mixed Effects, Random Ef- fects, Repeated Measures. 1 Introduction M"
8918,unknown,"Mixed-effects models are useful in describing relationships between a response variable and covariates in data that are grouped according to one or more classiﬁcation factors. Examples of such grouped data include longitudinal, repeated measures, and multilevel data, which frequently arise in many areas of application, such as clinical trials, epidemi- ological studies, and educational testing. Mi"
8919,unknown,"ological studies, and educational testing. Mixed-effects models assume that the form of 32 Austrian Journal of Statistics, V ol. 35 (2006), No. 1, 31–44 the intra-group model relating the response variable to the covariates is common to all groups, but some of the parameters that deﬁne the model are allowed to vary with group through the incorporation of random effects. By associating common rando"
8920,unknown,"observations in the same group, mixed-effects models ﬂexibly represent the covariance structure induced by the grouping of the data. Alternatively, the covariance matrix of the responses may be directly modelled via the covariance model adopted for the within-group error term (assumed to enter the model linearly). Of course, both approaches can be used in combination, giving mixed-effects models c"
8921,unknown,"models considerable ﬂexibility in representing the covariance structure present in grouped data. In practice, one is usually interested in jointly estimating the regression model param- eters (representing the relationship between the response and covariates of interest) and the covariance parameters, determining the correlation and variance structure of the re- sponse. The approach based on the u"
8922,unknown,"sponse. The approach based on the use of random effects is called the conditional model, while the one relying on direct modelling of the within-group error covariance structure is known as the marginal model. This paper describes and contrasts the two approaches in the context of linear mixed- effects (LME) models and nonlinear mixed-effects (NLME) models. A motivating exam- ple, the Dialyzer dat"
8923,unknown,"ple, the Dialyzer data, which will be used for illustration of both the LME and NLME models, is described in Section 2. In Section 3 it is shown that the two approaches are equivalent in the context of LME models, in the sense that both can be used to directly model the covariance matrix of the responses. Section 4 investigates extensions to the NLME model, in which the effect of the random effect"
8924,unknown,"NLME model, in which the effect of the random effects model on the covariance struc- ture of the response can only be determined approximately. Our conclusions are included in Section 5. 2 An Example: High-Flux Hemodialyzer V onesh and Carter (1992) describe and analyze data measured on high-ﬂux hemodialyzers to assess their in vivo ultraﬁltration characteristics. The ultraﬁltration rates (in ml/h"
8925,unknown,"20 high-ﬂux dialyzers were measured at 7 ascending transmembrane pressures (in dmHg). The in vitroevaluation of the dialyzers used bovine blood at ﬂow rates of either 200 dl/min or 300 dl/min. These data are available in the NLME library (Pinheiro and Bates, 2000) in S-PLUS and R. The plots of the ultraﬁltration rates versus transmembrane pressure by bovine blood ﬂow rate, displayed in Figure 1, r"
8926,unknown,"ﬂow rate, displayed in Figure 1, reveal that, as expected, the ultraﬁltration rate increases with transmembrane pressure up to a maximum, and that higher ultraﬁltration rates are attained with the 300 dl/min blood ﬂow dialyzers. These plots show a clear correlation in the measurements made on the same dialyzer and also indicate that the variability in the ultraﬁltration rates increases with transm"
8927,unknown,"ultraﬁltration rates increases with transmembrane pressure. In their original paper, V onesh and Carter (1992) use a nonlinear model to represent the relationship between ultraﬁltration rate and transmembrane pressure. An alternative analysis, based on a linear polynomial model, is presented in Littell et al. (1996). Both analyses use random effects to account for the within-dialyzer correlation, "
8928,unknown,J. Pinheiro 33 0 10 20 30 40 50 60 200 0.5 1.0 1.5 2.0 2.5 3.0 300 0.5 1.0 1.5 2.0 2.5 3.0 Transmembrane pressure (dmHg) Ultrafiltration rate (ml/hr) Figure 1: Hemodialyzer ultraﬁltration rates (in ml/hr) measured at 7 different transmem- brane pressures (in dmHg) on 20 high-ﬂux dialyzers. In vitro evaluation of dialyzers based on bovine blood ﬂow rates of 200 dl/min and 300 dl/min. also investiga
8929,unknown,"also investigating the use of marginal covariance models based on extended linear regres- sion models. We use this example illustrate and compare the conditional and marginal model approaches for both the LME model and the NLME model. 3 Linear Mixed-Effects Model The linear mixed-effects model for a normally distributed response grouped according to a single factor with M levels, proposed by Laird"
8930,unknown,"yi = Xiβ + Zibi + ϵi , (1) where i is the group index, yi is an ni-dimensional vector of observed responses, Xi and Zi are known ni × p and ni × q regression matrices corresponding to the p-dimensional ﬁxed effects vector β and the q-dimensional random effects vector bi respectively, and ϵi is an ni-dimensional vector of within-group errors. The bi are assumed to be independently distributed as N "
8931,unknown,"The bi are assumed to be independently distributed as N (0, Ψ) and the ϵi are as- sumed to be independently distributed asN (0, Λi), independent of the bi. The Ψ covari- ance matrix may be unstructured or structured – e.g. diagonal (Jennrich and Schluchter, 1986), being parameterized by a set of parameters θ. The Λi matrices are typically as- sumed to depend on i only through their dimensions, bei"
8932,unknown,"sumed to depend on i only through their dimensions, being parameterized by a ﬁxed, generally small, set of parameters λ – e.g. an AR(1) structure (Box et al., 1994). Several methods of parameter estimation have been proposed for LME models and we consider here the two most widely used and available in statistical software: maximum likelihood (ML) and restricted maximum likelihood (REML). Descripti"
8933,unknown,"likelihood (ML) and restricted maximum likelihood (REML). Descriptions and compar- isons of the various estimation methods used for LME models can be found, for example, 34 Austrian Journal of Statistics, V ol. 35 (2006), No. 1, 31–44 in Searle et al. (1992) and V onesh and Chinchilli (1997). For a Bayesian perspective see Wakeﬁeld et al. (1994). Even though the random effects are useful and intui"
8934,unknown,"Even though the random effects are useful and intuitive quantities to represent between- group differences in the coefﬁcients, they are not observable in practice. Therefore, likeli- hood estimation and inference generally rely on the marginal distribution of the observed response vectors yi. Because of the linearity of the random effects in the LME model (1), the assumptions on the random effects"
8935,unknown,"the assumptions on the random effects and the within-group errors, and the properties of the multivariate normal distribution, it can be shown that theyi are marginally distributed as independent N(Xiβ, Σi) random vectors, where the marginal covariance matrix is given by: var (yi) =Σi = ZiΨZ′ i   random eﬀects + Λi within−group error . (2) There are two ways in which the LME model (1) can "
8936,unknown,". (2) There are two ways in which the LME model (1) can account for within-group corre- lation and heteroscedasticity (non-constant variance): through the random effects bi and through the within-group errors ϵi. Because the random effects bi are ﬁxed by group, not varying with observation, the within-group observations share the same random ef- fects and are, therefore, correlated. This is repres"
8937,unknown,"i component of Σi. Note, also, that the diagonal elements of ZiΨZ′ i need not be constant, so that it can also accommodate heteroscedasticity. This component of the marginal covariance matrix Σi is the one favored in the conditional model approach, with Λi assuming a simple form (typically Λi = σ2Ii, with Ii denoting the identity matrix of order ni.) The within-group error contribution to the marg"
8938,unknown,"The within-group error contribution to the marginal covariance matrix is given directly by Λi, which can be non-diagonal (correlation) and have different diagonal elements (het- eroscedasticity). In the marginal model approach one sets Σi = Λi, so that the entire covariance structure is determined by the within-group error. Of course, in practice, one may use both components in (2) when modelling "
8939,unknown,"example, one may use the random effects component to account mostly for the correlation and the within-group component to account for heteroscedasticity via the use of a variance function (Pinheiro and Bates, 2000). Some care should be exercised when using both components in a model, as they may very well compete with each other in explaining the marginal covariance and lead to nearly or fully ove"
8940,unknown,"marginal covariance and lead to nearly or fully overparameterized models. As a simple example, consider the case of an LME model with a single random in- tercept, that is, with random effects model given by 1ibi, where bi is a scalar. The cor- responding random effect component of Σi is then equal to ψ1i1′ i = ψJi, with Ji rep- resenting an ni × ni matrix of ones. If we assume a compound symmetry "
8941,unknown,"resenting an ni × ni matrix of ones. If we assume a compound symmetry structure for the within-group covariance, that is, Λi = σ2((1 − ρ)Ii + ρJi), the resulting marginal covariance would have diagonal terms equal to σ2 + ψ and off-diagonal terms σ2ρ + ψ, that is, an overparameterized compound symmetry structure. Another example of overpa- rameterization would result from the use of an unstructure"
8942,unknown,"Λi together with any random effects model. Conditional on the parameters that determine Σi, the (RE)ML estimate of the ﬁxed effects β and its covariance matrix are given by ˆβ = ( M∑ i=1 X′ iΣ−1 i Xi )−1 M∑ i=1 X′ iΣ−1 i yi and var ( ˆβ ) = ( M∑ i=1 X′ iΣ−1 i Xi )−1 . (3) J. Pinheiro 35 The (RE)ML estimates of the parameters determining Σi can not be expressed in closed form, except in trivial cas"
8943,unknown,"form, except in trivial cases, and numerical optimization of the (restricted) likelihood function must be employed. The MLE of β is then obtained by replacing Σi in (3) with their corresponding estimates. Note that both ˆβ and its estimated variance depend on the covariance model through the marginal covariance matrices Σi only. Therefore, methods that lead to similar esti- mates of the Σi will al"
8944,unknown,"mates of the Σi will also lead to similar inferences on the ﬁxed effects, including conﬁ- dence intervals and tests of hypothesis. Therefore, if the main questions of interest asso- ciated with the LME model are related to inferences about the ﬁxed effects (like would be the case in clinical trials, for example), then the conditional and marginal approaches may lead to equivalent conclusions, prov"
8945,unknown,"may lead to equivalent conclusions, provided similar estimates ofΣi can be obtained with both methods. However, if one is interested in issues related to the covariance model, like inter-subject variability or spatial correlation, it may be that only one of the approaches can be used to address the speciﬁc question of interest. This will, of course, be applica- tion dependent, so no general recomm"
8946,unknown,"of either approach. 3.1 Dialyzer Example: Linear Model Version As an empirical model to describe the relationship between the ultraﬁltration rates and the transmembrane pressure in the Dialyzer example of Section 2, Littell et al. (1996) pro- posed the use of a fourth-order linear mixed-effects polynomial allowing for differences in the ﬁxed effects between the dialyzers with in vitro ultraﬁltrati"
8947,unknown,"and 300 dl/min. After several model building steps to determine which parameters should vary with in vitro ultraﬁltration class and which should be assigned random effects to accommo- date the inter-subject variation, the following LME model was chosen to represent the ultraﬁltration rate yij at the jth transmembrane pressure xij for the ith subject. yij = (β0+γ0Qi+b0i) + (β1+γ1Qi+b1i) xij + (β2+b"
8948,unknown,"ij + β3x3 ij + β4x4 ij + ϵij , (4) bi =   b0i b1i b2i  ind ∼N (0, Ψ) , ϵ ij ind ∼N ( 0, σ2xδ ij ) , where Qi is a binary variable taking values 0 for 200 dl/min hemodialyzers and 1 for 300 dl/min hemodialyzers; β0, β1, β2, β3, and β4 are, respectively, the intercept, linear, quadratic, cubic, and quartic ﬁxed effects corresponding to 200 dl/min dialyzers;γk is the blood ﬂow effect associated w"
8949,unknown,"blood ﬂow effect associated with the ﬁxed effect βk, k = 0, 1; bi is the vector of random effects, assumed independent for different i; and ϵij is the within-group error, assumed independent for different i, jand independent of the random effects. Different structures can be used to represent the random effects covariance matrix Ψ, as illustrated further below. The variance of the ϵij is allowed t"
8950,unknown,"below. The variance of the ϵij is allowed to change with the transmembrane pressure ac- cording to a power model with parameter δ (estimated together with the other parameters in the model.) This was needed due to heteroscedastic behavior observed in the residuals from the LME ﬁt with homocedastic within-subject errors. 36 Austrian Journal of Statistics, V ol. 35 (2006), No. 1, 31–44 The LME model"
8951,unknown,"The LME model in (4) uses random effects to account for the correlation among within-subject measurements and some of the heteroscedasticity in the response as well. An additional variance function model is used for the within-subject error to properly ac- commodate the observed heteroscedasticity, but the approach can be considered primarily a conditional model, as discussed previously. An altern"
8952,unknown,"An alternative marginal model to represent the data using the empirical linear polyno- mial model is given below. yij = (β0 + γ0Qi) + (β1 + γ1Qi) xij + β2x2 ij + β3x3 ij + β4x4 ij + ϵij , (5) ϵij ∼ N ( 0, σ2xδ ij ) corr(ϵij, ϵik) =ρ|j−k| , that is, the same ﬁxed effects and variance function as in (4) are used, but the within- subject correlation is now modeled by an AR(1) structure in the within-"
8953,unknown,"subject correlation is now modeled by an AR(1) structure in the within-subject errors. We also considered marginal models similar to (5), but with within-subject correlation structure given by a Gaussian spatial correlation model with a nugget effect (Pinheiro and Bates, 2000), an ARMA(1,3) model, and an unstructured model (that is, with 21 different correlations allowed for the within-subject mea"
8954,unknown,"correlations allowed for the within-subject measurement pairs). The conditional and marginal models described above for the Dialyzer data were ﬁtted using, respectively, the functionslme and gls in the NLME library in S-PLUS, with REML estimation in all cases. Figure 2 shows the scatter plots of the conditional model estimates for variance and covariance parameters inΣi versus the corresponding es"
8955,unknown,"different marginal models. 0 5 10 15 20 0 5 10 15 20 25 30 AR(1) + + + + + + + Gaussian + + + + + + + ARMA(1,3) + + + + + + + 0 5 10 15 20 0 5 10 15 20 25 30 Unstructured + + + + + + + Marginal model Conditional model +Covariance Variance Figure 2: Conditional versus marginal estimates for variance and covariance parameters in the Dialyzer linear models. Dotted line represents y = x reference line"
8956,unknown,"J. Pinheiro 37 There is generally good agreement between the estimates from the conditional and marginal models, with greater discrepancies observed for larger parameter values. The un- structured correlation marginal model has the best agreement with the conditional model with regard to the variances, but not the covariances. It is instructive to examine the estimation results for the ﬁxed effect"
8957,unknown,## LME fit (conditional model) Value Std.Error DF t-value p-value (Intercept) -16.393 0.8273 115 -19.814 <.0001 pressure 90.616 3.2660 115 27.745 <.0001 QB -2.777 0.8278 18 -3.355 0.0035 I(pressure^2) -49.692 4.2295 115 -11.749 <.0001 I(pressure^3) 12.474 2.1509 115 5.799 <.0001 I(pressure^4) -1.277 0.3575 115 -3.573 0.0005 pressure:QB 7.031 0.7856 115 8.949 <.0001 ## AR(1) marginal model Value St
8958,unknown,## AR(1) marginal model Value Std.Error t-value p-value (Intercept) -16.405 0.8453 -19.406 <.0001 pressure 90.240 3.8747 23.290 <.0001 QB -2.615 0.7399 -3.535 0.0006 I(pressure^2) -48.592 5.0571 -9.609 <.0001 I(pressure^3) 11.802 2.4749 4.769 <.0001 I(pressure^4) -1.153 0.3980 -2.897 0.0044 pressure:QB 6.615 0.8105 8.161 <.0001 ## Spatial Gaussian marginal model Value Std.Error t-value p-value (In
8959,unknown,"(Intercept) -16.357 0.7930 -20.626 <.0001 pressure 89.759 3.5459 25.314 <.0001 QB -2.391 0.7441 -3.214 0.0016 I(pressure^2) -48.095 4.5640 -10.538 <.0001 I(pressure^3) 11.663 2.2276 5.236 <.0001 I(pressure^4) -1.144 0.3588 -3.188 0.0018 pressure:QB 6.419 0.8396 7.645 <.0001 ## ARMA(1,3) marginal model Value Std.Error t-value p-value (Intercept) -16.297 0.7878 -20.686 <.0001 pressure 89.489 3.5166 "
8960,unknown,"QB -2.385 0.7351 -3.244 0.0015 I(pressure^2) -47.780 4.5177 -10.576 <.0001 I(pressure^3) 11.534 2.2032 5.235 <.0001 I(pressure^4) -1.127 0.3549 -3.174 0.0019 pressure:QB 6.408 0.8285 7.734 <.0001 38 Austrian Journal of Statistics, V ol. 35 (2006), No. 1, 31–44 ## Unstructured marginal model Value Std.Error t-value p-value (Intercept) -15.901 0.7827 -20.316 <.0001 pressure 87.770 3.7826 23.203 <.00"
8961,unknown,QB -2.027 0.6547 -3.097 0.0024 I(pressure^2) -45.890 5.1765 -8.865 <.0001 I(pressure^3) 10.611 2.6619 3.986 0.0001 I(pressure^4) -0.975 0.4447 -2.192 0.0302 pressure:QB 6.309 0.6930 9.103 <.0001 The ﬁxed effects estimates and respective standard errors obtained in the LME ﬁt are similar to the estimates obtained in the marginal model ﬁts. The conclusions about the signiﬁcance of the model paramete
8962,unknown,"signiﬁcance of the model parameters are the same in all ﬁve models. The greater dis- crepancies are observed for the unstructured correlation model with respect to the other models, but those differences are not pronounced and within the estimated precisions (overlapping conﬁdence intervals). The main conclusion from the exercise above is that the conditional and marginal approaches lead to simila"
8963,unknown,"rameters and similar estimated covariance matrices ˆΣi. 4 Nonlinear Mixed-Effects Model Nonlinear mixed-effects (NLME) models are mixed-effects models in which the response function is nonlinear in at least some of the underlying parameters. Several different nonlinear mixed effects models have been proposed in the literature (Sheiner and Beal, 1980; Mallet et al., 1988; Lindstrom and Bates, 1990;"
8964,unknown,"1980; Mallet et al., 1988; Lindstrom and Bates, 1990; V onesh and Carter, 1992; Davidian and Gallant, 1992; Wakeﬁeld et al., 1994). We adopt here the NLME model proposed by Lindstrom and Bates (1990), which can be viewed as a hierarchical model that gener- alizes both the linear mixed-effects model of Section 3 and the usual nonlinear regression model for independent data (Bates and Watts, 1988). "
8965,unknown,"on the ith group is described as yij = f(φij, xij) +ϵij , i = 1, . . . , M , j= 1, . . . , ni , (6) where f is a nonlinear function of a group-speciﬁc vector of parameters φij and the vec- tor of covariates xij, M is the total number of groups, and ni is the number of obser- vations in the ith group. Like in the LME model (1), the within-group error vectors ϵi = (ϵi1, . . . , ϵini)′ are assumed to"
8966,unknown,"ϵi = (ϵi1, . . . , ϵini)′ are assumed to be independently distributed as N(0, Λi), with the Λi parameterized by a ﬁxed parameter vector λ. In the second stage the group-speciﬁc parameters are modelled as φij = Aijβ + Bijbi . (7) Like in the LME model, β represents the ﬁxed effects and bi the random effects (varying with i but not with j), which are assumed to be independently distributed as N(0, Ψ"
8967,unknown,"Aij and Bij are design matrices for the ﬁxed and random effects respectively, which may depend on the values of some covariates at the jth observation. It is further assumed that the bi are independent of the ϵij. J. Pinheiro 39 Different methods have been proposed to estimate the parameters in the NLME model (Ramos and Pantula, 1995; Davidian and Giltinan, 1995; V onesh and Chinchilli, 1997); we "
8968,unknown,"we concentrate here on methods based on the likelihood function. As in the LME case, because the random effects bi are unobserved quantities, maximum likelihood estimation in NLME models is based on the marginal density of the responsesy, which is calculated as ℓ(β, Ψ, λ|y) = ∫ p(y|b, β, λ) p(b|Ψ) db . (8) Because the model function f in (6) can be nonlinear in the random effects, the integral in "
8969,unknown,"in (8) generally does not have a closed-form expression. To make the numerical opti- mization of the likelihood function a tractable problem, different approximations to (8) have been proposed. Some of these methods consist of taking a ﬁrst-order Taylor ex- pansion of the model function f around the expected value of the random effects ( i.e., 0) (Sheiner and Beal, 1980; V onesh and Carter, 1992),"
8970,unknown,"0) (Sheiner and Beal, 1980; V onesh and Carter, 1992), or around the conditional modes of the random effects ( ˆbi) (Lindstrom and Bates, 1990). We adopt here the approxima- tion suggested by Lindstrom and Bates (1990), which is implemented via an alternating algorithm comprising a linear mixed-effects (LME) step and a penalized nonlinear least squares (PNLS) step. This is the algorithm used in th"
8971,unknown,"squares (PNLS) step. This is the algorithm used in the nlme function of the NLME li- brary. Inferences on the model parameters, including hypothesis testing, are based on asymptotic results for the linear mixed-effects log-likelihood used in the LME step of the alternating algorithm (Pinheiro and Bates, 2000). Like for the LME model, the marginal covariance matrix of the responses in the NLME mode"
8972,unknown,"NLME model can be expressed as the sum of two components, one associated with the random effects and another with within-group errors. Lettingf(β, bi, Xi) = [f(φi1, xi1), . . . , f(φini, xini))]′ it follows var (yi) =Σi = var (f(β, bi, Xi))   random eﬀects + Λi within−group error . (9) Note that var (f(β, bi, Xi)) depends not only on Ψ, but also on β and Xi. One can obtain an estimate for "
8973,unknown,"tors b∗ 1, . . . ,b∗ N ind ∼N (0, ˆΨ), calculating the corresponding vectors f(ˆβ, b∗ k, Xi), k = 1, . . . , N, and deriving the associated sample covariance matrix. Alternatively, a ﬁrst- order Taylor expansion of f(β, ·, Xi) around bi = 0 can be used to obtain an LME-like approximation var (f(β, bi, Xi)) ≃ Zi(β, Xi)ΨZ′ i(β, Xi), where Zi(β, Xi) = ∂f(β, bi, Xi) ∂bi ⏐⏐⏐⏐ bi=0 . The same comments a"
8974,unknown,"∂bi ⏐⏐⏐⏐ bi=0 . The same comments and recommendations discussed in Section 3 for the LME model also apply here. The conditional approach attempts to explain the covariance structure of the response mostly via the random effects component of the marginal covariance, while the marginal approach focuses on the within-group component. Care should be exercised in not making both components too complex,"
8975,unknown,"in not making both components too complex, to avoid overparameterization problems. Unlike the LME case, however, the marginal covariance in the NLME model also de- pends on the ﬁxed effects, which makes the problem more complex. In addition, because 40 Austrian Journal of Statistics, V ol. 35 (2006), No. 1, 31–44 of the need to approximate the likelihood function of the NLME model to make the esti"
8976,unknown,"mation problem feasible, the relationship between the marginal covariance matrix in (9) and estimation results for the ﬁxed effects is not as clear cut as in the LME model case. Another important difference worth pointing out between the conditional and marginal models in the nonlinear case is in the interpretation of the parameters β. In the condi- tional model, the ﬁxed effects are associated wi"
8977,unknown,"tional model, the ﬁxed effects are associated with the response of typical individual (i.e., with bi = 0), while in the marginal model, these parameters are associated with the average response across the individuals in the population. 4.1 Dialyzer Example: Nonlinear Model Version An empirical fourth-order linear polynomial model proposed by Littell et al. (1996) was used in Section 3.1 to represe"
8978,unknown,"used in Section 3.1 to represent the Dialyzer data. The model originally proposed for these data by V onesh and Carter (1992) is an asymptotic regression model with an offset, which expresses the expected ultraﬁltration rate y at transmembrane pressure x as E (y) =φ1 {1 − exp [−exp(φ2) (x − φ3)]} . (10) Unlike the parameters in the empirical linear model of Section 3.1, the parameters in model (10"
8979,unknown,"model (10) have a physiological interpretation: φ1 is the maximum ultraﬁltration rate that can be attained, φ2 is the logarithm of the hydraulic permeability transport rate, and φ3 is the transmembrane pressure required to offset the oncotic pressure. V onesh and Carter (1992) suggest using different parameters in (10) for each blood ﬂow rate level. After several model building steps, the followin"
8980,unknown,"ﬂow rate level. After several model building steps, the following NLME version of (10) was selected to represent the ultraﬁltration rate yij at the jth transmembrane pressure xij for the ith subject yij = (β1 + γ1Qi + b1i) {1 − exp [−exp (β2 + γ2Qi + b2i) (xij − β3)]} + ϵij , (11) bi = [b1i b2i ] ind ∼N (0, Ψ) , ϵ ij ind ∼N ( 0, σ2xδ ij ) , where, as in Section 3.1, Qi is a binary indicator taking"
8981,unknown,"ij ) , where, as in Section 3.1, Qi is a binary indicator taking values 0 for 200 dl/min hemodia- lyzers and 1 for 300 dl/min hemodialyzers;β1, β2, and β3 are, respectively, the asymptotic ultraﬁltration rate, the log-transport rate, and the transmembrane pressure offset ﬁxed ef- fects corresponding to 200 dl/min dialyzers; γk is the blood ﬂow effect associated with the ﬁxed effect βk, k = 1, 2; b"
8982,unknown,"the ﬁxed effect βk, k = 1, 2; bi is the vector of random effects, assumed independent for different i; and ϵij is the within-group error, assumed independent for different i, jand independent of the random effects. The variance of the ϵij is allowed to change with the transmembrane pressure according to a power model with parameterδ (estimated together with the other parameters in the model.) Rand"
8983,unknown,"Random effects are used in the NLME model (11) to account for within-subject cor- relation, as well as some of the heteroscedasticity in the response. A variance function model is used in the within-subject covariance matrix Λi to accommodate the remaining heteroscedasticity in the data. Overall, this approach falls within the conditional model category. J. Pinheiro 41 Alternatively, a marginal no"
8984,unknown,"used to represent the data. yij = (β1 + γ1Qi) {1 − exp [−exp (β2 + γ2Qi) (xij − β3)]} + ϵij , (12) ϵij ind ∼N ( 0, σ2xδ ij ) , corr(ϵij, ϵik) =ρ|j−k| . This model has the same ﬁxed effects and variance function as (11), but the correlation among measurements is now modelled by an AR(1) structure in the within-subject errors. As in the linear model analysis of Section 3.1, three additional marginal"
8985,unknown,"As in the linear model analysis of Section 3.1, three additional marginal nonlinear models similar to (5) were considered, with within-subject correlation structures: Gaussian spatial correlation with a nugget effect, ARMA(1,3), and unstructured. The conditional and marginal models described above were ﬁtted using, respectively, the functions nlme and gnls in the NLME library in S-PLUS, with appro"
8986,unknown,"timation for the NLME model and exact ML estimation for the marginal models. The simulation approach outlined previously (based on sampling random effects from their es- timated distribution) was used to estimate the marginal covariance matrices in the NLME model. Note that, because of the dependency of Σi on β, different matrices are obtained for 200 dl/min and 300 dl/min dialyzers. For simplicit"
8987,unknown,"dialyzer estimates. 0 5 10 15 20 0 10 20 30 40 AR(1) + + + + + + + Gaussian + + + + + + + ARMA(1,3) + + + + + + + 0 5 10 15 20 0 10 20 30 40 Unstructured + + + + + + + Marginal model Conditional model +Covariance Variance Figure 3: Conditional versus marginal estimates for variance and covariance parameters in the Dialyzer nonlinear models. Conditional model estimates correspond to 200 dl/min dial"
8988,unknown,"dialyzers. Dotted line represents y = x reference line. 42 Austrian Journal of Statistics, V ol. 35 (2006), No. 1, 31–44 Figure 3 shows the scatter plots of the conditional model estimates for variance and covariance parameters in Σi associated with 200 dl/min dialyzers versus the correspond- ing estimates using the different marginal models. There is now considerably less agreement between the es"
8989,unknown,"There is now considerably less agreement between the estimates from the conditional and marginal models than in the linear model case depicted in Figure 2. This is consistent with the fact that, for nonlinear models, the two approaches are not equivalent, leading to different representations of the marginal covariance structures. As in the linear case, it is instructive to investigate the impact o"
8990,unknown,effects. ## NLME fit (conditional model) Value Std.Error DF t-value p-value Asym.(Intercept) 46.133 1.2359 116 37.326 <.0001 Asym.QB 16.974 1.8689 116 9.082 <.0001 lrc.(Intercept) 0.658 0.0598 116 10.992 <.0001 lrc.QB -0.454 0.0796 116 -5.702 <.0001 c0 0.217 0.0035 116 62.971 <.0001 ## AR(1) marginal model Value Std.Error t-value p-value Asym.(Intercept) 46.911 1.5343 30.576 <.0001 Asym.QB 16.400 
8991,unknown,"lrc.(Intercept) 0.542 0.0538 10.069 <.0001 lrc.QB -0.339 0.0748 -4.538 <.0001 c0 0.215 0.0043 50.527 <.0001 ## Spatial Gaussian marginal model Value Std.Error t-value p-value Asym.(Intercept) 47.122 1.6343 28.833 <.0001 Asym.QB 15.877 2.6108 6.081 <.0001 lrc.(Intercept) 0.502 0.0572 8.775 <.0001 lrc.QB -0.301 0.0807 -3.731 3e-04 c0 0.214 0.0042 50.653 <.0001 ## ARMA(1,3) marginal model Value Std.E"
8992,unknown,Value Std.Error t-value p-value Asym.(Intercept) 47.037 1.5952 29.487 <.0001 Asym.QB 15.955 2.5515 6.253 <.0001 lrc.(Intercept) 0.510 0.0568 8.976 <.0001 lrc.QB -0.308 0.0802 -3.835 2e-04 c0 0.214 0.0042 50.476 <.0001 ## Unstructured marginal model Value Std.Error t-value p-value Asym.(Intercept) 47.229 1.3340 35.405 <.0001 Asym.QB 15.938 2.1380 7.455 <.0001 lrc.(Intercept) 0.451 0.0485 9.289 <.00
8993,unknown,"lrc.QB -0.274 0.0683 -4.017 1e-04 c0 0.209 0.0041 50.672 <.0001 J. Pinheiro 43 Even though there are noticeable differences in the estimates of the marginal covari- ance matrices, the ﬁxed effects estimates obtained via the marginal and conditional ap- proaches are fairly similar and lead to the same inferences about the differences between the two types of dialyzers. The greater discrepancies are"
8994,unknown,"the two types of dialyzers. The greater discrepancies are observed between the unstruc- tured marginal model and the NLME conditional model, but even these are not of consid- erable magnitude. 5 Conclusions This paper describes and contrasts the conditional and marginal modelling approaches in the context of linear and nonlinear regression models. In the linear case, the marginal covariance matrix"
8995,unknown,covariance matrix corresponding to the responses in a linear mixed-effects model is ex- pressed as a sum of two components: one determined solely by the random effects model (associated with the conditional approach) and another determined by the within-group error covariance (associated with the marginal approach.) The two approaches are equiv- alent in the sense that they directly model the marg
8996,unknown,"alent in the sense that they directly model the marginal covariance of the responses and that both can use the same estimation method (maximum likelihood or restricted maxi- mum likelihood.) In the nonlinear case, however, the association between random effects model and the marginal covariance matrix is less direct, depending also on the ﬁxed effects model. In addition, because the random effects"
8997,unknown,"addition, because the random effects typically occur nonlinearly in an NLME model, for computational reasons exact likelihood methods are seldom used. Likelihood methods, on the other hand, can be easily implemented with the marginal approach. As a consequence, the two approaches can not be considered as equivalent, even though similar inferences on the ﬁxed effects are usually obtained. Perhaps t"
8998,unknown,"Perhaps the most important differences between the two approaches have to do with the questions that they are intended to answer. If the primary questions of interest refer to the ﬁxed effects only, then, depending on the choice of random effects and within-group covariance, both methods could be equivalent in the linear case, and generally nearly equivalent in the nonlinear case. However, if one "
8999,unknown,"equivalent in the nonlinear case. However, if one is interested in using predicted random effects to explore the impact of covariates on inter-subject variation, like is often done in pharmacokinetics/pharmacodynamics modeling (Davidian and Giltinan, 1995), then the conditional approach would be the natural choice. References Bates, D. M., and Watts, D. G. (1988).Nonlinear regression analysis and "
9000,unknown,"New York: Wiley. Box, G. E. P., Jenkins, G. M., and Reinsel, G. C. (1994).Time series analysis: Forecasting and control (3rd ed.). San Francisco: Holden-Day. Davidian, M., and Gallant, A. R. (1992). Smooth nonparametric maximum likelihood estimation for population pharmacokinetics, with application to quinidine. Journal of Pharmacokinetics and Biopharmaceutics, 20, 529–556. 44 Austrian Journal of "
9001,unknown,"44 Austrian Journal of Statistics, V ol. 35 (2006), No. 1, 31–44 Davidian, M., and Giltinan, D. M. (1995). Nonlinear models for repeated measurement data. London: Chapman & Hall. Jennrich, R. I., and Schluchter, M. D. (1986). Unbalanced repeated measures models with structural covariance matrices. Biometrics, 42(4), 805–820. Laird, N. M., and Ware, J. H. (1982). Random-effects models for longitudi"
9002,unknown,"Biometrics, 38, 963–974. Lindstrom, M. J., and Bates, D. M. (1990). Nonlinear mixed-effects models for repeated measures data. Biometrics, 46, 673–687. Littell, R. C., Milliken, G. A., Stroup, W. W., and Wolﬁnger, R. D. (1996).Sas system for mixed models. Cary, NC: SAS Institute Inc. Mallet, A., Mentre, F., Steimer, J.-L., and Lokiek, F. (1988). Nonparametric maxi- mum likelihood estimation for po"
9003,unknown,"mum likelihood estimation for population pharmacokinetics, with applications to Cyclosporine. Journal of Pharmacokinetics and Biopharmaceutics, 16, 311–327. Pinheiro, J. C., and Bates, D. M. (2000). Mixed-effects models in S and S-PLUS . New York: Springer-Verlag. Ramos, R. Q., and Pantula, S. G. (1995). Estimation of nonlinear random coefﬁcient models. Statistics & Probability Letters, 24, 49–56."
9004,unknown,"Searle, S. R., Casella, G., and McCulloch, C. E. (1992). Variance components. New York: Wiley. Sheiner, L. B., and Beal, S. L. (1980). Evaluation of methods for estimating popu- lation pharmacokinetic parameters. I. Michaelis–Menten model: Routine clinical pharmacokinetic data. Journal of Pharmacokinetics and Biopharmaceutics , 8(6), 553–571. V onesh, E. F., and Carter, R. L. (1992). Mixed-effects"
9005,unknown,"repeated measures. Biometrics, 48, 1–18. V onesh, E. F., and Chinchilli, V . M. (1997).Linear and nonlinear models for the analysis of repeated measures. New York: Marcel Dekker. Wakeﬁeld, J. C., Smith, A. F. M., Racine-Poon, A., and Gelfand, A. E. (1994). Bayesian analysis of linear and nonlinear population models using the Gibbs sampler.Applied Statistics, 43, 201–221. Authors’ address: Jos´e C."
9006,unknown,"Jos´e C. Pinheiro Department of Biostatistics Novartis Pharmaceuticals One Health Plaza East Hanover, NJ 07934 U.S.A. E-Mail: jose.pinheiro@novartis.com 1 RANDOM FORESTS Leo Breiman Statistics Department University of California Berkeley, CA 94720 January 2001 Abstract Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independe"
9007,unknown,vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to spl
9008,unknown,"error rates that compare favorably to Adaboost (Freund and Schapire[1996]), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression. 2 1. Rand"
9009,unknown,"applicable to regression. 2 1. Random Forests 1.1 Introduction Significant improvements in classification accuracy have resulted from growing an ensemble of trees and letting them vote for the most popular class. In order to grow these ensembles, often random vectors are generated that govern the growth of each tree in the ensemble. An early example is bagging (Breiman [1996]), where to grow each "
9010,unknown,made from the examples in the training set. Another example is random split selection (Dietterich [1998]) where at each node the split is selected at random from among the K best splits. Breiman [1999] generates new training sets by randomizing the outputs in the original training set. Another approach is to select the training set from a random set of weights on the examples in the training set. 
9011,unknown,"on the examples in the training set. Ho [1998] has written a number of papers on ""the random subspace"" method which does a random selection of a subset of features to use to grow each tree. In an important paper on written character recognition, Amit and Geman [1997] define a large number of geometric features and search over a random selection of these for the best split at each node. This latter"
9012,unknown,"of these for the best split at each node. This latter paper has been influential in my thinking. The common element in all of these procedures is that for the kth tree, a random vector Θ k is generated, independent of the past random vectors Θ 1, ... ,Θ k−1 but with the same distribution; and a tree is grown using the training set and Θ k, resulting in a classifier h(x,Θ k ) where x is an input ve"
9013,unknown,"instance, in bagging the random vector Θ is generated as the counts in N boxes resulting from N darts thrown at random at the boxes, where N is number of examples in the training set. In random split selection Θ consists of a number of independent random integers between 1 and K. The nature and dimensionality of Θ depends on its use in tree construction. After a large number of trees is generated,"
9014,unknown,"We call these procedures random forests. Definition 1.1 A random forest is a classifier consisting of a collection of tree- structured classifiers {h(x,Θ k ),k=1, ...} where the {Θ k} are independent identically distributed random vectors and each tree casts a unit vote for the most popular class at input x . 1.2 Outline of Paper Section 2 gives some theoretical background for random forests. Use "
9015,unknown,Strong Law of Large Numbers shows that they always converge so that overfitting is not a problem. We give a simplified and extended version of the 3 Amit and Geman [1997] analysis to show that the accuracy of a random forest depends on the strength of the individual tree classifiers and a measure of the dependence between them (see Section 2 for definitions). Section 3 introduces forests using the
9016,unknown,"Section 3 introduces forests using the random selection of features at each node to determine the split. An important question is how many features to select at each node. For guidance, internal estimates of the generalization error, classifier strength and dependence are computed. These are called out-of-bag estimates and are reviewed in Section 4. Sections 5 and 6 give empirical results for two "
9017,unknown,"for two different forms of random features. The first uses random selection from the original inputs; the second uses random linear combinations of inputs. The results compare favorably to Adaboost. The results turn out to be insensitive to the number of features selected to split each node. Usually, selecting one or two features gives near optimum results. To explore this and relate it to strengt"
9018,unknown,"carried out in Section 7. Adaboost has no random elements and grows an ensemble of trees by successive reweightings of the training set where the current weights depend on the past history of the ensemble formation. But just as a deterministic random number generator can give a good imitation of randomness, my belief is that in its later stages Adaboost is emulating a random forest. Evidence for t"
9019,unknown,"conjecture is given in Section 8. Important recent problems, i.e.. medical diagnosis and document retrieval , often have the property that there are many input variables, often in the hundreds or thousands, with each one containing only a small amount of information. A single tree classifier will then have accuracy only slightly better than a random choice of class. But combining trees grown using"
9020,unknown,"features can produce improved accuracy. In Section 9 we experiment on a simulated data set with 1,000 input variables, 1,000 examples in the training set and a 4,000 example test set. Accuracy comparable to the Bayes rate is achieved. In many applications, understanding of the mechanism of the random forest ""black box"" is needed. Section 10 makes a start on this by computing internal estimates of "
9021,unknown,estimates of variable importance and binding these together by reuse runs. Section 11 looks at random forests for regression. A bound for the mean squared generalization error is derived that shows that the decrease in error from the individual trees in the forest depends on the correlation between residuals and the mean squared error of the individual trees. Empirical results for regression are i
9022,unknown,"for regression are in Section 12. Concluding remarks are given in Section 13. 2 Characterizing the Accuracy of Random Forests 2.1 Random Forests Converge 4 Given an ensemble of classifiers h1(x),h2(x), ... ,hK (x), and with the training set drawn at random from the distribution of the random vector Y, X, define the margin function as mg (X ,Y )= avkI(hk (X )=Y )− max j≠ Y avkI(hk (X )= j). where I"
9023,unknown,"where I(•) is the indicator function. The margin measures the extent to which the average number of votes at X ,Y for the right class exceeds the average vote for any other class. The larger the margin, the more confidence in the classification. The generalization error is given by PE * = PX ,Y (mg (X ,Y )< 0) where the subscripts X ,Y indicate that the probability is over the X ,Y space. In rando"
9024,unknown,"In random forests, hk(X )= h(X ,Θ k ). For a large number of trees, it follows from the Strong Law of Large Numbers and the tree structure that: Theorem 1.2 As the number of trees increases, for almost surely all sequences Θ 1 ,... PE * converges to PX ,Y (PΘ (h(X ,Θ )=Y )−max j≠Y PΘ (h(X ,Θ )= j)< 0) (1) Proof: see Appendix I. This result explains why random forests do not overfit as more trees a"
9025,unknown,"but produce a limiting value of the generalization error. 2.2 Strength and Correlation For random forests, an upper bound can be derived for the generalization error in terms of two parameters that are measures of how accurate the individual classifiers are and of the dependence between them. The interplay between these two gives the foundation for understanding the workings of random forests. We "
9026,unknown,"forests. We build on the analysis in Amit and Geman [1997]. Definition 2.1 The margin function for a random forest is mr (X ,Y )= PΘ (h(X ,Θ )=Y )− max j≠Y PΘ (h(X ,Θ )= j) (2) and the strength of the set of classifiers {h(x,Θ )} is s= E X ,Y mr (X ,Y ) (3) 5 Assuming s ≥ 0, Chebychev’s inequality gives PE * ≤ var(mr )/s2 (4) A more revealing expression for the variance of mr is derived in the fol"
9027,unknown,"A more revealing expression for the variance of mr is derived in the following: Let ˆj(X ,Y )=arg maxj≠Y PΘ (h(X ,Θ )= j) so mr (X ,Y )= PΘ (h(X ,Θ )=Y )− PΘ (h(X ,Θ )= ˆj(X ,Y )) = EΘ [I(h(X ,Θ )=Y )− I(h(X ,Θ )= ˆj(X ,Y ))]. Definition 2.2 The raw margin function is rmg (Θ ,X ,Y )= I(h(X ,Θ )=Y )− I(h(X ,Θ )= ˆj(X ,Y )). Thus, mr (X ,Y ) is the expectation of rmg(Θ ,X ,Y ) with respect to Θ . Fo"
9028,unknown,"function f the identity [EΘ f(Θ )]2 = EΘ ,Θ 'f(Θ )f(Θ ') holds where Θ ,Θ ' are independent with the same distribution, implying that mr (X ,Y )2 = EΘ ,Θ 'rmg (Θ ,X ,Y )rmg (Θ ',X ,Y ) (5) Using (5) gives var(mr )=EΘ ,Θ '(covX ,Y rmg (Θ ,X ,Y )rmg (Θ ',X ,Y )) = EΘ ,Θ '(ρ(Θ ,Θ ')sd(Θ )sd(Θ ')) (6) where ρ (Θ ,Θ ') is the correlation between rmg (Θ ,X ,Y )andrmg (Θ ',X ,Y ) holding Θ ,Θ ' fixed and"
9029,unknown,"Θ ,Θ ' fixed and sd(Θ ) is the standard deviation of rmg (Θ ,X ,Y ) holding Θ fixed. Then, var(mr )= ρ (EΘ sd(Θ ))2 (7) ≤ ρ EΘ var(Θ ) where ρ is the mean value of the correlation; that is, 6 ρ = EΘ ,Θ '(ρ(Θ ,Θ ')sd(Θ )sd(Θ '))/EΘ ,Θ '(sd(Θ )sd(Θ ')) Write EΘ var(Θ )≤ EΘ (EX ,Y rmg (Θ ,X ,Y ))2 − s2 ≤ 1− s2. (8) Putting (4), (7), and (8) together yields:"
9030,unknown,"≤ 1− s2. (8) Putting (4), (7), and (8) together yields: Theorem 2.3 An upper bound for the generalization error is given by PE * ≤ ρ (1− s2 )/s2 Although the bound is likely to be loose, it fulfills the same suggestive function for random forests as VC-type bounds do for other types of classifiers. It shows that the two ingredients involved in the generalization error for random forests are the st"
9031,unknown,"are the strength of the individual classifiers in the forest, and the correlation between them in terms of the raw margin functions. The c/s2 ratio is the correlation divided by the square of the strength. In understanding the functioning of random forests, this ratio will be a helpful guide--the smaller it is, the better. Definition 2.4 The c/s2 ratio for a random forest is defined as c / s2 = ρ "
9032,unknown,"c / s2 = ρ / s2 There are simplifications in the two class situation. The margin function is mr (X ,Y )=2PΘ (h(X ,Θ )=Y )−1 The requirement that the strength is positive (see (4)) becomes similar to the familiar weak learning condition EX ,Y PΘ (h(X ,Θ ) = Y )>.5. The raw margin function is 2I(h(X ,Θ )=Y )−1 and the correlation ρ is between I(h(X ,Θ )=Y ) and I(h(X ,Θ ')=Y ) . In particular, if th"
9033,unknown,"ρ = EΘ ,Θ '[ρ(h(⋅,Θ ),h(⋅,Θ ')] so that ρ is the correlation between two different members of the forest averaged over the Θ , Θ ' distribution. 7 For more than two classes, the measure of strength defined in (3) depends on the forest as well as the individual trees since it is the forest that determines ˆj(X ,Y ). Another approach is possible. Write PE * = PX ,Y (PΘ (h(X ,Θ )=Y )− max j≠Y PΘ (h(X"
9034,unknown,"≤ PX ,Y (PΘ (h(X ,Θ )=Y )− PΘ (h(X ,Θ )= j)<0) j ∑ . Define sj= EX ,Y (PΘ (h(X ,Θ )=Y )− PΘ (h(X ,Θ )= j)) to be the strength of the set of classifiers {h(x,Θ )} relative to class j. Note that this definition of strength does not depend on the forest. Using Chebyshev's inequality, assuming all s j>0 leads to PE *≤ var( j ∑ PΘ (h(X ,Θ )=Y )− PΘ (h(X ,Θ )= j))/sj 2 (9) and using identities similar t"
9035,unknown,"2 (9) and using identities similar to those used in deriving (7), the variances in (9) can be expressed in terms of average correlations. I did not use estimates of the quantities in (9) in our empirical study but think they would be interesting in a multiple class problem. 3. Using Random Features Some random forests reported in the literature have consistently lower generalization error than oth"
9036,unknown,"generalization error than others. For instance, random split selection (Dieterrich [1998]) does better than bagging. Breiman's introduction of random noise into the outputs (Breiman [1998c]) also does better. But none of these three forests do as well as Adaboost (Freund and Schapire [1996]) or other algorithms that work by adaptive reweighting (arcing) of the training set (see Breiman [1998b], Di"
9037,unknown,"Dieterrich [1998], Bauer and Kohavi [1999]). To improve accuracy, the randomness injected has to minimize the correlation ρ while maintaining strength. The forests studied here consist of using randomly selected inputs or combinations of inputs at each node to grow each tree. The resulting forests give accuracy that compare favorably with Adaboost. This class of procedures has desirable characteri"
9038,unknown,"i) Its accuracy is as good as Adaboost and sometimes better. ii) It's relatively robust to outliers and noise. iii) It's fas ter than bagging or boosting. iv) It gives useful internal estimates of error, strength, correlation and variable importance. v) It's simple and easily parallelized. 8 Amit and Geman [1997] grew shallow trees for handwritten character recognition using random selection from "
9039,unknown,"recognition using random selection from a large number of geometrically defined features to define the split at each node. Although my implementation is different and not problem specific, it was their work that provided the start for my ideas 3.1 Using Out-Of-Bag Estimates to Monitor Error, Strength, and Correlation In my experiments with random forests, bagging is used in tandem with random feat"
9040,unknown,"random feature selection. Each new training set is drawn, with replacement, from the original training set. Then a tree is grown on the new training set using random feature selection. The trees grown are not pruned. There are two reasons for using bagging. The first is that the use of bagging seems to enhance accuracy when random features are used. The second is that bagging can be used to give o"
9041,unknown,"bagging can be used to give ongoing estimates of the generalization error (PE*) of the combined ensemble of trees, as well as estimates for the strength and correlation. These estimates are done out-of-bag, which is explained as follows. Assume a method for constructing a classifier from any training set. Given a specific training set T, form bootstrap training sets T k, construct classifiers h( x"
9042,unknown,"Tk) and let these vote to form the bagged predictor. For each y ,x in the training set, aggregate the votes only over those classifiers for which T k does not containing y ,x. Call this the out-of-bag classifier. Then the out-of-bag estimate for the generalization error is the error rate of the out-of-bag classifier on the training set. Tibshirani [1996] and Wolpert and Macready [1996], proposed u"
9043,unknown,"estimates as an ingredient in estimates of generalization error. Wolpert and Macready worked on regression type problems and proposed a number of methods for estimating the generalization error of bagged predictors. Tibshirani used out-of-bag estimates of variance to estimate generalization error for arbitrary classifiers. The study of error estimates for bagged classifiers in Breiman [1996b], giv"
9044,unknown,"Breiman [1996b], gives empirical evidence to show that the out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set. In each bootstrap training set, about one-third of the instances are left out. Therefore, the out-of-bag estimates are based on combining only about one- thir"
9045,unknown,"third as many classifiers as in the ongoing main combination. Since the error rate decreases as the number of combinations increases, the out-of-bag estimates will tend to overestimate the current error rate. To get unbiased out- of-bag estimates, it is necessary to run past the point where the test set error converges. But unlike cross-validation, where bias is present but its extent unknown, the"
9046,unknown,"unknown, the out-of-bag estimates are unbiased. Strength and correlation can also be estimated using out-of-bag methods. This gives internal estimates that are helpful in understanding classification accuracy 9 and how to improve it. The details are given in Appendix II. Another application is to the measures of variable importance (see Section 10). 4.Random Forests Using Random Input Selection Th"
9047,unknown,"The simplest random forest with random features is formed by selecting at random, at each node, a small group of input variables to split on. Grow the tree using CART methodology to maximum size and do not prune. Denote this procedure by Forest-RI. The size F of the group is fixed. Two values of F were tried. The first used only one randomly selected variable, i.e., F=1. The second took F to be th"
9048,unknown,"took F to be the first integer less than log 2M+1, where M is the number of inputs. My experiments use 13 smaller sized data sets from the UCI repository, 3 larger sets separated into training and test sets and 4 synthetic data sets. The first 10 sets were selected because I had used them in past research. Table 1 gives a brief summary. Table 1 Data Set Summary Data Set Train Size Test Size Inputs"
9049,unknown,glass 214 -- 9 6 breast cancer 699 -- 9 2 diabetes 768 -- 8 2 sonar 208 -- 60 2 vowel 990 -- 10 11 ionosphere 351 -- 34 2 vehicle 846 -- 18 4 soybean 685 -- 35 19 German credit 1000 ` -- 24 2 image 2310 -- 19 7 ecoli 336 -- 7 8 votes 435 -- 16 2 liver 345 -- 6 2 letters 15000 5000 16 26
9050,unknown,"liver 345 -- 6 2 letters 15000 5000 16 26 sat-images 4435 2000 36 6 zip-code 7291 2007 256 10 waveform 300 3000 21 3 twonorm 300 3000 20 2 threenorm 300 3000 20 2 ringnorm 300 3000 20 2 10 On each of the 13 smaller sized data sets, the following procedure was used: a"
9051,unknown,"10 On each of the 13 smaller sized data sets, the following procedure was used: a random 10% of the data was set aside. On the remaining data, random forest was run twice, growing and combining 100 trees--once with F=1, and the second time with F=int(log 2M+1). The set aside 10% was then put down each forest to get a test set error for both. The test set error selected corresponded to the lower va"
9052,unknown,lower value of the out-of-bag estimate in the two runs. This was repeated 100 times and the test set errors averaged. The same procedure was followed for the Adaboost runs which are based on combining 50 trees. The use for 100 trees in random forests and 50 for Adaboost comes from two sources. The out-of-bag estimates are based on only about a third as many trees as are in the forest. To get relia
9053,unknown,"as are in the forest. To get reliable estimates I opted for 100 trees. The second consideration is that growing random forests is many times faster than growing the trees based on all inputs needed in Adaboost. Growing the 100 trees in random forests was considerably quicker than the 50 trees for Adaboost. In the runs on the larger data sets, the random forest results for the first two data sets w"
9054,unknown,"data sets were based on combining 100 trees; the zip-code procedure combined 200. For Adaboost, 50 trees were combined for the first three data sets and 100 for zip-code. The synthetic data was described in Breiman [1996] and also used in Schapire et al [1997]. There were 50 runs. In each run, a new training set of size 300 and test set of size 3000 were generated. In random forests 100 trees were"
9055,unknown,were combined in each run--50 in Adaboost. The results of these runs are given in Table 2. The second column are the results selected from the two group sizes by means of lowest out-of-bag error. The third column is the test set error using just one random feature to grow the trees. The fourth column contains the out-of-bag estimates of the generalization error of the individual trees in the fores
9056,unknown,computed for the best setting (single or selection). This estimate is computed by using the left-out instances as a test set in each tree grown and averaging the result over all trees in the forest. Table 2 Test Set Errors (%) Data Set Adaboost Forest-RI Selection Single Input One Tree glass 22.0 20.6 21.2 36.9 breast cancer 3.2 2.9 2.7 6.3 diabetes 26.6 24.2 24.3 33.1 sonar 15.6 15.9 18.0 31.7
9057,unknown,breast cancer 3.2 2.9 2.7 6.3 diabetes 26.6 24.2 24.3 33.1 sonar 15.6 15.9 18.0 31.7 vowel 4.1 3.4 3.3 30.4 ionosphere 6.4 7.1 7.5 12.7 vehicle 23.2 25.8 26.4 33.1 German credit 23.5 24.4 26.2 33.3 image 1.6 2.1 2.7 6.4 11 ecoli 14.8 12.8 13.0 24.5 votes 4.8 4.1 4.6 7.4 liver 30.7 25.1 24.7 40.6 letters 3.4 3.5 4.7 19.8 sat-images 8.8 8.6 10.5 17.2 zip-code 6.2 6.3 7.8 20.6 waveform 17.8 17.2 17.3
9058,unknown,sat-images 8.8 8.6 10.5 17.2 zip-code 6.2 6.3 7.8 20.6 waveform 17.8 17.2 17.3 34.0 twonorm 4.9 3.9 3.9 24.7 threenorm 18.8 17.5 17.5 38.4 ringnorm 6.9 4.9 4.9 25.7 The error rates using random input selection compare favorably with Adaboost. The comparison might be even more favorable if the search is over more values of F instead of the preset two. But the procedure is not overly sensitive to th
9059,unknown,"of F instead of the preset two. But the procedure is not overly sensitive to the value of F. The average absolute difference between the error rate using F=1 and the higher value of F is less than 1%. The difference is most pronounced on the three large data sets. The single variable test set results were included because in some of the data sets, using a single random input variable did better th"
9060,unknown,"using a single random input variable did better than using several. In the others, results were only slightly better than use of a single variable. It was surprising that using a single randomly chosen input variable to split on at each node could produce good accuracy. Random input selection can be much faster than either Adaboost or Bagging. A simple analysis shows that the ratio of R2I compute "
9061,unknown,"unpruned tree construction using all variables is F *log 2(N )/M where F is the number of variables used in Forest-RI, N is the number of instances, and M the number of input variables. For zip-code data, using F=1, the ratio is .025, implying that Forest-RI is 40 times faster. An empirical check confirmed this difference. A Forest-RI run (F=1) on the zip-code data takes 4.0 minutes on a 250 Mhz M"
9062,unknown,"Mhz Macintosh to generate 100 trees compared to almost three hours for Adaboost. For data sets with many variables, the compute time difference may be significant. 5. Random Forests Using Linear Combinations of Inputs If there are only a few inputs, say M, taking F an appreciable fraction of M might lead an increase in strength but higher correlation. Another approach consists of defining more fea"
9063,unknown,"defining more features by taking random linear combinations of a number of the input variables. That is, a feature is generated by specifying L, the number of variables to be combined. At a given node, L variables are randomly selected and added together with coefficients that are uniform random numbers on [-1,1]. F linear combinations are generated, and then a search is made over these for the be"
9064,unknown,"the best split. This procedure is called Forest-RC . 12 We use L=3 and F=2, 8 with the choice for F being decided on by the out-of-bag estimate. We selected L=3 because with O(M 3) different combinations of the input variables, larger values of F should not cause much of a rise in correlation while increasing strength. If the input variables in a data set are incommensurable, they are normalized b"
9065,unknown,"incommensurable, they are normalized by subtracting means and dividing by standard deviations, where the means and standard deviations are determined from the training set. The test set results are given in Table 3 where the third column contains the results for F=2. The fourth column contains the results for individual trees computed as for Table 2. `` Table 3 Test Set Errors (%) Data Set Adaboos"
9066,unknown,Selection Two Features One Tree glass 22.0 24.4 23.5 42.4 breast cancer 3.2 3.1 2.9 5.8 diabetes 26.6 23.0 23.1 32.1 sonar 15.6 13.6 13.8 31.7 vowel 4.1 3.3 3.3 30.4 ionosphere 6.4 5.5 5.7 14.2 vehicle 23.2 23.1 22.8 39.1 German credit 23.5 22.8 23.8 32.6 image 1.6 1.6 1.8 6.0 ecoli 14.8 12.9 12.4 25.3 votes 4.8 4.1 4.0 8.6 liver 30.7 27.3 27.2 40.3 letters 3.4 3.4 4.1 23.8 sat-images 8.8 9.1 10.2
9067,unknown,"liver 30.7 27.3 27.2 40.3 letters 3.4 3.4 4.1 23.8 sat-images 8.8 9.1 10.2 17.3 zip-code 6.2 6.2 7.2 22.7 waveform 17.8 16.0 16.1 33.2 twonorm 4.9 3.8 3.9 20.9 threenorm 18.8 16.8 16.9 34.8 ringnorm 6.9 4.8 4.6 24.6 Except for the three larger data sets, use of F=8 is superflous; F=2 achieves close to the minimum. On the larger data sets, F=8 gives better results. Forest-RC does exceptionally well"
9068,unknown,"does exceptionally well on the synthetic data sets. Overall, it compares more favorably to Adaboost than Forest-RI. In Tables 2 and 3 there are some entries for which the selected entry is less than the one input value or with Forest-RC, less than the two-feature value. The reason this happens is that when the error rates corresponding to the two values of F are close together, then the out-of-bag"
9069,unknown,"almost at random. 13 A small investigation was carried out to see if performance on the three larger data sets could be improved. Based on the runs with the satellite data in Section 6, we conjectured that the strength keeps rising in the larger data sets while the correlation reaches an asymptote more quickly. Therefore we did some runs with F=100 on the larger data sets using 100, 100 and 200 tr"
9070,unknown,"forests respectively. On the satellite data, the error dropped to 8.5%, on the letters data to 3.0%, but the zip-code test set error did not decrease. Acting on an informed hunch, I tried Forest-RI with F=25. The zip-code test set error dropped to 5.8%. These are the lowest test set errors so far achieved on these three data sets by tree ensembles. 5.1 Categorical Variables Some or all of the inpu"
9071,unknown,"Categorical Variables Some or all of the input variables may be categoricals and since we want to define additive combinations of variables, we need to define how categoricals will be treated so they can be combined with numerical variables. My approach is that each time a categorical variable is selected to split on at a node, to select a random subset of the categories of the variable, and defin"
9072,unknown,"random subset of the categories of the variable, and define a substitute variable that is one when the categorical value of the variable is in the subset and zero outside. Since a categorical variable with I values can be coded into I-1 dummy 0-1 variables, we make the variable I-1 times as probable as a numeric variable to be selected in node splitting. When many of the variables are categorical,"
9073,unknown,"low value of F results in low correlation, but also low strength. F must be increased to about two-three times int(log 2M+1) to get enough strength to provide good test set accuracy. For instance, on the DNA data set having 60 four-valued categorical values, 2,000 examples in the training set and 1,186 in the test set, using Forest-RI with F=20 gave a test set error rate of 3.6% (4.2% for Adaboost"
9074,unknown,"gave a test set error rate of 3.6% (4.2% for Adaboost). The soybean data has 685 examples, 35 variables, 19 classes, and 15 categorical variables. Using Forest-RI with F=12 gives a test set error of 5.3% (5.8% for Adaboost). Using Forest-RC with combinations of 3 and F=8 gives an error of 5.5%. One advantage of this approach is that it gets around the difficulty of what to do with categoricals tha"
9075,unknown,"with categoricals that have many values. In the two-class problem, this can be avoided by using the device proposed in Breiman et al [1985] which reduces the search for the best categorical split to an O(I) computation. For more classes, the search for the best categorical split is an O(2 I-1) computation. In the random forest implementation, the computation for any categorical variable involves o"
9076,unknown,variable involves only the selection of a random subset of the categories. 6. Empirical Results on Strength and Correlation The purpose of this section is to look at the effect of strength and correlation on the generalization error. Another aspect that we wanted to get more understanding of was the lack of sensitivity in the generalization error to the 14 group size F. To conduct an empirical stu
9077,unknown,"14 group size F. To conduct an empirical study of the effects of strength and correlation in a variety of data sets, out-of-bag estimates of the strength and correlation, as described in Section 3.1, were used. We begin by running Forest-RI on the sonar data (60 inputs, 208 examples) using from 1 to 50 inputs. In each iteration, 10% of the data was split off as a test set. Then F, the number of ra"
9078,unknown,"Then F, the number of random inputs selected at each node, was varied from 1 to 50. For each value of F, 100 trees were grown to form a random forest and the terminal values of test set error, strength, correlation, etc. recorded. Eighty iterations were done, each time removing a random 10% of the data for use as a test set, and all results averaged over the 80 repetitions. Altogether, 400,000 tre"
9079,unknown,"trees were grown in this experiment. The top graph of Figure 1, plots the values of strength and correlation vs. F. The result is fascinating. Past about F=4 the strength remains constant; adding more inputs does not help. But the correlation continues to increase. The second graph plots the test set errors and the out-of-bag estimates of the generalization error against F. The out-of-bag estimate"
9080,unknown,"generalization error against F. The out-of-bag estimates are more stable. Both show the same behavior--a small drop from F=1 out to F about 4-8, and then a general, gradual increase. This increase in error tallies with the beginning of the constancy region for the strength. (Figure 1 about here) Figure 2 has plots for similar runs on the breast data set where features consisting of random combinat"
9081,unknown,"consisting of random combinations of three inputs are used. The number of these features was varied from 1 to 25. Again, the correlation shows a slow rise, while the strength stays virtually constant, so that the minimum error is at F=1. The surprise in these two figures is the relative constancy of the strength. Since the correlations are slowly but steadily increasing, the lowest error occurs wh"
9082,unknown,"only a few inputs or features are used. (Figure 2 about here) Since the larger data sets seemed to have a different behavior than the smaller, we ran a similar experiment on the satellite data set. The number of features, each consisting of a random sum of two inputs, was varied from 1 to 25, and for each, 100 classifiers were combined. The results are shown in Figure 3. The results differ from th"
9083,unknown,"results differ from those on the smaller data sets. Both the correlation and strength show a small but steady increase. The error rates show a slight decrease. We conjecture that with larger and more complex data sets, the strength continues to increase longer before it plateaus out. (Figure 3 about here) Our results indicate that better (lower generalization error) random forests have lower corre"
9084,unknown,lower correlation between classifiers and higher strength. The randomness used in tree construction has to aim for low correlation ρ while maintaining reasonable strength. This conclusion has been hinted at in previous work. 15 Dietterich [1998] has measures of dispersion of an ensemble and notes that more accurate ensembles have larger dispersion. Freund [personal communication] believes that one
9085,unknown,believes that one reason why Adaboost works so well is that at each step it tries to decouple the next classifier from the current one. Amit et al [1999] give an analysis to show that the Adaboost algorithm is aimed at keeping the covariance between classifiers small. 7. Conjecture: Adaboost is a Random Forest Various classifiers can be modified to use both a training set and a set of weights on t
9086,unknown,"on the training set. Consider the following random forest: a large collection of K different sets of non-negative sum-one weights on the training set is defined. Denote these weights by w (1),w (2), ...w (K ). Corresponding to these weights are probabilities p(1),p(2), ...p(K ) whose sum is one. Draw from the integers 1, ..., K according to these probabilities. The outcome is Θ . If Θ= k grow the "
9087,unknown,"h(x,Θ ) using the training set with weights w(k). In its original version, Adaboost (Freund and Schapire [1996]) is a deterministic algorithm that selects the weights on the training set for input to the next classifier based on the misclassifications in the previous classifiers. In our experiment, random forests were produced as follows: Adaboost was run 75 times on a data set producing sets of n"
9088,unknown,"times on a data set producing sets of non-negative sum-one weights w (1),w (2), ...w (50) (the first 25 were discarded). The probability for the kth set of weights is set proportional to Q (w k )=log[(1−error(k))/error(k)] where error(k) is the w(k) weighted training set error of the kth classifier. Then the forest is run 250 times. This was repeated 100 times on a few data sets, each time leaving"
9089,unknown,"test set and then averaging the test set errors. On each data set, the Adaboost error rate was very close to the random forest error rate. A typical result is on the Wisconsin Breast Cancer data where Adaboost produced an average of 2.91% error and the random forest produced 2.94%. In the Adaboost algorithm, w (k+1)= φ(w k() ) where φ is a function determined by the base classifier. Denote the kth"
9090,unknown,"the base classifier. Denote the kth classifier by h(x,w k ). The vote of the kth classifier is weighted by Q (w k ) so the normalized vote for class j at x equals I(h(x k ∑ ,w k )= j)Q (w k )/ Q (w k ) k ∑ . (10) For any function f defined on the weight space, define the operator T f(w )= f(φ(w )) . We conjecture that T is ergodic with invariant measure π (dw ). Then (10) will converge to EQ π [I("
9091,unknown,"Then (10) will converge to EQ π [I(h(x,w )= j)] where the distribution Q π (dw )=Q (w )π (dw )/ Q (v)π (dv)∫ . If this conjecture is true, then Adaboost is equivalent to a random forest where the weights on the training set are selected at random from the distribution Q π . 16 Its truth would also explain why Adaboost does not overfit as more trees are added to the ensemble--an experimental fact t"
9092,unknown,"added to the ensemble--an experimental fact that has been puzzling. There is some experimental evidence that Adaboost may overfit if run thousands of times (Grove and Schuurmans [1998]), but these runs were done using a very simple base classifier and may not carry over to the use of trees as the base classifiers. My experience running Adaboost out to 1,000 trees on a number of data sets is that t"
9093,unknown,"data sets is that the test set error converges to an asymptotic value. The truth of this conjecture does not solve the problem of how Adaboost selects the favorable distributions on the weight space that it does. Note that the distribution of the weights will depend on the training set. In the usual random forest, the distribution of the random vectors does not depend on the training set. 8. The E"
9094,unknown,"set. 8. The Effects of Output Noise Dietterich [1998] showed that when a fraction of the output labels in the training set are randomly altered, the accuracy of Adaboost degenerates, while bagging and random split selection are more immune to the noise. Since some noise in the outputs is often present, robustness with respect to noise is a desirable property. Following Dietterich [1998] the follow"
9095,unknown,"property. Following Dietterich [1998] the following experiment was done which changed about one in twenty class labels (injecting 5% noise). For each data set in the experiment, 10% at random is split off as a test set. Two runs are made on the remaining training set. The first run is on the training set as is. The second run is on a noisy version of the training set. The noisy version is gotten b"
9096,unknown,"is gotten by changing, at random, 5% of the class labels into an alternate class label chosen uniformly from the other labels. This is repeated 50 times using Adaboost (deterministic version), Forest-RI and Forest-RC. The test set results are averaged over the 50 repetitions and the percent increase due to the noise computed. In both random forests, we used the number of features giving the lowest"
9097,unknown,"the number of features giving the lowest test set error in the Section 5 and 6 experiments. Because of the lengths of the runs, only the 9 smallest data sets are used. Table 4 gives the increases in error rates due to the noise. Table 4 Increases in Error Rates Due to Noise (%) Data Set Adaboost Forest-RI Forest-RC glass 1.6 .4 -.4 breast cancer 43.2 1.8 11.1 diabetes 6.8 1.7 2.8 sonar 15.1 -6.6 4"
9098,unknown,"diabetes 6.8 1.7 2.8 sonar 15.1 -6.6 4.2 ionosphere 27.7 3.8 5.7 soybean 26.9 3.2 8.5 ecoli 7.5 7.9 7.8 votes 48.9 6.3 4.6 17 liver 10.3 -.2 4.8 Adaboost deteriorates markedly with 5% noise, while the random forest procedures generally show small changes. The effect on Adaboost is curiously data set dependent, with the two multiclass data sets, glass and ecoli, along with diabetes, least effected "
9099,unknown,"diabetes, least effected by the noise. The Adaboost algorithm iteratively increases the weights on the instances most recently misclassified. Instances having incorrect class labels will persist in being misclassified. Then, Adaboost will concentrate increasing weight on these noisy instances and become warped. The random forest procedures do not concentrate weight on any subset of the instances a"
9100,unknown,"instances and the noise effect is smaller. 9 Data With Many Weak Inputs Data sets with many weak inputs are becoming more common, i.e. in medical diagnosis, document retrieval, etc. The common characteristics is no single input or small group of inputs can distinguish between the classes. This type of data is difficult for the usual classifiers--neural nets and trees. To see if there is a possibil"
9101,unknown,"To see if there is a possibility that Forest-RI methods can work, the following 10 class, 1,000 binary input data, was generated: (rnd is a uniform random number, selected anew each time it appears) do j=1,10 do k=1,1000 p(j,k)=.2*rnd+.01 end do end do do j=1,10 do k=1, nint(400*rnd) !nint=nearest integer k=nint(1000*rnd) p(j,k)=p(j,k)+.4*rnd end do end do do n=1,N j=nint(10*rnd) do m=1,1000 if (r"
9102,unknown,"x(m,n)=1 else x(m,n)=0 end if y(n)=j ! y(n) is the class label of the nth example end do end do 18 This code generates a set of probabilities {p(j,m)} where j is the class label and m is the input number. Then the inputs for a class j example are a string of M binary variables with the mth variable having probability p(j,m) of being one. For the training set, N=1,000. A 4,000 example test set was "
9103,unknown,"For the training set, N=1,000. A 4,000 example test set was also constructed using the same {p(j,k)}. Examination of the code shows that each class has higher underlying probability at certain locations. But the total over all classes of these locations is about 2,000, so there is significant overlap. Assuming one knows all of the {p(j,k)}, the Bayes error rate for the particular {p(j,m)} computed"
9104,unknown,"of the {p(j,k)}, the Bayes error rate for the particular {p(j,m)} computed in our run is 1.0%. Since the inputs are independent of each other, the Naive Bayes classifier, which estimates the {p(j,k)} from the training data is supposedly optimal and has an error rate of 6.2%. This is not an endorsement of Naive Bayes, since it would be easy to create a dependence between the inputs which would incr"
9105,unknown,"Bayes error rate. I stayed with this example because the Bayes error rate and the Naive Bayes error rate are easy to compute. I started with a run of Forest-RI with F=1. It converged very slowly and by 2,500 iterations, when it was stopped, it had still not converged. The test set error was 10.7%. The strength was .069 and the correlation .012 with a c/s2 ratio of 2.5. Even though the strength was"
9106,unknown,"Even though the strength was low, the almost zero correlation meant that we were adding small increments of accuracy as the iterations proceeded. Clearly, what was desired was an increase in strength while keeping the correlation low. Forest-RI was run again using F=int(log 2M+1)=10. The results were encouraging. It converged after 2,000 iterations. The test set error was 3.0%. The strength was .2"
9107,unknown,"3.0%. The strength was .22, the correlation .045 and c/s2=.91. Going with the trend, Forest-RI was run with F=25 and stopped after 2,000 iterations. The test set error was 2.8%. Strength was .28, correlation .065 and c/s2 = .83. It's interesting that Forest-RI could produce error rates not far above the Bayes error rate. The individual classifiers are weak. For F=1, the average tree error rate is "
9108,unknown,"rate is 80%; for F=10, it is 65%; and for F=25, it is 60%. Forests seem to have the ability to work with very weak classifiers as long as their correlation is low. A comparison using Adaboost was tried, but I can't get Adaboost to run on this data because the base classifiers are too weak. 10. Exploring the Random Forest Mechanism A forest of trees is impenetrable as far as simple interpretations "
9109,unknown,"go. In some applications, analysis of medical experiments for example, it is critical to understand the interaction of variables that is providing the predictive accuracy. A start on this problem is made by using internal out-of-bag estimates, and verification by reruns using only selected variables. Suppose there are M input variables. After each tree is constructed, the values of the mth variabl"
9110,unknown,"the mth variable in the out-of-bag examples are randomly permuted and the out-of-bag data is run down the corresponding tree. The classification given for 19 each xn that is out of bag is saved. This is repeated for m=1,2, ... , M. At the end of the run, the plurality of out-of-bag class votes for x n with the mth variable noised up is compared with the true class label of xn to give a misclassifi"
9111,unknown,"The output is the percent increase in misclassification rate as compared to the out-of-bag rate (with all variables intact). We get these estimates by using a single run of a forest with 1,000 trees and no test set. The procedure is illustrated by examples. In the diabetes data set, using only single variables with F=1, the rise in error due to the noising of variables is given in Figure 4 Figure "
9112,unknown,"Figure 4 Measure of Variable Importance--Diabetes Data -10 0 10 20 30 40 percent increase 1 2 3 4 5 6 7 8 variable The second variable appears by far the most important followed by variable 8 and variable 6. Running the random forest in 100 repetitions using only variable 2 and leaving out 10% each time to measure test set error gave an error of 29.7%, compared with 23.1% using all variables. But "
9113,unknown,"compared with 23.1% using all variables. But when variable 8 is added, the error falls only to 29.4%. When variable 6 is added to variable 2, the error falls to 26.4%. The reason that variable 6 seems important, yet is no help once variable 2 is entered is a characteristic of how dependent variables affect prediction error in random forests. Say there are two variables x 1 and x 2 which are identi"
9114,unknown,"carry significant predictive information. Because each gets picked with about the same frequency in a random forest, noising each separately will result in the same increase in error rate. But once x 1 is entered as a predictive variable, using x2 in addition will not produce any decrease in error rate. In the diabetes data set, the 8th variable carries some of the same information as the second. "
9115,unknown,does not add predictive accuracy when combined with the second. 20 The relative magnitudes of rises in error rates are fairly stable with respect to the input features used. The experiment above was repeated using combinations of three inputs with F=2. The results are in the following graph: Figure 5 Measure of Variable Importance-Diabetes Data -10 0 10 20 30 percent increase 1 2 3 4 5 6 7 8 varia
9116,unknown,"1 2 3 4 5 6 7 8 variable Another interesting example is the voting data. This has 435 examples corresponding to 435 Congressmen and 16 variables reflecting their yes-no votes on 16 issues. The class variable is Republican or Democrat. To see which issues were most important, we again ran the noising variables program generating 1,000 trees. The lowest error rate on the original data was gotten usi"
9117,unknown,"inputs with F=5, so these parameters were used in the run. The results are graphed below: Figure 6 Measure of Variable Importance--Votes Data 21 -100 0 100 200 300 400 percent increase 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 variables Variable 4 stands out--the error triples if variable 4 is noised. We reran this data set using only variable 4. The test set error is 4.3%, about the same as if all"
9118,unknown,"set using only variable 4. The test set error is 4.3%, about the same as if all variables were used. The vote on 4 separates Republicans from Democrats almost as well as the vote on 4 combined with the votes on all other 15 issues. The approach given in this section is only a beginning. More research will be necessary to understand how to give a more complete picture. 11. Random Forests for Regres"
9119,unknown,"Random forests for regression are formed by growing trees depending on a random vector Θ such that the tree predictor h(x,Θ ) takes on numerical values as opposed to class labels. The output values are numerical and we assume that the training set is independently drawn from the distribution of the random vector Y,X . The mean-squared generalization error for any numerical predictor h(x) is EX ,Y "
9120,unknown,"h(x) is EX ,Y (Y −h(X ))2 (11) The random forest predictor is formed by taking the average over k of the trees { h(x,Θ k )}. Similarly to the classification case, the following holds: Theorem 11.1 As the number of trees in the forest goes to infinity, almost surely, EX ,Y (Y −avkh(X ,Θ k))2→ EX ,Y (Y − EΘ h(X ,Θ ))2 (12) Proof: see Appendix I. Denote the right hand side of (12) as PE*(forest)--the"
9121,unknown,"Denote the right hand side of (12) as PE*(forest)--the generalization error of the forest. Define the average generalization error of a tree as: 22 PE*(tree)= EΘ EX ,Y (Y −h(X ,Θ ))2 Theorem 11.2 Assume that for all Θ , EY = EX h(X ,Θ ). Then PE *(forest)≤ ρ PE *(tree) where ρ is the weighted correlation between the residuals Y −h(X ,Θ ) and Y −h(X ,Θ ') where Θ ,Θ ' are independent. Proof: PE *(f"
9122,unknown,"where Θ ,Θ ' are independent. Proof: PE *(forest)= EX ,Y [EΘ (Y − h(X ,Θ )]2 = EΘ EΘ 'EX ,Y (Y − h(X ,Θ ))(Y − h(X ,Θ ')) (13) The term on the right in (13) is a covariance and can be written as: E Θ E Θ '(ρ (Θ ,Θ ')sd(Θ )sd(Θ ') ) where sd(Θ )= EX ,Y (Y −h(X ,Θ ))2 . Define the weighted correlation as: ρ = E Θ E Θ '(ρ (Θ ,Θ ')sd(Θ )sd(Θ ') )/(E Θ sd(Θ ))2 (14) Then PE *(forest)=ρ (EΘ sd(Θ ))2 ≤ ρ"
9123,unknown,Then PE *(forest)=ρ (EΘ sd(Θ ))2 ≤ ρ PE *(tree). Theorem (11.2) pinpoints the requirements for accurate regression forests--low correlation between residuals and low error trees. The random forest decreases the average error of the trees employed by the factor ρ . The randomization employed needs to aim at low correlation. 12 Empirical Results in Regression In regression forests we use random feat
9124,unknown,"In regression forests we use random feature selection on top of bagging. Therefore, we can use the monitoring provided by out-of-bag estimation to give estimates of PE*(forest), PE*(tree) and ρ . These are derived similarly to the estimates in classification. Throughout, features formed by a random linear sum of two inputs are used. We comment later on how many of these features to use to determin"
9125,unknown,"PE*(tree) but the higher ρ . In our empirical study the following data sets are used: Table 5 Data Set Summary 23 Data Set Nr. Inputs #Training #Test Boston Housing 12 506 10% Ozone 8 330 10% Servo 4 167 10% Abalone 8 4177 25% Robot Arm 12 15,000 5000 Friedman#1 10 200 2000 Friedman#2 4 200 2000 Friedman#3 4 200 2000 O f these data sets, the Boston Housing, Abalone and Servo are available at the"
9126,unknown,"O f these data sets, the Boston Housing, Abalone and Servo are available at the UCI repository. The Robot Arm data was provided by Michael Jordan. The last three data sets are synthetic. They originated in Friedman [1991] and are also described in Breiman [1998]. These are the same data sets used to compare adaptive bagging to bagging (see Breiman [1999]), except that one synthetic data set (Peak2"
9127,unknown,"set (Peak20), which was found anomalous both by other researchers and myself, is eliminated. The first three data sets listed are moderate in size and test set error was estimated by leaving out a random 10% of the instances, running on the remaining 90% and using the left-out 10% as a test set. This was repeated 100 times and the test set errors averaged. The abalone data set is larger with 4,177"
9128,unknown,"instances and 8 input variables. It originally came with 25% of the instances set aside as a test set. We ran this data set leaving out a randomly selected 25% of the instances to use as a test set, repeated this 10 times and averaged. Table 6 gives the test set mean-squared error for bagging, adaptive bagging and the random forest. These were all run using 25 features, each a random linear combin"
9129,unknown,"a random linear combination of two randomly selected inputs, to split each node, each feature a random combination of two inputs. All runs with all data sets, combined 100 trees. In all data sets, the rule ""don't split if the node size is < 5"" was enforced. Table 6 Mean-Squared Test Set Error Data Set Bagging Adapt. Bag Forest Boston Housing 11.4 9.7 10.2 Ozone 17.8 17.8 16.3 Servo x10-2 24.5 25.1"
9130,unknown,Ozone 17.8 17.8 16.3 Servo x10-2 24.5 25.1 24.6 Abalone 4.9 4.9 4.6 Robot Arm x10-2 4.7 2.8 4.2 Friedman #1 6.3 4.1 5.7 24 Friedman #2x10+3 21.5 21.5 19.6 Friedman #3 x10-3 24.8 24.8 21.6 An interesting difference between regression and classification is that the correlation increases quite slowly as the number of features used increases. The
9131,unknown,"correlation increases quite slowly as the number of features used increases. The major effect is the decrease in PE*( tree). Therefore, a relatively large number of features are required to reduce PE*(tree) and get near optimal test-set error. The results shown in Table 6 are mixed. Random forest-random features is always better than bagging. In data sets for which adaptive bagging gives sharp dec"
9132,unknown,"decreases in error, the decreases produced by forests are not as pronounced. In data sets in which adaptive bagging gives no improvements over bagging, forests produce improvements. For the same number of inputs combined, over a wide range, the error does not change much with the number of features. If the number used is too small, PE*(tree) becomes too large and the error goes up. If the number u"
9133,unknown,"large, the correlation goes up and the error again increases. The in-between range is usually large. In this range, as the number of features goes up, the correlation increases, but PE*(tree) compensates by decreasing. Table 7 gives the test set errors, the out-of-bag error estimates, and the OB estimates for PE*(tree) and the correlation. Table 7. Error and OB Estimates Data Set Test Error OB Err"
9134,unknown,Table 7. Error and OB Estimates Data Set Test Error OB Error PE*(tree) Cor. Boston Housing 10.2 11.6 26.3 .45 Ozone 16.3 17.6 32.5 .55 Servo x10-2 24.6 27.9 56.4 .56 Abalone 4.6 4.6 8.3 .56 Robot Arm x10-2 4.2 3.7 9.1 .41 Friedman #1 5.7 6.3 15.3 .41
9135,unknown,"Friedman #1 5.7 6.3 15.3 .41 Friedman #2 x10+3 19.6 20.4 40.7 .51 Friedman #3 x10-3 21.6 22.9 48.3 .49 As expected, the OB Error estimates are consistently high. It is low in the robot arm data, but I believe that this is an artifact caused by separate training and test sets, where the test set may have a slightly higher error rate than the training set."
9136,unknown,"sets, where the test set may have a slightly higher error rate than the training set. As an experiment, I turned off the bagging and replaced it by randomizing outputs (Breiman [1998b]). In this procedure, mean-zero Gaussian noise is added to each of the outputs. The standard deviation of the noise is set equal to the standard deviation of the outputs. Similar to the bagging experiments, tree cons"
9137,unknown,"construction was done using 25 features, each a random linear combination of two randomly selected inputs, to split each node. The results are given in Table 8. 25 Table 8 Mean-Squared Test Set Error Data Set With Bagging With Noise Boston Housing 10.2 9.1 Ozone 17.8 16.3 Servo x10-2 24.6 23.2 Abalone 4.6 4.7 Robot Arm x10-2 4.2 3.9 Friedman #1 5.7 5.1 Friedman #2 x10+3 19.6 20.4"
9138,unknown,"Friedman #1 5.7 5.1 Friedman #2 x10+3 19.6 20.4 Friedman #3 x10-3 21.6 19.8 The error rates on the first two data sets are the lowest to date. Overall, adding output noise works with random feature selection better than bagging. This is illustrative of the flexibility of the random forest setting--various combinations of randomness can be added to see what works the best. 13 Remarks and Conclusion"
9139,unknown,"combinations of randomness can be added to see what works the best. 13 Remarks and Conclusions Random forests are an effective tool in prediction. Because of the Law of Large Numbers they do not overfit. Injecting the right kind of randomness makes them accurate classifiers and regressors. Furthermore, the framework in terms of strength of the individual predictors and their correlations gives ins"
9140,unknown,"the ability of the random forest to predict. Using out-of-bag estimation makes concrete the otherwise theoretical values of strength and correlation. For a while, the conventional thinking was that forests could not compete with arcing type algorithms in terms of accuracy. Our results dispel this belief, but lead to interesting questions. Boosting and arcing algorithms have the ability to reduce b"
9141,unknown,"reduce bias as well as variance (Schapire et al [1998]). The adaptive bagging algorithm in regression (Breiman [1999]) was designed to reduce bias and operates effectively in classification as well as in regression. But, like arcing, it also changes the training set as it progresses. Forests give results competitive with boosting and adaptive bagging, yet do not progressively change the training s"
9142,unknown,"progressively change the training set. Their accuracy indicates that they act to reduce bias. The mechanism for this is not obvious. Random forests may also be viewed as a Bayesian procedure. Although I doubt that this is a fruitful line of exploration, if it could explain the bias reduction, I might become more of a Bayesian. Random inputs and random features produce good results in classificatio"
9143,unknown,"so in regression. The only types of randomness used in this study is bagging and random features. It may well be that other types of injected randomness give better results. For instance, one of the referees has suggested use of random Boolean combinations of features. 26 An almost obvious question is whether gains in accuracy can be gotten by combining random features with boosting. For the large"
9144,unknown,"combining random features with boosting. For the larger data sets, it seems that significantly lower error rates are possible. On some runs, we got errors as low as 5.1% on the zip-code data, 2.2% on the letters data and 7.9% on the satellite data. The improvement was less on the smaller data sets. More work is needed on this; but it does suggest that different injections of randomness can produce"
9145,unknown,"on this; but it does suggest that different injections of randomness can produce better results. A recent paper (Breiman [2000]) shows that in distribution space for two class problems, random forests are equivalent to a kernel acting on the true margin. Arguments are given that randomness (low correlation) enforces the symmetry of the kernel while strength enhances a desirable skewness at abrupt "
9146,unknown,"boundaries. Hopefully, this sheds light on the dual role of correlation and strength. The theoretical framework given by Kleinberg [2000] for Stochastic Discrimination may also help understanding. References Amit, Y. and Geman, D. [1997] Shape quantization and recognition with randomized trees, Neural Computation 9, 1545-1588 Amit, Y., Blanchard, G., and Wilder, K. [1999] Multiple Randomized Class"
9147,unknown,"Classifiers: MRCL Technical Report, Department of Statistics, University of Chicago Bauer, E. and Kohavi, R. [1999] An Empirical Comparison of Voting Classification Algorithms, Machine Learning, 36, No. 1/2, 105-139. Breiman, L. [2000] Some infinity theory for predictor ensembles, Technical Report 579, Statistics Dept. UCB Breiman, L. [1999] Using adaptive bagging to debias regressions, Technical "
9148,unknown,"Report 547, Statistics Dept. UCB Breiman, L. [1998a], Arcing Classifiers, (discussion paper) Annals of Statistics, 26, 801-824 Breiman. L. [1998b] Randomizing Outputs To Increase Prediction Accuracy. Technical Report 518, May 1, 1998, Statistics Department, UCB (in press Machine Learning) Breiman, L. [1996a] Bagging Predictors, Machine Learning, 26, No. 2, 123-140 Breiman, L. [1996b] Out-of-bag es"
9149,unknown,"ftp.stat.berkeley.edu/pub/users/breiman/OOBestimation.ps Dietterich, T. [1998] An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting and Randomization, Machine Learning 1-22 Freund, Y. and Schapire, R. [1996] Experiments with a new boosting algorithm, Machine Learning: Proceedings of the Thirteenth International Conference, pp. 148-156 Grove, A"
9150,unknown,"Grove, A. and Schuurmans, D. [1998]. Boosting in the limit: Maximizing the margin of learned ensembles. In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98). Ho, T.K. [1998] The random subspace method for constructing decision forests, IEEE Trans. on Pattern Analysis and Machine Intelligence, 20,8, 832-844 27 Kleinberg, E. [2000] On the algorithmic implementatio"
9151,unknown,"discrimination, IEEE Trans. on Pattern Analysis and Machine Intelligence, 22,5 473-490 Schapire, R. ,Freund Y. Bartlett P. and Lee W. [1998] Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of Statistics, 26(5):1651-1686. Tibshirani, R. [1996] Bias, Variance, and Prediction Error for Classification Rules, Technical Report, Statistics Department, University of "
9152,unknown,"Wolpert, D.H. and Macready, W.G. [1997] An Efficient Method to Estimate Bagging's Generalization Error (in press, Machine Learning) Appendix I Almost Sure Convergence Proof of Theorem 1.2 It suffices to show that there is a set of probability zero C on the sequence space Θ 1,Θ 2, ... such that outside of C, for all x , 1 N I(h(Θ nn=1 N ∑ ,x)= j)→ PΘ (h(Θ ,x)= j). For a fixed training set and fixed"
9153,unknown,"For a fixed training set and fixed Θ , the set of all x such that h(Θ ,x)= j is a union of hyper-rectangles. For all h(Θ ,x) there is only a finite number K of such unions of hyper-rectangles, denoted by S1, ...SK . Define ϕ (Θ )= k if {x:h(Θ ,x)= j}= Sk. Let N k be the number of times that ϕ (Θ n )= k in the first N trials. Then 1 N I(h(Θ nn=1 N ∑ ,x)= j)= 1 N N k k ∑ I(x∈ Sk) By the Law of Large"
9154,unknown,"∑ ,x)= j)= 1 N N k k ∑ I(x∈ Sk) By the Law of Large Numbers, N k = 1 N I( n=1 N ∑ ϕ (Θ n )= k) converges a.s. to PΘ (ϕ (Θ )= k). Taking unions of all the sets on which convergence does not occur for some value of k gives a set C of zero probability such that outside of C, 1 N I(h(Θ nn=1 N ∑ ,x)= j)→ PΘ (ϕ (Θ )= k) k ∑ I(x∈ Sk). 28 The right hand side is PΘ (h(Θ ,x)= j). Proof of Theorem 9.1 There "
9155,unknown,"Proof of Theorem 9.1 There are a finite set of hyper-rectangles R1, ...RK , such that if yk is the average of the training sets y-values for all training input vectors in Rk then h(Θ ,x) has one of the values I(x∈ Sk )yk. The rest of the proof parallels that of Theorem 1.2. Appendix II Out-of Bag Estimates for Strength and Correlation At the end of a combination run, let Q (x,j)= I( k ∑ h(x,Θ k)= "
9156,unknown,"Q (x,j)= I( k ∑ h(x,Θ k)= j;(y,x)∉Tk,B )/ I( k ∑ (y,x)∉Tk,B ). Thus, Q (x,j) is the out-of-bag proportion of votes cast at x for class j, and is an estimate for PΘ (h(x,Θ )= j) . From Definition 2.1 the strength is the expectation of PΘ (h(x,Θ )= y)− max j≠y PΘ (h(x,Θ )= j). Substituting Q (x,j), Q (x,y) for PΘ (h(x,Θ )= j), PΘ (h(x,Θ )= y) in this latter expression and taking the average over the"
9157,unknown,"estimate. From equation (7), ρ =var(mr )/(EΘ sd(Θ ))2 . The variance of mr is E X ,Y [PΘ (h(x,Θ )= y)− max j≠y PΘ (h(x,Θ )= j)]2 − s2 (A1) where s is the strength. Replacing the first term in (A1) by the average over the training set of (Q (x,y)− max j≠y Q (x,j))2 and s by the out-of-bag estimate of s gives the estimate of var( mr ). The standard deviation is given by sd(Θ )=[p1+ p2 +(p1− p2 )2 ] "
9158,unknown,"12 (A2) where 29 p1 = E X ,Y (h(X ,Θ ) = Y ) p2 = E X ,Y (h(X ,Θ ) = ˆj(X ,Y )) . After the kth classifier is constructed, Q (x,j) is computed, and used to compute ˆj(x,y) for every example in the training set. Then, let p1 be the average over all (y,x) in the training set but not in the kth bagged training set of I(h(x,Θ k)= y). Then p2 is the similar average of I(h(x,Θ k)= ˆj(x,y)). Substitute t"
9159,unknown,"Then p2 is the similar average of I(h(x,Θ k)= ˆj(x,y)). Substitute these estimates into (A2) to get an estimate of sd(Θ k ). Average the sd(Θ k ) over all k to get the final estimate of sd(Θ ). 30 00 .. 11 .. 22 .. 33 .. 44 .. 55 .. 66 YY VVaarriiaabblleess 00 11 00 22 00 33 00 44 00 55 00 nnuummbbeerr ooff iinnppuuttss ccoorrrreellaattiioonn ssttrreennggtthh 00 55 11 00 11 55 22 00 22 55 ppeerrcc"
9160,unknown,ppeerrcceenntt 00 11 00 22 00 33 00 44 00 55 00 nnuummbbeerr ooff iinnppuuttss OOBB eerrrroorr tteesstt sseett eerrrroorr FFIIGGUURREE 11 EEFFFFEECCTT OOFF NNUUMMBBEERR OOFF IINNPPUUTTSS OONN SSOONNAARR DDAATTAA CCOORRRREELLAATTIIOONN AANNDD SSTTRREENNGGTTHH TTEESSTT SSEETT AANNDD OOBB EERRRROORR 31 0 .3 .6 .9 1.2Y Variables 0 5 10 15 20 25 number of features correlation strength 0 1 2 3 4 5percen
9161,unknown,strength 0 1 2 3 4 5percent 0 5 10 15 20 25 number of features OB error test set error FIGURE 2 EFFECT OF THE NUMBER OF FEATURES ON THE BREAST DATA SET CORRELATION AND STRENGTH TEST SET AND OB ERROR 32 00 .. 22 .. 44 .. 66 .. 88 11 YY VVaarriiaabblleess 00 55 11 00 11 55 22 00 22 55 nnuummbbeerr ooff ffeeaattuurreess ccoorrrreellaattiioonn ssttrreennggtthh 00 33 66 99 11 22 11 55 ppeerrcceenntt 00
9162,unknown,"ppeerrcceenntt 00 55 11 00 11 55 22 00 22 55 nnuummbbeerr ooff ffeeaattuurreess OOBB eerrrroorr tteesstt sseett eerrrroorr FFIIGGUURREE 33 EEFFFFEECCTT OOFF NNUUMMBBEERR OOFF FFEEAATTUURREESS OONN SSAATTEELLLLIITTEE DDAATTAA CCOORRRREELLAATTIIOONN AANNDD SSTTRREENNGGTTHH TTEESSTT SSEETT AANNDD OOBB EERRRROORR 33 Machine Learning, 28, 41±75 (1997) c   1997 Kluwer Academic Publishers, Boston. Manufa"
9163,unknown,"Multitask Learning*    caruana@cs.cmu.edu School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213 Editor: Abstract. Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shar"
9164,unknown,"better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for mult"
9165,unknown,"We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-"
9166,unknown,"conjecture there will be many opportunities for its use on real-world problems. Keywords: inductive transfer, parallel transfer, multitask learning, backpropagation, k-nearest neighbor, kernel regression, supervised learning, generalization 1. Introduction 1.1. Overview Multitask Learning (MTL) is an inductive transfer mechanism whose principle goal is to improve generalization performance. MTL im"
9167,unknown,"to improve generalization performance. MTL improves generalization by leveraging the domain-speci®c information contained in the training signals of related tasks. It does this by training tasks in parallel while using a shared representation. In effect, the training signals for the extra tasks serve as an inductive bias. Section 1.2 argues that inductive transfer is important if we wish to scale "
9168,unknown,"transfer is important if we wish to scale tabula rasa learning to complex, real-world tasks. Section 1.3 presents the simplest method we know for doing multitask inductive transfer, adding extra tasks (i.e., extra outputs) to a backpropagation net. Because the MTL net uses a shared hidden layer trained in parallel on all the tasks, what is learned for each task can help other tasks be learned bett"
9169,unknown,help other tasks be learned better. Section 1.4 argues that it is reasonable to view training signals as an inductive bias when they are used this way. Section 2 demonstrates that MTL works. We compare the performance of single task learning (STLÐlearning just one task at a time) and multitask learning in backpropagation on three problems. One of these problems is a real-world problem created by r
9170,unknown,"other than the author who did not consider using MTL when they collected the data. * This work was supported by ARPA grant F33615-93-1-1330, NSF grant BES-9315428, Agency for Health Care Policy and Research grant HS06468, and by Justsystem Pittsburgh Research Center.   Section 3 explains how MTL in backprop nets works. Section 3.1 suggests mechanisms that could improve generalization even if the"
9171,unknown,that could improve generalization even if the extra tasks' training signals are not relevant to the main task. We present an empirical test that rules out these mechanisms and thus ensures that the bene®t from MTL is due to the information in the extra tasks. In section 3.2 we present mechanisms that explain how MTL leverages the information in the extra training signals to improve generalization.
9172,unknown,training signals to improve generalization. In section 3.3 we show that MTL in backprop nets is able to determine how tasks are related without being given an explicit training signal for task relatedness. Section 4 may be the most important part of this paper. It shows that there are many opportunities for MTL (and for inductive transfer in general) on real-world problems. At ®rst glance most of 
9173,unknown,"®rst glance most of the problems one sees in machine learning today do not look like multitask problems. We believe most current problems in machine learning appear to be single task because of the way we have been trained. ManyÐin fact, we believe mostÐ real-world problems are multitask problems and performance is being sacri®ced when we treat them as single task problems. Sections 1-4 use the si"
9174,unknown,"Sections 1-4 use the simplest MTL algorithm we know of, a backprop net with multiple outputs sharing a single, fully connected hidden layer. But MTL is a collection of ideas, techniques, and algorithms, not one algorithm. In Section 5 we present MTL algorithms for k-nearest neighbor and decision trees. While these algorithms look rather different from MTL in backprop nets, there is strong overlap "
9175,unknown,"from MTL in backprop nets, there is strong overlap of mechanisms and issues; all MTL algorithms must address essentially the same set of problems, even if the speci®c mechanism in each algorithm is different. Inductive transfer is not new, and many backprop nets used multiple outputs before MTL came along. Related work is presented in Section 6. Section 7 discusses many issues that arise in MTL an"
9176,unknown,"arise in MTL and brie¯y mentions future work. Section 8 is a summary. 1.2. Motivation The standard methodology in machine learning is to learn one task at a time. Large problems are broken into small, reasonably independent subproblems that are learned separately and then recombined (see, for example, Waibel's excellent work on connectionist glue [Waibel 1989]). This paper argues that sometimes th"
9177,unknown,"1989]). This paper argues that sometimes this methodology is counterproductive because it ignores a potentially rich source of information available in many real-world problems: the information contained in the training signals of other tasks drawn from the same domain. An arti®cial neural network (or a decision tree, or a ...) trained tabula rasa on a single, isolated, very dif®cult task is unlik"
9178,unknown,"isolated, very dif®cult task is unlikely to learn it well. For example, a net with a 1000x1000 pixel input retina is unlikely to learn to recognize complex objects in real-world scenes given the number of training patterns and training time likely to be available. Might it be better to require the learner to learn many things simultaneously? Yes. If the tasks can share what they learn, the learner"
9179,unknown,"can share what they learn, the learner may ®nd it is easier to learn them together than in isolation. Thus, if we simultaneously train a net to recognize object outlines, shapes, edges, regions, subregions, textures, re¯ections, highlights, shadows, text, orientation, size, distance, etc., it may learn better to recognize complex objects in the real world. This approach is Multitask Learning.    "
9180,unknown,"approach is Multitask Learning.    1.3. MTL in Backpropagation Nets Figure 1 shows four separate arti®cial neural nets (ANNs). Each net is a function of the same inputs, and has one output. Backpropagation is applied to these nets by training each net in isolation. Because the four nets are not connected, it is not possible for what one net learns to help another net. We call this approach Single"
9181,unknown,". . . . . . . . . . . . Task 1 Task 2 Task 3 Task 4 INPUTS INPUTSINPUTS INPUTS Figure 1. Single Task Backpropagation (STL) of four tasks with the same inputs. Figure 2 shows a single net with the same inputs as the four nets in Figure 1, but which has four outputs, one for each task the nets in Figure 1 were being trained on. Note that these four outputs are fully connected to a hidden layer that "
9182,unknown,"these four outputs are fully connected to a hidden layer that they share.1 Backpropagation is done in parallel on the four outputs in the MTL net. Because the four outputs share a common hidden layer, it is possible for internal representations that arise in the hidden layer for one task to be used by other tasks. Sharing what is learned by different tasks while tasks are trained in parallel is th"
9183,unknown,"tasks are trained in parallel is the central idea in multitask learning [Suddarth & Kergosien 1990; Dietterich, Hild & Bakiri 1990, 1995; Suddarth & Holden 1991; Caruana 1993a, 1993b, 1994, 1995; Baxter 1994, 1995, 1996; Caruana & de Sa 1996]. MTL is an inductive transfer method that uses the domain speci®c information contained in the training signals of related tasks. It does this by learning th"
9184,unknown,"in the training signals of related tasks. It does this by learning the multiple tasks in parallel while using a shared representation. In backpropagation, MTL allows features developed in the hidden layer for one task to be used by other tasks. It also allows features to be developed to support several tasks that would not have been developed in any STL net trained on the tasks in isolation. Impor"
9185,unknown,"trained on the tasks in isolation. Importantly, MTL also allows some hidden units to become specialized for just one or a few tasks; other tasks can ignore hidden units they do not ®nd useful by keeping the weights connected to them small. 1.4. Training Signals as an Inductive Bias MTL is one way of achieving inductive transfer between tasks. The goal of inductive transfer is to leverage additiona"
9186,unknown,"transfer is to leverage additional sources of information to improve the performance of learning on the current task. Inductive transfer can be used to improve generalization accuracy, the speed of learning, and the intelligibility of learned models. In this paper we focus solely on improving accuracy. We are not concerned about the computational     INPUTS . . . Task 1 Task 2 Task 3 Task 4"
9187,unknown,"    INPUTS . . . Task 1 Task 2 Task 3 Task 4 Figure 2. Multitask Backpropagation (MTL) of four tasks with the same inputs. cost of learning nor the intelligibility of what is learned. One way transfer improves generalization is by providing a stronger inductive bias than would be available without the extra knowledge. This can yield better generalization with a ®xed training set, or reduce"
9188,unknown,"extra knowledge. This can yield better generalization with a ®xed training set, or reduce the number of training patterns needed to achieve some ®xed level of performance. Inductive bias is anything that causes an inductive learner to prefer some hypotheses over other hypotheses. Bias-free learning is impossible; much of the power of an inductive learner follows directly from the power of its indu"
9189,unknown,"learner follows directly from the power of its inductive bias [Mitchell 1980]. Multitask learning uses the training signals for related tasks as an inductive bias to improve gener- alization. One does not usually think of training signals as a bias; but when the training signals are for tasks other than the main task, it is easy to see that, from the point of view of the main task, the other tasks"
9190,unknown,"the main task, the other tasks may serve as a bias. The multitask bias causes the inductive learner to prefer hypotheses that explain more than one task. For this multitask bias to exist, the inductive learner must be biased to prefer hypotheses that have utility across multiple tasks. 2. Does MTL Work? Before jumping into how multitask learning works and when to use it, we ®rst demonstrate that i"
9191,unknown,"that it does work. We do this not only to convince the reader that multitask learning is worthwhile, but because the examples will help the reader develop intuitions about how multitask learning works and where it is applicable. In this section we present three applications of MTL in backprop nets. The ®rst uses simulated data for an ALVINN-like road-following domain. The second uses real data col"
9192,unknown,collected with a robot-mounted camera. This data was collected speci®cally to demonstrate    MTL. The third domain applies MTL to a medical decision-making domain. The data in this domain was collected by other researchers who did not consider using MTL when collecting the data. 2.1. 1D-ALVINN 1D-ALVINN uses a road image simulator ®rst developed by Pomerleau to permit rapid testing of learning id
9193,unknown,"testing of learning ideas for road-following domains [Pomerleau 1992]. The original simulator generates synthetic road images based on a number of user de®ned parameters such as the road width, number of lanes, angle and ®eld of view of the camera. We modi®ed the simulator to generate 1-D road images comprised of a single 32-pixel horizontal scan line instead of the original 2-D 30x32-pixel image."
9194,unknown,"line instead of the original 2-D 30x32-pixel image. We did this to speed learning so more thorough experimentation could be doneÐtraining moderate sized nets with the full 2-D retina was computationally too expensive to allow many replications. Nevertheless, 1D- ALVINN retains much of the complexity of the original 2-D domain; the main complexity lost is that road curvature is no longer visible, a"
9195,unknown,"pixels) makes learning easier. The principle task in both 1D-ALVINN and 2D-ALVINN is to predict steering direction. For our MTL experiments, eight additional tasks were used:  whether the road is one or two lanes  location of centerline (2-lane roads only)  location of left edge of road  location of right edge of road  location of road center  intensity of road surface  intensity of region "
9196,unknown, intensity of region bordering road  intensity of centerline (2-lane roads only) These additional tasks are all computable from the internal variables in the simulator. We modi®ed the simulator so that the training signals for these extra tasks were added to the synthetic data along with the training signal for the main steering task. Table 1 shows the performance of ten runs of single and multi
9197,unknown,"Table 1 shows the performance of ten runs of single and multitask learning on 1D- ALVINN using nets with one hidden layer. The MTL net has 32 inputs, 16 hidden units, and 9 outputs. The 36 STL nets have 32 inputs, 2, 4, 8 or 16 hidden units, and 1 output each.2 Note that the size of the MTL nets was not optimized. The entries under the STL and MTL headings are the generalization error for nets of "
9198,unknown,speci®ed size when early stopping is used to halt training. The bold STL entries are the STL runs that yielded best performance. The last two columns compare STL and MTL. The ®rst column is the percent reduction in error of MTL over the best STL run. Negative percentages indicate MTL performs better. This test is biased in favor of STL because it compares single runs of MTL on an unoptimized net s
9199,unknown,"it compares single runs of MTL on an unoptimized net size with several independent runs of STL that use different random seeds and are able to ®nd near-optimal net size. The last column is the percent improvement of MTL over the average STL performance. Differences marked with an ª*º are statistically signi®cant at 0.05 or better. Note that on the important steering task, MTL outperforms STL 15±30"
9200,unknown,"the important steering task, MTL outperforms STL 15±30%. It does this without having access to any extra training patterns: exactly the same training patterns are used for both STL and MTL. The only difference is that the MTL training patterns have the training    Table 1. Performance of STL and MTL with one hidden layer on tasks in the 1D-ALVINN domain. The bold entries in the STL columns are th"
9201,unknown,better are marked with an *. ROOT-MEAN SQUARED ERROR ON TEST SET TASK Single Task Backprop (STL) MTL Change MTL Change MTL 2HU 4HU 8HU 16HU 16HU to Best STL to Mean STL 1 or 2 Lanes .201 .209 .207 .178 .156 -12.4% * -21.5% * Left Edge .069 .071 .073 .073 .062 -10.1% * -13.3% * Right Edge .076 .062 .058 .056 .051 -8.9% * -19.0% * Line Center .153 .152 .152 .152 .151 -0.7% -0.8% Road Center .038 .03
9202,unknown,"Road Greylevel .054 .055 .055 .054 .038 -29.6% * -30.3% * Edge Greylevel .037 .038 .039 .038 .038 2.7% 0.0% Line Greylevel .054 .054 .054 .054 .054 0.0% 0.0% Steering .093 .069 .087 .072 .058 -15.9% * -27.7% * signals for all nine tasks, whereas the STL training patterns have training signals for only one task at a time. 2.2. 1D-DOORS 1D-ALVINN is not a real domain; the data is generated with a si"
9203,unknown,"1D-ALVINN is not a real domain; the data is generated with a simulator. To test MTL on a more realistic problem, we created an object recognition domain similar in some respects to 1D-ALVINN. In 1D-DOORS, the main tasks are to locate doorknobs and to recognize door types (single or double) in images of doors collected with a robot-mounted color camera. Figure 3 shows several door images from the d"
9204,unknown,"Figure 3 shows several door images from the database. As with 1D-ALVINN, the problem was simpli®ed by using horizontal stripes from the images, one for the green channel and one for the blue channel. Each stripe is 30 pixels wide (accomplished by applying Gaussian smoothing to the original 150 pixel-wide image) and occurs at the vertical height in the image where the doorknob is located. Ten tasks"
9205,unknown," horizontal location of doorknob  single or double door  horizontal location of doorway center  width of doorway  horizontal location of left door jamb  horizontal location of right door jamb  width of left door jamb  width of right door jamb  horizontal location of left edge of door  horizontal location of right edge of door As this is a real domain, the training signals for these tasks"
9206,unknown,"As this is a real domain, the training signals for these tasks had to be acquired manually. We used a mouse to click on the appropriate features in each image in the training and test sets. Since it was necessary to process each image manually to acquire the training signals for the two main tasks, it was not that dif®cult to acquire the training signals for the extra tasks. The dif®culty of 1D-DO"
9207,unknown,"tasks. The dif®culty of 1D-DOORS precludes running as exhaustive a set of experiments as with 1D-ALVINN; comparison could be done only for the two tasks we considered most important: doorknob location and door type. STL was tested on nets using 6, 24, and 96    Figure 3. Sample single and double doors from the 1D-DOORS domain. hidden units. MTL was tested on nets with 120 hidden units. The result"
9208,unknown,"STL and MTL are in Table 2. MTL generalizes 20±30% better than STL on these tasks, even when compared to the best of three different runs of STL. Once again, note that the training patterns used for STL and MTL are identical except that the MTL training patterns contain additional training signals. It is the information contained in these extra training signals that helps the hidden layer learn a "
9209,unknown,"layer learn a better internal representation for the door recognition domain, and this better representation in turn helps the net better learn to recognize door types and the location of the doorknobs. Table 2. Performance of STL and MTL on the two main tasks in 1D-DOORS. The bold entries in the STL columns are the STL runs that performed best. Differences statistically signi®cant at 0.05 or bett"
9210,unknown,"with an *. ROOT-MEAN SQUARED ERROR ON TEST SET TASK Single Task Backprop (STL) MTL Change MTL 6HU 24HU 96HU 120HU to Best STL Doorknob Loc .085 .082 .081 .062 -23.5% * Door Type .129 .086 .096 .059 -31.4% * The 1D-ALVINN domain used simulated data. Although the simulator was not built with MTL in mind, it was modi®ed to make extra task signals available in the training data. The 1D-DOORS domain us"
9211,unknown,"data. The 1D-DOORS domain used real data collected from a real camera on a real robot wandering around a real hallway. Although every attempt was made to keep this domain challenging (e.g., the robot was not kept parallel to the hallway and the distance to the doors and illumination was allowed to vary), it is still a domain contrived speci®cally to demonstrate MTL. How well will MTL work on a rea"
9212,unknown,"for it? 2.3. Pneumonia Prediction Of the 3,000,000 cases of pneumonia each year in the U.S., 900,000 are admitted to the hospital for treatment and testing. Most pneumonia patients recover given appropriate    treatment, and many can be treated effectively without hospitalization. Nonetheless, pneu- monia is serious: 100,000 of those hospitalized for pneumonia die from it, and many more are at el"
9213,unknown,"are at elevated risk if not hospitalized. A primary goal in medical decision making is to accurately, swiftly, and economically identify patients at high risk from diseases like pneumonia so they may be hospitalized to receive aggressive testing and treatment; patients at low risk may be more comfortably, safely, and economically treated at home. In this problem the diagnosis of pneumonia has alre"
9214,unknown,"has already been made. The goal is not to diagnose if the patient has pneumonia, but to determine how much risk the illness poses to the patient. Because some of the most useful tests for predicting pneumonia risk are usually measured after one is hospitalized, they will be available only if preliminary assessment indicates hospitalization and further testing is warranted. But low risk patients ca"
9215,unknown,hospitalization and further testing is warranted. But low risk patients can often be identi®ed using measurements made prior to admission to the hospital. We have a database in which all patients were hospitalized. It is the extra lab tests made once these patients are admitted to the hospital that we will use as extra tasks for MTL; they cannot be used as inputs because they usually will not be a
9216,unknown,"must be made. The Medis Pneumonia Database [Fine et al. 1995] contains 14,199 pneumonia cases collected from 78 hospitals in 1989. Each patient in the database was diagnosed with pneumonia and hospitalized. 65 measurements are available for most patients. These include 30 basic measurements acquired prior to hospitalization, such as age, sex, and pulse, and 35 lab results, such as blood counts or "
9217,unknown,"pulse, and 35 lab results, such as blood counts or blood gases, usually not available until after hospitalization. The database indicates how long each patient was hospitalized and whether the patient lived or died. 1,542 (10.9%) of the patients died. The most useful decision aid for this problem would predict which patients will live or die. But this is too dif®cult. In practice, the best that ca"
9218,unknown,"dif®cult. In practice, the best that can be achieved is to estimate a probability of death (POD) from the observed symptoms. In fact, it is suf®cient to learn to rank patients by their POD so lower-risk patients can be discriminated from higher risk patients; patients at least risk may then be considered for outpatient care. The performance criteria used by others working with the Medis database ["
9219,unknown,"al. 1995] is the accuracy with which one can select prespeci®ed fractions of the patient population who will live. For example, given a population of 10,000 patients, ®nd the 20% of this population at least risk. To do this we learn a risk model and a threshold for this model that allows 20% of the population (2000 patients) to fall below it. If 30 of the 2000 patients below this threshold die, th"
9220,unknown,"2000 patients below this threshold die, the error rate is 30/2000 = 0.015. We say that the error rate for FOP 0.20 is 0.015 (FOP stands for ªfraction of populationº). In this paper we consider FOPs 0.1, 0.2, 0.3, 0.4, and 0.5. Our goal is to learn models and model thresholds, such that the error rate at each FOP is minimized. The Medis database contains results from 35 lab tests that usually will "
9221,unknown,"The Medis database contains results from 35 lab tests that usually will be available only after patients are hospitalized. These results typically will not be available when the model is used because the patients will not yet have been admitted. We use MTL to bene®t from the future lab results. The extra lab values are used as extra backprop outputs, as shown in Figure 4. The hope is that the extr"
9222,unknown,in Figure 4. The hope is that the extra outputs will bias the shared hidden layer towards representations that better capture important features of each patient's condition.3    We developed a method called Rankprop that learns to rank patients without learning to predict mortality. ªRankpropº is short for ªbackpropagation using sum-of-squares errors on repeatedly re-estimated ranksº. Rankprop ou
9223,unknown,"on repeatedly re-estimated ranksº. Rankprop outperforms traditional backprop using sum- of-squares errors (SSE) on targets 0=lives,1=dies by 10%-40% on this domain, depending on which FOP is used for comparison. It is the best performer we know of on this database. See [Caruana, Baluja & Mitchell 1995] for details about rankprop and why it outperforms SSE on this domain. Age Sex Chest Pain Asthmat"
9224,unknown,"Chest Pain Asthmatic Diabetic Heart Mumur Wheezing Stridor Mortality Rank Hematocrit White Blood Cell Count Potassium . . . . . . INPUTS INPUT LAYER OUTPUT LAYER SHARED HIDDEN LAYER RANKPROP OUTPUT FUTURE LABS . . . Figure 4. Using future lab results as extra outputs to bias learning for the main rankprop risk prediction task. The lab tests would help most if they could be used as inputs, but will"
9225,unknown,"predicted, so we use them as extra MTL outputs instead. The STL net has 32 hidden units and one output for the rankprop risk prediction. The MTL net has 64 hidden units. (Preliminary experiments suggested 32 hidden units was near optimal for STL, and that MTL would perform somewhat better with nets as large as 512 hidden units.) Table 3 shows the mean performance of ten runs of rankprop using STL "
9226,unknown,"and MTL. The bottom row shows the improvement over STL with rankprop. Although MTL lowers the error at each FOP compared with STL, only the differences at FOP 0.3, 0.4, and 0.5 are statistically signi®cant with ten trials. We also tried using feature nets on this problem. Feature nets [Davis & Stentz 1995] is a competing approach that trains nets to predict the missing future measurements and uses"
9227,unknown,"the predictions, or the hidden layers learned for these predictions, as extra inputs. On this pneumonia problem feature nets did not yield bene®ts comparable to MTL.    Table 3. Error Rates (fraction deaths) for STL with Rankprop and MTL with Rankprop on Fractions of the Population predicted to be at low risk (FOP) between 0.0 and 0.5. MTL makes 5±10% fewer errors than STL. FOP 0.1 0.2 0.3 0.4 0."
9228,unknown,"STL Rankprop .0083 .0144 .0210 .0289 .0386 MTL Rankprop .0074 .0127 .0197 .0269 .0364 % Change -10.8% -11.8% -6.2% * -6.9% * -5.7% * 3. How Does MTL Work? Why are tasks learned better when trained on a net that learns other related tasks at the same time? Is it because information in the extra tasks is helping the hidden layer learn a better internal representation, or is something else less inter"
9229,unknown,"a better internal representation, or is something else less interesting happening? And, if multitask learning is exploiting the information in the training signals of related tasks, how does it do this? This section addresses these questions. 3.1. Ruling Out Alternate Explanations There are many potential reasons why adding extra outputs to a backprop net might improve generalization performance. "
9230,unknown,"generalization performance. For example, adding noise to backpropagation sometimes improves generalization [Holmstrom & Koistinen 1992]. To the extent that tasks are uncorrelated, their contribution to the aggregate gradient (the gradient that sums the error fed back from each layer's outputs) can appear as noise to other tasks. Thus uncorrelated tasks might improve generalization by acting as a s"
9231,unknown,"tasks might improve generalization by acting as a source of noise. Another possibility is that adding tasks might change weight updating dynamics to somehow favor nets with more tasks. For example, adding extra tasks increases the effective learning rate on the input-to-hidden layer weights relative to the hidden layer-to-output weights. Maybe larger learning rates on the ®rst layer improves learn"
9232,unknown,"learning rates on the ®rst layer improves learning. A third possibility is net capacity; MTL nets share the hidden layer between all tasks. Perhaps reduced capacity improves generalization on these problems. It is possible to devise experiments to disprove each of these explanations, but the number of possible explanation that would have to be ruled out is not small. It would be better to show the"
9233,unknown,"show the bene®ts of MTL depend on the training signals for the extra tasks being related to the main task, as the following experiment does: Take an MTL training set. For each case in the training set, there is a set of input features, the main task training signal, and a set of extra task training signals. Shuf¯e the extra task training signals among all the cases in the training set, i.e., rando"
9234,unknown,"training set, i.e., randomly reassign the training signals for the extra tasks among the cases. This breaks the relationship between the main task and the extra tasks without altering other properties of the extra tasks; the distributions of the extra tasks remains unchanged. If MTL depends on the extra information in the training signals being meaningfully related to the main task, shuf¯ing will "
9235,unknown,"main task, shuf¯ing will eliminate that relationship, and thus should eliminate the bene®ts of MTL. If the bene®ts from MTL depend on some other property of having multiple outputs, shuf¯ing will not affect this and the bene®ts should remain after shuf¯ing. This    shuf¯e test is similar to the heuristic used in [Valdes-Perez & Simon 1994] to discover complex patterns in data. We've run the shuf¯"
9236,unknown,"We've run the shuf¯e test on the problems in Section 2. In each case, shuf¯ing the extra tasks reduces the performance of MTL to performance comparable to STL. We conclude that the bene®ts observed with MTL are due to the information in the extra training signals serving as a domain-speci®c inductive bias for these problems, not to some other bene®t achievable with unrelated extra outputs. The shu"
9237,unknown,"The shuf¯e test does not completely rule out the possibility that the bene®t of MTL is due to restricting net capacityÐextra tasks can consume net capacity even after they have been shuf¯ed. To rule out net capacity as the possible explanation for MTL, we always compare MTL with STL run at many different net sizes, or are careful to optimize the net size for STL before running the experiments. Usu"
9238,unknown,"size for STL before running the experiments. Usually, we do not optimize the net size for MTL. We've also done experiments using MTL nets larger than the sum of all the STL nets combined. In these experiments, the MTL nets still outperform STL. It is clear that the extra tasks are not improving performance merely by restricting net capacity. 3.2. MTL Mechanisms Knowing that the performance improve"
9239,unknown,"Knowing that the performance improvement from MTL is due to the extra information in the training signals of related tasks is different from knowing how that bene®t occurs. This section summarizes several mechanisms that help MTL backprop nets to generalize better. The mechanisms all derive from the summing of error gradient terms at the hidden layer for the different tasks. Each, however, exploit"
9240,unknown,"the different tasks. Each, however, exploits a different relationship between tasks. We have discovered additional mechanisms (some of which are special cases of the ones presented here) and have run tests on carefully contrived problems to verify that each mechanism actually works. More detail can be found in [Caruana 1994, 1997]. 3.2.1. Statistical Data Ampli®cation Data ampli®cation is an effec"
9241,unknown,"Data ampli®cation is an effective increase in sample size due to extra information in the training signals of related tasks. Ampli®cation occurs when there is noise in the training signals. Consider two tasks,  and  , with independent noise added to their training signals, that both bene®t from computing a hidden layer feature  of the inputs. A net learning both  and  can, if it recognize"
9242,unknown,"learning both  and  can, if it recognizes that the two tasks share  , use the two training signals to learn  better by averaging  through the different noise processes. 3.2.2. Attribute Selection Consider two tasks,  and  , that use a common hidden layer feature  . Suppose there are many inputs to the net. A net learning  will, if there is limited training data or signi®cant noise, so"
9243,unknown,"noise, sometimes have dif®culty distinguishing inputs relevant to  from those irrelevant to it. A net learning both  and  , however, will better select the attributes relevant to     because data ampli®cation provides better training signals for  , allowing it to better determine which inputs to use to compute  . Attribute selection is a consequence of data ampli®cation. 3.2.3. Eavesdropp"
9244,unknown,"Consider a hidden layer feature  , useful to tasks,  and   , that is easy to learn when learning  , but dif®cult to learn when learning   (either because   uses  in a more complex way, or because the residual error in   learned without  is noisier). A net learning  will learn  , but a net learning just   may not. If the net learning   also learns  ,   can eavesdrop on the hid"
9245,unknown,"learns  ,   can eavesdrop on the hidden layer learned for  (e.g.,  ) and thus learn better. Once the connection is made between   and the evolving representation for  , the extra information from  about  will help the net learn  better via the other mechanisms. The simplest case of eavesdropping is what Abu-Mostafa calls catalytic hints where     , i.e., the net is being told explic"
9246,unknown,"[Abu-Mostafa 1990]. 3.2.4. Representation Bias Because nets are initialized with random weights, backprop is a stochastic search procedure; multiple runs rarely yield identical nets. Suppose there are two local minima,  and  , a net can ®nd for task  . Suppose a net learning task   also has two minima,  and  . Both share the minima at  (i.e., both would perform well if the net entered that"
9247,unknown,"share the minima at  (i.e., both would perform well if the net entered that region of weight space), but do not overlap at  and  . We ran two experiments. In the ®rst, we selected the minima so that nets trained on  alone are equally likely to ®nd  or  , and nets trained on   alone are equally likely to ®nd  or  . Nets trained on both  and   usually fall into  for both tasks.4 This s"
9248,unknown, for both tasks.4 This shows that MTL tasks prefer hidden layer representations that other tasks prefer. Search is biased towards representations in the intersection of what would be learned for  or  alone. T T' Representations Findable by Backprop Best Reps In the second experiment we selected the minima so that  has a strong preference for  over  : a net trained on  always falls into  
9249,unknown," : a net trained on  always falls into  .  , however, still has no preference between  or  . When both  and   are trained on one net,  falls into  as expected: the bias from   is unable to pull it to  . Surprisingly,   usually falls into  , the minima it does not share with  !  creates a ªtideº in the hidden layer representation towards  that ¯ows away from  .  has no pre"
9250,unknown,"that ¯ows away from  .  has no preference for  or  , but is subject to the tide created     by  . Thus   usually falls into  ; it would have to ®ght the tide from  to fall into  . MTL tasks prefer NOT to use hidden layer representations that other tasks prefer NOT to use. 3.3. Backprop MTL Discovers How Tasks Are Related Section 3.2 presents mechanisms that allow MTL to exploit differe"
9251,unknown,"Section 3.2 presents mechanisms that allow MTL to exploit different kinds of relationships between tasks. But MTL nets are not told how tasks are related. Do MTL backprop nets discover how tasks are related? Yes. Backprop nets, though primarily used for supervised learning, perform a limited kind of unsupervised learning on the hidden layer features learned for different tasks (different outputs)."
9252,unknown,"features learned for different tasks (different outputs). The details of how this unsupervised learning occurs and how well it works are not yet fully understood. It is worthwhile, however, to demonstrate here that backprop does discover task relatedness. We devised a set of test problems called the Peaks Functions. Each peak function is of the form: IF (?1 > 1/2), THEN ?2, ELSE ?3 where ?1, ?2, a"
9253,unknown,"where ?1, ?2, and ?3 are instantiated from the alphabet   A,B,C,D,E,F without duplication. There are 120 such functions: P001 = IF (A > 1/2) THEN B, ELSE C P002 = IF (A > 1/2) THEN B, ELSE D ... P014 = IF (A > 1/2) THEN E, ELSE C ... P024 = IF (B > 1/2) THEN A, ELSE F ... P120 = IF (F > 1/2) THEN E, ELSE D The variables A±F are de®ned on the real interval [0,1]. A±F are provided as inputs to a ba"
9254,unknown,"a backprop net learning peaks functions. The values for A±F are given to the net via an encoding, rather than as simple continuous inputs. A net learning peaks functions must not only learn the functions, but must learn to properly decode the input encodings. The details of the encoding we used are not particularly interesting; nearly any learnable encoding will work. The encoding we used has ten "
9255,unknown,"inputs altogether. We trained one MTL net on all 120 peaks functions. This net has 60 inputs and 120 outputs, one for each of the 120 peaks functions. We ªopenedº the net to see how much different outputs shared the hidden layer. We did a sensitivity analysis for each output with each hidden unit. There are 120 outputs and 64 hidden units, so we did 15,360 separate sensitivity analyses. By compari"
9256,unknown,"sensitivity analyses. By comparing the sensitivity of output P001 to each hidden unit with that of output P002 to each hidden unit, we were able to measure how much outputs P001 and P002 shared the hidden layer. We used a non-parametric rank correlation test to measure sharing because we were uncertain of the distributions of sensitivities.     The relatedness of two peaks functions depends on how"
9257,unknown,"    The relatedness of two peaks functions depends on how many variables they have in common, and whether they use those variables in the same way. For example, P001 does not share any variables with P120, so it is not related to P120 (see above). P001 shares two variables with P024, though neither of these are used in the same way; P001 is moderately related to P024. P001 also shares two variable"
9258,unknown,"related to P024. P001 also shares two variables with P014, and both variables are used the same way. Thus P001 is more related to P014 than to P024. Figure 5 shows the rank correlation of hidden unit sensitivities between tasks as a function of how related they are. In the graph, the data point at ª0 features in commonº compares how much tasks having no features in common share the hidden layer. T"
9259,unknown,how much tasks having no features in common share the hidden layer. The two data points at ª3 features in commonº show the degree of hidden unit sharing between tasks that use the same three features (though these features are not necessarily in the same places in the tasks). The line labelled ªany featureº disregards the position of the features in the tasks. Tasks that have one feature in common
9260,unknown,"Tasks that have one feature in common might or might not use that common feature the same way. The line labelled ªtest must matchº, however, requires that the feature in the conditional test be the same. Thus if two tasks have one feature in common, this feature must be the feature used in the conditional. -0.1 0 0.1 0.2 0.3 0.4 0.5 0 1 2 3 Rank Correlation of the 64 Hidden Units Number of Feature"
9261,unknown,"Number of Features in Common ""any_feature"" ""test_must_match"" Figure 5. Sharing in the hidden layer as a function of the similarity between tasks. Tasks that are more related share more hidden units. The general trend of both lines is that tasks share hidden units more if they are more related. The small negative correlation for tasks that do not share any variables suggests that a complete lack of"
9262,unknown,"that a complete lack of relatedness between functions leads to anticorrelated sharing, i.e., outputs for unrelated functions tend to use different hidden units. The correlations for the ªtest must matchº line is higher than the correlations for the ªany featureº line. This suggests that overlap in the conditional IF test is more important for hidden layer sharing than overlap in the THEN or ELSE p"
9263,unknown,"than overlap in the THEN or ELSE part of the tasks.     There are other relationships between peaks functions we could examine. For every relationship between peaks functions we examined, relatedness was positively correlated with hidden unit sharing. This suggests that, for the peaks functions at least, backpropaga- tion using a shared hidden layer is able to discover how tasks are related on hid"
9264,unknown,features without being given explicit training signals about task relatedness. 4. Is MTL Broadly Applicable? Section 2 demonstrated the bene®ts of MTL. Section 3 showed these bene®ts are due the domain knowledge contained in the extra training signals. How often will training data be available for extra tasks that are usefully related to the main task? This section presents nine kinds of domains w
9265,unknown,nine kinds of domains where training signals for useful extra tasks will often be available. We believe most real-world problems fall into one or more of these kinds of domains. This claim might sound surprising given that few of the test problems in machine learning repositories are multitask problems. We believe that most problems traditionally used in machine learning have been preprocessed to 
9266,unknown,"MTL before learning was applied. 4.1. Using the Future to Predict the Present Often valuable features become available after predictions must be made. These features cannot be used as inputs because they will not be available at run time. If learning is done of¯ine, however, they can be collected for the training set and used as extra MTL tasks. The predictions the learner makes for these extra ta"
9267,unknown,"The predictions the learner makes for these extra tasks will probably be ignored when the system is used; their main function is to provide extra information to the learner during training. One application of learning from the future is medical risk prediction, such as the pneumonia risk problem from Section 2.3. In that problem, we used lab tests that were available in the training setÐbut which "
9268,unknown,available in the training setÐbut which would not be available when making predictions for patientsÐas extra output tasks. The valuable information contained in those future measurements helped bias the net towards a hidden layer representation that better supported risk prediction from the features that would be available at run time. Future measurements are available in many of¯ine learning prob
9269,unknown,"Future measurements are available in many of¯ine learning problems. As a very different example, a robot or autonomous vehicle can more accurately measure the size, location, and identity of objects in the future if it passes near them. For example, road stripes can be detected reliably as a vehicle passes alongside them, but detecting them far ahead of a vehicle is beyond the current state-of-the"
9270,unknown,"a vehicle is beyond the current state-of-the-art. Since driving brings future road closer to the car, stripes can be measured accurately when passed and added to the training set. They can't be used as inputs because they will not be available in time when driving autonomously. As MTL outputs, though, they provide extra information that helps learning without requiring they be available at run tim"
9271,unknown,"without requiring they be available at run time.     4.2. Multiple Representations and Metrics Sometimes capturing everything that is important in one error metric or in one output rep- resentation is dif®cult. When alternate metrics or output representations capture different, but useful, aspects of a problem, MTL can be used to bene®t from them. An example of using MTL with different metrics is "
9272,unknown,"An example of using MTL with different metrics is again the pneumonia domain from Section 2.3. There we used the rankprop error metric [Caruana, Baluja & Mitchell 1995] designed speci®cally for this domain. Rankprop outperforms backprop using traditional SSE by 10-40% on this problem. Rankprop, however, can have trouble learning to rank cases at such low risk that virtually all patients survive. R"
9273,unknown,"cases at such low risk that virtually all patients survive. Rankprop still outperforms SSE on these low-risk patients, but this is where it has the most dif®culty learning a stable rank. Interestingly, SSE is at its best in regions of the space with high purity, as in regions where most cases have low risk. Suppose we add an additional SSE output to a network learning to predict risk using rankpro"
9274,unknown,"Adding an extra SSE output to the rankprop MTL net has the expected effect. It lowers error at the rankprop output for the low-risk FOPs, while slightly increasing error at the higher-risk FOPs. Table 4 shows the results with rankprop before and after adding the extra SSE output. Note that the extra SSE output is completely ignored when predicting patient risk. It has been added solely because it "
9275,unknown,risk. It has been added solely because it provides a useful bias to the net during training. Table 4. Adding an extra SSE task to MTL with rankprop improves MTL performance where SSE performs well (FOPs near 0.0 or 1.0) but hurts MTL performance where SSE performs poorly (FOPs near 0.5). FOP 0.1 0.2 0.3 0.4 0.5 w/o SSE .0074 .0127 .0197 .0269 .0364 with SSE .0066 .0116 .0188 .0272 .0371 % Change -
9276,unknown,"with SSE .0066 .0116 .0188 .0272 .0371 % Change -10.8% * -8.7% * -4.6% * +1.1% +1.9% Similarly, it is not always apparent what output encoding will work best. Alternate codings of the main task can be used as extra outputs the same way alternate error metrics were used above. For example, distributed output representations often help parts of a problem be learned better because the parts have sepa"
9277,unknown,"problem be learned better because the parts have separate error gradients. But if prediction requires all outputs in the distributed representation to be correct at the same time, a non- distributed representation can be more accurate. MTL is one way to merge these con¯icting requirements and obtain both bene®ts by using both output representations. 4.3. Time Series Prediction Applications of this"
9278,unknown,"Applications of this type are a subclass of using the future to predict the present where future tasks are identical to the current task except that they occur at a later time. This is a large enough subclass to warrant special attention. The simplest way to use MTL for time series prediction is to use a single net with multiple outputs, each output corresponding to the same task at a different ti"
9279,unknown,"outputs, each output corresponding to the same task at a different time. Figure 2 showed     an MTL net with four outputs. If output k referred to the prediction for the time series task at time    , this net makes predictions for the same task at four different times. Often, the output used for prediction would be the middle one (temporally) so that there are tasks earlier and later than it trai"
9280,unknown,"earlier and later than it trained on the net. Or, as input features temporally ªslideº across the inputs, one can collect the outputs from a sequence of predictions and combine them. We tested MTL on time sequence data in a robot domain where the goal is to predict future sensory states from the current sensed state and the planned action. For example, we were interested in predicting the sonar re"
9281,unknown,"we were interested in predicting the sonar readings and camera image that would be sensed N meters in the future given the current sonar and camera readings, for N between 1 and 8 meters. As the robot moves, it collects a stream of sense data. (Strictly speaking, this sense data is a time series only if the robot moves at constant speed. We use dead reckoning to determine the distance the robot tr"
9282,unknown,"to determine the distance the robot travelled, so our data might be described as a spatial series.) We used a backprop net with four sets of outputs. Each set predicts the sonar and camera image that will be sensed at a future distance. Output set 1 is the prediction for 1 meter, set 2 is for 2 meters, set 3 is for 4 meters, and set 4 for 8 meters. The performance of this net on each prediction di"
9283,unknown,"net on each prediction distance is compared in Table 5 with separate STL nets learning to predict each distance separately. Each entry is the SSE averaged over all sense predictions. Error increases with distance, and MTL outperforms STL at all distances except 1 meter. Table 5. STL and MTL on robot sensory prediction tasks. The tasks are to predict what the robot will sense 1, 2, 4, and 8 meters "
9284,unknown,"easier 1 meter task may be hurt by MTL. METERS 1 2 4 8 STL .074 .098 .145 .183 MTL .076 .094 .131 .165 % Change +2.7% -4.1% -9.7% * -10.9% * The loss of accuracy at 1 meter is not statistically signi®cant, but there is an interesting trend in MTL improvement as a function of distance: MTL seems to help the harder, long-range prediction tasks more. We conjecture that this may not be uncommon. That "
9285,unknown,"MTL may help harder tasks most, possibly at the expense of easier tasks, because there is more room for improvement with harder tasks and more to loose with easy tasks. Where possible, one should use STL for the tasks on which it works best, and use MTL for the tasks on which it works best. But it is important to include tasks best trained with STL on the MTL net to help the MTL tasks. Why does MT"
9286,unknown,"Why does MTL provide a bene®t with time series data? One explanation is that pre- dictions at different time scales (or different distance scales) often partially depend on different processes. When learning a task with a short time scale, the learner may ®nd it dif®cult to recognize the longer-term processes, and vice-versa. Training both scales on a single net improves the chances that both shor"
9287,unknown,"and combined to make predictions.     4.4. Using Non-Operational Features Some features are impractical to use at run time because they are too expensive to compute, or because they need human expertise that won't be around or that will be too slow. Training sets, however, are often small, and we usually have the luxury to spend more time preparing them. Where it is practical to compute non-operat"
9288,unknown,"these may be used as extra MTL outputs. A good example of this is in scene analysis where human expertise is often required to label important features. Usually the human will not be in the loop when the learned system is used. Does this mean features labelled by humans cannot be used for learning? No. If the labels can be acquired for the training set, they can be used as extra tasks for the lear"
9289,unknown,"as extra tasks they will not be required later when the system is used. A good example of this is the 1D-DOORS domain, where we used a mouse to de®ne features in the images of doorways collected from a robot-mounted camera. A human had to process each image to capture the training signals for the two main tasks, the doorknob location and doorway center, so it was easy to collect the additional fea"
9290,unknown,"center, so it was easy to collect the additional features at the same time. Using the extra features as extra tasks improved performance considerably on the two main tasks. 4.5. Using Extra Tasks to Focus Attention Learners often learn to use large, ubiquitous patterns in the inputs, while ignoring small or less common inputs that are useful. MTL can be used to coerce the learner to attend to patt"
9291,unknown,"patterns in the input it would otherwise ignore. This is done by forcing it to learn internal representations to support tasks that depend critically on input patterns it might otherwise ignore. A good example of this is the road-following domain in Section 2.1. Here, STL nets often ignore lane markings when learning to steer because lane markings are usually a small part of the image, are not alw"
9292,unknown,"a small part of the image, are not always present, and frequently change appearance (e.g., single vs. double centerlines and solid vs. dashed centerlines). If a net learning to steer is also required to learn to recognize road stripes as an extra output task, the net will learn to attend to those parts of the image where stripes occur. To the extent that the stripe tasks are learnable, the net wil"
9293,unknown,"the extent that the stripe tasks are learnable, the net will develop internal representations to support them. Since the net is also learning to steer using the same hidden layer, the steering task can use whatever parts of the stripe hidden representation are useful for steering. 4.6. Sequential Transfer Sometimes we already have a domain theory for related tasks from prior learning. The data use"
9294,unknown,"data used to train these models, however, may no longer be available. Can MTL bene®t from the prior learned models without the training data? Yes. One can use the model to generate synthetic data and use the training signals in the synthetic data as extra MTL tasks. This approach to sequential transfer elegantly sidesteps the catastrophic interference problem (forgetting old tasks while learning n"
9295,unknown,"problem (forgetting old tasks while learning new ones), and is applicable even where the analytic methods of evaluating domain theories used by other serial transfer methods are     not available. For example, EBNN [Thrun & Mitchell 1994; Thrun 1996] requires that the domain theory be differentiable, but the MTL approach to sequential transfer does not. This approach is most effective when the pri"
9296,unknown,"approach is most effective when the prior learned models are accurate. If the prior models are poor, they can be a poor source of inductive bias. Some serial transfer mechanisms have explicit mechanisms for reducing transfer when prior learning does not appear to be accurate for the task at hand [Thrun & Mitchell 1994; Thrun 1996]. An issue that arises when synthesizing data from prior models is w"
9297,unknown,An issue that arises when synthesizing data from prior models is what distribution to sample from. One approach is to use the distribution of the training patterns for the current task. Pass the current training patterns through the prior learned models and use the predictions those models make as extra MTL outputs when learning the new main task. This sampling may not always be satisfactory. If t
9298,unknown,"This sampling may not always be satisfactory. If the models are complex (suggesting a large or carefully constructed sample would be needed to represent them with high ®delity), but the new sample of training data is small, it is bene®cial to sample the prior model at more points than the current sample. See [Craven & Shavlik 1994] for a thorough discussion of synthetic data sampling. 4.7. Multipl"
9299,unknown,"synthetic data sampling. 4.7. Multiple Tasks Arise Naturally Often the world gives us sets of related tasks to learn. The traditional approach to separate these into independent problems trained in isolation is counterproductive; related tasks can bene®t each other if trained together. An early, almost accidental, use of multitask transfer in backprop nets is NETtalk [Sejnowski & Rosenberg 1986]. "
9300,unknown,"in backprop nets is NETtalk [Sejnowski & Rosenberg 1986]. NETtalk learns the phonemes and stresses to give a speech synthesizer to pronounce the words given it as inputs. NETtalk used one net with many outputs, partly because the goal was to control a synthesizer that needed both phonemes and stresses at the same time. Although they never analyzed the contribution of multitask transfer to NETtalk,"
9301,unknown,"contribution of multitask transfer to NETtalk, there is evidence that NETtalk is harder to learn using separate nets [Dietterich, Hild & Bakiri 1990, 1995]. A more recent example of multiple tasks arising naturally is Mitchell's Calendar Ap- prentice System (CAP) [Dent et al. 1992; Mitchell et al. 1994]. In CAP, the goal is to learn to predict the    ,      ,    "
9302,unknown,"   ,      ,    , and    of the meetings it schedules. These tasks are functions of the same data and can share many common features. Early results using MTL decision trees (see Section 5.2) on this domain suggest that training these four tasks together yields better performance than training them in isolation, as is done in the CAP system. 4.8. Quantizat"
9303,unknown,"in isolation, as is done in the CAP system. 4.8. Quantization Smoothing Often the world gives us quantized information. For example, the training signal may result from human assessment into one of several categorical variables (e.g., poor, medium, good), or it may result from a natural process that quantizes some underlying smoother function (e.g., physical measurements made with limited precisio"
9304,unknown,"(e.g., physical measurements made with limited precision, or patient outcomes such as lives or dies). Although quantization sometimes makes problems easier to learn, usually it makes learning harder.    If there are extra training signals available that are less quantized than the main task, or that are quantized differently, these may be useful as extra tasks. What is learned for less quantized "
9305,unknown,"quantized extra tasks is helpful because it sometimes can be learned more easily due its greater smoothness. Extra tasks that are not smoother, but which result from a different quantization process, sometimes also help because, together with the main task, it may be possible to better interpolate the coarse quantization of both tasks. In effect, each task can serve to ®ll in some of the gaps crea"
9306,unknown,"serve to ®ll in some of the gaps created by quantization in the other. One example of quantization smoothing occurs in the pneumonia domain. In this domain, the main taskÐmortality probabilityÐis heavily and stochastically quantized: a patient either lives or dies. But one of the extra features in the database is the length of stay in the hospital. If length of stay is related to risk and the seve"
9307,unknown,"the hospital. If length of stay is related to risk and the severity of illness, then it is clear that the length of stay extra task can help the net better interpolate risk between the crudely quantized values lives or dies. In this case, the relationship between length of stay and risk may be complex. For example, patients at very high risk might have short stays in the hospital because they do n"
9308,unknown,"hospital because they do not live long. While the potential complexity of the relationship between a quantized task and some related, less quantized task can make bene®tting from the less quantized task more dif®cult, some bene®t will often arise. 4.9. Some Inputs Work Better as Outputs Many domains where MTL is useful are domains where it is impractical to use some features as inputs. MTL provide"
9309,unknown,"features as inputs. MTL provides a way of bene®ting from these features (instead of just ignoring them) by using them as extra tasks. Might some features that can be used as inputs be better used as outputs? Surprisingly, yes. It is possible to construct problems with features that are more useful when used as outputs than as inputs. Consider the following function: F1(A,B) = SIGMOID(A+B), SIGMOID"
9310,unknown,"Consider the following function: F1(A,B) = SIGMOID(A+B), SIGMOID(x) = 1   1    Consider the backprop net shown in Figure 6a with 20 inputs, 16 hidden units, and one output trained to learn F1(A,B). Data for F1(A,B) is generated by randomly sampling values A and B uniformly from the interval [-5,5]. The net input is 10-bit binary codes for A and B. The ®rst 10 inputs receive the coding for"
9311,unknown,"target output is the unary real (unencoded) value F1(A,B). Table 6 shows the mean performance of 50 trials of Net 1a with backpropagation and early stopping. For each trial, we generate new random training, halt, and test sets. Training sets contain 50 patternsÐenough for good performance, but not so much that there is not room for improvement. The halt and test sets contain 1000 cases each to min"
9312,unknown,"effect of sampling error. Now consider the related function: F2(A,B) = SIGMOID(A-B). Suppose, in addition to the 10-bit binary codings for A and B, the net is given the unencoded value F2(A,B) as an extra input feature. Will this extra input help it learn F1(A,B) better? Probably not. A+B and A-B do not correlate for random A and B. (The    binary inputs coding for A binary inputs coding for B Ma"
9313,unknown,R e g u l a r I n p u t s fully connected hidden layer A:STD binary inputs coding for A binary inputs coding for B Main Output R e g u l a r I n p u t s Extra Input fully connected hidden layer B:STD+IN binary inputs coding for A binary inputs coding for B Main Output R e g u l a r I n p u t s Extra Output fully connected hidden layer C:STD+OUT Figure 6. Three net architectures for learning F1. A:
9314,unknown,"B:STD+IN is a net that uses the extra feature as an extra input. C:STD+OUT is MTL, the extra feature is used as an extra output, not as an input. Table 6. Performance of STL, STL with an extra input, and MTL (STL with an extra output) on F1. Using the extra feature as an MTL output works better than using it as an extra input. Network Trials Mean RMSE Signi®cance STD (STL w/o extra input) 50 0.064"
9315,unknown,"STD+IN (STL with extra input) 50 0.0647 ns MTL (STD with extra output) 50 0.0631 0.013 * absolute value of the correlation coef®cients for our training sets is typically less than 0.01.) This hurts backprop's ability to learn to use F2(A,B) to predict F1(A,B). The net in Figure 6b has 21 inputs ± 20 for the binary codes for A and B, and an extra input for F2(A,B). The 2nd line in Table 6 shows the"
9316,unknown,"F2(A,B). The 2nd line in Table 6 shows the performance of STL with the extra input for the same training, halting, and test sets. Performance is not signi®cantly differentÐthe extra information contained in the feature F2(A,B) does not help backpropagation learn F1(A,B) when used as an extra input. If using F2(A,B) as an extra input does not help backpropagation learn F1(A,B), should we ignore F2("
9317,unknown,"we ignore F2(A,B)? No. F1(A,B) and F2(A,B) are strongly related. They both need to compute the same subfeatures, A and B. If, instead of using F2(A,B) as an extra input, it is used as an extra output that must be learned, it will bias the shared hidden layer to learn A and B better, and this will help the net better learn to predict F1(A,B). Figure 6c shows a net with 20 inputs for A and B, and 2 "
9318,unknown,"Figure 6c shows a net with 20 inputs for A and B, and 2 outputs, one for F1(A,B) and one for F2(A,B). The performance of this net is evaluated only on the output for F1(A,B), but backpropagation is done on both outputs. The 3rd line in Table 6 shows the mean performance of the MTL net on F1(A,B). Using F2(A,B) as an extra output improves performance on F1(A,B). Using the extra feature as an extra "
9319,unknown,"as an extra input. F1(A,B) and F2(A,B) were carefully contrived. We have devised less contrived functions that demonstrate similar effects, and have seen evidence of this behavior in real-world problems [Caruana & de Sa 1997]. One particularly interesting class of problems where    some features are more useful as outputs than as inputs is when there is noise present in the features; noise in ext"
9320,unknown,"5. Is MTL Just for Backprop Nets? In MTL with backprop nets, the representation used for multitask transfer is a hidden layer shared by all tasks. Many learning methods do not have a representation naturally shared between tasks. Can MTL be used with these methods? Yes. This section presents an algorithm and results for MTL with case-based methods such as k-nearest neighbor and kernel regression, "
9321,unknown,5.1. MTL in KNN and Kernel Regression K-nearest neighbor (KNN) and kernel regression (also called locally weighted averaging (LCWA)) use a distance metric de®ned on attributes to ®nd training cases close to the new case:                      1               2 The principle difference between KNN and LCWA is the kernel used for prediction. Whereas K
9322,unknown,"Whereas KNN uses a kernel that is uniform for the  closest neighbors and drops to 0 for cases further away, LCWA uses a kernel that decreases smoothly (and usually rapidly) with increasing distance. The performance of KNN and LCWA depends on the quality of the distance metric. Search for good attribute weights can be cast as an optimization problem using cross validation to judge the performance "
9323,unknown,"validation to judge the performance of different sets of weights. We use gradient descent and leave-one-out cross validation, which is particularly ef®cient with case-based methods like KNN and LCWA. Finding good attribute weights is essential to good performance with KNN and LCWA. MTL can be used to ®nd better weights. The basic approach is to ®nd attribute weights that yield good performance not"
9324,unknown,"from the domain.        ""!          # %$ & 1 '   !   (        '    0 causes learning to ignore the extra task, ' *) 1 causes learning to give as much weight to performance on the extra task as on the main task, and ' ,+ 1 causes learning to pay more attention to performance on the extra task than on the main task. We applied MTL LCWA to the pneumonia domain from "
9325,unknown,"We applied MTL LCWA to the pneumonia domain from Section 2.3. As before, the main task is to predict a fraction of the population at least risk, and the extra tasks are to predict     0.025 0.0255 0.026 0.0265 0.027 0.0275 0.028 0.0285 0.029 0.0295 0 0.5 1 1.5 2 Mortality At FOP = 0.3 Weight of Extra Tasks Compared with Main Task Figure 7. Error rate at FOP 0.3 as a function of   , the parameter t"
9326,unknown,"tasks.   0 is STL; the extra tasks are ignored.   1 is MTL with the same weight given to the main task and each extra task.   2 is MTL with most weight given to the extra tasks instead of the main task. the results of lab tests available on the training set but that will not be available for future patients. Figure 7 shows the error rates for FOP 0.3 as a function of ' (for simplicity, we pr"
9327,unknown,results here where each '  takes on the same value). '   0 is STL; all extra tasks are ignored. '   1 0 is MTL giving equal weight to each extra task and to the main task; the feature weights attempt to perform well on all the tasks. Note that the error rate is lowest when learning pays comparable attention to the main task and to the extra tasks.5 Similar graphs were obtained for other FOPs. Ta
9328,unknown,"STL ( '   0) and MTL (with '   1 0) for the ®ve FOPs. As with backpropagation, MTL performs 5±10% better than STL on risk prediction. Table 7. Error rates of STL LCWA and MTL LCWA (   1) on the pneumonia problem using training sets with 1000 cases. FOP 0.1 0.2 0.3 0.4 0.5 STL LCWA .0147 .0216 .0285 .0364 .0386 MTL LCWA .0141 .0196 .0259 .0340 .0364 % Change -4.3% -9.3% -9.1% * -6.6% * -5.7% * F"
9329,unknown,"Figure 8 shows the performance of STL( '   0) and MTL (with '   1 0) as a function of the size of the training set. The error bars are the standard errors of the estimates. The     error rates for MTL are lower than STL for all training set sizes. For smaller training set sizes, MTL yields performance comparable to STL given 25% to 75% more data. 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 0"
9330,unknown,"0.065 0.07 0 200 400 600 800 1000 1200 1400 1600 Error Rate at FOP 0.3 Number of Training Patterns MTL STL Figure 8. Performance of STL (    0) and MTL (with    1) as the number of training patterns varies. For 100±800 training patterns, STL needs about 50% more data to perform as well as MTL. 5.2. MTL Decision Tree Induction Traditional decision trees are single task: leaves denote a class (or "
9331,unknown,"Traditional decision trees are single task: leaves denote a class (or class probabilities) for only one task. Multitask decision trees, where each leaf denotes classes for more than one task, are possible, but why use them? Just as it was important to ®nd good feature weights in KNN/LCWA, in top-down induction of decision trees (TDIDT) it is important to ®nd good splits. In STL TDIDT, the only inf"
9332,unknown,"good splits. In STL TDIDT, the only information available to judge splits is how well they separate classes on a single task. In MTL TDIDT splits can be evaluated by how well they perform on multiple tasks. If tasks are related, preferring splits that have utility to multiple tasks will improve the quality of the selected splits. The basic recursive step in TDIDT [Quinlan 1984, 1992] is to determi"
9333,unknown,"The basic recursive step in TDIDT [Quinlan 1984, 1992] is to determine what split to add at the current node in a growing decision tree. Typically this is done using an information gain metric that measures how much class purity is improved by the available splits. The basic approach in MTL TDIDT is to compute the information gain of each split for each task individually, combine the gains, and se"
9334,unknown,"As in MTL KNN/LCWA, ' parameters are introduced to control how much emphasis is given to the extra tasks. Weighting the extra tasks this way yields better performance than the simpler approach presented in [Caruana 1993], which combined task gains by averaging them; recursive splitting algorithms often suffer when the data becomes sparse     low in the tree, so it is important early splits are sen"
9335,unknown,"See [Caruana 1997] for more detail about how the ' parameters can be learned ef®ciently in MTL TDIDT. See [Dietterich, Hild & Bakiri 1990, 1995] for the earliest discussion we know of the potential bene®ts of MTL in decision trees. 6. Related Work It is common to train neural nets with multiple outputs. Usually these outputs encode a single task. For example, in classi®cation tasks it is common to"
9336,unknown,"single task. For example, in classi®cation tasks it is common to use one output for each class (see, for example, [Le Cun et al. 1989]). But using one net for a few strongly related tasks is also not new. The classic NETtalk [Sejnowski & Rosenberg 1986] application uses one net to learn both phonemes and their stresses. Using one net is natural for NETtalk because the goal is to learn to control a"
9337,unknown,"the goal is to learn to control a synthesizer that needs both phoneme and stress commands at the same time. NETtalk is an early example of MTL. But the builders of NETtalk viewed the multiple outputs as codings for a single problem, not as independent tasks that bene®ted by being trained together. If one graphs the NETtalk learning curves for the phoneme and stress tasks separately, one observes t"
9338,unknown,"stress tasks separately, one observes that the stress tasks begin to overtrain long before the phoneme tasks reach peak performance. Better performance could easily been obtained in NETtalk by doing early stopping on each output individually, or by balancing the learning rates of the different outputs so they all reach peak performance at roughly the same time. [Dietterich, Hild & Bakiri 1990, 199"
9339,unknown,"[Dietterich, Hild & Bakiri 1990, 1995] performed a thorough comparison of NETtalk and ID3 on the NETtalk text-to-speech domain. One explanation they considered as to why backpropagation outperformed ID3 on this problem is that backpropagation bene®ts from sharing hidden units between different outputs, something ID3 does not do. They conclude that although hidden unit sharing (i.e., MTL) does help"
9340,unknown,"that although hidden unit sharing (i.e., MTL) does help, it is not the largest difference between the two learning methods, and suggest that adding sharing to ID3 probably would not be worthwhile. Transferring learned structure between related tasks is not new. The early work on sequential transfer of learned structure between neural nets [Pratt et al. 1991; Pratt 1992; Sharkey & Sharkey 1992] cle"
9341,unknown,"Sharkey & Sharkey 1992] clearly demonstrates that what is learned for one task can be used as a bias for other tasks. Unfortunately, this work failed to ®nd improvements in generalization performance; the focus was on speeding up learning. More recently, Mitchell and Thrun devised a serial transfer method called Explanation-Based Neural Nets (EBNN) [Thrun & Mitchell 1994; Thrun 1995, 1996] based o"
9342,unknown,"[Thrun & Mitchell 1994; Thrun 1995, 1996] based on tangent prop [Simard et al. 1992] that yields improved generalization on sequences of learned tasks. [O'Sullivan & Thrun 1996] devised a serial transfer mechanism for KNN that clusters previously learned tasks into sets of related tasks. KNN attribute weights learned for previous tasks in the cluster most similar to the new task are used for the n"
9343,unknown,"most similar to the new task are used for the new task when the number of training patterns for the new task are too small to support accurate learning. Both of these approaches differ from MTL, where the goal is to learn a better model for one task by learning all available extra tasks in parallel. O'Sullivan is currently exploring a thesis that combines sequential transfer and MTL. Some approach"
9344,unknown,"Some approaches to inductive transfer have both parallel and sequential components. [Breiman & Friedman 1995] present a method called Curds & Whey that takes advantage     of correlations between different prediction tasks. Models for different tasks are trained separately (i.e., via STL), but predictions from the separately learned models are combined before making the ®nal predictions. This shar"
9345,unknown,before making the ®nal predictions. This sharing of the predictions of the models instead of the internal structure learned by the models is quite different from MTL; combining the two methods is straightforward and might be advantageous in some domains. Omohundro presents algorithms for ªFamily Discoveryº where the goal is to learn a parameterized family of stochastic models [Omohundro 1996]. By 
9346,unknown,"family of stochastic models [Omohundro 1996]. By interleaving learning of different functions drawn from the family of functions, the algorithms learn the structure of the function family and can make better predictions. [Hinton 1986] suggested that generalization in arti®cial neural nets would improve if nets learned to better represent underlying regularities of the domain. Suddarth and Abu- Mos"
9347,unknown,Mostafa were among the ®rst to recognize that this might be accomplished by providing extra information at the outputs of a net. [Suddarth & Kergosien 1990; Suddarth & Holden 1991] used extra outputs to inject rule hints into networks about what they should learn. This is MTL where the extra tasks are carefully engineered to coerce the net to learn speci®c internal representations. The centerline 
9348,unknown,"internal representations. The centerline extra tasks in the 1D-ALVINN domain in Section 2.1 are examples of rule-injection hints. [Abu-Mostafa 1990, 1993, 1995] provides hints to backprop nets via extra terms in the error signal backpropagated for the main task output. The extra error terms constrain what is learned to satisfy desired properties of main task such as monotonicity [Sill & Abu-Mostaf"
9349,unknown,"such as monotonicity [Sill & Abu-Mostafa 1997], symmetry, or transitivity with respect to certain sets of inputs. MTL, which does not use extra error terms on the main task output, could easily be used in concert with Abu-Mostafa's hints. MTL is similar in some ways to clustering and unsupervised learning. For example, small changes to the indices in COBWEB's [Fisher 1987] probabilistic informatio"
9350,unknown,"changes to the indices in COBWEB's [Fisher 1987] probabilistic informationmetric yields a metric suitable for judging splits in multitask decision trees. Whereas COBWEB considers all features as tasks to predict, MTL decision trees allow the user to specify which signals are inputs and which are training signals. This not only makes it easier to create additional tasks without committing to extra "
9351,unknown,"tasks without committing to extra training information being available at run time, but makes learning simpler in domains where some features cannot reasonably be predicted. [Martin 1994, Martin & Billman 1994] explore how concept formation systems such as COBWEB can be extended to acquire overlapping concept descriptions. Their OLOC system is an incremental concept learner that learns overlapping"
9352,unknown,"system is an incremental concept learner that learns overlapping probabilistic descriptions that improve predictive accuracy. de Sa's Minimizing Disagreement Algorithm [de Sa 1994] is an unsupervised learning method similar in spirit to MTL. In MDA, multiple unsupervised learning tasks are trained in parallel and bias each other via supervisory signals from the other unsupervised tasks. Attempts h"
9353,unknown,"Attempts have been made to develop theories of parallel transfer in arti®cial neural nets [Abu-Mostafa 1993; Baxter 1994, 1995, 1996].6 Unfortunately, it is dif®cult to use the theory developed so far to draw conclusions about real-world uses of MTL. Limitations of the current theory include:   it yields worst-case bounds that are too loose to insure extra tasks will help. For example, it is possi"
9354,unknown,"example, it is possible to create synthetic problems where increasing the number of tasks hurts performance instead of helping it. Results with these problems are consistent with the theory, but only because the bounds are loose enough to allow it.       it lacks a well-de®ned notion of task relatedness and makes assumptions about sharing in the hidden layer that often are not satis®ed. For exampl"
9355,unknown,"in the hidden layer that often are not satis®ed. For example, we usually ®nd that optimal performance requires increasing the number of units in the shared hidden layer as the number of tasks increases. This con¯icts with assumptions made by the theory that the hidden layer size remain constant as the number of tasks increases.   it is unable to account for behaviors of the search procedure that a"
9356,unknown,"  it is unable to account for behaviors of the search procedure that are critical in practice. As one example, if early stopping is not done correctly, MTL often hurts performance instead of helping it. The current theory is unable to account for important phenomena like this. Developing a theory of MTL that better agrees with what is observed in practice may be dif®cult. Perhaps the hardest obsta"
9357,unknown,"dif®cult. Perhaps the hardest obstacle standing in the way of better MTL theory is the dif®culty of de®ning task relatedness. An improved theory of MTL in arti®cial neural nets also would need to address open questions about the effective capacity of neural nets and take into account important behaviors of training procedures like backprop, such as their susceptibility to local minima, pressure to"
9358,unknown,"susceptibility to local minima, pressure towards sharing, etc. [Munro & Parmanto 1997] use extra tasks to improve the generalization performance of a committee machine that combines the predictions of multiple learned experts. Because committee machines work better if the errors made by different committee members are decorrelated, they use a different extra task for each committee member to bias "
9359,unknown,"decorrelated, they use a different extra task for each committee member to bias how it learns the main task. Each committee member learns the main task in a slightly different way, and the performance of the committee as a whole improves. Committee machines trained with extra tasks can be viewed as MTL with architectures more complex than the simple, fully connected MTL architectures presented her"
9360,unknown,"fully connected MTL architectures presented here. One interesting feature of committee MTL architectures is that multiple copies of the main task are used, and this improves performance on the main task. Sometimes this same effect is observed with simpler, fully connected MTL nets, too [Caruana 1993]. [Dietterich & Bakiri 1995] examine a much more sophisticated approach to bene®tting from multiple"
9361,unknown,"multi-bit error-correcting codes as the output representation. One application of MTL is to take features that will be missing at run time but that are available for the training set, and use them as outputs instead of inputs. There are other ways to handle missing values. One approach is to treat each missing feature as a separate learning problem, and use predictions for missing values as inputs"
9362,unknown,"learning problem, and use predictions for missing values as inputs. (We tried this on the pneumonia problem and did not achieve performance comparable to MTL, but in some domains this works well.) Other approaches to missing data include marginalizing over the missing values in learned probabilistic models [Little & Rubin 1987; Tresp, Ahmad & Neuneier 1994], and using EM to iteratively reestimate "
9363,unknown,"& Neuneier 1994], and using EM to iteratively reestimate missing values from current estimates of the data density [Ghahramani & Jordan 1994, 1997]. Of particular interest in this direction is work on learning Bayesian Networks [Cooper & Herskovits 1992; Spirtes, Glymour, & Scheines 1993; Jordan & Jacobs, 1994]. Because Bayes nets have sound statistical semantics (which makes handling missing valu"
9364,unknown,"statistical semantics (which makes handling missing values easier) and usually are more comprehensive models than those learned with STL, Bayes nets also are able to bene®t from extra tasks like those used by MTL. It is not clear yet if Bayes nets represent a competitive approach to MTL, the main issue being that the extra complexity inherent in     many Bayes net models may increase the number of"
9365,unknown,"good performance. 7. Discussion and Future Work 7.1. Predictions for Multiple Tasks MTL trains many tasks in parallel on one learner, but this does not mean one learned model should be used to make predictions for many tasks. The reason for training multiple tasks on one learner is so one task can bene®t from the information contained in the training signals of other tasks, not to reduce the numbe"
9366,unknown,"signals of other tasks, not to reduce the number of models that must be learned. Where tradeoffs can be made between mediocre performance on all tasks and optimal performance on any one task, usually it is best to optimize performance on tasks one at a time, and allow performance on the extra tasks to degrade. The task weights in MTL KNN/LCWA and MTL TDIDT make this tradeoff explicit; the learner "
9367,unknown,"better performance on the main task. Where predictions for several tasks are required (as in CAP, Section 4.8), it may be important to train a separate MTL model for each required task. With backprop MTL, however, using an architecture that treats all tasks equally, and that has suf®cient capacity in the shared hidden layer to allow parts of the hidden layer to become dedicated to single tasks, of"
9368,unknown,"tasks, often allows models to be learned for all tasks during one training run. If early stopping is used, it is important to apply it to each task individually; not all tasks trainÐ or overtrainÐat the same rate. The easiest way to do this is to take snapshots of the network when performance on each task is best, instead of trying to halt training on some tasks while other tasks are still being t"
9369,unknown,"tasks while other tasks are still being trained. If some tasks train much faster than others, reducing the learning rate on tasks that have already achieved their best performance is one way to prevent them from overtraining so much that they drag the other slower tasks into overtraining. 7.2. Learning Rate in Backprop MTL Usually better performance is obtained in backprop MTL when all tasks learn"
9370,unknown,"rates and reach best performance at roughly the same time. If the main task trains long before the extra tasks, it cannot bene®t from what has not yet been learned for the extra tasks. If the main task trains long after the extra tasks, it cannot shape what is learned for the extra tasks. Moreover, if the extra tasks begin to overtrain, they may cause the main task to overtrain too because of the "
9371,unknown,"task to overtrain too because of the overlap in hidden layer representation. The easiest way to control the rate at which different tasks learn is to adjust the learning rate on each output task. One way to do this is to train a net using equal learning rates for all tasks, and then train again a second time, reducing the learning rate for tasks that learned fastest. A few iterations of this proce"
9372,unknown,learned fastest. A few iterations of this process usually suf®ce to bring most tasks to peak performance at roughly the same time. Early stopping on tasks individually is then used to pick the optimal stopping point for each task. We are currently testing an algorithm     that automates this tuning of learning rates. Instead of using a learning rate for each task that is constant throughout traini
9373,unknown,"that is constant throughout training, it adaptively adjusts each task's learning rate during training based on how much progress that task has made. Tasks ahead of schedule have their learning rate reduced until slower tasks catch up. This method still requires at least one prior training run to estimate how far each task will get before it begins to overtrain. 7.3. Parallel vs. Sequential Transfe"
9374,unknown,MTL is parallel transfer. It might seem that sequential transfer [Pratt & Mostow 1991; Pratt 1992; Sharkey & Sharkey 1992; Thrun & Mitchell 1994; Thrun 1995] would be easier. This may not be the case. The advantages of parallel transfer are:   The full detail of what is being learned for all tasks is available to all tasks because all tasks are being learned at the same time.   In many application
9375,unknown,"  In many applications, the extra tasks are available in time to be learned in parallel with the main task(s). Parallel transfer does not require one to de®ne a training sequenceÐ the order in which tasks are trained often makes a difference in serial transfer.   Tasks often bene®t each other mutually, something a linear sequence cannot capture. For example, if task 1 is learned before task 2, tas"
9376,unknown,"For example, if task 1 is learned before task 2, task 2 can't help task 1. This not only reduces performance on task 1, but can also reduce task 1's ability to help task 2. When tasks naturally arise serially, it is straightforward to use parallel transfer for se- quential transfer. If the training data can be stored, perform MTL using whatever tasks have become available, re-learning as new tasks"
9377,unknown,"become available, re-learning as new tasks arise. If training data cannot be stored, synthetic data can be generated from prior learned models (see Section 4.6). Interestingly, while it is easy to use parallel transfer to do serial transfer, it is not so easy to use serial transfer to do parallel transfer. Note that it is possible to combine serial and parallel transfer; O'Sullivan is currently ex"
9378,unknown,"learning in robots. 7.4. Computational Cost The main goal of multitask learning is to improve generalization. But what effect does MTL have on training time? In backprop nets, an MTL net is usually larger than an STL net and thus requires more computation per backprop pass. If all tasks eventually need to be learned, training the MTL net often requires less computation than training the individual"
9379,unknown,"STL nets. If most of the extra tasks are being trained just to help one or a few main tasks, then the MTL net will require more computation. However, we often ®nd that tasks trained with MTL need fewer epochs than the same tasks trained alone, which partially compensates for the extra computational cost of each MTL epoch. In k-nearest neighbor, kernel regression, and decision trees, MTL adds littl"
9380,unknown,"In k-nearest neighbor, kernel regression, and decision trees, MTL adds little to the cost of training the models. The only extra cost is the computation needed to evaluate performance on multiple tasks instead of just one task. This small constant factor is easily dominated     by other more expensive steps, such as computing distances between cases, ®nding nearest neighbors, ®nding the best thres"
9381,unknown,"neighbors, ®nding the best threshold for splits of continuous attributes in decision trees, etc. The main additional cost of using MTL with these algorithms is cross-validating the 'parameters the control the relative weight of the main and extra tasks. 7.5. Architecture The applications of MTL backprop presented in Section 2 use a single fully connected hidden layer shared equally by all tasks. S"
9382,unknown,"hidden layer shared equally by all tasks. Sometimes, more complex net architectures work better. For example, sometimes it is bene®cial to have a small private hidden layer for the main task, and a larger hidden layer shared by both the main task and extra tasks. But too many private hidden layers (e.g., a private hidden layer for each task) reduce sharing and the bene®ts of MTL. We do not current"
9383,unknown,"bene®ts of MTL. We do not currently have principled ways to determine what architecture is best for each problem. Fortunately, simple architectures often work well, even if not optimally. [Ghosn & Bengio 1997] experiment with several different architectures for MTL in backprop nets. Regularization methods such as weight decay can be used with MTL. By reducing the effective number of free parameter"
9384,unknown,"effective number of free parameters in the model, regularization promotes sharing. Too strong a bias for sharing, however, can hurt performance. If tasks are more different than they are alike (the usual case), it is important to allow tasks to learn fairly independent models and overlap only where there is common hidden structure. This is one reason why MTL performance often drops if the size of "
9385,unknown,"MTL performance often drops if the size of the shared hidden layer is much smaller than the sum of the sizes of the STL hidden layers that would provide good performance on the tasks when trained separately. 7.6. What Are Related Tasks? One of the most important open problems in inductive transfer is to better characterize, either formally or heuristically, what related tasks are. The lack of an a"
9386,unknown,"either formally or heuristically, what related tasks are. The lack of an adequate de®nition of task relatedness is one of the obstacles preventing the development of more useful theories of inductive transfer. Some of the characteristics of a theory of relatedness are already clear. For example, if two tasks are the same function of the inputs, but with independent noise processes added to the tas"
9387,unknown,"processes added to the task signals, clearly the two tasks are related. As another example, if two tasks are to predict different aspects of the health of the same individual, these tasks are more related than two tasks to predict different aspects of the health of different individuals. Finally, just because two tasks help each other when trained together does not necessarily mean they are relate"
9388,unknown,"mean they are related: sometimes injecting noise through an extra output on a backprop net improves generalization on other outputs by acting as a regularizer at the hidden layer, but this does not mean the noise task is related to the other tasks. We may never have a theory of relatedness that allows us to reliably predict which tasks will help or hurt each other when used for inductive transfer."
9389,unknown,"will help or hurt each other when used for inductive transfer. Because of this, we are now focussing part of our effort on ways of ef®ciently determining which tasks are bene®cially related to each other. Of particular interest is recent work on feature selection that shows     generalization performance sometimes improves if as many as half of the input features available on some of the large pro"
9390,unknown,"available on some of the large problems in the UCI repository are ignored, i.e., not used as inputs [Liu & Setiono 1996]. It would be interesting to test those problems to see if some of the ªignoredº features might be well used as extra outputs (as was done in Section 4.9). 7.7. When Inductive Transfer Hurts MTL does not always improve performance. In the pneumonia domain, performance dropped for"
9391,unknown,"dropped for high-risk cases when an extra SSE output was added to the rankprop net (see Section 4.2). This was consistent with our model of the relative strengths and weaknesses of the main and extra task on this problem. MTL is a source of inductive bias. Some inductive biases help. Some inductive biases hurt. It depends on the problem. For now, the safest approach is to treat MTL as a tool that "
9392,unknown,"safest approach is to treat MTL as a tool that must be tested on each problem. Fortunately, on most problems where we have tried MTL, it helps. Algorithms that automatically adjust the MTL bias using cross-validation, such as those used for TDIDT and KNN, are important steps for making MTL useful in practice. 7.8. MTL Thrives on Complexity Perhaps the most important lesson we have learned from app"
9393,unknown,Perhaps the most important lesson we have learned from applying MTL to real problems is that the MTL practitioner must get involved before the problem and data have been sanitized. MTL bene®ts from extra information that often would be engineered away because traditional STL techniques would not be able to use it. The opportunities for MTL often decrease as one gets further removed from the raw da
9394,unknown,often decrease as one gets further removed from the raw data or from the data collection process. MTL provides new ways of using information that may not be obvious from the traditional STL point-of-view. 8. Summary Acquiring domain-speci®c inductive bias is subject to the usual knowledge acquisition bottleneck. Multitask learning allows inductive bias to be acquired via the training signals for r
9395,unknown,"for related additional tasks drawn from the same domain. This paper demonstrates that the bene®t of using extra tasks can be substantial. Through careful experiments, we are able to show that the bene®ts of multitask learning are due to the extra information contained in the training signals for the extra tasks, not due to some other property of backpropagation nets that might be achieved in anoth"
9396,unknown,"nets that might be achieved in another way. We are also able to elucidate a number of mechanisms that explain how multitask learning improves generalization. Most of the work presented in this paper uses multitask learning in backprop nets. We have, however, developed algorithms for multitask learning in k-nearest neighbor and decision trees. The ability to use multitask learning with inductive me"
9397,unknown,"decision trees. The ability to use multitask learning with inductive methods as different as arti®cial neural nets, decision trees, and k-nearest neighbor speaks to the generality of the basic idea. Perhaps more importantly, we have been able to identify a number of situations   that commonly arise in real-world domains where multitask learning should be applicable. This is surprisingÐfew of the"
9398,unknown,"This is surprisingÐfew of the standard test problems used in machine learning today are multitask problems. We conjecture that as machine learning is applied to unsanitized, real-world problems, the opportunities for multitask learning will increase. Acknowledgments We thank Greg Cooper, Michael Fine, and other members of the Pitt/CMU Cost-Effective Health Care group for help with the Medis Pneumo"
9399,unknown,"Health Care group for help with the Medis Pneumonia Database; Dean Pomerleau for the use of his road simulator; Tom Mitchell, Reid Simmons, Joseph O'Sullivan, and other members of the Xavier Robot Project for help with Xavier the robot; and Tom Mitchell, David Zabowski, and other members of the Calendar Apprentice Project for help in collecting and using the CAP data. The work to characterize whic"
9400,unknown,"in collecting and using the CAP data. The work to characterize which features are more useful as inputs or as outputs is joint work with Virginia de Sa. Rankprop was developed with Shumeet Baluja. This work has bene®ted from discussions with many people, most notably Tom Mitchell, Herb Simon, Dean Pomerleau, Tom Dietterich, Shumeet Baluja, Jonathan Baxter, Virginia de Sa, Scott Fahlman, Andrew Moo"
9401,unknown,"Jonathan Baxter, Virginia de Sa, Scott Fahlman, Andrew Moore, Sebastian Thrun, and Dave Touretzky. We also thank the anonymous reviewers for their thorough reviews and excellent suggestions. Notes 1. More complex architectures than a fully connected hidden layer sometimes work better. See Section 7.5 2. A similar experiment using nets with 2 hidden layers containing 2, 4, 8, 16, or 32 hidden units"
9402,unknown,STL and 32 hidden units per layer for MTL yielded similar results. 3. It is interesting to note that other researchers who tackled this problem using this database ignored the the extra lab tests because they knew the lab tests would not be available at run time and did not see ways to use them other than as inputs. 4. In these experiments the nets have suf®cient capacity to ®nd independent minima
9403,unknown,"forced to share the hidden layer representations. 5. If separate  are learned for each extra task, some  may be near 0 while others may be larger than 1. 6. Baxter's theory does not exactly apply to the backprop MTL described in this paper because it assumes each task has independent training patterns. In MTL, the extra training signals are usually, though not always, available for the same tr"
9404,unknown,"available for the same training patterns as the main task. References Abu-Mostafa, Y. S., ªLearning from Hints in Neural Networks,º Journal of Complexity, 1990, 6(2), pp. 192±198. Abu-Mostafa, Y. S., ªHints and the VC Dimension,º Neural Computation, 1993, 5(2). Abu-Mostafa, Y. S., ªHints,º Neural Computation, 1995, 7, pp. 639-671. Baluja, S. and Pomerleau, D. A., ªUsing the Representation in a Neu"
9405,unknown,"Speci®c Focus of Attention,º Proceedings of the International Joint Conference on Arti®cial Intelligence 1995, IJCAI-95, Montreal, Canada, 1995, pp. 133-139. Baxter, J., ªLearning Internal Representations,º Ph.D. Thesis, The Flinders Univeristy of South Australia, Dec. 1994.    Baxter, J., ªLearning Internal Representations,º Proceedings of the 8th ACM Conference on Computational Learning Theory,"
9406,unknown,"Learning Theory, (COLT-95), Santa Cruz, CA, 1995. Baxter, J., ªA Bayesian/Information Theoretic Model of Bias Learning,º, Proceedings of the 9th International Conference on Computational Learning Theory, (COLT-96), Desenzano del Gardo, Italy, 1996. Breiman, L. and Friedman, J. H., ªPredicting Multivariate Responses in Multiple Linear Regression,º 1995, ftp://ftp.stat.berkeley.edu/pub/users/breiman"
9407,unknown,"ftp://ftp.stat.berkeley.edu/pub/users/breiman/curds-whey-all.ps.Z. Caruana, R., ªMultitask Learning: A Knowledge-Based Source of Inductive Bias,º Proceedings of the 10th International Conference on Machine Learning, ML-93, University of Massachusetts, Amherst, 1993, pp. 41-48. Caruana, R., ªMultitask Connectionist Learning,º Proceedings of the 1993 Connectionist Models Summer School, 1994, pp. 372"
9408,unknown,"1994, pp. 372-379. Caruana, R., ªLearning Many Related Tasks at the Same Time with Backpropagation,º Advances in Neural Information Processing Systems 7, (Proceedings of NIPS-94), 1995, pp. 656-664. Caruana, R., Baluja, S., and Mitchell, T., ªUsing the Future to ªSort Outº the Present: Rankprop and Multitask Learning for Medical Risk Prediction,º Advances in Neural Information Processing Systems 8"
9409,unknown,"NIPS-95), 1996, pp. 959-965. Caruana, R. and de Sa, V. R., ªPromoting Poor Features to Supervisors: Some Inputs Work Better As Outputs,º to appear in Advances in Neural Information Processing Systems 9, (Proceedings of NIPS-96), 1997. Caruana, R., ªMultitask Learning,º Ph.D. Thesis, School of Computer Science, Carnegie Mellon University, 1997. Cooper, G. F. and Herskovits, E., ªA Bayesian Method f"
9410,unknown,"Machine Learning, 1992, 9, pp. 309-347. Cooper, G. F., Aliferis, C. F., Ambrosino, R., Aronis, J., Buchanan, B. G., Caruana, R., Fine, M. J., Glymour, C., Gordon, G., Hanusa, B. H., Janosky, J. E., Meek, C., Mitchell, T., Richardson, T., and Spirtes, P., ªAn Evaluation of Machine Learning Methods for Predicting Pneumonia Mortality,º Arti®cial Intelligence in Medicine 9, 1997, pp. 107-138. Craven, "
9411,unknown,"Proceedings of the 11th International Conference on Machine Learning, ML-94, Rutgers University, New Jersey, 1994, pp. 37-45. Davis, I. and Stentz, A., ªSensor Fusion for Autonomous Outdoor Navigation Using Neural Networks,º Proceed- ings of IEEE's Intelligent Robots and Systems Conference, 1995. Dent, L., Boticario, J., McDermott, J., Mitchell, T., and Zabowski, D., ªA Personal Learning Apprentic"
9412,unknown,"Proceedings of 1992 National Conference on Arti®cial Intelligence, 1992. de Sa, V. R., ªLearning Classi®cation with Unlabelled Data,º Advances in Neural Information Processing Systems 6, (Proceedings of NIPS-93), 1994, pp. 112-119. Dietterich, T. G., Hild, H., and Bakiri, G., ªA Comparative Study of ID3 and Backpropagation for English Text-to-speech Mapping,º Proceedings of the Seventh Internation"
9413,unknown,"pp. 24-31. Dietterich, T. G., Hild, H., and Bakiri, G., ªA Comparison of ID3 and Backpropagationfor English Text-to-speech Mapping,º Machine Learning, 18(1), 1995, pp. 51-80. Dietterich, T. G. and Bakiri, G., ªSolving Multiclass Learning Problems via Error-Correcting Output Codes,º Journal of Arti®cial Intelligence Research, 1995, 2, pp. 263-286. Fine, M. J., Singer, D., Hanusa, B. H., Lave, J., a"
9414,unknown,"Using the MedisGroups Comparative Hospital Database,º American Journal of Medicine, 1993. Fisher, D. H., ªConceptual Clustering, Learning from Examples, and Inference,º Proceedings of the 4th Interna- tional Workshop on Machine Learning, 1987. Ghahramani, Z. and Jordan, M. I., ªSupervised Learning from Incomplete Data Using an EM Approach,º Advances in Neural Information Processing Systems 6, (Pro"
9415,unknown,"Advances in Neural Information Processing Systems 6, (Proceedings of NIPS-93,) 1994, pp. 120-127. Ghahramani, Z. and Jordan, M. I., ªMixture Models for Learning from Incomplete Data,º Computational Learning Theory and Natural Learning Systems, Vol. IV, R. Greiner, T. Petsche and S.J. Hanson (eds.), Cambridge, MA, MIT Press, 1997, pp. 67-85. Ghosn, J. and Bengio, Y., ªMulti-Task Learning for Stock "
9416,unknown,"Processing Systems 9, (Proceedings of NIPS-96), 1997. Hinton, G. E., ªLearning Distributed Representations of Concepts,º Proceedings of the 8th International Confer- ence of the Cognitive Science Society, 1986, pp. 1-12.    Holmstrom, L. and Koistinen, P., ªUsing Additive Noise in Back-propagation Training,º IEEE Transactions on Neural Networks, 1992, 3(1), pp. 24-38. Jordan, M. and Jacobs, R., ª"
9417,unknown,"6, pp. 181-214. Le Cun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackal, L. D., ªBackpropa- gation Applied to Handwritten Zip-Code Recognition,º Neural Computation, 1989, 1, pp. 541-551. Little, R. J. A. and Rubin, D. B., Statistical Analysis with Missing Data, 1987, Wiley, New York. Liu, H. and Setiono, R., ªA Probibilistic Approach to Feature SelectionÐA Filte"
9418,unknown,"13th International Conference on Machine Learning, ICML-96, Bari, Italy, 1996, pp. 319-327. Martin, J. D., ªGoal-directed Clustering,º Proceedings of the 1994 AAAI Spring Symposium on Goal-directed Learning, 1994. Martin, J. D. and Billman, D. O., ªAcquiring and Combining Overlapping Concepts,º Machine Learning, 1994, 16, pp. 1-37. Mitchell, T., ªThe Need for Biases in Learning Generalizations,º R"
9419,unknown,"Mitchell, T., Caruana, R., Freitag, D., McDermott, J., and Zabowski, D., ªExperience with a Learning Personal Assistant,º Communications of the ACM: Special Issue on Agents, July 1994, 37(7), pp. 80-91. Munro, P. W. and Parmanto, B., ªCompetition Among Networks Improves Committee Performance,º to appear in Advances in Neural Information Processing Systems 9, (Proceedings of NIPS-96), 1997. Omohund"
9420,unknown,"NIPS-95), 1996, pp. 402-408. O'Sullivan, J. and Thrun, S., ªDiscovering Structure in Multiple Learning Tasks: The TC Algorithm,º Proceedings of the 13th International Conference on Machine Learning, ICML-96, Bari, Italy, 1996, pp. 489-497. Pomerleau, D. A., ªNeural Network Perception for Mobile Robot Guidance,º Carnegie Mellon University: CMU-CS-92-115, 1992. Pratt, L. Y., Mostow, J., and Kamm, C."
9421,unknown,"Proceedings of AAAI-91, 1991. Pratt, L. Y., ªNon-literal Transfer Among Neural Network Learners,º Colorado School of Mines: MCS-92-04, 1992. Pratt, L. Y., Mostow, J., and Kamm, C. A., ªDirect Transfer of Learned Information Among Neural Networks,º Proceedings of AAAI-91, 1991. Quinlan, J. R., ªInduction of Decision Trees,º Machine Learning, 1986, 1, pp. 81-106. Quinlan, J. R., C4.5: Programs for M"
9422,unknown,"Quinlan, J. R., C4.5: Programs for Machine Learning, Morgan Kaufman Publishers, 1992. Rumelhart, D. E., Hinton, G. E., and Williams, R. J., ªLearning Representations by Back-propagating Errors,º Nature, 1986, 323, pp. 533-536. Sejnowski, T. J. and Rosenberg, C. R., ªNETtalk: A Parallel Network that Learns to Read Aloud,º John Hopkins: JHU/EECS-86/01, 1986. Sharkey, N. E. and Sharkey, A. J. C., ªAd"
9423,unknown,"Exeter: R257, 1992. Sill, J. and Abu-Mostafa, Y., ªMonotonicity Hints,º to appear in Neural Information Processing Systems 9, (Proceedings of NIPS-96), 1997. Spirtes, P., Glymour, C., and Scheines, R., Causation, Prediction, and Search,, 1993, Springer-Verlag, New York. Simard, P., Victorri, B., LeCun, Y., and Denker, J., ªTangent PropÐA Formalism for Specifying Selected Invari- ances in an Adapti"
9424,unknown,"NIPS-91) 1992, pp. 895-903. Suddarth, S. C. and Holden, A. D. C., ªSymbolic-neural Systems and the Use of Hints for Developing Complex Systems,º International Journal of Man-Machine Studies, 1991, 35(3), pp. 291-311. Suddarth, S. C. and Kergosien, Y. L., ªRule-injection Hints as a Means of Improving Network Performance and Learning Time,º Proceedings of the 1990 EURASIP Workshop on Neural Networks"
9425,unknown,"Learning Time,º Proceedings of the 1990 EURASIP Workshop on Neural Networks, 1990, pp. 120-129. Thrun, S. and Mitchell, T., ªLearning One More Thing,º Carnegie Mellon University: CS-94-184, 1994. Thrun, S., ªLifelong Learning: A Case Study,º Carnegie Mellon University: CS-95-208, 1995. Thrun, S., ªIs Learning the N-th Thing Any Easier Than Learning The First?,º Advances in Neural Information Proce"
9426,unknown,"Processing Systems 8, (Proceedings of NIPS-95), 1996, pp. 640-646. Thrun, S., Explanation-Based Neural Network Learning: A Lifelong Learning Approach, 1996, Kluwer Academic Publisher. Tresp, V., Ahmad, S., and Neuneier, R., ªTraining Neural Networks with De®cient Data,º Advances in Neural Information Processing Systems 6, (Proceedings of NIPS-93), 1994, pp. 128-135.    Valdes-Perez, R., and Simon"
9427,unknown,"Proceedings of the 11th International Conference on Machine Learning, ML-94, Rutgers University, New Jersey, 1994, pp. 326-334. Waibel, A., Sawai, H., and Shikano, K., ªModularity and Scaling in Large Phonemic Neural Networksº IEEE Transactions on Acoustics, Speech and Signal Processing, 1989, 37(12), pp. 1888-1898. Received Date Accepted Date Final Manuscript Date Statistical Modelling Epiphany T"
9428,unknown,"Epiphany Term 2023 Lecture notes Lecturer Dr Tahani Coolen-Maturi Email tahani.maturi@durham.ac.uk Oﬃce MCS 3030 Oﬃce hour See Blackboard Learn Ultra Online information Blackboard Learn Ultra (or Ultra for short) will be used to post lecture notes, problems sheets, tutorials and computer practicals. Information on learning outcomes and assessment for the module may also be found in the Faculty Han"
9429,unknown,"https://www.dur.ac.uk/faculty.handbook/module_description/?year=2022&module_code=MATH2697. Literature (C) Crawley. The R book (2007): Wiley, ISBN 9786610900978. (electronic resource). (K) Krzanowski. Principles of Multivariate Analysis: A User’s Perspective , (2000): Oxford, ISBN 0198507089. (M) Mardia, Kent, & Bibby, Multivariate Analysis : Academic Press, 1979, ISBN 0124712509. (MC) Petersen & P"
9430,unknown,"(MC) Petersen & Petersen (2008), The Matrix Cookbook (Version November 15, 2012), (PDF). (N) Neter, Kutner, Nachtsheim, & Wasserman. Applied Linear Statistical Models (several editions with diﬀerent combinations of authors 1974–2004): McGraw-Hill, ISBN 0256117365. (R) Rice. Mathematical Statistics and Data Analysis (3rd edn., 2006): Brooks/Cole, ISBN 0495110892. (RA) Raykov. Basic Statistics — an "
9431,unknown,"(RA) Raykov. Basic Statistics — an introduction with R (2013). Access via MyiLibrary. (V) Venables, Smith, and the R Development Core Team. An Introduction to R (HTML) (PDF). (W) Weisberg. Applied Linear Regression (3rd edn., 2005): Wiley-Interscience, ISBN 0471663794. Based on lecture notes developed by Prof. Jochen Einbeck at Durham. 1 Contents 1 Introduction 4 1.1 Supervised and unsupervised le"
9432,unknown,1.2 Basics of multivariate analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.2.1 Multivariate normal distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.2.2 Variance matrix estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.3 Mahalanobis distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
9433,unknown,1.3.1 Checking multivariate normality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.3.2 Outlier detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2 Linear models: Assumptions & Estimation 18 2.1 Model speciﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.1.1 Linear model assumptions . . . . . . . . . . 
9434,unknown,2.1.1 Linear model assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.2 Estimation of model parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.2.1 Least-squares (LS) estimation of β . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.2.2 Estimation of σ2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
9435,unknown,2.2.2 Estimation of σ2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.3 Statistical properties of ˆβand s2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 2.3.1 Expectation and variance of ˆβ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 2.3.2 Variance of linear combinations of ˆβ . . . . . . . . . . . . . . . . . . . . . . . . . . 
9436,unknown,2.3.2 Variance of linear combinations of ˆβ . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.3.3 Expectation of s2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3 Linear models: Inference & Prediction 31 3.1 Inference for linear model parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.1.1 Sampling distribution . . . . . . . . . . . . 
9437,unknown,3.1.1 Sampling distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.1.2 Signiﬁcance tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.1.3 Conﬁdence intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.2 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
9438,unknown,4 Factors 39 4.1 Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 4.3 Factorial experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 5 Analysis of variance 47 5.1 Explaining variation . . . . . . . . 
9439,unknown,5.1.1 The ANOVA table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 5.1.2 Sequential ANOVA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 6 Model selection 56 6.1 Submodels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 6.2 Selection criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . 
9440,unknown,6.3 Selection methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 6.3.1 Forward selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 6.3.2 Backward elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 6.3.3 Stepwise selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9441,unknown,2 7 Regression diagnostics & transformations 63 7.1 Regression diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 7.1.1 Leverage values and studentised residuals . . . . . . . . . . . . . . . . . . . . . . . 63 7.1.2 Inﬂuence analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 7.1.3 Model checking through residual diagnostic
9442,unknown,"7.1.3 Model checking through residual diagnostics . . . . . . . . . . . . . . . . . . . . . . 71 7.2 Box-Cox transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 A Some matrix algebra 79 3 Chapter 1 Introduction 1.1 Supervised and unsupervised learning Most statistical problems can be interpreted as a learning problem. One has observed some data sample, and "
9443,unknown,"was sampled. We start directly with a simple illustrative environmental data example. Example 1.1 Scallop data. Scallops are small bivalves that live in deep waters and grow in shells, much the way oysters do. Fig. 1.1 (left) provides a graphical representation of n = 127 locations (represented by two variables, longitude long and latitude lat) at which scallops were collected in a 1990 survey cru"
9444,unknown,"continental shelf oﬀ Long Island, New York, USA. The set of locations forms a bivariate data cloud of points ( longi,lati), i = 1,...n . One can put these together to form a data matrix Z=   long1 lat1 ... ... longn latn .   One may be interested in using Z in order to gain some information on the scallop population that it represents. For instance, properties of interest could be • the mean"
9445,unknown,"• the mean, ¯Z= (long,lat); • the variance-covariance structure of Z (Sec. 2.4); • main directions of variability; here: south-west to north-east (‘Principal component analysis’, we will not cover this!) • the existence of outliers (Sec. 2.5); • when assuming a certain distributional shape: the parameter estimates and their properties (‘Max- imum Likelihood Estimation’, Sec. 1.2.2); • the existenc"
9446,unknown,"• the existence of clusters or other features (we will not do this!) ... We continue with the same data set but consider an additional variable. Beyond the locations, we have additional information available on the abundance of scallops caught by the cruiser. Speciﬁcally, we are given Y = (y1,...,y n)T, where yi is the logarithm of the number of scallops found at location (longi,lati). We display "
9447,unknown,"We observe from this plot that there is some roughly linear relationship between the variables ( long, lat) contained in the original data matrix, Z, and the response vector, Y. The task is to quantify this relationship in some suitable form. 4 −73.5 −73.0 −72.5 −72.0 39.0 39.5 40.0 40.5 41.0 scallops$long scallops$lat long lat log(tcatch) Figure 1.1: Longitudes and latitudes of scallop locations."
9448,unknown,"log–abundances. Note that the view on the data has changed compared to the initial situation. We have now two types of variables: Those ones stored in Z play the role of an input, and that one stored in Y play the role of an output. We wish to gain some insight how the physical system considered transforms a given input into an output. We will achieve this through a statistical model which relates"
9449,unknown,"the elements of Z. For example, a simple model that would spring into mind could be yi = β1 + β2longi + β3lati + ϵi, (1.1) where the parameters β1, β2, and β3 have to be estimated, and ϵ is some noise or error. In the example considered, such noise may stem from daily change of environmental conditions (weather, tides), unknown quantities related to the cruiser (speed, experience, etc.), measureme"
9450,unknown,"causing random variation. The model (1.1) is an example for a linear regression model. We will deal with this model in detail in Chapter 3. There we will also study how to obtain estimates ˆβ1, ˆβ2, ˆβ3 of the unknown parameters β1, β2, and β3, thus enabling us to produce ﬁtted values ˆyi = ˆβ1 + ˆβ2longi + ˆβ3lati. How do we know whether these are good? Here, the observed (known) values yi can pl"
9451,unknown,"teacher: When all yi are relatively close to the ˆyi, then the model is good (in some sense), otherwise it is bad; i.e., one aims to minimize n∑ i=1 (yi −ˆyi)2. (1.2) Therefore this kind of statistical learning is often referred to as supervised learning. In contrast, learning problems involving just a data matrixZ, as described earlier in this section, are referred to asunsupervised learning prob"
9452,unknown,"learning problems (Table 1.1). 5 Table 1.1: Supervised and unsupervised learning data structure Aim Learning type Z Investigate properties of Z Unsupervised learning [Z,Y] Learn how Z aﬀects Y Supervised learning Example 1.2 (R Code for Example 1.1) #install.packages(""remotes"") #you need to do that once > remotes::install_github(""tmaturi/sm2data"") > library(sm2data) > ?scallops > dim(scallops) [1]"
9453,unknown,"[1] 127 4 > names(scallops) [1] ""lat"" ""long"" ""tcatch"" ""y"" # Note: y=log(tcatch) > scallops[1:6,] > head(scallops) # Both commands above do the same, namely to display the first 6 rows: lat long tcatch y 1 40.38333 -71.85000 1 0.00000 2 40.13333 -72.08333 2 0.69315 3 40.10000 -72.31667 7 1.94591 4 40.01667 -72.40000 13 2.56495 5 39.90000 -72.56667 530 6.27288 6 39.81667 -72.48333 2750 7.91936 # Fin"
9454,unknown,"# Find overall mean of positions > c(mean(scallops$long), mean(scallops$lat)) # or, equivalently: > colMeans(scallops[,c(""long"",""lat"")]) long lat -72.73215 39.91798 # Some graphical display in 2D: > plot(scallops$long, scallops$lat) # Fig. 1.1 (left) # Now involve the response, y: > library(lattice) > cloud(y~long+lat, data=scallops) # Fig. 1.1 (right) # An additional tidbit > plot(scallops$long, "
9455,unknown,"# An additional tidbit > plot(scallops$long, scallops$lat, cex=0.75) > for (j in 1:126){ segments(scallops$long[j],scallops$lat[j],scallops$long[j+1], scallops$lat[j+1], col=2) } # gives the route presumably taken by the cruiser 6 1.2 Basics of multivariate analysis Random vectors and densities Let Xj,j = 1,...,q be a collection of real-valued random quantities (r.q.’s). Then X =   X1 ... Xq  "
9456,unknown,"X =   X1 ... Xq   forms a q- dimensional random vector (r.v.) and x=   x1 ... xq   a particular realization of X. The probabilistic behavior of X is entirely determined by the distribution function of X, F(x) = F(x1,...,x q) = P(X1 ≤x1,...,X q ≤xq). An overview of important operations with density functions is provided in Table 1.2 for general reference. Table 1.2: Common operations wi"
9457,unknown,"Table 1.2: Common operations with density functions. Let g : Rq −→R be any arbitrary (integrable) function, h : Rq −→Rp a bijective and diﬀerentiable function, p a positive integer, and y= (y1,...,y p)T. (D1) Marginalization ( p<q ) f(x1,...,x p) = ∫ xq ... ∫ xp+1 f(x1,...,x q) dxp+1 ...dx q (D2) Conditioning ( p<q ) f(xp+1,...,x q|x1,...,x p) = f(x1,...,x q)/f(x1,...,x p) (D3) Independence X1,..."
9458,unknown,"(D4) Expectation E(g(X)) = ∫ g(x)f(x) dx (D5) Change of variables The density of Y = h(X) is fY(y) = f(h−1(y))|dx/dy|. Expectation and variance The expectation of a r.v. X = (X1,...,X q)T is given by   m1 ... mq  = m= E(X) = ∫ xf(x) dx= ∫ ... ∫   x1 ... xq  f(x1,...,x q)dx1 ...dx q =   E(X1) ... E(Xq)   that is, the j–th component of mis just the expectation of the j–th component"
9459,unknown,"The variance is given by Var(X) = E((X−m)(X−m)T) = E(XXT) −mmT =   Σ11 ··· Σ1q ... ... ... Σq1 ··· Σqq  = Σ, (1.3) where Σij = Cov( Xi,Xj) = E(XiXj) −E(Xi)E(Xj) i̸= j Σjj ≡ σ2 j = Var(Xj) 7 In short, we write X ∼(m,Σ) meaning that X has some unspeciﬁed distribution with mean (expectation) mand variance Σ. Any variance matrix Σ found via (1.3) has the following properties: (i) Σ is symmetric,"
9460,unknown,"(i) Σ is symmetric, (ii) Σ is positive semi-deﬁnite. If a given matrix fulﬁls (i) and (ii) we call it a valid variance matrix, otherwise it is invalid . Important related concepts • The correlation matrix is deﬁned as R= (Rij)1≤i≤q,1≤j≤q with pairwise correlation coeﬃcients Rij = Σij√ ΣiiΣjj , where Rij = Rji ∈[−1,1] for i̸= j and Rii = 1 for i= 1,...,q . In matrix notation the correlation matrix "
9461,unknown,"matrix can be expressed as R= D−1/2ΣD−1/2, where D = diag(Σ 11,..., Σqq). A very useful feature of the correlation matrix R is that it is scale–invariant. • We call the random variables Xi and Xj uncorrelated if Rij = 0. If Xi and Xj are independent, then Σij = 0 and, consequently, Xi and Xj are uncorrelated. Hence, under (D3), all Xi, Xj, for i̸= j, are uncorrelated so Rbecomes the identity matri"
9462,unknown,"• Let Y ∈Rp be a further random vector. We deﬁne the covariance between r.v.’s X ∈Rq and Y ∈Rp as Cov(X,Y ) = (Cov(Xi,Yj))1≤i≤q,1≤j≤p = Cov(Y,X)T. • Sums of random vectors (of the same dimensionality): For X ∼(m,Σ), Y ∼( ˜m, ˜Σ), one has X±Y ∼ ( m±˜m,Σ ±Cov(X,Y ) ±Cov(Y,X) + ˜Σ ) = ( m±˜m,Σ ±2Cov(X,Y ) + ˜Σ ) (1.4) Note that the two covariance terms vanish if X and Y are independent. 1.2.1 Multiva"
9463,unknown,"1.2.1 Multivariate normal distribution We say that the random vector X = (X1,...,X q)T ∈Rq is multivariate normal (MVN) with parameters m∈Rq, Σ ∈Rq×q (pos. def.) if its density is f(x) = 1 (2π)q/2|Σ|1/2 exp { −1 2(x−m)TΣ−1(x−m) } , (1.5) where |Σ|≡ det(Σ). In short we write X ∼Nq(m,Σ). In general the opposite is not true; i.e. Σ ij = 0 ̸⇒f(xi,xj) = f(xi)f(xj). For instance, consider a random varia"
9464,unknown,"variable X1 with E(X1) = 0 and E(X3 1 ) = 0 and the random variable X2 = X2 1 . Clearly, X1 and X2 are dependent; however, Cov(X1,X2) = E(X1X2) −E(X1)E(X2) = E(X3) = 0. 8 Linear transformations and special cases A very useful identity related to linear transformations of multivariate normal r.v. is the following: if X ∼Nq(m,Σ), A∈Rr×q and b∈Rr, then AX+ b∼Nr(Am+ b,AΣAT). (1.6) Recall that E(AX+ b)"
9465,unknown,E(AX+ b) = E(AX) + E(b) = AE(x) + b= Am+ b Var(AX+ b) = E((AX+ b−(Am+ b))(AX+ b−(Am+ b))T) = E((A(X−m))(A(X−m))T) = E(A(X−m)(X−m)TAT) = AE((X−m)(X−m)T)AT = AΣAT. Eq. (1.6) is very useful as important properties may be derived in a very simple manner. Let look how we can use (1.6) for two important special cases. • Marginalization: The general way of ﬁnding marginal distributions involves multiple 
9466,unknown,"The general way of ﬁnding marginal distributions involves multiple integration; see (D1) in Table 1.2. For instance, we can ﬁnd the marginal of X1 by doing the following integration f(x1) = ∫ x2 ... ∫ xq f(x1,x2,...,x q)dx2 ...dx q, which is a rather tedious process! Here we can use (1.6) in order to avoid this. Speciﬁcally, if we have X ∼Nq(m,Σ) then if want to ﬁnd the marginal of Xj we simply de"
9467,unknown,"we have X ∼Nq(m,Σ) then if want to ﬁnd the marginal of Xj we simply deﬁne the row vector A= (0 ... 0 1 0 ... 0) ∈R1×q which is zero everywhere except at the j-th position where it equals 1. We also set b= 0. Then Xj = AX+ b∼N(mj,Σjj). So if the r.v. X is MVN, then each univariate component Xj is normally distributed. • Standardization Again we have that X ∼Nq(m,Σ) where Σ is positive deﬁnite and t"
9468,unknown,"Σ1/2 exists. Then, we can consider the following transformation Σ−1/2(X−m) = Σ−1/2X−Σ−1/2m. In the context of Eq. (1.6) we have A= Σ−1/2 and b= Σ−1/2m; therefore, Σ−1/2(X−m) ∼Nq(Σ−1/2m−Σ−1/2m,Σ−1/2ΣΣ−1/2) ∼Nq(0,Iq). The transformed r.v. has zero means and covariance matrix equal to the identity matrix (zero covariances/corellations and variances equal to one). Example 1.3 Bivariate normal distribu"
9469,unknown,"Let X = (X1 X2 ) , m= (m1 m2 ) , Σ = ( σ2 1 0 0 σ2 2 ) . Then f(x1,x2) = 1 2πσ1σ2 exp { −1 2(x1 −m1,x2 −m2) ( σ2 1 0 0 σ2 2 )−1 ( x1 −m1 x2 −m2 )} = 1 2πσ1σ2 exp { −1 2(x1 −m1,x2 −m2) ( 1/σ2 1 0 0 1 /σ2 2 )( x1 −m1 x2 −m2 )} = 1√ 2πσ1 exp { −1 2 (x1 −m1)2 σ2 1 } · 1√ 2πσ2 exp { −1 2 (x2 −m2)2 σ2 2 } = f(x1) ·f(x2) 9 Hence, when X1 and X2 are bivariate normal and uncorrelated, then they are also in"
9470,unknown,"Hence, when X1 and X2 are bivariate normal and uncorrelated, then they are also independent. Obvi- ously, this also holds the other way round: When X1 and X2 are normally distributed and independent, then they are BVN. Example 1.4 Visualizing bivariate normal distributions The following code visualizes the density f(x,y) of a bivariate normal distribution generated by two independent random quanti"
9471,unknown,"1.2. > x<- seq(-8,20, length=51) # defines the range of x-values plotted > y<- seq(-2,8, length=51) > dens <- matrix(0,51,51) # creates an ""empty"" matrix of appropriate # dimension which will be used for values of f(x,y) > for (i in 1:51){ > for (j in 1:51){ > dens[i,j]<- dnorm(x[i],6,3)*dnorm(y[j],3,1) # uses independence (D3) # to generate the joint density > } > } > persp(x, y, dens, theta=40, "
9472,unknown,"> } > } > persp(x, y, dens, theta=40, phi=20) # plots the density in 3D x y dens Figure 1.2: Plot of bivariate density function f(x1,x2) with uncorrelated components. Another way of visualizing multivariate densities are through “contours”. Contours are deﬁned as curves of equal density. For the MVN, these curves are ellipsoids (x−m)TΣ−1(x−m) = c2, for some constant c. For contours in R we use the"
9473,unknown,"10 > contour(x, y, dens) # contour plot of density The corresponding graph is shown in Fig. 1.3. −5 0 5 10 15 20 −2 0 2 4 6 8 Figure 1.3: Contour plot of bivariate density function f(x1,x2) with uncorrelated components. Normality, independence and correlation Simple results of the type above have let to the widespread misconception that, whenX1 and X2 are each normally distributed and uncorrelated"
9474,unknown,"vided below. Table 1.3 summarizes the relationship between multivariate independence/correlation/normality in diagrammatic form. Table 1.3: Relationship between normality, multivariate normality, independence, and correlation X1,...,X q are... MVN normal independent ←→ ↕ ↓ uncorrelated −→ otherwise −→ Example 1.5 Uncorrelated normal variables which are not independent. 11 Let X ∼N(0,1) and Y = WX,"
9475,unknown,"mass W = { −1, with probability 1/2, 1, with probability 1/2. First, let us examine the covariance of X and Y: Cov(X,Y ) = E(XY) −E(X)E(Y) ( E(X) = 0) = E(X2W) ( X2,W independent) = E(X2)E(W) ( E(W) = 0) = 0. Thus, X and Y are uncorrelated. Next, we will prove that Y is also normally distributed by showing that Pr(Y ≤x) = Pr(X ≤x). Pr(Y ≤x) = E(Pr(Y ≤x|W)) = Pr(X ≤x)Pr(W = 1) + Pr(−X ≤x)Pr(W = −1)"
9476,unknown,"= Pr(X ≤x)Pr(W = 1) + Pr(X ≥−x)Pr(W = −1) = Pr(X ≤x)1 2 + Pr(X ≤x)1 2 (symmetry of normal) = Pr(X ≤x). So, X and Y are both normally distributed and uncorrelated. However, they are not independent. For instance, |Y|= |X|. Conditional normality Any r.v. X = (X1 ...X q)T can be represented in terms of random subvectors. For instance, for ( p<q ) we may write X = (Y Z)T, where Y = (X1 ...X p)T ∈Rp an"
9477,unknown,"X ∼Nq(m,Σ) we have ( Y Z ) ∼Nq (( mY mZ ) , ( ΣY ΣYZ ΣZY ΣZ )) , where mY ∈Rp, mZ ∈Rq−p, ΣY ∈Rp×p, ΣYZ ∈Rp×(q−p), ΣZY ∈R(q−p)×p and ΣZ ∈R(q−p)×(q−p). A further property of the MVN relates to conditional distributions. Speciﬁcally, The conditional distribution of Y given a realization of Z = z is again MVN; namely, Y |Z = z∼Np( ˜mY, ˜ΣY), ˜mY = mY + ΣYZ Σ−1 Z (z−mZ), ˜ΣY = ΣY −ΣYZ Σ−1 Z ΣZY . When "
9478,unknown,"Np(mY,ΣY). Example 1.6 The conditional distribution in the bivariate correlated case. If X = (X1 X2)T BVN with non-zero correlation then the conditional distribution of X1 is X1 |X2 = x2 ∼N ( m1 + ρσ1 σ2 (x2 −m2),(1 −ρ2)σ2 1 ) , where ρ= σ12 σ1σ2 is the correlation. 12 1.2.2 Variance matrix estimation The setup is as follows: We have data xi = (xi1,...,x iq)T, i = 1,...,n , which are n independent"
9479,unknown,"identically distributed (iid) observations generated from a r.v. X ∼(m,Σ) ∈Rq, and which form together a data matrix X=   xT 1 ... xT n  =   x11 ... x 1q ... ... ... xn1 ... x nq  . We denote by ¯x= 1 n n∑ i=1 xi = 1 n n∑ i=1   xi1 ... xiq  =   1 n ∑n i=1 xi1 ... 1 n ∑n i=1 xiq  =   ¯x1 ... ¯xq   the overall mean, which forms an unbiased estimate of the expectation m "
9480,unknown,"the overall mean, which forms an unbiased estimate of the expectation m meaning that if we use the estimator ˆm= ¯xthen E( ˆm) = m. The goal of this subsection is to estimate Σ. Firstly, recall Σ = Var(X) = E ( (X−m)(X−m)T) . Replacing all expectations by means andmby ¯x(if the former is unknown), a natural candidate estimator for Σ is given by ˆΣ = 1 n n∑ i=1 (xi −¯x)(xi −¯x)T Q. 2.1 = 1 n n∑ i=1"
9481,unknown,"for Σ is given by ˆΣ = 1 n n∑ i=1 (xi −¯x)(xi −¯x)T Q. 2.1 = 1 n n∑ i=1 xixT i −¯x¯xT ∈Rq×q. In fact, this turns out to be the Maximum Likelihood estimator for Σ (under the MVN assumption), therefore we give this estimator the suﬃx ML. It can be shown that ˆΣML is not unbiased for Σ, as E( ˆΣML) = ( 1 −1 n ) Σ ̸= Σ However, since lim n−→∞ ( ˆΣML) = lim n−→∞ ( 1 −1 n ) Σ = Σ, we say that ˆΣML is as"
9482,unknown,"An unbiased estimator of Σ is obtained through the sample variance matrix , ˆΣsample = 1 n−1 n∑ i=1 (xi −¯x)(xi −¯x)T = n n−1 ˆΣML, where essentially the fraction n/(n−1) is a bias correction factor. We generally prefer ˆΣsample to ˆΣML. Strictly, when talking of iid observations x1,..., xn, we mean that x1,..., xn are realizations of nindependent and identically distributed random vectors, which "
9483,unknown,"course not imply that the components X1,...,X q need to be independent! 13 1.3 Mahalanobis distance Given: Multivariate distribution X ∼(m,Σ). Question: What is (a fair measure of) the distance between a point x = ( x1,...,x q)T and the mean m= (m1,...,m q)T? Example 1.7 (Continuation of Example 1.4). Back to the BVN with uncorrelated components. Consider the contour plot shown in Fig. 1.2 in Exam"
9484,unknown,"Example 1.4. One ﬁnds easily, by taking the length along the coordinate axes, that the (Euclidean) distance from, say, (12,3)T to m= (6,3)T is 6, while the distance from (6 ,6)T to mis only 3. But does this reﬂect accurately the eﬀort to cover the distance? From the perspective of a hiker, the way from (12,3)T to the summit is probably the easier one, as one starts from a higher basis already. In "
9485,unknown,"In more mathematical terms, taking the Euclidean distance dE = √ (x1 −m1)2 + (x2 −m2)2 ignores the diﬀering variability of the components of the random vector. To account for this, one can consider the standardized Euclidean distance dM = √(x1 −m1 σ1 )2 + (x2 −m2 σ2 )2 = = √(x1 −m1 x2 −m2) ( 1 σ2 1 0 0 1 σ2 2 )( x1 −m1 x2 −m2 ) which gives, for the example above, the values 2 and 3, respectivel"
9486,unknown,"the hiker. — One can generalize this notion to arbitrary random vectors: For any X ∼(m,Σ), with Σ pos. def., one deﬁnes the Mahalanobis distance (to the mean) through dM(x,m,Σ) = √ (x−m)TΣ−1(x−m). If X is a MVN, i.e. X ∼Nq(m,Σ), then points with equal Mahalanobis distance to the mean lie on the same contour, and one has, d2 M(X,m,Σ) = ( X−m)TΣ−1(X−m) = = ( X−m)TΣ−1/2Σ−1/2(X−m) = = [ Σ−1/2(X−m) ]T "
9487,unknown,"= [ Σ−1/2(X−m) ]T    ≡Y∼Nq(0,I) [ Σ−1/2(X−m) ] = = YTY = q∑ j=1 Y2 j N(0,1)2 ∼χ2 q Note: The χ2 property remains true (rule of thumb n ≥10q) when m and Σ are replaced by ˆm= ¯x and ˆΣML or ˆΣsample). In the following subsections we discuss important applications of the Mahalanobis distance. Firstly, we discuss how to verify the assumption of multivariate normality. 14 1.3.1 Checking multiv"
9488,unknown,"14 1.3.1 Checking multivariate normality For given data x1,... xn ∈Rq sampled from a random vector X ∼(m,Σ), we wish to check whether or not X is a MVN. Let di = d2 M(xi,m,Σ), (1.7) where mand Σ may be replaced by appropriate estimates ˆmand ˆΣ, respectively. We know that, if X is MVN, then d2 M(X,m,Σ) ∼χ2 q. Hence, under multivariate normality, we would expect the values di to follow closely a χ2"
9489,unknown,"be easily checked in complete analogy to the well-known “normal probability plot” which is used to check a sample for univariate normality. Speciﬁcally, the QQ plot for checking MVN is constructed as follows: 1. Sort the values of di, yielding ordered values d(i), i= 1,...,n . 2. Compute the quantiles qi = χ2 q,1−(i−0.5)/n, i.e. the quantiles of the χ2 q distribution with a probability mass of i−0"
9490,unknown,mass of i−0.5 n to their left hand side. 3. Plot d(i) (vertical) versus qi (horizontal). 4. Compare the plotted points to a straight line through the origin with slope equal to 1. Deviations from this line indicate deviations from MVN. Example 1.8 Fuel consumption data. We consider data collected for the analysis of fuel consumption in 48 US states. There are nine variables in this data set but we
9491,unknown,"them: TAX (cents per gallon), DLIC (% population with driving licences), INC (average income in $1000’s), ROAD (1000’s of miles). #install.packages(""remotes"") #you need to do that once > remotes::install_github(""tmaturi/sm2data"") > library(sm2data) > ?fuelcons > head(fuelcons) # STATE POP TAX NLIC INC ROAD FUELC DLIC FUEL #1 ME 1029 9.0 540 3.571 1.976 557 52.5 541 #2 NH 771 9.0 441 4.092 1.250 40"
9492,unknown,"#3 VT 462 9.0 268 3.865 1.586 259 58.0 561 #4 MA 5787 7.5 3060 4.870 2.351 2396 52.9 414 #5 RI 968 8.0 527 4.399 0.431 397 54.4 410 #6 CN 3082 10.0 1760 5.342 1.333 1408 57.1 457 > fuel <- fuelcons[,c(""TAX"", ""DLIC"", ""INC"", ""ROAD"")] > pairs(fuel) We ﬁrstly estimate mean and variance: > m <- colMeans(fuel) > Sigma <- var(fuel) A χ2 probability plot for checking MVN via the Mahalanobis distances is o"
9493,unknown,"> d<- mahalanobis(fuel, m, Sigma) > plot(qchisq( (1:48-0.5)/48,4),sort(d)) > abline(a=0,b=1) The resulting plot is provided in Figure 1.4. We see some deviation from the straight line both in the middle part and in the right tail. Some deviations in the tail are hard to avoid even for relatively well–behaved data. The deviation in the middle part is potentially more relevant, as it concerns more d"
9494,unknown,"data. We conclude that there is some moderate violation of multivariate normality in this data. Any further analysis based on the MVN ‘working assumption’ should be done with special care. 15 Figure 1.4: QQ plot d(i) versus qi for Fuel data set. 0 2 4 6 8 10 12 0 2 4 6 8 10 12 14 qchisq((1:48 − 0.5)/48, 4) sort(d) We listed the steps required for producing the QQ plot but the question is what is t"
9495,unknown,"We listed the steps required for producing the QQ plot but the question is what is the justiﬁcation of this procedure as a valid check of multivariate normality. The justiﬁcation is as follows. The ordered values can be used to evaluate the empirical distribution function (EDF) which depends on the sample and is given by Fn(d(i)) = ˆPr(d≤d(i)) = #{i: d≤d(i)} n = i n, where the random variable d is"
9496,unknown,"Mahalanobis distances that are less than or equal to d(i) for i = 1 ,...,n . On the other hand, the corresponding theoretical values of the χ2 q distribution function are Pr ( d≤χ2 q,1−i n ) = i n. As a result the sets of values {χ2 q,1−1 n ,...,χ 2 q,0}and {d(1),...,d (n)}should align approximately along a straight line passing through the origin with slope equal to 1 if the data are MVN. Note th"
9497,unknown,"we use 1 −(i−0.5)/ninstead of 1 −i/n(in step 2 above). This is due to symmetry reasons; for example, the expectation is that the observed value of d(2) to fall likely in the middle of the interval deﬁned by χ2 q,1−1 n and χ2 q,1−2 n . 1.3.2 Outlier detection For a data sample x1,..., xn ∈Rq and a speciﬁc data point x∈Rq [which may or may not be one of the xi], we wish to test H0 : x is not an outl"
9498,unknown,"the xi], we wish to test H0 : x is not an outlier versus H1 : x is an outlier at signiﬁcance level α. The understanding of the word ‘outlier’ is here ‘an observation that deviates so strongly from the other sample values that it is unlikely to have been generated by the same mechanism’. We can equate the word ‘mechanism’ with ‘random vector’. A pragmatic assumption is required for the distribution"
9499,unknown,"distribution of X; we choose X ∼Nq(m,Σ), where mand Σ are usually unknown. 16 Under this assumption, we know that d2 M(X,m,Σ) ∼χ2 q and this would then also be true for xunder the null hypothesis. Hence, we reject H0 if d2 M(x,m,Σ) >χ2 q,α. (1.8) where m and Σ need to be replaced by estimates if unknown. Typical choices for α are α = 0.05 or α= 0.025, with the latter being more common for this par"
9500,unknown,"α= 0.025, with the latter being more common for this particular test. Note that the test may give misleading results if the distribution of X is not MVN, or the sample size n is small. Since mand Σ need to be estimated from x1,..., xn, it would be conceptually preferable if xwas not part of these data (as otherwise xmay have a strong impact on ˆmand ˆΣ and hence avoid its detection as an outlier, "
9501,unknown,"its detection as an outlier, an eﬀect known as masking). Despite these concerns, the test is commonly carried out on all nobservations simultaneously, that is ˆmand ˆΣ are obtained from the complete data set, and then it is established for which of thenobservations (1.8) holds. By construction, each of the n observations is then classiﬁed as outlier with 100 ×α% probability even it is not outlying"
9502,unknown,"that at least one observation will be incorrectly classiﬁed as outlier , will be 1 −(1 −α)n ≈nα>>α . This can be shown as follows Pr(H0 rejected for at least one xi) = 1 −Pr(H0 not rejected for all xi) = 1 −(1 −α)n = 1 −(1n −nα+ .................   very small ) ≈nα. This is known as the ‘multiple testing problem’. Several techniques have been proposed to address this issue; such as replacing χ"
9503,unknown,"q,α by χ2 q,α/n (Bonferroni correction). One will often ﬁnd in practice that not any outliers are detected using this rule; one could say that the Bonferroni correction is ‘overly conservative’ in detecting outliers. The choice of α = 0.025 (rather than 0.05) which is often used for this test is presumably driven by the attempt to mitigate somewhat for the multiple testing without needing to carry"
9504,unknown,"this methodology as a useful ‘diagnostic device’ rather than a formal statistical test. Example 1.9 (Continuation of Example 1.8) Let us initially investigate “by hand” whether there are potential outliers. Therefore, we use the function identify() as follows: > plot(fuel$ROAD, fuel$INC) > identify(fuel$ROAD, fuel$INC) # [1] 7 12 37 .... > plot(fuel$ROAD, fuel$TAX) > identify(fuel$ROAD, fuel$TAX) "
9505,unknown,"# [1] 7 12 37 .... > plot(fuel$ROAD, fuel$TAX) > identify(fuel$ROAD, fuel$TAX) # [1] 7 12 37 ... Now let’s see whether formal outlier tests (using α= 0.05 and α= 0.025) based on the Mahalanobis distance conﬁrm this result: > which(d > qchisq(0.95,4)) # [1] 6 7 12 37 45 > which(d > qchisq(0.975,4) ) # [1] 7 37 45 Yes, this roughly conﬁrms the result above (noting that outliers in multivariate space"
9506,unknown,"be outlying in any individual coordinate direction!). 17 Chapter 2 Linear models: Assumptions & Estimation In Section 1.2, we were interested in studying the characteristics of a data matrix Z (or of the random vector from which it was generated). In this chapter, we are given data of type [ Z,Y], with Z∈Rn×q, Y ∈Rn×1, and wish to gain insight into the character of the (statistical) dependence of "
9507,unknown,"2.1 Model speciﬁcation Example 2.1 Scallop data – Cont. of Example 1.1 . For the scallop data, we have given Z=   long1 lat1 ... ... longn latn  ∈Rn×2, Y =   y1 ... yn  =   log(tcatch1) ... log(tcatchn)  . A potentially useful model for these data was already suggested in (1.1), namely yi = β1 + β2longi + β3lati + ϵi, i = 1,...,n which, in matrix notation, takes the shape Y =  "
9508,unknown," y1 ... yn  =   1 long 1 lat1 ... ... ... 1 long n latn     β1 β2 β3  +   ϵ1 ... ϵn  = [1,Z]β+ ϵ≡Xβ+ ϵ. Generally, the linear model in matrix form is given by Y = Xβ+ ϵ, (2.1) where all notation is explained in Table 2.1. Taking the i-th row of (2.1), one can represent the linear model via yi = p∑ j=1 βjxij + ϵi = xT i β+ ϵi, (2.2) which we will occasionally write without the "
9509,unknown,"which we will occasionally write without the index i, y= p∑ j=1 βjxj + ϵ= xTβ+ ϵ, (2.3) Note that the predictor variables xj,j = 1,...,p (each of which corresponds to a column of X) may be transformations or functions of the covariate variables which constitute the data matrix Z. In other words, X is not necessarily equal to [1,Z]. An example for such a situation is provided in Example 2.2. 18 Tab"
9510,unknown,"18 Table 2.1: Matrix notation used for the linear model. Notations: y : the response variable (also regressand, dependent variable, endogenous variable), x1,...,x p : the predictor variables (also regressors, independent or explanatory or exogenous variables) β1,...,β p : ﬁxed unknown coeﬃcients ϵ : noise or error variable. Model:   y1 ... ... yn  =   x11 ... x 1p x21 ... x 2p ... ."
9511,unknown,"xn1 ... x np     β1 β2 ... βp  +   ϵ1 ... ... ϵn   or Y = Xβ+ ϵ where YT = (y1,...,y n) is the vector of responses, X is the design matrix βT = (β1,...,β p) is the p−dimensional parameter vector, ϵT = (ϵ1,...,ϵ n) is the vector of errors. 19 Example 2.2 Cement data. The data below are from an experiment on the tensile strength of cement: Hald, A. (1952) Statistical Theory "
9512,unknown,"curing time (days) Tensile strength (kg/cm 2) 1 13.0 13.3 11.8 2 21.9 24.5 24.7 3 29.8 28.0 24.1 24.2 26.2 7 32.4 30.4 34.5 33.1 35.7 28 41.8 42.6 40.3 35.7 37.3 The plot in Figure 2.1 shows that the relationship between ‘strength’ and ‘time’ is (i) non-linear, (ii) appears to be increasing to a maximum (as should be expected) and (iii) the variability in strength increases with curing time. 0 5 1"
9513,unknown,"increases with curing time. 0 5 10 15 20 25 15 20 25 30 35 40 time strength Figure 2.1: “tensile strength” of cement versus “curing time”. We want to be able to make reasonable predictions about tensile strength for any given value of curing time between the experiment’s extremes. These predictions should minimally give (i) the typical strength, together with (ii) some measure of its uncertainty. "
9514,unknown,That there must be some uncertainty is clear from the data—diﬀerent samples with the same curing time have diﬀerent strengths. The fact that repeated trials were done for each curing time is very helpful as we can do a rough check of whether the variability is about the same at each curing time or whether it is more complicated than that. Note that the values of ‘curing time’ are chosen in advance
9515,unknown,"Look at the summaries time strength sstrength 1 12.70 0.79 2 23.70 1.56 3 26.46 2.46 7 33.22 2.03 28 39.54 2.95 where the strength are the group averages, while thesstrength values are the group standard deviations; for example, 39.54 is anestimate of E[strength|time = 28] and (2.95)2 is an estimate of Var[strength|time = 28]. 20 A plot of these standard deviations against the group means (not sho"
9516,unknown,the standard deviations are increasing (perhaps linearly) as the means increase. This is not desirable as one of the underlying assumptions of prevalent regression models is that the standard deviations are constant (as we will formalize later). When we see this happening a standard trick is transform the data: and here we transform to y= log(tensile strength). Doing this we ﬁnd: time ¯ y s y 1 2.
9517,unknown,time ¯ y s y 1 2.540 0.064 2 3.164 0.067 3 3.272 0.092 7 3.502 0.061 28 3.675 0.076 which has made the standard deviations roughly the same for each group of measurements. We see from the plot in Figure 2.2 that the relationship between time and log(strength) is still far from linear. Obviously cement cures to some maximum strength so we are looking for a mathematical function which tends to a ﬁni
9518,unknown,"E(y|time) = β1 + β2/time for some constants β1 >0, β2 <0. Indeed, the plot in Figure 2.3 makes this look a plausible thing to do and the least squares line is shown. Translating all this to the notation introduced above, this means 0 5 10 15 20 25 2.6 2.8 3.0 3.2 3.4 3.6 3.8 time log strength Figure 2.2: log “tensile strength” of cement versus “curing time”. y = log(strength) x1 = 1 , x2 = 1 /time"
9519,unknown,"and in matrix notation Z=   time1 ... timen  , Y =   log(strength1) ... log(strengthn)  , X=   1 1 /time1 ... ... 1 1 /timen  . 21 0.0 0.2 0.4 0.6 0.8 1.0 2.6 2.8 3.0 3.2 3.4 3.6 3.8 1/time log strength Figure 2.3: log “tensile strength” of cement versus “curing rate” with least squares line. 2.1.1 Linear model assumptions Consider again the equation (2.2). This, in itself, does "
9520,unknown,"sition. The term xT i β expresses the tendency of the response to vary in some systematic fashion with xi, and the term ϵi expresses variation (“scattering”) around this line (curve, plane...) of statistical relationship. Of course, one has to ensure to do this in a meaningful way: If the ﬁtted line (etc.) does not come close to the data then the decomposition does not make sense. Therefore, a lin"
9521,unknown,"a couple of assumptions, which are listed in Table 2.2. Thereby the ϵi, i = 1 ,...n are considered as random variables, and the assumptions pertain to the distribution of the random variables. Note that then ϵ= (ϵ1,...,ϵ n)T takes the role of a random vector, and we could write (A1) and (A2) in condensed form as ϵ∼(0,σ2I). But, if ϵis random vector, then also Y is a random vector. Considering Xβ a"
9522,unknown,"form as ϵ∼(0,σ2I). But, if ϵis random vector, then also Y is a random vector. Considering Xβ as a constant, we see that Y ∼(Xβ,σ2I). Including the assumption (A3), these distributions take the form ϵ∼Nn(0,σ2I) and, via (1.6), Y ∼Nn(Xβ,σ2I), (2.4) respectively. We call any model of type Y = Xβ+ ϵwhich adheres to at least (A1) and (A2) a linear model. Normality (A3) is not strictly necessary for the"
9523,unknown,"it will be required for any further inference (conﬁdence or prediction intervals, signiﬁcance testing, etc.). It will be pointed out in each Section whether or not we actually require it. Table 2.2: Assumptions of the linear model. (A1) Linearity E(ϵi) = 0, i.e. E(yi|xi) = xT i β Homoscedasticity Var( ϵi) = σ2 (A2) Independence Cov( ϵi,ϵj) = 0 }Var(ϵ) = σ2I (A3) Normality ϵi is normally distribute"
9524,unknown,"(A3) Normality ϵi is normally distributed 22 2.2 Estimation of model parameters In this section, we assume (A1) and (A2) to hold for subsection 2.2.2, while (A3) is not strictly necessary. We wish to estimate the model parameters, i.e. βand σ2. 2.2.1 Least-squares (LS) estimation of β We ﬁnd estimates ˆβ= ( ˆβ1,..., ˆβp)T of β= (β1,...,β p)T by minimizing R(β) = R(β1,...,β p) = n∑ i=1 ϵ2 i = ϵTϵ= "
9525,unknown,"i=1 ϵ2 i = ϵTϵ= (Y −Xβ)T(Y −Xβ). The solution to this minimization problem is derived as follows. R(β) = YTY −YTXβ−βTXTY + βTXTXβ = YTY −2YTXβ+ βTXTXβ. Now using the identities ∂aTβ ∂β = aand ∂βTAβ ∂β = 2Aβ(for Asymmetric) we derive the gradient and set it equal to zero; speciﬁcally ∂R(β) ∂β = −2(YTX)T + 2XTXβ = −2XTY + 2XTXβ != 0 ⇒XTXβ = XTY. (2.5) The above is a system of plinear equations, whic"
9526,unknown,"the context of statistical estimation. Example 2.3 Consider the simple linear regression model yi = a+ bxi + ϵi,i = 1,...,n. With βT = ( a,b), XT = ( 1 ... 1 x1 ... x n ) , XTX = ( n ∑xi∑xi ∑x2 i ) , XTY = ( ∑yi∑xiyi ) and Y = (y1 ... yn)T, the normal equations take the familiar form na+ b ∑ xi = ∑ yi (I) a ∑ xi + b ∑ x2 i = ∑ xiyi (II) By multiplying (I) with 1 n ∑xi, subtracting it from (II), an"
9527,unknown,"n ∑xi, subtracting it from (II), and after some algebra we ﬁnd ˆb= ∑xiyi −n¯x¯y∑x2 i −n¯x2 , (2.6) and by plugging this solution into (I) we obtain ˆa= ¯y∑x2 i −¯x∑xiyi∑x2 i −n¯x2 . (2.7) In the general case of p predictors if XTX is non-singular (invertible) the LS estimator is ˆβ= ( XTX )−1 XTY (2.8) 23 which shows that each ˆβj is a known linear combination of the responses y1,...,y n. We will "
9528,unknown,"that ˆβis an unbiased estimator for β. Remarks on Computation: (1) The solution of (2.5) requires that XTX is invertible, which is the case if rank( XTX) = p. Since rank(XTX) = rank(X) ([R], Section 14.3, Lemma A), the normal equations have a formal solution if and only if the columns of Xare linearly independent. Also, a necessary condition is in any case n≥p. (2) In practice, modern software nev"
9529,unknown,"(2) In practice, modern software never uses (2.8) for the computation of the LS estimator ˆβ, but uses decompositions of XTX (Cholesky decomposition [MC], Sec 5.5.1) or X (such as the QR decomposition [R], Sec 14.8) which allow for more eﬃcient computation. The R software uses by default the QR decomposition. Properties of XTX, residuals and ﬁtted values • The matrix is symmetric: ( XTX)T = XT(XT)"
9530,unknown,"• The matrix is positive semi-deﬁnite: for a∈Rp and a̸= 0 we have aTXTXa = (Xa)T(Xa) =∑ j b2 j ≥0, where (b1,...,b n)T = b= Xa. The second property can be potentially used to verify that ˆβ is indeed the vector which minimizes the objective function R(β). Speciﬁcally we have to look at the matrix of the second partial derivatives (the Hessian). We have that ∂R(β) ∂βT∂β = ∂ ∂βT (−2YTX+ 2XTX) = 2XTX"
9531,unknown,"(the Hessian). We have that ∂R(β) ∂βT∂β = ∂ ∂βT (−2YTX+ 2XTX) = 2XTX. This matrix is positive semi-deﬁnite (since XTX is positive semi-deﬁnite) and, therefore, its eigenvalues are all non-negative. We would need all eigenvalues to be strictly positive to conclude that we have a minimum. Without going into further detail one can show that ˆβ is indeed a minimum. One can also reach this conclusion i"
9532,unknown,"necessarily a minimum. So, R(β) is minimized at ˆβwith minimum R(ˆβ) = (Y −Xˆβ)T(Y −Xˆβ) = ˆϵTˆϵ. The known vector ˆϵ= Y −Xˆβ of “residuals” ˆϵi = yi −xT i ˆβ estimates the unknown vector of random errors ϵi; and the known vector ˆY = Xˆβ= X ( XTX )−1 XTY ≡HY of “ﬁtted values” ˆyi = xT i ˆβestimates the unknown vector Xβ of the (conditional) expectation of yi: E[yi|xi] = xT i β(i= 1,...,n ). The h"
9533,unknown,"i β(i= 1,...,n ). The hat matrix H will be considered in more detail later in this chapter. Obviously, yi = ˆyi + ˆϵi, or in vector form, Y = ˆY + ˆϵ (observed= ﬁtted plus residual ). The ﬁtted values and residuals play an important role in prediction and model diagnostics. Relationship with the ML estimator The LS estimator of βis completely non-parametric in the sense that it does not require an"
9534,unknown,"The LS estimator of βis completely non-parametric in the sense that it does not require any of the assumtions listed in Table 2.2. If we were to use ML estimation on the other hand, we would require all of the assumptions in Table 2.2. However, the resulting estimate would be the same (see Q2.3 in sheet 2) and given by equation (2.8)! It should be noted that in general LS and ML estimation do not "
9535,unknown,"in the same solutions. For positive semi-deﬁnite matrices the second partial derivative test is generally inconclusive (meaning we may have a minimum or a saddle point). 24 2.2.2 Estimation of σ2 Note that because E[ ϵ2 i] = Var[ϵi] + (E[ϵi])2 = σ2 + 02 = σ2, it follows that E [ϵTϵ n ] = 1 nE [∑ i ϵ2 i ] = 1 n ∑ i E [ ϵ2 i ] = σ2. (2.9) But ϵTϵ/n is unknown since the values of the random error ϵi "
9536,unknown,"that a natural candidate estimator is n−1 ∑ iˆϵ2 i. In fact, one can show that: (i) this is the ML estimator and (ii) this estimator is biased. A “bias-corrected” estimate of σ2, based on the known residuals ˆϵi, is s2 = ˆϵTˆϵ n−p = ∑n 1 ˆϵ2 i n−p = R(ˆβ) n−p = RSS n−p (2.10) where, as is customary, RSS denotes R(ˆβ) and is called the “Residual Sum of Squares” with n−p degrees-of-freedom. In other"
9537,unknown,"R, s is referred to as “residual standard error”. We will see later how dividing by n−p rather than n makes s2 an unbiased estimator of σ2. There is a related geometrical reason we divide by n−p in equation (2.10); namely, that there are p constraints on the residuals so they only have n−p degrees-of-freedom, not n, as is the case with the unconstrained unknown random errors ϵ= (ϵ1,...,ϵ n)T which"
9538,unknown,"are given by XTˆϵ= XT(Y −Xˆβ) = XTY −XTXˆβ= XTY −XTX ( XTX )−1 XTY = 0. This translates to   x11 ... x n1 ... ... ... x1p ... x np      XT   ˆϵ1 ... ˆϵn      ˆϵ =   0 ... 0  . Multiplying the ﬁrst row of XT with ˆϵyields ∑n i=1 xi1ˆϵi = 0. This means that as soon as the ﬁrst n−1 residuals are known the last residual ˆϵn is also known. In total, we havepsuch constraints, ∑"
9539,unknown,"i=1 xijˆϵi = 0, for j = 1,...,p , so ˆϵhas n−p degrees of freedom. Also, note that if there is an intercept in the model, then ∑n i=1 ˆϵi = 0; that is, the residuals sum to zero, because x1 = (x11,...,x n1)T = (1,..., 1)T. We will see later that these constraints are used as a basis for model checking. Example 2.4 Measurement model: The regression model with p= 1, yi = µ+ ϵi, (i= 1,...,n ) is ofte"
9540,unknown,"is often referred to as the “measurement model”, as µ may e.g. represent the value of an unknown physical constant which is to be estimated from y1,...,y n. One has X=   1 ... 1   , β= µ Hence, XTX = n, XTY = y1 + ··· + yn so that nˆµ = y1 + ··· + yn; that is, ˆµ = y, which is just the sample mean. Also, ˆyi = y, ˆϵi = yi −y. Thus, RSS = ∑(yi −y)2 and df = n−1, and therefore the variance of "
9541,unknown,"variance of the measurement errors is estimated as s2 = 1 n−1 n∑ i=1 (yi −y)2, which corresponds to the well-known expression for sample standard deviation . This example highlights that common and simple statistical estimation problems can be expressed as special cases of linear regression and that is also the case for more complicated problems. 25 Example 2.5 (Continuation of Example 2.2) We use"
9542,unknown,"We use the cement data to illustrate model parameter estimation using R. #install.packages(""remotes"") #you need to do that once > remotes::install_github(""tmaturi/sm2data"") > library(sm2data) > ?cement Regard the row labels as case numbers i. The number of cases is n= 21. If we want to ﬁt a simple linear regression model relating log(strength) to 1 /time, as suggested in Example 2.2, we can use th"
9543,unknown,"function lm: see its help ﬁle. To ﬁt the model to the cement data and obtain estimates, residuals, ﬁtted values and model matrix, we do as follows: > cement.lm <- lm(log(strength) ~ I(1/time), data=cement) > cement.lm Call: lm(formula = log(strength) ~ I(1/time), data = cement) Coefficients: (Intercept) I(1/time) 3.688 -1.146 > coeffs <- cement.lm$coefficients # LEAST SQUARES ESTIMATES OF INTERCEP"
9544,unknown,> coeffs (Intercept) I(1/time) 3.687818 -1.145528 > cement.lm$residuals # RESIDUALS FROM FITTED MODEL 1 2 3 4 5 6 0.02265927 0.04547395 -0.07419055 -0.02856765 0.08361883 0.09174896 ....................................................................... 19 20 21 0.04944472 -0.07175606 -0.02791343 > cement.lm$fitted.values # FITTED VALUES 1 2 3 4 5 6 7 8 2.542290 2.542290 2.542290 3.115054 3.115054
9545,unknown,....................................................................... 17 18 19 20 21 3.646907 3.646907 3.646907 3.646907 3.646907 > X<- model.matrix(cement.lm) > X # DESIGN MATRIX (Intercept) I(1/time) 1 1 1.00000000 2 1 1.00000000 3 1 1.00000000 4 1 0.50000000 5 1 0.50000000 6 1 0.50000000 7 1 0.33333333 8 1 0.33333333 ......................... ......................... 20 1 0.03571429 21 1 0.0
9546,unknown,"......................... ......................... 20 1 0.03571429 21 1 0.03571429 26 Notice that the envelope I() is needed because we are employing a function of the original covariate time. If a covariate is directly used as predictor, then this envelope can be omitted. As seen above, coeﬃcients ˆβ, the residuals ˆϵ, and the ﬁtted values ˆY are available from the object cement.lm for further p"
9547,unknown,"cement.lm for further processing. For example, we can obtain the residual sum of squares RSS, its degrees-of-freedom df and the residual standard error s as follows: > RSS <- sum(cement.lm$residuals^2) > RSS [1] 0.1085086 > df <- cement.lm$df > df [1] 19 > s <- sqrt(RSS/df) > s # ESTIMATE OF SIGMA [1] 0.07557102 # quicker, extract s directly from lm object: > summary(cement.lm)$sigma [1] 0.0755710"
9548,unknown,"[1] 0.07557102 # Create Figure 3.3 plot(X[,2], log(cement$strength)) abline(coeffs) 27 2.3 Statistical properties of ˆβ and s2 2.3.1 Expectation and variance of ˆβ Using assumption (A1), which implies E[ Y] = Xβ, one has E[ˆβ] = E[ ( XTX )−1 XTY] = ( XTX )−1 XTE[Y] = ( XTX )−1 XTXβ = β. Under the variance assumption (A2), one gets Var[ˆβ] =Var[ ( XTX )−1 XTY] = ( XTX )−1 XTVar[Y] [( XTX )−1 XT ]T "
9549,unknown,"( XTX )−1 XTVar[Y] [( XTX )−1 XT ]T = ( XTX )−1 XT [ σ2In ] X ( XTX )−1 = ( XTX )−1 σ2 Example 2.6 (i) For the measurement model yi = µ+ ϵi, i= 1,...,n (Example 2.4), we have ˆµ = ¯y Var(ˆµ) = σ2(XTX)−1 = σ2/n (ii) For the simple linear regression model yi = a+ bxi + ϵi, i= 1,...,n, one has ˆβ= ( ˆa ˆb ) with ˆa, ˆb as in Example 2.3; that is ˆa= ¯y∑x2 i −¯x∑xiyi∑x2 i −n¯x2 , ˆb= ∑xiyi −n¯x¯y∑x2 i"
9550,unknown,"i −n¯x2 and Var(ˆβ) = σ2 ( n ∑xi∑xi ∑x2 i )−1 = σ2 1 n∑x2 i −(∑xi)2 ( ∑x2 i −∑xi −∑xi n ) or (2.11) = σ2 n 1∑x2 i −n¯x2 ( ∑x2 i −∑xi −∑xi n ) or (2.12) = σ2 n 1 (n−1) ∑(xi −¯x)2 n−1 ( ∑x2 i −∑xi −∑xi n ) = σ2 n 1 (n−1)S2 X ( ∑x2 i −∑xi −∑xi n ) . (2.13) Note that the entries of Var( ˆβ) tend to become small for n −→∞, meaning that the precision of estimator ˆβincreases with sample size. 28 2.3.2 V"
9551,unknown,"estimator ˆβincreases with sample size. 28 2.3.2 Variance of linear combinations of ˆβ Generally, we are interested in a speciﬁed linear combination cTβ; for example, for cT = (1,0,..., 0) we get cTβ = β1. Another important example is when we wish to draw inferences about E[ y|x0] = xT 0 β at some (possibly previously unseen) value x0 ∈Rp of the predictors. We estimate cTβ by the obvious unbiased "
9552,unknown,"unbiased estimator cT ˆβ. Clearly, one has E[ cT ˆβ] = cTβ, and because Var[cT ˆβ] = σ2 cT(XTX)−1c the standard deviation of cT ˆβis given by SD[cT ˆβ] = σ √ cT(XTX)−1c (2.14) The standard deviation (SD) of cT ˆβ as a measure of the precision of the estimate cT ˆβ is only useful if the value of σ is known. When it is not known we replace it by its estimate s and one obtains the standard error of c"
9553,unknown,"standard error of cT ˆβ SE[cT ˆβ] = s √ cT(XTX)−1c (2.15) In the special case c= (0,..., 0,1,0,... 0)T, with the 1 at j−th position, one gets for j = 1,...,p E[ ˆβj] = βj SD[ ˆβj] = σ √[( XTX )−1] jj (2.16) and SE[ ˆβj] = s √[( XTX )−1] jj (2.17) for the standard error (SE) of ˆβj, the estimate of SD[ ˆβj]. Example 2.7 (Continuation of Example 2.5). R produces convenient summary objects of ﬁtted l"
9554,unknown,"model objects. For instance, for the cement data, > summary(cement.lm) produces a considerable amount of information (most of which will only be relevant later), which can either be read from the summary display or directly accessed from its components > names(summary(cement.lm)) [1] ""call"" ""terms"" ""residuals"" ""coefficients"" [5] ""aliased"" ""sigma"" ""df"" ""r.squared"" [9] ""adj.r.squared"" ""fstatistic"" """
9555,unknown,"For now, we are interested in the coeﬃcient table which can be extracted via > summary(cement.lm)$coef Estimate Std. Error t value Pr(>|t|) (Intercept) 3.687818 0.02425278 152.05758 8.782730e-31 I(1/time) -1.145528 0.05290007 -21.65457 7.472821e-15 Here, the column Estimate contains the ˆβj, and the column Std.Error contains the SE( ˆβj). Let us try to verify these standard errors. According to ou"
9556,unknown,"> s * sqrt(diag(solve(t(X)%*%X))) (Intercept) I(1/time) 0.02425278 0.05290007 Note that we have made use of the residual standard error sas well as the design matrix X as obtained in Example 2.2. A slightly more convenient way to do this calculation is to make use of the summary component cov.unscaled, which is just ( XTX)−1: > XTXinv <- summary(cement.lm)$cov.unscaled > s * sqrt(diag(XTXinv)) (In"
9557,unknown,"> s * sqrt(diag(XTXinv)) (Intercept) I(1/time) 0.02425278 0.05290007 29 2.3.3 Expectation of s2 To ﬁnd the expectation of s2 we need assumptions (A1) and (A2) to hold. Also, we will make use of three properties for traces and of three preliminary results. The properties are the following: (M1) Tr( A+ B) = Tr(A) + Tr(B) (M2) Tr( AB) = Tr(BA) for symmetric AB and BA (M3) Tr( bbT) = bTb Next, the res"
9558,unknown,"(M3) Tr( bbT) = bTb Next, the results that we will require. (R1) We have that E[ ˆϵ] (A1) = E[ Y −Xˆβ] = Xβ−Xβ = 0. So from this we obtain the ﬁrst result Var[ˆϵ] = E[ˆϵˆϵT] −E[ˆϵ]E[ˆϵ]T    0 = E[ˆϵˆϵT]. (R2) Now consider the hat matrix H= X ( XTX )−1 XT. It is easy to show that HT = H. Thus, we have that HHT = H2 = X ( XTX )−1 XTX ( XTX )−1 XT = X ( XTX )−1 XT = H. From this we obtain the sec"
9559,unknown,"we obtain the second result (In −H)(In −H)T = In −HT −H+ HHT = In −H−H+ H = In −H. (R3) Finally, the third result is the following: Tr (H) = Tr ([ X ( XTX )−1] XT )(M2) = Tr ( XTX ( XTX )−1) = Tr (Ip) = p Now we can ﬁnd the expectation of s2. Recall from equation (2.10) that s2 = ˆϵTˆϵ/(n−p), so for simplicity we will start with E[ ˆϵTˆϵ]. E[ˆϵTˆϵ] (M3) = E[Tr ( ˆϵˆϵT ) ] = Tr ( E[ˆϵˆϵT] )(R1) = T"
9560,unknown,( Var[Y −ˆY] ) = Tr (Var[Y −HY]) = Tr (Var[(In −H)Y]) = Tr ( (In −H) Var[Y]   σ2In (A2) (In −H)T ) = σ2Tr ( (In −H)(In −H)T) (R2) = σ2Tr ((In −H)) (M1) = σ2 ( Tr(In)   n −Tr(H) ) (R3) = σ2(n−p). As a result we have that σ2 is an unbiased estimator since E[s2] = E[ˆϵTˆϵ] n−p = σ2. 30 Chapter 3 Linear models: Inference & Prediction 3.1 Inference for linear model parameters In this section we
9561,unknown,"In this section we assume throughout assumptions (A1) to (A3), i.e. we further assume normality. We give some results without proof; some we prove. 3.1.1 Sampling distribution The sampling distribution is the probability distribution of an estimator, when drawing repeatedly sam- ples of the same size from the population. In our context this has to be understood as follows: For a given (ﬁxed) set o"
9562,unknown,"y1,...,y n. Each set of responses leads to a certain estimate ˆβ, and we are looking at the distribution of all those estimates. The sampling distribution is of crucial importance for deriving conﬁdence intervals, critical values for hypothesis tests, etc. Using the results from Subsection 2.3.1 and Equation (1.6), the sampling distribution of ˆβ is given by ˆβ∼Np(β,σ2(XTX)−1) (3.1) and that of cT"
9563,unknown,"by ˆβ∼Np(β,σ2(XTX)−1) (3.1) and that of cTˆβis cT ˆβ∼N ( cTβ, σ2 cT(XTX)−1c ) . (3.2) Equations (3.1) and (3.2) are most times not particularly useful in practice because they involve the error variance σ2 which is usually unknown. For this reason we will proceed to show that (n−p)s2/σ2 ∼χ2 n−p, (3.3) which will prove to be a very useful result. First let us re-express equation (2.10) as s2 = 1 n−"
9564,unknown,"n−p(Y −Xˆβ)T(Y −Xˆβ), so that 1 σ2 (n−p)s2 = 1 σ2 (Y −Xˆβ)T(Y −Xˆβ) see Q2.4 = 1 σ2 (Y −Xβ)T(Y −Xβ)    A − 1 σ2 (β−ˆβ)TXTX(β−ˆβ)    B . Upon careful examination both A and B are sum of squares of standard normal variates and, therefore, follow chi-square distributions. Speciﬁcally, A= (Y −Xβ)T 1 σ2 (Y −Xβ) = (Y −E[Y])TVar[Y]−1(Y −E[Y]) ∼χ2 n 31 and B = (β−ˆβ)T 1 σ2 XTX(β−ˆβ) = (ˆβ−E[ˆβ])TV"
9565,unknown,"n 31 and B = (β−ˆβ)T 1 σ2 XTX(β−ˆβ) = (ˆβ−E[ˆβ])TVar[ˆβ]−1(ˆβ−E[ˆβ]) ∼χ2 p. Thus, from the properties of the chi-square distribution we have that A−B ∼χ2 n−p leading to the result in equation (3.3). Note that based on this result (under the further assumption (A3)), one can show that s2 is an unbiased estimator in an alternative and simpler way (in comparison to the proof presented previously in S"
9566,unknown,"previously in Subsection 2.3.3); namely, we have that E[(n−p)s2/σ2] = E[χ2 n−p] = n−p⇒E[s2] = σ2. Now, combining (3.2) and (3.3), one can arrive at a very useful result. We make use of the fact that s2 and cTˆβare independent (not proven here). Hence, cT ˆβ−cTβ SE[cT ˆβ] = cT ˆβ−cTβ s √ cT(XTX)−1c = cT ˆβ−cTβ σ √ cT(XTX)−1c    SD[cT ˆβ] 1√ χ2 n−p/(n−p) ∼ N(0,1)√ χ2 n−p/(n−p) = tn−p (3.4) is a "
9567,unknown,"ˆβj −βj SE[ ˆβj] ∼tn−p (3.5) Example 3.1 As an illustrative toy example for (3.1), we simulate 500 data sets of sample size 50 from the true function f(x) = 2x−1 The predictors x1,...,x 50 are initially drawn from a uniform distribution on [0 ,1], but are then kept constant during the simulation process. The response is simulated as yi = f(xi) + ϵi, where the ϵi,i = 1,..., 50 are drawn, in each of"
9568,unknown,"500 simulated data sets is exemplarily shown in Figure 3.1 (top left). For each of the 500 runs, a linear model of type y = a+ bx+ ϵ is ﬁtted to the data. The 500 estimates, say ˆβj = (ˆaj,ˆbj)T, j = 1,..., 500, of aand bare recorded. A scatterplot of all 500 estimates is provided in Figure 3.1 (top right). Do these form a bivariate normal distribution, as would be expected by result (3.1)? We che"
9569,unknown,"Construction of the QQ plots requires computation of Mahalanobis distances d2 M ( ˆβj,E(ˆβ),Var(ˆβ) ) , j = 1,..., 500. We consider two scenarios: on the left side in Figure 3.1 (bottom) we use the ‘true’ values E(ˆβ) = β = (−1,2)T and Var(ˆβ) = σ2(XTX)−1, with σ = 0.1, while the right plot uses estimates of mean and variance obtained directly from the scatterplot (see R code below). We see that t"
9570,unknown,"of the data follows very closely the straight line. Note that these are 500 observations, so a few deviating points at the upper end are tolerable! We also observe that the two plots look quite similar. Overall, these plots do not give evidence that BVN is violated (this is, of course, as it should be!). The accompanying R code is provided below. # Define design and ’true’ function > x <- runif(50"
9571,unknown,"> fx<- -1 + 2*x # Example data set > y <- fx+ rnorm(50,0,0.1) > fit <- lm(y~x) > plot(x,y) 32 Figure 3.1: Top left: Example data set simulated from the function f(x) = 2 x−1; right top: Plotted values of ˆbversus ˆaafter 500 runs; bottom: QQ plot using true (left) and estimated (right) population parameters. 0.0 0.2 0.4 0.6 0.8 1.0 −1.0 −0.5 0.0 0.5 1.0 x y true line estimated line −1.05 −1.00 −0."
9572,unknown,"−1.05 −1.00 −0.95 1.85 1.90 1.95 2.00 2.05 2.10 2.15 ahat bhat 0 2 4 6 8 10 12 14 0 5 10 15 20 qchisq((1:500 − 0.5)/500, 2) sort(d) 0 2 4 6 8 10 12 14 0 5 10 15 qchisq((1:500 − 0.5)/500, 2) sort(dhat) > abline(a=-1,b=2) > abline(a=fit$coef[1], b=fit$coef[2], col=2, lty=2) > leg.names<- c(""true line"", ""estimated line"") > legend(0,1, leg.names, lty=c(1,2), col=c(1,2)) # Simulation > ahat <- rep(0,50"
9573,unknown,"> ahat <- rep(0,500) > bhat <- rep(0,500) > for (j in 1:500){ y<- fx+ rnorm(50,0,0.1) fit<- lm(y~x) ahat[j] <- fit$coef[1] bhat[j] <- fit$coef[2] } > hat <- cbind(ahat, bhat) > plot(hat) # Is this a BVN? (If so, the squared Mahalanobis distances should be chi^2 with 2df) # Firstly, use ’true’ values of beta and Sigma > beta<- c(-1,2) 33 > var <- 0.1^2 *summary(fit)$cov.unscaled # this is sigma^2* "
9574,unknown,"> d<- mahalanobis(hat, beta, var) > plot(qchisq( (1:500-0.5)/500,2),sort(d)) > abline(a=0,b=1) # Secondly, estimate mean and variance from scatterplot > betahat<- colMeans(hat) > varhat <- var(hat) > dhat <- mahalanobis(hat, betahat, varhat) > plot(qchisq( (1:500-0.5)/500,2),sort(dhat)) > abline(a=0,b=1) 3.1.2 Signiﬁcance tests We reject a null hypothesis H0 that βj = β0 j at prescribed signiﬁcanc"
9575,unknown,"j at prescribed signiﬁcance level α(0 <α< 1) if T ≡ ⏐⏐⏐⏐⏐ ˆβj −β0 j SE[ ˆβj] ⏐⏐⏐⏐⏐>tn−p,α/2 (3.6) In particular, we reject βj = 0 at level α if ⏐⏐⏐ˆβj/SE[ ˆβj] ⏐⏐⏐> tn−p,α/2. The hypothesis H0 : βj = 0 is the most common hypothesis to test in the regression setting, because it provides evidence whether covariate Xj has a statistically signiﬁcant eﬀect on Y. Most statistical software packages do no"
9576,unknown,"Most statistical software packages do not carry out actual hypothesis tests, but compute, for an observed value of T, the observed signiﬁcance level p∗(“p-value”) as the probability p∗= P(|t|≥ T), (3.7) where t∼tn−p This can be seen as the probability of obtaining “by chance” (considering H0 is true) a value of tat least as extreme as the observed one T. The smaller p∗is, the greater is the eviden"
9577,unknown,"the data) against the null hypothesis. Note at this occasion, in R summary output, the column t-value contains the values ˆβj/SE[ ˆβj] used for the test H0 : βj = 0, and the column Pr(>|t|) contains the corresponding p−values. Example 3.2 (Continuation of Examples 2.5 and 2.6) Test H0 : β2 = −1 vs H1 : β2 ̸= −1, at level α = 0.05. We use from Example 2.5 that ˆβ2 = −1.146 and s= 0.0756. Of course,"
9578,unknown,"that we have to do this manually, one ﬁnds from Example 2.6 (b) SE( ˆβ2) = s 1√∑x2 i −n¯x2 = ...= 0.0529. (Note that xi = 1/timei, so ∑x2 i = 4.4140 and ¯x= 0.33617.) Then T = ⏐⏐⏐⏐ −1.146 + 1 0.0529 ⏐⏐⏐⏐= 2.760 >t21−2,0.025 = 2.093, so we reject H0 at the 5% level of signiﬁcance. 3.1.3 Conﬁdence intervals From (3.4) we have Pr (⏐⏐⏐⏐⏐ cTβ−cT ˆβ SE[cT ˆβ] ⏐⏐⏐⏐⏐≤tn−p,α/2 ) = 1 −α ⇐⇒ Pr ( −SE[cT ˆβ]tn"
9579,unknown,"⏐⏐⏐⏐⏐≤tn−p,α/2 ) = 1 −α ⇐⇒ Pr ( −SE[cT ˆβ]tn−p,α/2 ≤cTβ−cT ˆβ≤SE[cT ˆβ]tn−p,α/2 ) = 1 −α ⇐⇒ Pr ( cT ˆβ−SE[cT ˆβ]tn−p,α/2 ≤cTβ≤cT ˆβ+ SE[cT ˆβ]tn−p,α/2 ) = 1 −α. 34 Thus, a 100(1 −α)% conﬁdence interval (CI) for cTβhas limits cT ˆβ ±tn−p,α/2 ×SE[cT ˆβ]. In particular, for c= (0 ... 0 1 0 ... 0)T, with 1 at j−th position, one gets ˆβj ±tn−p,α/2 ×SE[ ˆβj] (3.8) Interpretation of CIs A conﬁdence inter"
9580,unknown,"This means that under repeated sampling we expect 100(1 −α)% of the samples to produce conﬁdence intervals which include the true value of βj. Note that this is not a probability statement for βj, which is considered a ﬁxed yet unknown quantity. Relationship between CIs and signiﬁcance tests Of course, there is a direct relationship between conﬁdence intervals and signiﬁcance tests. Speciﬁcally, w"
9581,unknown,"we have that The (1 −α) CI for ˆβj does not contain β0 j ⇐⇒reject H0 : βj = β0 j at signiﬁcance level α ⇐⇒T >tn−p,α/2 ⇐⇒p∗<α, and vice versa when the CI contains β0 j. Example 3.3 (Continuation of Example 2.7) R standard model output does not provide conﬁdence intervals for parameters. We can do this ourselves: > s<- summary(cement.lm)$sigma > XTXinv <- summary(cement.lm)$cov.unscaled > SE1<- s * "
9582,unknown,"> SE1<- s * sqrt(XTXinv[1,1]) # SE(\hat{\beta_1}) > SE2<- s * sqrt(XTXinv[2,2]) # SE(\hat{\beta_2}) > cement.lm$coef[1] + c(-1,1) * qt(0.975,19)* SE1 [1] 3.637057 3.738580 > cement.lm$coef[2] + c(-1,1) * qt(0.975,19)* SE2 [1] -1.256250 -1.034807 We observe that the latter interval does not contain the value −1, in conformity with the result from Example 3.2. Considerably simpler, we can do the sam"
9583,unknown,"> confint(cement.lm,1,level=0.95) 2.5 % 97.5 % (Intercept) 3.637057 3.73858 > confint(cement.lm,2,level=0.95) 2.5 % 97.5 % I(1/time) -1.25625 -1.034807 35 3.2 Prediction Suppose we are contemplating a further observation y0 ≡y|x0 at predictor value x0 = (x01 ...x 0p)T. The vector x0 may be either one of x1,..., xn or a new value: fortunately, we do not need to distinguish between the two cases, as"
9584,unknown,"between the two cases, as the subsequent theory is the same. However, what we do need to distinguish carefully between are the following problems: (i) Estimating the “population mean” E[ y|x0] = xT 0 β. (ii) Predicting the value y0 ≡y|x0. Problem (i) has been covered: the estimate is ˆE[y|x0] = xT 0 ˆβwith standard error given bys √ xT 0 (XTX)−1x0, and a 100(1 −α)% conﬁdence interval for E[y|x0] h"
9585,unknown,"xT 0 ˆβ± tn−p,α/2 ×SE[xT 0 ˆβ] = xT 0 ˆβ± tn−p,α/2 ×s √ xT 0 (XTX)−1x0 Problem (ii) is about prediction: we are trying to predict y0 = xT 0 β+ ϵ0 and the question in this case is what would be a reasonable estimate of y0. A ﬁrst rational observation is that the estimator should be of the from y∗ 0 = ˆy0 + ϵ0 in order to account for both the systematic part ( xT 0 β) and the random part ( ϵ0) of th"
9586,unknown,"Given assumption (A1) we also know that E[ ϵ0] = 0, so 0 is a reasonable estimate of ϵ0. Hence, our best estimate of y0 must be y∗ 0 = xT 0 ˆβ+ 0 = ˆy0, which is the same as the estimated population mean. However, things get diﬀerent when looking at the variance. The variance of our prediction is Var[y∗ 0] = Var[ˆy0 + ϵ0] = Var[ˆy0] + Var[ϵ0] = σ2xT 0 (XTX)−1x0 + σ2, so that the variance of the pr"
9587,unknown,"0 ˆβ. Note that ˆy0 and ϵ0 are independent so that their covariance is 0. One can further show that this is also the variance of the distance between y0 (which is unknown) and ˆy0; speciﬁcally, Var(y0 −ˆy0) = Var(y0) + Var(ˆy0) = σ2 + σ2xT 0 (XTX)−1x0, where again ˆy0, which is estimated based on the i.i.d observations y1,...,y n, is independent of y0. Thus, the standard error of prediction (in sh"
9588,unknown,"the standard error of prediction (in short: prediction error) is s √ 1 + xT 0 (XTX)−1x0. A so-called 100(1 −α)% prediction interval for y0 has limits xT 0 ˆβ ±tn−p,α/2 ×s √ 1 + xT 0 (XTX)−1x0 Note carefully that the estimates in both problems (i) and (ii) are the same, but the conﬁdence interval is narrower than the prediction interval. In fact, if nis “large”, the estimate of xT 0 ˆβis well-deter"
9589,unknown,"0 ˆβis well-determined with “small” standard error, so that the prediction error is approximately s, as we might anticipate (because (XTX)−1 ≈0 for large n). In this case, for example, a 95% prediction interval has approximate limits xT 0 ˆβ ±tn−p,0.025 s≈xT 0 ˆβ ±z0.025 s≈xT 0 ˆβ ±2 s, since the t-distribution can be approximated by a normal distribution for a large number of degrees of freedom."
9590,unknown,"freedom. Finally, note, for predictions to be valid, we invoke the same assumptions about the distribution of y|x0 as for the previous n cases: y0 is generated from the same linear model as y1,...,y n, and ϵ0 and ϵ1,...,ϵ n are independent N(0,σ2) errors. 36 Example 3.4 Measurement model: (Continuation of Example 2.4) There are no covariates and y= µ+ ϵ, so we want a conﬁdence interval for E[y] = "
9591,unknown,"There are no covariates and y= µ+ ϵ, so we want a conﬁdence interval for E[y] = µand a prediction interval for a further observation y0 = µ+ ϵ0. Here, p = 1, ˆµ = y, x0 = 1, ( XTX)−1 = 1 /n with Var[ˆµ] = σ2/n, and so SE[ˆµ] = s/√n, where s2 = ∑(yi−y)2/(n−1). Hence, the conﬁdence interval for µ has limits y± tn−1,α/2 × s√n and the corresponding prediction interval for y0 has limits y± tn−1,α/2 ×s "
9592,unknown,"y± tn−1,α/2 ×s √ 1 + 1 n ≃y± zα/2 ×s [if nis “large”]. Example 3.5 (Continuation of Example 3.3) Consider the tensile strength of cement at a given cure time with model E[log(strength) |time] = β1 + β2/time The following shows how both conﬁdence intervals and prediction intervals can be computed with or without using the R function predict. Suppose we are interested in predicting tensile strength "
9593,unknown,"curing time of t0 = 10 days. > cement.lm # least squares fit of model # predict y=log(strength) at time = t0 = 10 days > t0 <- 10 > y0hat <- as.numeric(cement.lm$coef %*% c(1,1/t0)) # as.numeric(.) is required to transform the 1x1 matrix into a scalar # (would cause warning message at later stage otherwise) > y0hat [1] 3.573266 # CI for expected response: > SE0 <- as.numeric(s* sqrt( c(1, 1/t0)%*%"
9594,unknown,"> CI <- y0hat + c(-1,1)* qt(0.975,19)* SE0 # PI for true response value: > PE0 <- as.numeric(s *sqrt(1+ c(1, 1/t0)%*%XTXinv %*% c(1,1/t0))) > PI <- y0hat +c(-1,1)* qt(0.975,19)* PE0 # now, on original strength scale: > exp(y0hat) [1] 35.63277 > exp(CI) [1] 34.12271 37.20965 > exp(PI) [1] 30.24326 41.98271 # or, using the R function predict: > t0 <- data.frame(time = 10) > Ci <- predict(cement.lm, "
9595,unknown,"> t0 <- data.frame(time = 10) > Ci <- predict(cement.lm, newdata = t0, interval = ""confidence"") > Pi <- predict(cement.lm, newdata = t0, interval = ""prediction"") # back to original strength scale: > exp(Ci) > exp(Pi) # same as above # Finally, note that several predictions can be made in the same call, # and that the level of the interval can be changed. For instance, 37 > t0 <- data.frame(time=c("
9596,unknown,"> exp(predict(cement.lm, newdata = t0, interval = ""prediction"", level=0.99)) # gives 99%PIs for all three times on original strength scale: # fit lwr upr # 1 35.63277 28.47727 44.58624 # 2 37.73326 30.11587 47.27738 # 3 38.46059 30.68108 48.21268 38 Chapter 4 Factors So far, we have mainly considered continuous covariates. Categorical covariates are called factors. Example 4.1 The data below come "
9597,unknown,"Example 4.1 The data below come from an experiment reported in Beall (1942), Transformation of data from entomological ﬁeld experiments , Biometrika. Six diﬀerent insect sprays were each applied to 12 plots and in each case the number of tobacco hornworms found in the plot is given. Spray Number of insects A 10 7 20 14 14 12 10 23 17 20 14 13 B 11 17 21 11 16 14 17 17 19 21 7 13 C 0 1 7 2 3 1 2 1 "
9598,unknown,"D 3 5 12 6 4 3 5 5 5 5 2 4 E 3 5 3 5 3 6 1 1 3 2 6 4 F 11 9 15 22 15 16 13 10 26 26 24 13 Spray is a factor, with levels A,...,F and r= 12 replicates per factor level. 4.1 Coding For inclusion into a linear model, factors need to be coded: For a factor Awith levels 1,...,a , xA j = 1{A=j}, i.e. an indicator taking the value 1 if the j−th factor level is attained, and 0 otherwise. In form of a codi"
9599,unknown,"coding matrix, Spray-Ex. A xA 1 xA 2 ... x A a−1 xA a A 1 1 ... B 2 1 ... ... ... ... E a −1 ... 1 F a ... 1 This type of coding is called dummy-coding. Other codings are possible as long as they enable a unique identiﬁcation of factor levels. Including all a indicators into the LM, one gets the unconstrained model E(y|A) = β0 + β1xA 1 + β2xA 2 + ... + βa−1xA a−1 + βaxA a i.e. E(y|A= j) = β0 + βj "
9600,unknown,"a−1 + βaxA a i.e. E(y|A= j) = β0 + βj (j = 1,...,a ) or yjk = β0 + βj + ϵjk (4.1) 39 for the k−th replicate at level j. In what follows, denote the number of replicates at level j by rj. (In Example 4.1, rj ≡r= 12 for j = 1,..., 6). In matrix notation, (4.1) takes the form   y11 ... y1r1 y21 ... y2r2 ... ... ya1 ... yara   =   1 "
9601,unknown,"1 ... ... 1 ... 1 ... ... ... 1 ... ... ... 1 ... ... 1 1     β0 β1 β2 ... βa−1 βa   +   ϵ11 ... ϵ1r1 ϵ21 ... ϵ2r2 ... ... ϵa1 ... ϵara   The design matrix Xis an n×(a+ 1) matrix. Clearly, its columns are not linear independent as the sum over the latter a columns gives the ﬁrst column. Hence, Rank( X) = a. As R"
9602,unknown,"over the latter a columns gives the ﬁrst column. Hence, Rank( X) = a. As Rank( XTX) = Rank( X), one has Rank( XTX) = a as well, so that XTX ∈Ra+1×a+1 is not invertible. Thus, the unconstrained model is not feasible in practice. This is why we need constraints on the parameters. There is no unique way how to do this, but popular constraints are: • The most common solution is a zero-constraint; e.g."
9603,unknown,"• The most common solution is a zero-constraint; e.g., to set β1 = 0 (R does this too). In this case, level 1 takes the role of a reference category. Thus, the second column of the design matrix (corresponding to the ﬁrst column of the coding matrix) can be cut oﬀ, as indicated through vertical dashed lines. This gives the constrained model E(y|A) = β0 + β2xA 2 + ... + βaxA a , (4.2) i.e. E(y|A= 1"
9604,unknown,"2 + ... + βaxA a , (4.2) i.e. E(y|A= 1) = β0 and E(y|A= j) = β0 + βj for j = 2,...a. This makes clear that the intercept represents the expected response for the reference category, and the parameters estimated for the other categories give the level eﬀect relative to this reference category. • Equivalently, one could set any otherβj, j = 2,...,a , equal to 0, in which case the (j+1)th column of t"
9605,unknown,"of the design matrix gets removed. • A diﬀerent type of constraint is the zero-sum constraint: ∑a j=1 βj = 0 (which arises naturally under eﬀect coding). Example 4.2 (Continuation of Example 4.1) We ﬁrstly enter the data in a vector. insects0 <- + c( + 10,7,20,14,14,12,10,23,17,20,14,13, + 11,17,21,11,16,14,17,17,19,21,7,13, + 0,1,7,2,3,1,2,1,3,0,1,4, + 3,5,12,6,4,3,5,5,5,5,2,4, + 3,5,3,5,3,6,1,1,"
9606,unknown,"+ 3,5,12,6,4,3,5,5,5,5,2,4, + 3,5,3,5,3,6,1,1,3,2,6,4, + 11,9,15,22,15,16,13,10,26,26,24,13 + ) 40 For the inclusion into a linear model, we need one row for each observation: > insects<- data.frame(""spray""= c(rep(""A"",12), rep(""B"",12), rep(""C"",12),rep(""D"",12), + rep(""E"",12), rep(""F"",12)), ""insects"" = insects0) > insects spray insects 1 A 10 2 A 7 3 A 20 4 A 14 5 A 14 6 A 12 7 A 10 8 A 23 9 A 17 10"
9607,unknown,"10 A 20 11 A 14 12 A 13 13 B 11 14 B 17 15 B 21 . . . 68 F 10 69 F 26 70 F 26 71 F 24 72 F 13 Fortunately, we don’t need to do the coding ourselves: R does this for us via the simple command as.factor(): > insects$spray <- as.factor(insects$spray) > insects$spray [1] A A A A A A A A A A A A B B B B B B B B B B B B C C C C C C C C C C C C D D [39] D D D D D D D D D D E E E E E E E E E E E E F F F F"
9608,unknown,"[39] D D D D D D D D D D E E E E E E E E E E E E F F F F F F F F F F F F Levels: A B C D E F Then we ﬁt a linear model to the relationship between spray and insects, and display the design matrix and the ﬁtted coeﬃcients: > fit <- lm(insects ~ spray, data= insects) > model.matrix(fit) ... > fit$coef (Intercept) sprayB sprayC sprayD sprayE sprayF 14.5000 0.8333 -12.4167 -9.5833 -11.0000 2.1667 Inte"
9609,unknown,"Interpretation: For the reference group A we expect 14.5 insects per plot. For group B, we expect 14.5 + 0.8333 insects per plot, and so on. 41 4.2 Experiments In an experiment, the experimenter must identify at least one factor which (s)he manipulates, and at least one response variable to measure. Each combination of factor levels that an experimental unit receives is called a treatment. The goa"
9610,unknown,"relationships, i.e. to decide whether (a substantial part of) the variation of the response can indeed be attributed to the diﬀerent levels of the factor. For such conclusions to be valid, certain principles must be followed: The three principles of experimental design: (i) Control: The experimenter sets the values of the factors and tries to eliminate any other source of variation. (ii) Randomize"
9611,unknown,"(ii) Randomize: Treatments have to be allocated at random, by the experimenter, to the individuals (i.e. individuals should not choose their treatment themselves, but also the experimenter should not allocate it just as he thinks best). This will • Reduce selection bias • Avoid confounding (i.e. reduce detrimental eﬀects of minor violations of (i)) (iii) Replicate. Make sure to collect more than o"
9612,unknown,"(iii) Replicate. Make sure to collect more than one observation for each administered treatment. One cannot conclude anything based on one observation! Example 4.3 nichts • The insects data are an example for a “randomized complete block design”, where each two treat- ments from A to F are randomized within blocks of 12 neighboring plots. • The cement data (Example 2.2) were also obtained from a d"
9613,unknown,"• The cement data (Example 2.2) were also obtained from a designed experiment (the discrete hardening times play the role of the “factor”, though we have not used it as a factor in the subsequent data analysis) Experimental design ﬁnds application in many scientiﬁc ﬁelds. A prominent example in medicine, for instance, are randomized control trials. These are clinical trials designed to investigate"
9614,unknown,"new experimental drugs. In this setting carefully stratiﬁed random samples of individuals are randomly assigned either to the control group (which usually receives a placebo dose) or to the treatment group (which receives the experimental drug). Typically, these are blinded studies; that is, the individuals do not know whether they belong to the control group or to the treatment group. In such set"
9615,unknown,"not know whether they belong to the control group or to the treatment group. In such settings, we have E[y|placebo] = β0 and E[y|drug] = β0 + β1 for an appropriate response variable y which accurately reﬂects the eﬃcacy of the drug under consideration for the particular condition that needs treatment. Obviously, here the interest lies on testing hypotheses of the form H0 : β1 ≤t vs. H1 : β1 > tfor"
9616,unknown,"critical threshold value t (rejecting H0 in this case implies that the drug is eﬀective). Signiﬁcance levels for this type of testing are far lower than the usual 5% level. However, it is not possible to carry out all studies as experiments. Consider, for instance, a model as in Question 2.2 where we have yi = µ+ δxi + ϵi. Here xi ∈{0,1}is a binary indicator/grouping variable. Now let us assume th"
9617,unknown,"Here xi ∈{0,1}is a binary indicator/grouping variable. Now let us assume that the grouping variable distinguishes between non-smokers ( xi = 0) and smokers ( xi = 1) and that the response yi is blood pressure. Obviously, we cannot allocate the treatment “smoking” at random, we can just observe the data, which are then likely to confound the treatment eﬀect with other eﬀects. For instance, smoking"
9618,unknown,"may be associated with other factors like, for instance, socio-economic status, which in turn inﬂuences 42 other factors (e.g. dietary habits) that may aﬀect the response of interest. Due to these confounding eﬀects the resulting estimate ˆδ may be seriously inﬂated/deﬂated. Investigative studies which do not follow the three principles above are called observational studies. Taken strictly, they "
9619,unknown,all the time). 4.3 Factorial experiments Experiments involving two or more factors are called factorial experiments. Certain technical terms in the language of factorial experiments are introduced and illustrated in the context of the example. Example 4.4 (Animal survival times) The data shown in Table 4.1 are the survival times (unit: 10 hours) of 48 animals ( experimental units) with four animal
9620,unknown,4 antidote treatments. The experiment was part of an investigation to combat the eﬀects of certain toxic agents. Random allocation is intended to reduce bias by “scrambling up” any residual variation in the response (such as possibly resulting from unavoidable initial weight diﬀerences in the animals) not caused directly by the eﬀects of the 12 combinations of poisons and antidote treatment. In th
9621,unknown,"statistical experimental design, each combination of poison and antidote treatment is called a treatment, and the observations corresponding to each treatment form a cell. Thus, there are 12 treatments here corresponding to the 12 combinations of poison and antidote treatment. This arrangement is called a 3 ×4 factorial design , which is complete since no cell is empty and is balanced as we have t"
9622,unknown,number of replicates (4) per cell. Poisons and antidote treatments are factors. The factor poison has 3 levels and the factor antidote treatment has 4 levels. antidote treatment poison A B C D I 0 ·31 0 ·82 0 ·43 0 ·45 0·45 1 ·10 0 ·45 0 ·71 0·46 0 ·88 0 ·63 0 ·66 0·43 0 ·72 0 ·76 0 ·62 II 0 ·36 0 ·92 0 ·44 0 ·56 0·29 0 ·61 0 ·35 1 ·02 0·40 0 ·49 0 ·31 0 ·71 0·23 1 ·24 0 ·40 0 ·38 III 0 ·22 0 ·30 
9623,unknown,III 0 ·22 0 ·30 0 ·23 0 ·30 0·21 0 ·37 0 ·25 0 ·36 0·18 0 ·38 0 ·24 0 ·31 0·23 0 ·29 0 ·22 0 ·33 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 animals$Poison mean of animals$Survival I II III animals$Treatment B D C A Table 4.1: Left: survival times (unit: 10 hours) of 48 animals comprising 4 replications of the 3 ×4 combinations of 3 poisons and 4 antidote treatments; right: interaction plot. The data are stor
9624,unknown,"for the inclusion into a linear model. Code to read and display the data, as well as to produce the There is an unfortunate double use of the word treatment here as it appears again in the name of one of the two factors, the antidote treatment. 43 interaction plot depicted in Table 4.1(right), is given below. Production of the “interaction plot”does not require ﬁtting any model, it is just a visua"
9625,unknown,"#> install.packages(""remotes"") # you need to do this once > remotes::install_github(""tmaturi/sm2data"") > library(sm2data) > data(animals) > animals Poison Treatment Survival 1 I A 0.31 2 II A 0.36 3 III A 0.22 4 I B 0.82 5 II B 0.92 6 III B 0.30 7 I C 0.43 8 II C 0.44 9 III C 0.23 10 I D 0.45 11 II D 0.56 12 III D 0.30 ... 47 II D 0.38 48 III D 0.33 > interaction.plot(animals$Poison,animals$Treatm"
9626,unknown,"47 II D 0.38 48 III D 0.33 > interaction.plot(animals$Poison,animals$Treatment, animals$Survival) A linear model (without interaction) for a response yin a complete factorial design replicated rtimes with two factors, Awith a levels and Bwith b levels, can be written as yjkℓ = µ+ τA j + τB k + ϵjkℓ (4.3) with components τA j : main eﬀect of level j of factor A(j = 1,...,a ) τB k : main eﬀect of le"
9627,unknown,"τB k : main eﬀect of level k of factor B(k= 1,...,b ) ϵjkℓ: error term of replicate ℓ for the j,k factor combination (ℓ= 1,...,r ). Similar as in Section 4.1, one ﬁnds that the unconstrained version of this model leads to a design matrix with a+ b+ 1 columns but rank 1 + (a−1) + (b−1) = a+ b−1, resulting in the necessity to impose one constraint on each of the two parameter collections τA 1 ,...,τ"
9628,unknown,"1 ,...,τ A a and τB 1 ,...,τ B b . The default constraints in R are τA 1 = τB 1 = 0. Model (4.3) assumes that the two factors impact additively onto the response. If this is not the case, they are said to interact; i.e. the eﬀect of the levels of factor Aonto the expected response varies with the levels of B, and vice versa. A full interaction model can be written in the form, yjkℓ = µ+ τA j + τB "
9629,unknown,"j + τB k + τAB jk + ϵjkℓ (4.4) where τA j , τB k , and ϵjkℓ are deﬁned as above, and τAB jk : interaction eﬀect of level j of factor Awith level k of factor B. The number of parameters in the unconstrained model appears to be 1 + a+ b+ ab= (a+ 1)(b+ 1). As there is no sense in having more than one parameter per cell, we will need appropriate constraints For the animal survival data, a= 3,b = 4,r ="
9630,unknown,"For the animal survival data, 1 + (a−1) + (b−1) = 1 + 2 + 3 = 6 44 which reduce this to eﬀectively p = ab parameters. One requires 1 + a+ b constraints to restore the design matrix to full rank. The default constraints in R are τA 1 = τB 1 = τAB 1k = τAB j1 = 0 for j = 1,...,a and k= 1,...,b . Even then βis a “big” vector compared with n= abr, and the residual degrees-of-freedom turn out to be n−a"
9631,unknown,"to be n−ab= ab(r−1), so we must have r >1; otherwise, with just one replicate (r= 1) there are no degrees-of-freedom left for estimating σ2— this means a “perfect ﬁt” with all of the residuals equal to zero. In analogy to the notation used in (4.2), we can re-express interaction model in (4.4) as E[y|A,B] = µ+ a∑ j=2 τA j xA j + b∑ k=2 τB k xB k + a∑ j=2 b∑ k=2 τAB jk xA j xB k (4.5) where the ( a"
9632,unknown,"where the ( a−1) + (b−1) predictors xA j , j= 2,...,a and xB k, k= 2,...b are such that xA j ∈{0,1} (absence or presence of level j of factor A) with ∑a j=2 xA j = 0 for the observations belonging to the cell of the 1st reference-treatment level and ∑a j=2 xA j = 1 otherwise (similarly for factor B). If the terms τAB jk are all zero then the following statements are equivalent: 1. There is deﬁned "
9633,unknown,"1. There is deﬁned to be no interaction between Aand B. 2. The eﬀects Aand Bare additive. 3. The diﬀerence in expected response between any two levels of Ais the same for all levels of B. 4. The diﬀerence in expected response between any two levels of Bis the same for all levels of A. Of course, in data there will virtually always be nonzero estimates of the above quantities, so we must assess whe"
9634,unknown,"Example 4.5 (Continuation of Example 4.4) This is an edited R output to ﬁt a model with main eﬀects for poison and treatment andno interaction to the animals survival data: > animals.addfit <- lm(Survival ~ Poison + Treatment, data = animals) > summary(animals.addfit) Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 0.45229 0.05592 8.088 4.22e-10 *** PoisonII -0.07313 0.05592 -1.308 "
9635,unknown,"PoisonII -0.07313 0.05592 -1.308 0.19813 PoisonIII -0.34125 0.05592 -6.102 2.83e-07 *** TreatmentB 0.36250 0.06458 5.614 1.43e-06 *** TreatmentC 0.07833 0.06458 1.213 0.23189 TreatmentD 0.22000 0.06458 3.407 0.00146 ** The ﬁrst 12 rows of the design matrix in R for the animal survival data for the model in (4.3) is shown below. In the full 48 ×6 model matrix, the ﬁrst 12 rows are repeated 3 more t"
9636,unknown,"4 replicates of the 3 ×4 design. > model.matrix(animals.addfit) (Intercept) PoisonII PoisonIII TreatmentB TreatmentC TreatmentD 1 1 0 0 0 0 0 2 1 1 0 0 0 0 3 1 0 1 0 0 0 45 4 1 0 0 1 0 0 5 1 1 0 1 0 0 6 1 0 1 1 0 0 7 1 0 0 0 1 0 8 1 1 0 0 1 0 9 1 0 1 0 1 0 10 1 0 0 0 0 1 11 1 1 0 0 0 1 12 1 0 1 0 0 1 . . . The following is an edited R output to ﬁt a model with main eﬀects for poison, treatment and"
9637,unknown,"interaction to the animals survival data: > animals.interfit <- lm(Survival ~ Poison + Treatment + Poison:Treatment, data = animals) > summary(animals.interfit) Estimate Std. Error t value Pr(>|t|) (Intercept) 0.41250 0.07457 5.532 2.94e-06 *** PoisonII -0.09250 0.10546 -0.877 0.3862 PoisonIII -0.20250 0.10546 -1.920 0.0628 . TreatmentB 0.46750 0.10546 4.433 8.37e-05 *** TreatmentC 0.15500 0.10546"
9638,unknown,TreatmentC 0.15500 0.10546 1.470 0.1503 TreatmentD 0.19750 0.10546 1.873 0.0692 . PoisonII:TreatmentB 0.02750 0.14914 0.184 0.8547 PoisonIII:TreatmentB -0.34250 0.14914 -2.297 0.0276 * PoisonII:TreatmentC -0.10000 0.14914 -0.671 0.5068 PoisonIII:TreatmentC -0.13000 0.14914 -0.872 0.3892 PoisonII:TreatmentD 0.15000 0.14914 1.006 0.3212 PoisonIII:TreatmentD -0.08250 0.14914 -0.553 0.5836 # Interpret
9639,unknown,"# Interpretation: # Expected response at Poison=III and Treatment=A: > predict(animals.interfit, newdata=data.frame( Poison=""III"", Treatment=""A"")) # this is 0.41250-0.20250 # Expected response at Poison=II and Treatment=B: > predict(animals.interfit, newdata=data.frame(Poison=""II"", Treatment=""B"")) # this is 0.41250 - 0.09250 + 0.46750 + 0.02750 Note, however, that the p−values give very little evi"
9640,unknown,"terms, so the simpler additive model appears to be more adequate from this analysis. 46 Chapter 5 Analysis of variance For the LM E(y|x) = xTβ with intercept, with predictors x= (1,x2,...,x p) under (A1-A3), we want to learn about the size of the contribution of diﬀerent sources of variation (predictors, error) towards the total variation in the response y, and draw from this conclusions on the re"
9641,unknown,"sources. 5.1 Explaining variation The idea is to partition the total variation in the response SST = Syy = n∑ i=1 (yi −¯y)2, into two components • the proportion of variation that is explained by the regression line (curve, hyperplane, etc). We shall call this component SSR, the sum of squares for regression , SSR = ∑n i=1(ˆyi −¯y)2, • the residual variation, i.e. proportion of variation that is u"
9642,unknown,"sum of squares for error , SSE = ∑n i=1(yi −ˆyi)2 [ = RSS]. Indeed, it holds SST = SSR + SSE. (5.1) This can be expressed in words as [variation in y ignoring x] = (5.2) = [variation in y due to regression on x] + [residual variation in y after regression on x] How can we use this decomposition in variation to assess whether or not the predictorsx= (x1,...,x p) are important in explaining the vari"
9643,unknown,"R2 ≡SSR SST = variation in y due to regression on x variation in y ignoring x which is the proportion of variation in y explained by x. Equivalently, by virtue of (5.1), we could consider the ratio SSR SSE = variation in y due to regression on x residual variation in y after regression on x “large” values of the ratio indicating that x explains a substantial proportion of the original variation in"
9644,unknown,"modiﬁed version of the latter does the job: 47 5.1.1 The ANOVA table It turns out that the following, so-called F–ratio, which is a slight adjustment to the previous ratio, obtained by dividing the numerator and denominator by their respective degrees-of-freedom, is the ratio to consider: F = SSR / (p−1) SSE / (n−p) = [variation in y due to regression on x] / (p−1) [residual variation in y after r"
9645,unknown,"/ (p−1) [residual variation in y after regression on x] / (n−p). (5.3) Provided our usual assumptions (A1-A3) hold, the sampling distribution of F under the null hypothesis that variation in y is not explained by x(equivalently, all βj except the intercept are zero) can be shown to be an F-distribution with numerator degrees-of-freedom p−1 and denominator degrees-of-freedom n−p(More detail on the "
9646,unknown,"against the hypothesis, can be calibrated using the p-value p∗= P [Fp−1,n−p ≥F] (5.4) where the observed value F is given by (5.3). The smaller the value of p∗, the greater is the evidence against the null hypothesis that E[y|x] = αwhen contrasted with thealternative hypothesis that E[y|x] = xTβ. This procedure is known as the overall F–test. An important, alternative way of displaying the F-ratio"
9647,unknown,"An important, alternative way of displaying the F-ratio is F = MSR MSE (5.5) where MSR = SSR /(p−1) and MSE = SSE /(n−p) are called the “ Mean Square Regression” and “Mean Square Error”, respectively. Note that MSE = s2, the estimate of the error variance σ2 in the full alternative model, while SST/(n−1) is the estimate of σ2 in the reduced null (“measurement”) model. All this information is usual"
9648,unknown,"Source df SS MS F p∗ Regression on x p−1 SSR MSR MSR/MSE P [ Fp−1,n−p >MSR/MSE ] Error n−p SSE MSE Total n−1 SST Note that the last row of the table (“Total”) is not really necessary, and could be omitted without loss of information. Example 5.1 (Continuation of Example 4.5) Consider additive model for animals data: > SSR <- sum( (animals.addfit$fitted- mean(animals$Survival))^2) > SSR [1] 1.95421"
9649,unknown,"> SSR [1] 1.954219 > SSE <- sum( (animals$Survival- animals.addfit$fitted)^2) > SSE [1] 1.050863 > F<-( SSR/(6-1) )/( SSE/(48-6)) [1] 15.62092 # p-value: > 1-pf(15.62092, 5,42) [1] 1.122701e-08 # Check with linear model summary: > summary(animals.addfit) # F-statistic: 15.62 on 5 and 42 DF, p-value: 1.123e-08 Summarizing in the ANOVA table, 48 Source df SS MS F p Regression 5 1.954 0.3908 15.621 1"
9650,unknown,"48 Source df SS MS F p Regression 5 1.954 0.3908 15.621 1 .12 ·10−8 Error 42 1.051 0.0250 We conclude that the predictors, as a whole, contribute signiﬁcantly (and substantially) towards the variation in the response. Table 5.1: A little bit more detail about the F-distribution. • Let U1 ∼χ2 ν1 and U2 ∼χ2 ν2 be independent. Then the ratio F = U1/ν1 U2/ν2 (5.6) is said to have an F-distribution wit"
9651,unknown,"degrees-of-freedom ν2: we write F ∼Fν1,ν2. • Hence, for the F-test in (5.4) to be valid, we would need to establish that SSR ∼χ2 p−1, SSE ∼χ2 n−p and that the two sums of squares are independent under the null hypothesis. To prove this is a little beyond the main aim of this course. • For reference, the density of an F-distribution for any point y >0 is f(y|ν1,ν2) = K(ν1,ν2) y(ν1/2)−1 (ν2 + ν1y)(ν"
9652,unknown,"(ν2 + ν1y)(ν2+ν1)/2 (5.7) and zero otherwise, and K(ν1,ν2) normalises the p.d.f. to integrate to 1. • It is worth noting that if F ∼Fν1,ν2, then E[F] = ν2/(ν2 −2). Hence, if the null hypothesis is true, we expect an F-ratio in the ANOVA table to be (slightly) in excess of 1 when ν2 is not too small. • The R functions rf, pf, qf and df have their usual meanings for the F-distribution. • As usual, w"
9653,unknown,"• As usual, we denote the quantile corresponding to the probability mass αin the right tail of Fν1,ν2 by Fν1,ν2,α. 49 5.1.2 Sequential ANOVA Can we decompose the ‘Regression’ component of the ANOVA table (as formulated in Section 5.1.1) further? That is, consider any sequence of m “nested” models M1 ⊂... ⊂Mm with design matrices X1 = 1,X2,..., Xj,..., Xm, where Xj is n×pj and Xj+1 is obtained by a"
9654,unknown,"X1 = 1,X2,..., Xj,..., Xm, where Xj is n×pj and Xj+1 is obtained by adding pj+1 −pj columns to Xj. In diﬀerent but equivalent terms, model Mj is nested in Mj+1 if they can be written as Mj : E[ y|x] = β1 + β2x2 + ··· + βpjxpj (5.8) Mj+1 : E[ y|x] = β1 + β2x2 + ··· + βpjxpj + ... + βpj+1xpj+1 (5.9) where pj+1 > pj, and x1,...,x pj are the same in both models. Note carefully, there is no requirement"
9655,unknown,"that the values of β1,...,β pj should be the same in the full and reduced models. To make this plain, we could have used (say) α1,...,α pj instead of β1,...,β pj to represent the coeﬃcients in the second model, but we choose not to do this. Both models assume homogeneity of error variance, but again the value σ2 in both models can be diﬀerent. In what follows, we do not consider non–nested models."
9656,unknown,"in both models can be diﬀerent. In what follows, we do not consider non–nested models. The idea is now to assess how residual variation changes as we add successively columns to a design matrix; that is, increase the number of terms in the model for E[ y|x]. The corresponding residual sums of squares are such that S1 ≥S2 ≥···≥ Sm and Sj −Sj+1 is the “sum of squares explained by the extra columns” "
9657,unknown,"Sj −Sj+1 is the “sum of squares explained by the extra columns” in Xj+1 with degrees-of-freedom pj+1 −pj; equivalently, by the extra pj+1 −pj model terms. We can decompose S1 as S1 = (S1 −S2) + (S2 −S3) + ··· + (Sj −Sj+1) + ··· + (Sm−1 −Sm) + Sm with corresponding degrees-of-freedom decomposition n−p1 = (p2 −p1) + (p3 −p2) + ··· + (pj+1 −pj) + ··· + (pm −pm−1) + (n−pm) The Sequential Analysis of V"
9658,unknown,"Source df SS MS F xp1+1,...,x p2 p2 −p1 S1 −S2 (S1 −S2)/(p2 −p1) F1 ... ... ... ... ... xpj+1,...,x pj+1 pj+1 −pj Sj −Sj+1 (Sj −Sj+1)/(pj+1 −pj) Fj ... ... ... ... ... xpm−1+1,...,x pm pm −pm−1 Sm−1 −Sm (Sm−1 −Sm)/(pm −pm−1) Fm−1 Error n−pm Sm Sm/(n−pm) Total n−p1 S1 where under the null hypothesis H 0 : βpj+1 = ··· = βpj+1 = 0 Fj = (Sj −Sj+1) /(pj+1 −pj) Sm/(n−pm) ∼Fpj+1−pj,n−pm j = 1,...,m −1 (5"
9659,unknown,"Sm/(n−pm) ∼Fpj+1−pj,n−pm j = 1,...,m −1 (5.10) and the corresponding p−value is obtained as usual. With X1 being a column of 1’s corresponding to an intercept term, there remain ( m−1)! diﬀerent sequential ANOVA’s depending on the order of the inclusion of the other columns. In general, all these ANOVA’s will be diﬀerent; that is, the values of SS, F, etc. for the same source of variation will be"
9660,unknown,"diﬀerent if the order of inclusion of terms is altered. However, in the special case of a balanced factorial design, the ANOVA table does not depend on the order of inclusion of terms. In this case, we say that the sources of variation are orthogonal to each other. 50 Example 5.2 (Student height/weight data) Personal characteristics were recorded for students taking Statistics in 1993/94. Supposey"
9661,unknown,"1,x2 = height,x3 = gender (with gender = 0 for male and gender = 1 for female). We determine the “full” model as E[y|height,gender] = β1 + β2 height +β3 gender + β4 height ·gender. (5.11) The model implies that we believe that weight is linearly related to height for both sexes, that is the straight line for men is E[y|height,gender = 0] = β1 + β2height (5.12) and that for women is E[y|height,gend"
9662,unknown,"and that for women is E[y|height,gender = 1] = (β1 + β3) + (β2 + β4)height. (5.13) Thus, β3 is the diﬀerence in intercepts and β4 is the diﬀerence in slopes. Hence, (i) β3 = 0 corresponds to a common intercept β1, (ii) β4 = 0 corresponds to a common slope β2, and (iii) β3 = β4 = 0 corresponds to a common straight line. In order to produce a sequential ANOVA table, we ﬁrst ﬁt just the intercept, th"
9663,unknown,"In order to produce a sequential ANOVA table, we ﬁrst ﬁt just the intercept, then height (a single straight line), then height and gender (two straight lines with the same slope but diﬀerent intercepts) and ﬁnally height and gender and height ×gender (two straight lines with diﬀerent slopes and intercepts). #install.packages(""remotes"") #you need to do that once > remotes::install_github(""tmaturi/s"
9664,unknown,"> library(sm2data) > ?student > head(student) gender height stone lb weight 1 0 74 13 0 82.55381 2 0 73 13 12 87.99692 3 0 73 9 4 58.96701 4 0 72 12 0 76.20352 5 0 72 10 7 66.67808 6 1 66 8 3 52.16312 > S1 <- sum(lm(weight ~ 1, data = student)$residuals^2) [1] 6202.927 > S2 <- sum(lm(weight ~ 1 + height, data = student)$residuals^2) [1] 1896.556 > S3 <- sum(lm(weight ~ 1 + height+gender, data = st"
9665,unknown,"[1] 1896.556 > S3 <- sum(lm(weight ~ 1 + height+gender, data = student)$residuals^2) [1] 1868.036 > S4 <- sum(lm(weight ~ 1 + height+gender+height:gender, data = student)$residuals^2) [1] 1850.411 Let Sj be the RSS of the j−th considered model and pj be the number of parameters in that model. Hence, S1 = 6202.9, p1 = 1, S2 = 1896.6, p2 = 2, S3 = 1868.0, p3 = 3, S4 = 1850.4, p4 = 4, so that S1 −S2 "
9666,unknown,"S1 −S2 = 4306 .3 is the sum of squares for height, S2 −S3 = 28 .6 is the sum of squares for gender, S3 −S4 = 17.6 is the sum of squares for height:gender, and S4 = 1850.4 is the residual sum of squares. The necessary R code to carry out sequential ANOVA by hand is > MSE <- S4/(43-4) > MSE [1] 47.44643 # F-values: > ((S1-S2)/1)/MSE [1] 90.7628 51 > ((S2-S3)/1)/MSE [1] 0.6011152 > ((S3-S4)/1)/MSE [1"
9667,unknown,"[1] 0.3714676 # compare each of these with quantile > qf(0.95, 1, 39) [1] 4.091279 which can be obtained much quicker using the function anova(), > anova(lm(weight ~ height*gender, data = student)) Analysis of Variance Table Response: weight Df Sum Sq Mean Sq F value Pr(>F) height 1 4306.4 4306.4 90.7628 9.911e-12 *** gender 1 28.5 28.5 0.6011 0.4428 height:gender 1 17.6 17.6 0.3715 0.5457 Residua"
9668,unknown,"Residuals 39 1850.4 47.4 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 [Note that the row denoted in R by Residuals is the same as what we call ‘Error’]. The table below shows how the estimates of the coeﬃcients β1,β2,β3 and β4 change as we increase the number of terms in the model. Intercept height gender height:gender 66.49 −103.211 2.458 −87.339 2.242 −2.451 −97.6607 2.3863 "
9669,unknown,"66.49 −103.211 2.458 −87.339 2.242 −2.451 −97.6607 2.3863 32.0842 −0.5149 When we ﬁt gender before height, that is, ﬁt the terms one-by-one from left-to-right in the model E[y|gender,height] = β1 + β2 gender + β3 height +β4 gender ·height the ANOVA table below seems to tell a diﬀerent story—two lines with the same slope but with diﬀerent intercepts are required. The result is not surprising, as we"
9670,unknown,"alone (ignoring height) at the ﬁrst ﬁt. > anova(lm(weight ~ gender + height + gender:height, data = student)) Analysis of Variance Table Response: weight Df Sum Sq Mean Sq F value Pr(>F) gender 1 2679.56 2679.56 56.4755 4.251e-09 *** height 1 1655.33 1655.33 34.8884 6.981e-07 *** gender:height 1 17.62 17.62 0.3715 0.5457 Residuals 39 1850.41 47.45 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.0"
9671,unknown,"Residuals 39 1850.41 47.45 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 The table below shows how the estimates of the coeﬃcients change as we increase the number of terms in the model in which gender enters before height, corresponding to the sequential ANOVA table in the R output above. 52 Intercept gender height gender:height 66.49 72.87 −16.15 −87.339 −2.451 2.242 −97.6607"
9672,unknown,"66.49 72.87 −16.15 −87.339 −2.451 2.242 −97.6607 32.0842 2.3863 −0.5149 Note, in particular, that the parameter estimates for the full model (last row) do not depend on the order of inclusion. Important special cases: • If m = 2, with M1 corresponding to the measurement model, and M2 to any other linear model with p2 ≥2, then the sequential ANOVA table constructed above corresponds to the ANOVA ta"
9673,unknown,"as produced in Section 5.1.1, and the task of testing H0 : M1 versus H1 : M2 corresponds to the overall F–test. • Assume j+ 1 = m, that is, the larger of the two models corresponds to the “full” model. In this case, the test H0 : Mj versus H1 : Mm is also known as partial F-test. In the context of partial F–tests, the model Mj = Mm−1 is then often referred to as “reduced model”. Note that, in orde"
9674,unknown,"to test a reduced against a full model, one does not need to specify, or make any calculations, concerning the “lower” models M1,...,M j−1, since all what is needed to calculate the test statistic Fj are the residual sums of squares of the two involved models. Example 5.3 (Continuation of Example 5.2) We are now interested in the question of whether the same straight line suﬃces for both genders. "
9675,unknown,"(5.12) serves as our null hypothesis, and the full model (5.11) as the alternative. There are two ways of approaching this problem: Firstly, we can merge the information from two lines of the sequential ANOVA table: > ((28.5+17.6)/2)/47.4 # merging two lines of [1] 0.48634 # ANOVA table above > 1-pf(0.48634, 2, 39) [1] 0.6185519 Or secondly, we can answer this question through a partial F–test: > "
9676,unknown,"> student.fit = lm(weight ~ height*gender, data = student) > RSS <- sum(student.fit$residuals^2) # RSS[Full] > RSS [1] 1850.411 > df <- student.fit$df # df[Full] > df [1] 39 > student.fit0 <- lm(weight ~ height, data = student) # Reduced model > RSS0 <- sum(student.fit0$residuals^2) # RSS[Reduced] > RSS0 [1] 1896.556 > df0 <- student.fit0$df # df[Reduced] > df0 [1] 41 > F <- ((RSS0-RSS)/(df0-df))/"
9677,unknown,"> F <- ((RSS0-RSS)/(df0-df))/(RSS/df) # F-value > F [1] 0.4862914 > p <- 1-pf(0.4862914,2,39) # p-value p = P[F(2,39) > 0.4862914] > p [1] 0.6185812 53 Of course, both approaches are equivalent and give the same result. All the above can be more conve- niently carried out automatically by applying the R function anova to the two ﬁts: > anova(student.fit0, student.fit) Analysis of Variance Table Mo"
9678,unknown,Model 1: weight ~ height Model 2: weight ~ height + gender + height:gender Res.Df RSS Df Sum of Sq F Pr(>F) 1 41 1896.56 2 39 1850.41 2 224.3 0.4863 0.6186 Example 5.4 (Animal survival times) > anova(animals.addfit) Analysis of Variance Table Response: Survival Df Sum Sq Mean Sq F value Pr(>F) Poison 2 1.03301 0.51651 20.643 5.704e-07 *** Treatment 3 0.92121 0.30707 12.273 6.697e-06 *** Residuals 
9679,unknown,"Residuals 42 1.05086 0.02502 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 This table indicates that variation in survival time for this “main eﬀects only” model is signiﬁcantly inﬂuenced by both factors, poison and treatment. > anova(animals.interfit) Analysis of Variance Table Response: Survival Df Sum Sq Mean Sq F value Pr(>F) Poison 2 1.03301 0.51651 23.2217 3.331e-07 *** T"
9680,unknown,Treatment 3 0.92121 0.30707 13.8056 3.777e-06 *** Poison:Treatment 6 0.25014 0.04169 1.8743 0.1123 Residuals 36 0.80072 0.022247 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 This table indicates that there is little evidence for an interaction. Notice that the ANOVA lines for poison and treatment are the same for the both ﬁts (with and without an interaction term in the model)
9681,unknown,"Also, the residual sum of squares for the model without interaction is the sum of the sums of squares for residuals and the interaction for the model with interaction; and similarly for the degrees-of-freedom. We change the order of inclusion of terms: > anova(lm(Survival~Treatment+Poison+ Poison:Treatment, data=animals) ) Analysis of Variance Table Response: Survival Df Sum Sq Mean Sq F value Pr("
9682,unknown,"Df Sum Sq Mean Sq F value Pr(>F) Treatment 3 0.92121 0.30707 13.8056 3.777e-06 *** Poison 2 1.03301 0.51651 23.2217 3.331e-07 *** Treatment:Poison 6 0.25014 0.04169 1.8743 0.1123 Residuals 36 0.80073 0.02224 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 54 If we ﬁt treatment before poison, we see that the ANOVA lines are the same, except re-shuﬄed according to the order of ﬁtti"
9683,unknown,"“balanced” factorial design. Notice that in an ANOVA table there is a row for each source of variation (i.e. for each factor), unlike in the summary(lm) output, where is a row for each (combination of) factor level(s). 55 Chapter 6 Model selection 6.1 Submodels In this section we assume that there is a given correctly speciﬁed model E[y|x] = xTβ= x1β1 + ...x pβp, (6.1) (featuring A1-A3), but we wa"
9684,unknown,"as good”. Our goal is to ﬁnd which terms, if any, can be deleted without important loss of information. Let I be the subset of indices of those terms to be included in a submodel and Dthe remainder to be deleted. The cardinality of these sets is denoted by pI and pD, respectively, with pI+ pD = p. The full mean function in (6.1) can be written E[y|x] = xT IβI+ xT DβD The mean function for the subm"
9685,unknown,E[y|x] = xT IβI+ xT DβD The mean function for the submodel is EI[y|x] = xT IβI (6.2) where EI indicates expectation with respect to an hypothesised mean function for the submodel with pI terms which may or may not be correct: expectations or variances without such a subscript are with respect to the correct full model. Dropping a term xj can be particularly helpful when it is nearly a linear combi
9686,unknown,"in the model, as dropping xj is equivalent to replacing it in the full model by the linear combination. “Good” submodels Iwill have RSSI close to the RSS of the full model, and pI as small as possible. One will not achieve both goals arbitrarily well at the same time; one has to make a trade-oﬀ between goodness-of-ﬁt and parsimony of the model. One needs selection criteria, also called information"
9687,unknown,"large number of them; we introduce some important criteria below. 6.2 Selection criteria (1) Mallows’ CI is given by CI= RSSI s2 + 2pI−n (6.3) Equivalent expressions for CI are CI= RSSI−RSS s2 + pI−pD= pD(FD−1) + pI (6.4) where the subscript I refers to statistics computed from the submodel, while those without a subscript are computed from the full model. FD is the F-statistic for testing the nul"
9688,unknown,"H0 that βD= 0. 56 Under H0, one has E(CI) = pD(E(FD) −1) + pI ≈pD(1 −1) + pI = pI, indicating that good candidates for submodel mean functions will have CI≤pI (with pI as small as possible). The last term in (6.4) shows that CI≤pI if and only FD≤1. Thus, we would tend to delete a set of terms Dwhen the F-statistic FD is less than 1. (2) A criterion closely related to CI is the AIC (Akaike Informat"
9689,unknown,"minimize AICI= −2LI+ 2pI. Here LI is the log-likelihood L(βI,σ) evaluated at βI = ˆβI and a suitable estimate ˆσ of σ. This can be seen as a maximum likelihood technique which penalizes large values of pI. One can show (Question 4.5) that AICI is equivalent to CI under (A1)-(A3) if σ2 in AICI is estimated by the common error variance estimator s2 under the full model (6.1). (3) A simpler idea is t"
9690,unknown,"(3) A simpler idea is to minimize the residual mean square s2 I= RSSI dfI with dfI= n−pI. This selects the submodel with the smallest estimated error variance σ2. (4) A variant of this is Tukey’s rule: Minimize s2 I/dfI= RSSI (n−pI)2 This criterion aims to simultaneously minimize the residual mean square and maximise its degrees- of-freedom, with stronger emphasis on the latter compared to method "
9691,unknown,"of-freedom, with stronger emphasis on the latter compared to method (3). We will mainly consider methods (1) and (3) henceforth, since (2) and (4) can be considered as variants of these. Example 6.1 The following data comprises a response,“heat evolved” y = heat (in calories per gram of cement) during hardening of Portland cement considered as a function of the amounts of four covari- ates, chemic"
9692,unknown,"tri.silicate; “tetracalcium alumino ferrite” ferite; and “dicalcium silicate” di.silicate, all as per- centages of the weights of the clinkers. > data(cement, package=""MASS"") > cement <- cement[,c(5,1,2,3,4)] > names(cement)<- c(""heat"", ""aluminate"", ""tri.silicate"", ""ferite"", ""di.silicate"") > cement heat aluminate tri.silicate ferite di.silicate 1 78.5 7 26 6 60 2 74.3 1 29 15 52 3 104.3 11 56 8 20"
9693,unknown,"4 87.6 11 31 8 47 5 95.9 7 52 6 33 6 109.2 11 55 9 22 7 102.7 3 71 17 6 8 72.5 1 31 22 44 9 93.1 2 54 18 22 10 115.9 21 47 4 26 11 83.8 1 40 23 34 12 113.3 11 66 9 12 13 109.4 10 68 8 12 57 We will assume the “largest model” has predictor terms x1 = aluminate, x2 = tri.silicate, x3 = ferite, x4 = di.silicate and E[y|x1,x2,x3,x4] = β0 + β1x1 + β2x2 + β3x3 + β4x4 is the full model, which we designat"
9694,unknown,"model, which we designate as “1 + x1 + x2 + x3 + x4”, and Var[ y|x1,x2,x3,x4] = σ2. The sequential ANOVA table below for this model, seems to indicate that the model 1+x1 +x2 might be a parsimonious model for these data, but we must be careful, as we know that an ANOVA can depend on the order of ﬁtting terms. > cement.fit.full <- lm(heat~ aluminate + tri.silicate + ferite + di.silicate, data = cem"
9695,unknown,data = cement) > anova(cement.fit.full) Analysis of Variance Table Response: heat Df Sum Sq Mean Sq F value Pr(>F) aluminate 1 1450.08 1450.08 242.3679 2.888e-07 *** tri.silicate 1 1207.78 1207.78 201.8705 5.863e-07 *** ferite 1 9.79 9.79 1.6370 0.2366 di.silicate 1 0.25 0.25 0.0413 0.8441 Residuals 8 47.86 5.98 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Table 6.1 shows CI a
9696,unknown,"I applied to all 24 = 16 possible submodels. Both criteria suggest the three models 1 + x1 + x2, 1 + x1 + x2 + x3 and 1 + x1 + x2 + x4 as contenders. The submodel 1 + x1 + x2 is preferred because it has fewer terms and also CI= 2.68 <3 = pI. Model contenders pI dfI RSSI s2 I CI 1 1 12 2715 .76 226 .31 442 .99 1 + x1 2 11 1265 .69 115 .06 202 .55 1 + x2 2 11 906 .34 82 .39 142 .49 1 + x3 2 11 1939 "
9697,unknown,1 + x4 2 11 883 .87 80 .35 138 .73 1 + x1 + x2 3 10 57 .90 5 .79 2 .68 1 + x1 + x3 3 10 1227 .07 122 .71 198 .09 1 + x1 + x4 3 10 74 .76 7 .48 5 .50 1 + x2 + x3 3 10 415 .44 41 .54 62 .44 1 + x2 + x4 3 10 868 .88 86 .89 138 .23 1 + x3 + x4 3 10 175 .74 17 .57 22 .37 1 + x1 + x2 + x3 4 9 48 .11 5 .35 3 .04 1 + x1 + x3 + x4 4 9 50 .84 5 .64 3 .50 1 + x1 + x2 + x4 4 9 47 .97 5 .33 3 .02 1 + x2 + x3 +
9698,unknown,"1 + x2 + x3 + x4 4 9 73 .81 8 .20 7 .34 1 + x1 + x2 + x3 + x4 5 8 47 .86 5 .98 5 .00 Table 6.1: All subset models for the cement data. For each submodel with pI terms, RSSI is the residual sum of squares; s2 I is the estimate of the error variance σ2; CI = RSSI s2 + 2pI−n is Mallows’ criterion (s2 = 5.98 is estimate of σ2 in full model). 58 6.3 Selection methods Considering 2k (for a model with an"
9699,unknown,"Considering 2k (for a model with an intercept: k= p−1) submodels is manageable when k≤10: there are only 2 10 = 1024 submodels to consider, but 2 20 ≃1,000,000 submodels can be computationally time-consuming. A popular alternative is stepwise regression with two ﬂavours: forward selection, in which terms are sequentially added to the mean function and backward elimination, in which terms are seque"
9700,unknown,"sequentially removed from the mean function. A third method alternates between the selection and elimination criteria. 6.3.1 Forward selection Suppose at some stage there are pI terms xI in the model with value CI for Mallows’ criterion. (i) Add, in turn, each of the remaining p−pI terms xj to the current base set xI and for each of the resulting submodels with pI+ 1 terms note the value of CI∪{j}"
9701,unknown,(ii) Find the term added in (i) which gives the smallest value of CI∪{j}; (iii) If the smallest value of CI∪{j} in (ii) is less than or equal to CI then add the corresponding term to the base set xI; otherwise STOP. Example 6.2 (Continuation of Example 6.1) Forward selection applied to the cement data with CI: Submodel pI CI 1 1 442 .99 +x1 2 202 .55 +x2 2 142 .49 +x3 2 315 .15 +x4 2 ∗138.73 1 + x
9702,unknown,"1 + x4 2 138 .73 +x1 3 ∗5.50 +x2 3 138 .23 +x3 3 22 .37 1 + x4 + x1 3 5 .50 +x2 4 ∗3.02 +x3 4 3 .50 1 + x4 + x1 + x2 4 STOP 3 .02 +x3 5 5 .00 Table 6.2: Forward selection applied to the cement data with CI. A ∗ indicates which submodel is selected at a particular stage. The forward selected submodel is 1 + x4 + x1 + x2; that is, 1 + di.silicate + aluminate + tri.silicate. 6.3.2 Backward eliminatio"
9703,unknown,"di.silicate + aluminate + tri.silicate. 6.3.2 Backward elimination As with forward selection of terms, suppose at some stage there are pI terms xI in the model with value CI for Mallows’ criterion. (i) Delete, in turn, each of the pI terms xj from the current base set xI and for each of the resulting submodels with pI−1 terms note the value of CI\{j}; 59 (ii) Find the term removed in (i) which giv"
9704,unknown,59 (ii) Find the term removed in (i) which gives the smallest value of CI\{j}; (iii) If the smallest value of CI\{j}in (ii) is less than or equal to CIthen remove the corresponding term from the base set xI; otherwise STOP. Example 6.3 (Continuation of Example 6.2) Backward elimination applied to the Portland cement data with CI: Submodel pI CI 1 + x1 + x2 + x3 + x4 5 5 .00 −x1 4 7 .34 −x2 4 3 .50
9705,unknown,"−x2 4 3 .50 −x3 4 ∗3.02 −x4 4 3 .04 1 + x1 + x2 + x4 4 3 .02 −x1 3 138 .23 −x2 3 5 .50 −x4 3 ∗2.68 1 + x1 + x2 3 STOP 2 .68 −x1 2 142 .49 −x2 2 202 .55 Table 6.3: Backward elimination applied to the cement data with CI. A ∗ indicates which sub- model is selected at a particular stage. The backward selected submodel is 1 + x1 + x2; that is, 1+ aluminate + tri.silicate. 6.3.3 Stepwise selection This"
9706,unknown,at each stage test whether any term in the current submodel can be eliminated according to the backward elimination method. Stop when both the forward and backward criteria are satisﬁed. Example 6.4 (Continuation of Example 6.3) Here we apply the R functions lm and step to the Portland cement data to carry out backward elimination and forward selection — look also at the help for step. The results
9707,unknown,"in the previous two tables. Note that in what follows the notation Cp is used instead of CI and p instead of pI whereas in our development p is used exclusively for the number of terms in the full model. Backward Elimination: First we ﬁt “largest” model using lm. > cement.fit.full <- lm(heat ~ aluminate + tri.silicate + ferite + di.silicate, data = cement) Now we apply step to the full model ﬁt ce"
9708,unknown,"AIC ≡Cp. > step(cement.fit.full, scale = summary(cement.fit.full)$sigma^2) Start: AIC= 5 heat ~ aluminate + tri.silicate + ferite + di.silicate Df Sum of Sq RSS Cp 60 - ferite 1 0.109 47.973 3.0182 - di.silicate 1 0.247 48.111 3.0413 - tri.silicate 1 2.972 50.836 3.4968 <none> 47.864 5.0000 - aluminate 1 25.951 73.815 7.3375 Step: AIC= 3.02 heat ~ aluminate + tri.silicate + di.silicate Df Sum of S"
9709,unknown,"- di.silicate 1 9.93 57.90 2.6782 <none> 47.97 3.0182 - tri.silicate 1 26.79 74.76 5.4959 - aluminate 1 820.91 868.88 138.2259 Step: AIC= 2.68 heat ~ aluminate + tri.silicate Df Sum of Sq RSS Cp <none> 57.90 2.6782 - aluminate 1 848.43 906.34 142.4864 - tri.silicate 1 1207.78 1265.69 202.5488 Call: lm(formula = heat ~ aluminate + tri.silicate, data = cement) Coefficients: (Intercept) aluminate tri"
9710,unknown,"Coefficients: (Intercept) aluminate tri.silicate 52.5773 1.4683 0.6623 Forward Selection: Here we start with the ﬁt to the “smallest” model. > cement.fit.intercept <- lm(heat ~ 1, data = cement) Now we apply step to the intercept-only model ﬁt cement.fit.intercept to do forward selection using AIC ≡Cp. Note the use of direction and scope, which are not necessary for backward elimination. > step(ce"
9711,unknown,"> step(cement.fit.intercept, scale = summary(cement.fit.full)$sigma^2, scope = list(lower = cement.fit.intercept, upper = cement.fit.full), direction = ""forward"") Start: AIC= 442.92 heat ~ 1 Df Sum of Sq RSS Cp + di.silicate 1 1831.90 883.87 138.73 + tri.silicate 1 1809.43 906.34 142.49 + aluminate 1 1450.08 1265.69 202.55 + ferite 1 776.36 1939.40 315.15 <none> 2715.76 442.92 Step: AIC= 138.73 he"
9712,unknown,Step: AIC= 138.73 heat ~ di.silicate Df Sum of Sq RSS Cp + aluminate 1 809.10 74.76 5.4959 + ferite 1 708.13 175.74 22.3731 + tri.silicate 1 14.99 868.88 138.2259 <none> 883.87 138.7308 Step: AIC= 5.5 61 heat ~ di.silicate + aluminate Df Sum of Sq RSS Cp + tri.silicate 1 26.789 47.973 3.0182 + ferite 1 23.926 50.836 3.4968 <none> 74.762 5.4959 Step: AIC= 3.02 heat ~ di.silicate + aluminate + tri.s
9713,unknown,"Df Sum of Sq RSS Cp <none> 47.973 3.0182 + ferite 1 0.109 47.864 5.0000 Call: lm(formula = heat ~ di.silicate + aluminate + tri.silicate, data = cement) Coefficients: (Intercept) di.silicate aluminate tri.silicate 71.6483 -0.2365 1.4519 0.4161 Stepwise selection: Using direction = ‘‘both’’ gives (for this data set!) the same result as backward elimination. Try yourself! 62 Chapter 7 Regression dia"
9714,unknown,"62 Chapter 7 Regression diagnostics & transformations 7.1 Regression diagnostics There are two inter-related strands. Residuals are used to check the linear model assumptions, and inﬂuential observations (yet to deﬁne) are those cases featuring a particularly large impact on ˆβor s2, or both. The key concept for the understanding of both is the hat matrix, from which important diagnostic tools are"
9715,unknown,"tools are derived. For a LM Y = Xβ+ ϵ, recall that ﬁtted values are given by ˆY = Xˆβ= X(XTX)−1XTY = HY = H(Xβ+ ϵ) = Xβ+ Hϵ (7.1) with n×n hat matrix H= X(XTX)−1XT = ( xT i (XTX)−1xj ) 1≤i≤n,1≤j≤n ≡(hij)1≤i≤n,1≤j≤n where xT i comprises the values of the predictors for case i. In (7.1) we have used that (H5) HX = X. The hat matrix has several other interesting properties which are summarized in Tab"
9716,unknown,"from (7.1) that ˆY has the same mean function as Y but with error term Hϵ in place of ϵ. 7.1.1 Leverage values and studentised residuals Taking the i−th row of ˆY = HY, one ﬁnds that ˆyi = ... + hiiyi + ... where the so-called leverage values hi ≡hii = xT i (XTX)−1xi (also called: potential inﬂuence values, hat values) measure the impact of the i−th case on its own ﬁtted value. It is always 0 ≤hi "
9717,unknown,"1 n ≤hi ≤1. Residuals are given by ˆϵ= Y −ˆY = (I−H)Y = (I−H)(Xβ+ ϵ) = (I−H)ϵ Thus the residuals ˆϵare the same known linear functions of both the data Y and the random errors ϵ, and Var[ˆϵ] = Var[(I−H)ϵ] = (I−H)Var[ϵ](I−H)T = (I−H)2σ2I= (I−H)σ2. (7.2) Table 7.2 summarizes properties of errors and residuals. Equation (7.2) implies that Var[ˆϵi] = (1 −hi)σ2 and Cov[ˆϵi, ˆϵj] = −hijσ2. 63 Table 7.1:"
9718,unknown,"63 Table 7.1: Properties of the hat matrix H: (H1) H2 = H (idempotent) (H2) HT = H (symmetric) (H3) rank [ H] = p (H4) Tr [ H] = p (H5) HX = X and hence XTH= XT (H6) If model has intercept: H1 = 1; that is, ∑n i=1 hij = ∑n i=1 hji = 1 (Row and column sums of H are 1). (H7) H is positive semi–deﬁnite Table 7.2: Properties of residuals ˆϵand random error ϵ Assumption Errors Residuals – ˆY T ˆϵ= 0 (s"
9719,unknown,"– XTˆϵ= 0 (see Section 2.2.2). – If model has intercept term then 1Tˆϵ= ∑ˆϵi = 0. (A1) E[ ϵ] = 0 E[ˆϵ] = 0 (A2) Var[ ϵ] = σ2I Var[ˆϵ] = σ2(I−H) (A3) ϵ∼N(0,σ2I) ˆϵ∼N(0,σ2(I−H)) If ϵi ∼N(0,σ2) then ˆϵi ∼N(0,(1 −hi)σ2). Hence ˆϵi σ√1 −hi ∼N(0,1) Diagnostic procedures are based on residuals ˆϵ, which we would like to assume behave like the unobserved errors ϵ. Usefulness of such an assumption depends "
9720,unknown,"Replacing σ by its estimate s, we obtain the studentised residuals ri = ˆϵi s√1 −hi It can be shown that E[ ri] = 0 and Var[ri] ≈1. Hence in a plot of ri versus i, a case with a studentised residual greater in magnitude than 2 or 3 will suggest a possible outlier. We speak of ‘internally’ studen- tised residuals when all data, including case i, are used to estimate σ, otherwise one speaks of ‘exte"
9721,unknown,"studentised residuals. The behaviour of ˆϵi, hi, and ri is illustrated in Figure 7.1. This plot shows a bivariate data set featuring some horizontal and vertical outlines, and a regression line (for y versus x) ﬁtted to all data points. The △symbol within the bulk of the data is a data point where all of ˆ ϵi, hi, and ri are small. The vertical outlier (+) has ˆϵi large and hi small, and, hence, r"
9722,unknown,"top right corner has hi large, but it is not outlying with respect to the regression model, so ˆ ϵi is small and ri still moderately sized. In contrast, the point in the bottom right corner ( ⋄) has large ˆϵi, large hi, and a very large value of ri. 7.1.2 Inﬂuence analysis While we use residuals to check the model, inﬂuence analysis assumes that the model is correct and studies robustness of concl"
9723,unknown,"64 0 5 10 15 −5 0 5 10 x y Figure 7.1: Illustration of ˆ ϵi, hi, and ri (see description in text and turn by 90 degrees!). 65 when ˆβor s2 change substantially when deleting it (we will only investigate the behavior ofˆβhenceforth). One idea is to look at the leverage values hi ≡hii. Since Var[ˆϵi] = (1 −hi)σ2 ↘0 as hi ↗1 then essentially no matter what the value of yi if hi ≃1 then ˆϵi ≃0 so that"
9724,unknown,"then ˆϵi ≃0 so that the regression “plane” almost passes through yi; that is ˆyi ≃yi. If case i has hi ≃1 then it is potentially very inﬂuential in ﬁtting the model. That is, without such a case i, the estimated regression coeﬃcients and error variance may change substantially. However, a case can have large potential inﬂuence (leverage) but little actual inﬂuence. Thus, an overall inﬂuence measur"
9725,unknown,"measure should involve both Y and X: the hi only involve X, not Y. The better idea is to delete cases from data, one at a time, and compare results with those from the ﬁt to the full model. Those cases that cause major changes in analysis are called inﬂuential. Notation: Subscript (i) means i-th case deleted; for example, ˆβ(i) = (XT (i)X(i))−1XT (i)Y(i) In the above, ˆβ(i),X(i),Y(i) are respectiv"
9726,unknown,"(i)Y(i) In the above, ˆβ(i),X(i),Y(i) are respectively p×1,(n−1) ×p,(n−1) ×1. Cook’s distance We want to compare ˆβ(i) with ˆβ using a single number. There are lots of possibilities but one popular choice is Cook’s distance, Di, which for case i is Di = (ˆβ(i) −ˆβ)T(XTX)(ˆβ(i) −ˆβ) ps2 = ( ˆY(i) −ˆY)T( ˆY(i) −ˆY) ps2 [W,p.200] = 1 pr2 i hi 1 −hi Note that (a) ˆY(i) = Xˆβ(i) is n×1, not (n−1) ×1. ("
9727,unknown,"(b) Since p is ﬁxed, Di is determined by two factors r2 i and hi: it increases as r2 i increases and as hi increases (because h/(1 −h) is an increasing function of h); (c) the “studentised residual” ri is a random quantity reﬂecting the “lack of ﬁt” to the model of case i; (d) the “potential inﬂuence” hi is a non-random quantity reﬂecting the potential for case i to be inﬂu- ential, measuring the "
9728,unknown,"ential, measuring the distance of xi from the “centre of gravity” xof x1,x2,... xn; (e) Cases for which Di is “large” have substantial inﬂuence on ˆY and ˆβ; and their deletion may result in substantial changes in conclusions; (f) Cases for which Di ≥1/2 (rule of thumb!) should be considered carefully to see what eﬀect their deletion would have. Example 7.1 Fuel consumption We will carry out inﬂue"
9729,unknown,"see also Example 1.8). The response is the fuel consumption FUEL (“gallons per person”). There are several covariates, of which we consider the following ones: TAX (cents per gallon), DLIC (% population with driving licences), INC (average income in $1000’s), ROAD (1000’s of miles). The data and a pairs plot are provided below: #install.packages(""remotes"") #you need to do that once > remotes::inst"
9730,unknown,"> remotes::install_github(""tmaturi/sm2data"") > library(sm2data) > ?fuelcons > fuelcons STATE POP TAX NLIC INC ROAD FUELC DLIC FUEL 1 ME 1029 9.00 540 3.571 1.976 557 52.5 541 2 NH 771 9.00 441 4.092 1.250 404 57.2 524 3 VT 462 9.00 268 3.865 1.586 259 58.0 561 66 4 MA 5787 7.50 3060 4.870 2.351 2396 52.9 414 5 RI 968 8.00 527 4.399 0.431 397 54.4 410 6 CN 3082 10.00 1760 5.342 1.333 1408 57.1 457 "
9731,unknown,7 NY 18366 8.00 8278 5.319 11.868 6312 45.1 344 8 NJ 7367 8.00 4074 5.126 2.138 3439 55.3 467 9 PA 11926 8.00 6312 4.447 8.577 5528 52.9 464 10 OH 10783 7.00 5948 4.512 8.507 5375 55.2 498 11 IN 5291 8.00 2804 4.391 5.939 3068 53.0 580 12 IL 11251 7.50 5903 5.126 14.186 5301 52.5 471 13 MI 9082 7.00 5213 4.817 6.930 4768 57.4 525 14 WI 4520 7.00 2465 4.207 6.580 2294 54.5 508 15 MN 3896 7.00 2368 
9732,unknown,15 MN 3896 7.00 2368 4.332 8.159 2204 60.8 566 16 IA 2883 7.00 1689 4.318 10.340 1830 58.6 635 17 MO 4753 7.00 2719 4.206 8.508 2865 57.2 603 18 ND 632 7.00 341 3.718 4.725 451 54.0 714 19 SD 579 7.00 419 4.716 5.915 501 72.4 865 20 NE 1525 8.50 1033 4.341 6.010 976 67.7 640 21 KS 2258 7.00 1496 4.593 7.834 1466 66.3 649 22 DE 565 8.00 340 4.983 0.602 305 60.2 540 23 MD 4056 9.00 2073 4.897 2.449 
9733,unknown,23 MD 4056 9.00 2073 4.897 2.449 1883 51.1 464 24 VA 4764 9.00 2463 4.258 4.686 2604 51.7 547 25 WY 1781 8.50 982 4.574 2.619 819 55.1 460 26 NC 5214 9.00 2835 3.721 4.746 2953 54.4 566 27 SC 2665 8.00 1460 3.448 5.399 1537 54.8 577 28 GA 4720 7.50 2731 3.846 9.061 2979 57.9 631 29 FL 7259 8.00 4084 4.188 5.975 4169 56.3 574 30 KY 3299 9.00 1626 3.601 4.650 1761 49.3 534 31 TN 4031 7.00 2088 3.640
9734,unknown,31 TN 4031 7.00 2088 3.640 6.905 2301 51.8 571 32 AL 3510 7.00 1801 3.333 6.594 1946 51.3 554 33 MS 2263 8.00 1309 3.063 6.524 1306 57.8 577 34 AR 1978 7.50 1081 3.357 4.121 1242 54.7 628 35 LA 3720 8.00 1813 3.528 3.495 1812 48.7 487 36 OK 2634 6.58 1657 3.802 7.834 1695 62.9 644 37 TX 11649 5.00 6595 4.045 17.782 7451 56.6 640 38 MT 719 7.00 421 3.897 6.385 506 58.6 704 39 ID 756 8.50 501 3.635 
9735,unknown,39 ID 756 8.50 501 3.635 3.274 490 66.3 648 40 WY 345 7.00 232 4.345 3.905 334 67.2 968 41 CO 2357 7.00 1475 4.449 4.639 1384 62.6 587 42 NM 1065 7.00 600 3.656 3.985 744 56.3 699 43 AZ 1945 7.00 1173 4.300 3.635 1230 60.3 632 44 UT 1126 7.00 572 3.745 2.611 666 50.8 591 45 NV 527 6.00 354 5.215 2.302 412 67.2 782 46 WN 3443 9.00 1966 4.476 3.942 1757 57.1 510 47 OR 2182 7.00 1360 4.296 4.083 1331
9736,unknown,"47 OR 2182 7.00 1360 4.296 4.083 1331 62.3 610 48 CA 20468 7.00 12130 5.002 9.794 10730 59.3 524 > pairs(fuelcons[,c(""FUEL"", ""TAX"", ""DLIC"", ""INC"", ""ROAD"")]) We ﬁrst ﬁt a multiple linear regression model to the fuel consumption data and look at the residuals. > fuel.lm <- lm(FUEL~ TAX+ DLIC+ INC+ROAD, data = fuelcons) > summary(fuel.lm) # Part of the summary follows Coefficients: Estimate Std. Erro"
9737,unknown,67 FUEL 5 6 7 8 9 10 3.0 3.5 4.0 4.5 5.0 400 600 800 5 6 7 8 9 10 TAX DLIC 45 55 65 3.0 4.0 5.0 INC 400 500 600 700 800 900 45 50 55 60 65 70 0 5 10 15 0 5 10 15 ROAD Figure 7.2: A matrix scatterplot of the fuel consumption data. (Intercept) 377.291 185.541 2.033 0.048207 * TAX -34.790 12.970 -2.682 0.010332 * DLIC 13.364 1.923 6.950 1.52e-08 *** INC -66.589 17.222 -3.867 0.000368 *** ROAD -2.426 
9738,unknown,"ROAD -2.426 3.389 -0.716 0.477999 Residual standard error: 66.31 on 43 degrees of freedom Multiple R-Squared: 0.6787, Adjusted R-squared: 0.6488 > e <- fuel.lm$res > plot(e) # gives a plot of e_i against i > identify(e) # case 40 outlying? Externally and internally studentised residuals would be computed through > r0 <- rstudent(fuel.lm) # Externally studentised > r <- rstandard(fuel.lm) # Interna"
9739,unknown,"> plot(r) # gives a plot of r_i against i. > identify(r) # also here, case 40 stands out -- clear outlier with r>3. Next, we extract leverage values from the output of function lm.influence: > fuel.inf <- lm.influence(fuel.lm) # object containing influence information about regression object fuel.lm > names(fuel.inf) # [1] ""hat"" ""coefficients"" ""sigma"" ""wt.res"" 68 > h <- fuel.inf$hat # hat: a vecto"
9740,unknown,> h [1] 0.09634480 0.07402210 0.08474641 0.12515339 0.09231812 0.22880802 [7] 0.28322744 0.11069764 0.06076012 0.04790797 0.03582251 0.23047670 [13] 0.05563302 0.04330312 0.04408112 0.06413393 0.03814984 0.07341915 [19] 0.19037667 0.18745045 0.09536828 0.10941373 0.11270322 0.07370097 [25] 0.05175408 0.09124983 0.06577900 0.06465282 0.02652023 0.10906237 [31] 0.07816256 0.10890532 0.13778638 0.080
9741,unknown,"[37] 0.31510460 0.03963847 0.17091219 0.09972312 0.05434435 0.07451915 [43] 0.05698576 0.14884860 0.26597947 0.07231159 0.05632373 0.08792289 > plot(h) # gives a plot of h_i against i > identify(h) # here, case 37 attains the top value Cook’s distances are easily obtained through > d <- cooks.distance(fuel.lm) # a vector of the Cooks distances D_i > plot(d) # gives a plot of D_i against i. Notice "
9742,unknown,"> identify(d) # how case 40 stands out even though d[40] is # is only 0.3089626. The produced plots of e, r, h and d are provided in Figure 7.3. Summarizing, case 40 (Wyoming) is clearly distinctive and likely to be an outlier, yet not actually inﬂuential in the sense of our rule of thumb (D40 is smaller than 1/2). Case 37 (Texas) has a relatively large leverage value and can be called potentially"
9743,unknown,"We can now consider ﬁtting the full model without some of these possibly inﬂuential observations. For example, suppose we omit cases 37 (Texas) and 40 (Wyoming). We do this as > newfuel.lm <- lm(FUEL~ TAX+ DLIC+ INC+ROAD, data = fuelcons, subset=c(-37,-40)) Part of the new summary is: > summary(newfuel.lm) Estimate Std. Error t value Pr(>|t|) (Intercept) 433.744 157.866 2.748 0.00888 ** TAX -31.66"
9744,unknown,"TAX -31.663 11.155 -2.838 0.00702 ** DLIC 11.728 1.661 7.063 1.34e-08 *** INC -66.407 14.533 -4.569 4.43e-05 *** ROAD -1.192 3.078 -0.387 0.70061 Residual standard error: 55.8 on 41 degrees of freedom Multiple R-Squared: 0.7009, Adjusted R-squared: 0.6717 Comparing summary(newfuel.lm) with summary(fuel.lm), we notice some improvements and changes, none particularly dramatic. 69 0 10 20 30 40 −100 "
9745,unknown,case number e_i 0 10 20 30 40 −2 −1 0 1 2 3 case number r_i 0 10 20 30 40 0.05 0.15 0.25 case number h_i 0 10 20 30 40 0.00 0.10 0.20 0.30 case number d_i Figure 7.3: Residual and inﬂuence plots for fuel consumption data: Residuals ˆ ϵi; studentised residuals ri; potential inﬂuences hi; and Cook’s distance Di. 70 7.1.3 Model checking through residual diagnostics We want to check (A1) the form of E
9746,unknown,"We want to check (A1) the form of E[ y|x] for all x, equivalently E[ϵ|x] = 0 for all x; (A2’) Var[ y|x] = σ2 for all x, equivalently Var[ϵ|x] = σ2 for all x; (A3) y|x∼N(E[y|x],σ2) for all x, equivalently, ϵ|x∼N(0,σ2) for all x; (A2”) whether the errors ϵi ≡ϵ|xi,i = 1,...,n , are independent. We will use residuals to check these four assumptions. In particular, we use the following previously estab"
9747,unknown,"established results to check items (A1) and (A2’) above. (a) XTˆϵ= 0 (b) ˆY T ˆϵ= 0 Equation (a) implies 0 = ∑n i=1 xijˆϵi for j = 1,...,p , which corresponds exactly to the numerator of the correlation coeﬃcient between the residuals and the j-th predictor (if there is an intercept in the model). Hence, one has Corr[xj,ˆϵ] = 0 j = 1,...,p where xj is the jth column of the design matrix X. Similar"
9748,unknown,"Similarly (b) implies that the sample correlation Corr[ ˆY,ˆϵ] = 0. Thus, if the model assumptions (A1) and (A2’) are correct, plots of ˆϵi versus xij for j = 1....,p and ˆϵi versus ˆyi should be patternless. Otherwise, • curvature suggests form of E[ y|x] is wrong • a trumpet shaped plot suggests form of Var[ y|x] = σ2 for all xis wrong • possibly both of the above! A Normal quantile plot can be "
9749,unknown,"A Normal quantile plot can be used to check assumption (A3). A plot ui = Φ−1 (i−0.5 n ) versus r(i) where r(1) < r(2) < ··· < r(n) are the ordered studentised residuals ri = ˆϵi/s√1 −hi, should look like straight line u= r through the origin with slope 45 degrees; otherwise, a curved plot suggests some form of non-normality. A pattern in plot of ˆϵi versus i or (if available) ti, the time at which"
9750,unknown,"non-independent errors, violating assumption (A2”). For standard graphical diagnostics inR, suppose myfit.lm results ﬁtting a linear model using function lm in R. Then type plot(myfit.lm) and, as instructed, hit return to produce the following four plots: (i) residuals versus ﬁtted values plot; (ii) Normal Q-Q plot of residuals; (iii) scale-location plot: √ |internally studentised residuals|versus"
9751,unknown,"(iv) Either ri vs hi (continuous predictors) or ri vs “Factor level combinations”. 71 Example 7.2 (Continuation of Example 7.1) Diagnostic plots for fuel consumption data. The corresponding plots are provided in Figure 7.4. # Residual plots (check for patterns) > plot(fuelcons$TAX, fuel.lm$res) > plot(fuelcons$DLIC, fuel.lm$res) > plot(fuelcons$INC, fuel.lm$res) > plot(fuelcons$ROAD, fuel.lm$res) "
9752,unknown,"# all ok. > plot(fuel.lm$fitted, fuel.lm$res) # slight trumpet-shape? # check for normality > qqnorm(fuel.lm$res) > qqline(fuel.lm$res) # or > qqnorm(rstandard(fuel.lm)) > qqline(rstandard(fuel.lm)) # ok (small deviations in boundary region are acceptable). # check for residual autocorrelation and outliers > plot(fuel.lm$res) # ok. # R standard diagnostics # plots not shown --- please produce your"
9753,unknown,"> par(mfrow=c(2,2)) > plot(fuel.lm) # top left: residual vs fitted # no strong pattern here, but somewhat increasing spread # top right: r_i vs Gauss quantiles # no strong indication against normality (boundaries are tolerable) # bottom left: sqrt(|r_i|) vs fitted # slight tendency to heteroscedasticity (""Location-scale-plot"") # bottom right: r_i vs h_i # No influential observations 72 5 6 7 8 9 1"
9754,unknown,−100 0 50 150 fuelcons$TAX fuel.lm$res 45 50 55 60 65 70 −100 0 50 150 fuelcons$DLIC fuel.lm$res 3.0 3.5 4.0 4.5 5.0 −100 0 50 150 fuelcons$INC fuel.lm$res 0 5 10 15 −100 0 50 150 fuelcons$ROAD fuel.lm$res 400 500 600 700 −100 0 50 150 fuel.lm$fitted fuel.lm$res −2 −1 0 1 2 −100 0 50 150 Normal Q−Q Plot: Residuals Theoretical Quantiles Sample Quantiles −2 −1 0 1 2 −2 −1 0 1 2 3 Normal Q−Q Plot: St
9755,unknown,"Theoretical Quantiles Sample Quantiles 0 10 20 30 40 −100 0 50 150 Index fuel.lm$res Figure 7.4: Diagnostic plots for fuel consumption data, in the order as produced in Example 7.2. 73 Example 7.3 (Animals data — Continuation of Example 5.4) Figures 7.5 and 7.6 show the standard R diagnostic plots for the main eﬀects-plus-interaction ﬁt to both survival and reciprocal of survival in the animal fac"
9756,unknown,"of the 4 treatments and 3 poisons investigated. The residuals-versus-ﬁtted values plot in Figure 7.5 indicates strongly that variance increases with mean, and this is supported in the scale-location plot. The Q-Q plot gives no support for the normality assumption. However, inspection of the corresponding plots in Figure 7.6 for the main eﬀects-plus- interaction ﬁt to the reciprocal of survival (ra"
9757,unknown,on this scale of the response. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 −0.4 −0.2 0.0 0.2 0.4 Fitted values Residuals Residuals vs Fitted 41 23 29 −2 −1 0 1 2 −2 −1 0 1 2 3 Theoretical Quantiles Standardized residuals Normal Q−Q 41 23 29 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 0.5 1.0 1.5 Fitted values Standardized residuals Scale−Location 41 23 29 −3 −2 −1 0 1 2 3 4 Factor Level Combinations Standardized resi
9758,unknown,"Standardized residuals A C D B Treatment : Constant Leverage: Residuals vs Factor Levels 41 23 29 Figure 7.5: Diagnostic plots for main eﬀects-plus-interaction ﬁt to survival in animal factorial experiment (Example 4.4). We also compare the ANOVA tables associated with the “main eﬀects plus interaction ﬁt for both suvival and its reciprocal, rate of dying. # > install.packages(""remotes"") # you nee"
9759,unknown,"> remotes::install_github(""tmaturi/sm2data"") > library(sm2data) > data(animals) > par(mfrow=c(2,2)) > plot(lm(Survival~ Poison * Treatment, data=animals)) > anova(lm(Survival ~ Poison * Treatment, data = animals)) Df Sum Sq Mean Sq F value Pr(>F) Poison 2 1.03301 0.51651 23.2217 3.331e-07 *** Treatment 3 0.92121 0.30707 13.8056 3.777e-06 *** 74 Poison:Treatment 6 0.25014 0.04169 1.8743 0.1123 Resi"
9760,unknown,"74 Poison:Treatment 6 0.25014 0.04169 1.8743 0.1123 Residuals 36 0.80073 0.02224 The corresponding ANOVA table for “rate of dying” ﬁtted to the same model, given below, shows a increase in sensitivity compared with the previous analysis: the F-ratios for the main eﬀects Poison and Treatment are larger (leading to smaller signiﬁcance probabilities) and the F-value for interaction is smaller. > plot"
9761,unknown,"smaller. > plot(lm(1/Survival~Poison* Treatment, data=animals)) > anova(lm(1/Survival ~ Poison * Treatment, data = animals)) Df Sum Sq Mean Sq F Pr(>F) Poison 2 34.877 17.4386 72.635 2.310e-13 Treatment 3 20.414 6.8048 28.343 1.376e-09 Poison:Treatment 6 1.571 0.2618 1.090 0.3867 Residual 36 8.643 0.2401 2 3 4 −1.0 −0.5 0.0 0.5 1.0 Fitted values Residuals Residuals vs Fitted 38 47 26 −2 −1 0 1 2 −"
9762,unknown,−1 0 1 2 3 Theoretical Quantiles Standardized residuals Normal Q−Q 38 47 26 2 3 4 0.0 0.5 1.0 1.5 Fitted values Standardized residuals Scale−Location 38 47 26 −2 −1 0 1 2 3 Factor Level Combinations Standardized residuals B D C A Treatment : Constant Leverage: Residuals vs Factor Levels 38 47 26 Figure 7.6: Diagnostic plots for main eﬀects-plus-interaction ﬁt to reciprocal survival in animal facto
9763,unknown,"factorial experiment (Example 4.4). This indicates that variation in survival time for the ﬁt to “main eﬀects plus interaction” model is signiﬁcantly inﬂuenced by both factors, poison and treatment, while there is little evidence for interaction (even less than for the untransformed model). The discussion so far indicates that the simple additive model 1/Survival ∼ Poison + Treatment for rate of d"
9764,unknown,"rate of dying as the response will be considered in detail in Section 7.2. 75 Some remedies There are many things we could do, but two common situations are: (1) if a plot of ˆϵi versus xij is curved, we may need to include an extra model term, or possibly transform xj. (2) if a plot of ˆϵi versus ˆyi is trumpet shaped, we may need to transform the response y to stabilise its variance. An interest"
9765,unknown,"An interesting device to address (2) is the Box–Cox transformation. 7.2 Box-Cox transformations Suppose we seek some power transformation of a positive response y of the form y(λ) = {yλ−1 λ λ̸= 0 log y λ = 0 such that y(λ) satisﬁes the standard model assumptions; namely, (A1) E[ y(λ) |x] = xTβ (A2’) Var[ y(λ) |x] = σ2 for all x (A3) y(λ)|x∼N(xTβ,σ2) for all x (A2”) y(λ) 1 ,...,y (λ) n are independ"
9766,unknown,"(A3) y(λ)|x∼N(xTβ,σ2) for all x (A2”) y(λ) 1 ,...,y (λ) n are independent As λvaries over (−2,2), y(λ) encompasses the reciprocal transformation (λ= −1), log (λ= 0), square root (λ= 1 2 ), the original scale ( λ= 1) and the square transformation ( λ= 2). If the yi are not positive we apply the transformation to yi + γ, where γ is chosen to make all the yi + γ positive. The Box-Cox method chooses λ"
9767,unknown,"The Box-Cox method chooses λvia a likelihood argument as follows. Taking account of the Jacobian of the transformation from y(λ) to y, namely yλ−1, the density of yi is f(yi ⏐⏐xi) = yλ−1 i (2πσ2) 1 2 exp [ − 1 2σ2 ( y(λ) i −xT i β )2] Consequently, the log-likelihood for β,σ2,λ based on independent y1,...,y n is L(β,σ2,λ) = −1 2 [ nlog(2πσ2) + 1 σ2 n∑ i=1 ( y(λ) i −xT i β )2 ] + (λ−1) n∑ i=1 log y"
9768,unknown,"log yi If λ is regarded as ﬁxed, the maximum likelihood estimates of βand σ2 are ˆβλ = (X T X)−1X T Y(λ) ˆσ2 λ = RSS(ˆβλ) n where RSS(ˆβλ) is the residual sum of squares for the regression ofY(λ) = (y(λ) 1 ,...,y (λ) n ) T on the columns of X. Note when λ= 1 we get the ordinary least squares estimate of βand ˆσ2 is a multiple of the usual estimate s2 of σ2. The proﬁle log likelihood for λ, written"
9769,unknown,"Lp(λ) ≡max β,σ2 L(β,σ2,λ) = L(ˆβλ,ˆσ2 λ,λ) = −n 2 log RSSλ + (λ−1) ∑ log yi + n 2 (log n−1), where the latter term n 2 (log n−1) does not depend on λ and can therefore be omitted. A plot of Lp(λ) summarises the information concerning λ. An approximate 1 −αconﬁdence interval is the set of λ values for which Lp(λ) ≥Lp(ˆλ) −1 2χ2 1,α where χ2 1,α is the quantile of the chi-squared distribution with 1"
9770,unknown,"1,α is the quantile of the chi-squared distribution with 1 degree-of-freedom corresponding to the probability mass α in the right tail; for example, α= 0.05 gives χ2 1,0.05 = 3.84 for a 95% interval [W, p. 289]. 76 Example 7.4 (Continuation of Example 7.3) Suppose we want to choose a value of λsuch that a main eﬀects only model (no interaction) plus the usual assumptions of independent Gaussian er"
9771,unknown,"appropriate. We type > library(MASS) > boxcox(lm(Survival~Poison +Treatment, data=animals)) which generates Figure 7.7. The conﬁdence interval contains −1, supporting the reciprocal transforma- tion. The conﬁdence interval in Figure 7.8, which shows the corresponding plot for the model using the reciprocal survival times contains 1. This gives further support to this model. > boxcox(lm((1/Survival"
9772,unknown,−2 −1 0 1 2 −20 −10 0 10 20 30 lambda loglik for survival additive fit for animals data 95% Figure 7.7: Box-Cox transformation log-likelihood plot for main eﬀects ﬁt to survival in animal factorial experiment. Nur das das Bild richtig dasteht. 77 −2 −1 0 1 2 −110 −100 −90 −80 −70 −60 lambda loglik for reciprocal survival additive fit for animals data 95% Figure 7.8: Box-Cox transformation log-like
9773,unknown,"in animal factorial experiment. 78 Appendix A Some matrix algebra Generally, we denote • real-valued matrices by capital letters A,B,... , • real-valued vectors by small letters a,b,... , • random vectors by U,V,W,X,Y,Z , and their realizations by u,v,w,x,y,z. Further, • X, Y, and Z denote design matrices (see Section 3), response vectors, and data matrices, respec- tively. Below we provide a list"
9774,unknown,"Below we provide a list of some important and useful terms and formulas from matrix algebra. Deﬁne therefore a square matrix A∈Rn×n, a matrix B∈Rn×m, and a vector b∈Rn. • The transpose BTis the m×n matrix obtained from B by interchanging rows with columns; • The trace Tr(A) is the sum of the n diagonal elements of A; • The inverse A−1 is the n×nmatrix such that AA−1 = A−1A= I, where Iis the identi"
9775,unknown,"having 1’s along the diagonal and 0’s otherwise. • A matrix Ais said to be symmetric if A= AT. • A matrix Ais said to be orthogonal if AAT = ATA= I, i.e. if AT = A−1. • A symmetric matrix Ais said to be positive deﬁnite if and only if bTAb>0 for all vectors b̸= 0, and positive semi-deﬁnite if the inequality is not strict (i.e., ≥). Similarly, Ais said to be negative deﬁnite if and only if bTAb<0 f"
9776,unknown,"not strict. • The eigenvectors vi and eigenvalues λi of a matrix Asatisfy Avi = λivi, i= 1,...,n . • The principal minors of a symmetric matrix Aare the determinants of any k×k submatrix of A (where the submatrix is formed by removing any q−k rows and the same q−k columns). • The leading principal minors of a symmetric matrix A are the determinants det( Ak×k),k = 1,...,n , where Ak×k are obtained "
9777,unknown,"1,...,n , where Ak×k are obtained by taking only the ﬁrst k rows and k columns of A. • If A is positive deﬁnite, then there exists precisely one positive deﬁnite matrix C ∈Rn×n with CC = A; we then deﬁne the square root of a matrix as A1/2 = C. Of course, these are real-valued matrices resp. vectors too; they just have a special signiﬁcance which is highlighted through an explicit denotation. 79 T"
9778,unknown,"79 Table A.1: Properties of matrix operators. Let A∈Rn×n,B∈Rn×m, b∈Rn. (M1) Let C∈Rn×n. Then Tr(A+ C) = Tr(A) + Tr(C) (M2) Let C∈Rm×n. Then Tr(BC) = Tr(CB) (M3) A special case of (M2) is: Tr( bbT) = bTb (M4) Tr( A) = ∑n i=1 λi (M5) det( A) = ∏n i=1 λi (M6) Ais positive (semi-) deﬁnite ⇔ all eigenvalues of Aare positive (non-negative) (M7) Ais negative (semi-) deﬁnite ⇔ all eigenvalues of Aare nega"
9779,unknown,"all eigenvalues of Aare negative (non-positive) (M8) Ais positive deﬁnite ⇔A−1 is positive deﬁnite. (M9) Ais invertible ⇔det(A) ̸= 0. (M10) Ais positive deﬁnite =⇒det(A) >0. (M11) Let D= ( a b c d ) . Then det( D) = ad−bc (M12) With Das in (M11), one has D−1 = 1 det(D) ( d −b −c a ) . (M13) Let a∈Rn. Then ∂aTb ∂b = ∂bTa ∂b = a. (M14) if Ais symmetric, then ∂bTAb ∂b = 2Ab. • The determinant det(A) "
9780,unknown,"• The determinant det(A) is a rather complex function mapping a square matrix A to a scalar (geometrically, this is the volume of the parallelepiped spanned by the columns or rows of the matrix); see [K], Appendix A3. • The rank of a matrix B, denoted by rank(B), is deﬁned as the largest number of linearly indepen- dent column vectors (or, equivalently, rows) in B. This corresponds just to the dim"
9781,unknown,"vector space spanned by the columns (or rows) of B. • The derivative (gradient) of a scalar function g(b) : Rn −→R w.r.t. b is deﬁned as the vector ∂g(b) ∂b = ( ∂g(b) ∂b1 ,..., ∂g(b) ∂bn )T . Some important properties of these operators are summarized in Table A.1. A generally useful resource, where these (and many other) properties of matrices can be found, is the Matrix Cookbook [MC]. 80 _h. ,%'"
9782,unknown,"[MC]. 80 _h. ,%'+, _. 1994 Universal Cokriging Under Intrinsic Coregionalization ~ Jeffrey D. Helterbrand 2""+ and Noel Cressie 2 [ lhlPF l/l(' illll'ill.*i( corr IIIodi'/. i~ J~ol]l prlllkll~ lllld ~c~olld~lrx nl('(l~Hll'ttl('lll.~ lit(' ,:1~111~11~1~' ill ~1// ',lllllll/l' IOi lllitql~. Ilk' ~~JIl~'llll~*llill k'~'~&lll~lRlll Ilixdottl i~ I11r i ~l'ik, lllk, 111~lldd~"
9783,unknown,"Ilhl]\ Iht' StlltlU ~ohlliqql ~1~ Iltliv{lritllt' kris L, Oll Iht' [lt+illRil'~ l)rol'l's,~ (l/Ollt"" HIIIII'I'CF, I'l't'l'lll , ~tllll[)l{'~ hllVC h('UIt l,,ix'l'll iihClk"" 111911Z~'ll; sdloncltll~V l+lklik, itlk r Ildl,ghls hdvl' (~[(lll'rt'd Illld('l 1111~ ,/~dlllgl dt.pt'lldl'lll(' Slrllc llll(' ]ills Iiii1(' Jdt'lllillls Ihr ('tllldJlJotls IIIId{'l"" II]lJ( ]l st'i llllddr~ [?lfllt'l?ltlllllll"
9784,unknown,"e ~ ll*<'/il/ IIIldl't"" f/IU r ~lllllllf]Oll Q/ inlrinsi~ I'ordk'il~lllllizr ..|n il/llXllr is k, ixUII IlSllll~ iI ~J~Aldl~l'l '1 I)]llloIlllllll (11111 ~mlcrt( ittm ( +~ll('Ctlll'dllOll~ co/]c('tcd./r,.n a rd.k'ioll ~)1 tit(' ,'x,'l'x'(/(h/ ]'('~t ,%'ild. KEY %VORI)S: mulli\ ariatc spaltal dep(_'nduncc rnndcls, partial cmariancc, restricted linear models. INTRODUCTION Cokriging produces predicto"
9785,unknown,"INTRODUCTION Cokriging produces predictors that use nc)l only inlomlation from direct mea- ',urenlents of the spatial component process being considered, but also the in Ionllation frona nlcasurements of a secondary component process. Let s~. s,, 9 . , % be sample locations xvherc |neasurcments on both component processes ,re avuilablc. For the time being, v,e shall LISF, UlllC no missing data at "
9786,unknown,",re avuilablc. For the time being, v,e shall LISF, UlllC no missing data at the sample Locations. Let Z,(sa) be the measuremenl for the ith component at location s a (k I ..... n: i = I, 2). and Z(st) denote the bivariate vector at location s a. \Vc shall retcr to the first component process, ZL(. ) us the primar) process and /,(') as the secondary' process. Define the data vector Z =- (Z(sl)/. Z("
9787,unknown,".... Z(s,,)/) ~. Suppose we arc interested in predicting the primary process Z L iS41) at unsanlpled location s,. The cokriging predictor for Z I(s,} given the sawtple infl+m+mtion is t+t the It + FI]I, ZI (s,,) \' ~"" = ~ ~ X,aZ,(s a) (1) Rccci~cd II Noxcmhcr It,)~)2: accepted f~ .hmtlar,, It4""43 I)cparllncnt ol ,~[allSlids. Io'.'.a Y, lalc Unixcrsil 3 . Ames. ]ox~a 5{RJl I I'rcscnl addrcsn: Jcflr"
9788,unknown,",,h',. Indiunu 41~2,~5. 205 ~F~,=,2 "",1.!1 'J~ 1~2041 ~l.'ql+~ql - ~'4~ I HI'*4 hlqum4h.uknl \,~,,, udn,,u h,t \141hcm,ntn~ d I,c,,Ioc'+"
9789,unknown,"206 Helterbrand and Cressie where the cokriging weights { X/k: k = 1 ..... n; i = 1, 2} satisfy the uniform unbiasedness constraint, E(E~= i E~= i XikZi(sD) = E(Z~ (so)). These cokriging weights are selected to minimize E Zl(so) - kikZi(sk) (2) i=1 ""= subject to the unbiasedness constraint. To obtain the optimal cokriging weights requires knowledge of models of the spatial correlation for both the"
9790,unknown,"the spatial correlation for both the primary and the secondary components as well as for the cross-correlation between the two processes. The bivariate spatial dependence model is said to be intrinsically coregionalized if the assumed spatial dependence structure is I E 1 C (s,, s,,,) --- = I_C21(Sk, Sin) C22(Sk, Sm)J 7, ~ ~-3 where c0(s k, Sin) = COV [Zi(s~), Zj(Sm)], c(s,, S,,,) is a valid covar"
9791,unknown,"3' > 0, and I ol -< I. Under the intrinsic coregionalization model, if both primary and secondary measurements are available at all sample data locations, the conventional geo- statistical wisdom is that cokriging provides exactly the same solution as uni- variate kriging on the primary component alone. Specifically, all secondary cokriging weights are zero. In the Summer 1992 issue of Geostatisti"
9792,unknown,"hlterdisciplinar3., Geostatistics Newsletter, an example was given where, under the intrinsic coregionalization assumption with all sample data available, the secondary cokriging weights were nonzero. The question was posed regarding when secondary information is useful under the assumption of intrinsic core- gionalization. In this note we clarify when the secondary cokriging weights may be non- z"
9793,unknown,"zero. In particular, we investigate properties of the cokriging model under in- trinsic coregionalization and determine conditions where cokriging provides the same solution as univariate kriging. We also investigate what happens to the secondary cokriging weights when an additional primary or secondary obser- vation is available at a location s,, + ~. The results of this paper extend imme- diatel"
9794,unknown,"diately to the m-variate cokriging case (cf. Myers, 1982) if an m-variate intrinsic coregionalization model is assumed. Finally, cokriging under intrinsic core- gionalization is illustrated using a dataset of plutonium and americium concen- trations collected from a nuclear testing area on the Nevada Test Site. But first, we argue that the intrinsic coregionalization model is very restrictive and "
9795,unknown,inappropriate. This is done in order to dispel any thoughts one might have that multivariate spatial statistics is an unnecessary complication in which the in- corporation of covariate infomlation is not useful.
9796,unknown,"Universal Cokriging 207 The Intrinsic Coregionalization Model is Restrictive The intrinsic coregionalization assumption for a bivariate spatial depen- dence model is very restrictive and is often not appropriate. Should microscale variation and/or measurement error variation be present in a bivariate spatial dependence model, the model is no longer intrinsically coregionalized. Even when a small n"
9797,unknown,"when a small nugget effect due to microscale variation is present, the secondary cokriging weights may be nonzero. For a simple example in the plane, suppose that there are two sample locations, s~, and s2, one on either side of the point So where the primary value Z~ (So) is to be predicted. The closer location, s~, is 50 meters due west to So, and s2 is 150 meters due east. We wish to predict Z~"
9798,unknown,"Z~ (so) using information Z(s~) and Z(s2). Suppose the bivariate autocovariance function is an intrinsically coregionalized exponential model given by (3) with 3' = 1, 0 = 0.5, and spatial-dependence parameter -3/200. That is, the bivariate spatial dependence model can be expressed as, [, 0;] (3 ) C(s~, Sin) = exp [Isk - stall (4) o.5 so that the univariate covariogram is isotropic. The resulting "
9799,unknown,"o.5 so that the univariate covariogram is isotropic. The resulting simple cokriging weights (simple cokriging will be discussed momentarily) are Xll = .468, ~u2 = .082, X2~ = 0, and X22 = 0. We see that the secondary cokriging weights are zero. Now incorporate microscale variation into the bivariate spatial de- pendence structure. Specifically, consider C (sk, s,,) = 1 exp 0.5 EOo' ol ) + exp -- s"
9800,unknown,"C (sk, s,,) = 1 exp 0.5 EOo' ol ) + exp -- s.,ll (5) 0.25 Under this modified spatial dependence model we obtain the simple cokriging weights, ;ktl = .289, Xt2 = .056, ~,21 = .072, and X22 = .011. Here, the secondary information is used in the simple cokriging predictor. A second remark is that the assumption of intrinsic coregionalization carries with it the assumption of a symmetric spatial cros"
9801,unknown,"with it the assumption of a symmetric spatial cross-covariance function. That is, cO(s k, s,n) = cji(sk, Sin). This symmetry need not be tree in general (e.g., Ver Hoef and Cressie, 1993). Third, it isoften necessary to estimate unknown parameters present in the valid univariate covariance function c(- ,-) given in Eq. (3). In this case, sec- ondary information should be used in conjunction with p"
9802,unknown,"ondary information should be used in conjunction with primary information to obtain the most efficient estimators. Thus, the secondary information should not be ignored. For these reasons, one should not view multivariate spatial prediction as"
9803,unknown,"208 Helterbrand and Cressie an unneccessary complication to a univariate problem. As in standard multivari- ate analysis, multivariate spatial prediction allows covariate knowledge to yield more precise predictions. SIMPLE COKRIGING We can determine the conditions under which the intrinsically coregion- alized cokriging model produces zero secondary weights by exploring its prop- erties in the sim"
9804,unknown,"erties in the simple cokriging environment. Consider the following decompo- sition for the component processes. Let Zi(s) = ~i(s) + 5i(s); s ~ D, i = 1, 2 (6) where D is the spatial domain of interest, E(Z,(s)) = p.i(s), t~i(s) is referred to as the large-scale variation term, and 6~(s) is a zero-mean process including smooth small-scale variation, microscale variation, and measurement error (see "
9805,unknown,"Cressie, 1991, p. 112 for further details). The process 5i is referred to as the small-scale variation for the ith component. The large-scale structure for a component process is usually unknown. Often we pavameterize #,(') in a linear manner. That is. we assume ~i(s) = Xi(s)r~/ (7) where X,(s) is most generally a (pi  l) vector of fixed ""explanatory"" variables for the ith process and [~, is a (p"
9806,unknown,"for the ith process and [~, is a (p,  1) vector of corresponding unknown parameters. The parameters [~, are referred to as the large-scale parameters for the ith process; i = 1, 2. In simple cokriging, it is assumed that the p~(') processes are known in full. For example, in Eq. (7), all p = Pl + P2 large-scale parameters, p = (p~, p~)r, are fixed and known, say equal to pc, = ([~,,r ~OT)T. The l"
9807,unknown,"parameter space for simple cokriging is thus f2 = { I~""} (8) a single point in p-dimensional space. Since p.j(-) is known for i = 1,2, simple cokriging lbcuses on the prediction of 5t (so) based on 5i(sk) = Zi(sk) - ~i(sk); k = 1, 2 ..... n, i = 1, 2 (9) We now establish that the secondary simple cokriging weights will always be zero under the intrinsic coregionalization assumption. We arrive at t"
9808,unknown,"Universal Cokriging 209 Simple Cokriging Equations Consider the simple cokriging equations for predicting 8~ (so) based on 6 = (6 (sl) T, 6(s2) T, .... 6(s~)r) r, which are r {X,,.c,,(sk, s,,,) + X2,,,c,2(sk, s,,,)} = c~(sk, So), rn=l n {)KImCI2(S k, Sin) q- )k2mC22(Sk, Sin)} = CI2(Sk, S0), m= I k= 1,2 ..... n k= 1,2 ..... n (10) By the intrinsic coregionalization assumption, we know cl~(sk, s,,,)"
9809,unknown,"(10) By the intrinsic coregionalization assumption, we know cl~(sk, s,,,) = c(sk, s,,,), cl2(s~, s,,,) = c21(s ~, s,,,) = ""ypc(sk, s,,,), and C2e(S k, Sin) = 3'2C'(Sk. S,,,), which reduces the simple cokriging equations to Pl {X,,,, + 3,o;',z,,,}c(s~, s,,,) = c(sk, So), m = I tl {3,pX.,, + ""r2X2,,,}c(s~, s,,,) = 3,pc(sk, so), rr 1 k = 1,2 ..... n k = 1.2 ..... n (11) The simple cokriging equations"
9810,unknown,"{X.,, + 7pX2,,,}c(sk, s,,,) = c(sk, so), k = I, 2 ..... n m = ] I 1 ~] )""Jm + 3' X2,, ' C(Sk, %.) = C(S~, SO), k = 1, 2 ..... n (12) m = 1 p If we define X,,* -= XI,,, + "",/OX2,,, and k,,** --- X~,~ + (3'/P) X2,,, (m = 1,2 ..... n), we observe that each set of equations represents exactly the equations for standard univariate simple kriging. Furthermore, since in general lal ~ 1 and we must have X"
9811,unknown,"we must have X,* = X**; m = 1, 2 ..... n, we see that the secondary weights {X2m: m = 1, 2 ..... n} must be zero. (If ]Pl = 1, Eq. (12) does not have a unique solution). Least Squares Solution for k Recall that 6 = (8(s0 r, ~i(s2) r ..... ~5(s.)7) r. Denote the corresponding variance-covariance matrix for ti as T =- cov (6). Under the intrinsic coregion- alization assumption, T takes on a special "
9812,unknown,"210 Helterbrand and Cressie [ c(s~, sO c(s~, s2) c(st, s2) c(s2, s~) T = LC(St, S,) ... p-y -y- j ""'"" c(s,, s,,)l P3' 3 `2 (13) = ((a,jB)),,,t ,  where | denotes the Kronecker product [recall A ..... | B,  q Next, let c --- cov (6, 6~ (so)), which is a 2n x 1 vector. Note that, under the intrinsic coregionalization assumption, c can be expressed as e = ([c(sl, so), c(s2, So) ..... c(s,,, so) ] |"
9813,unknown,"e = ([c(sl, so), c(s2, So) ..... c(s,,, so) ] | [1, pT]) T -= co | [I. pT] 7. (I4) Minimizing Eq. (2) with respect to {hit: k = 1, 2 .... , n; i = 1. 2} results in the following least squares solution for k --- (X~, h~2, X2~, X22 ..... X,,t, X,,Dr: (E' ~.=T-'c = (2 | (Co| [1, p~,l r) P3' 3 `2 J / [ 1 0""7 -i = C-' * ] ) (c0 @ [1, p'),] T) P'Y 312 J / I :1' 1 p = C ~co | [1, p.y]T (15) P3' ""Y- J Thu"
9814,unknown,"I :1' 1 p = C ~co | [1, p.y]T (15) P3' ""Y- J Thus, we have ~. = C Ic o| [1,017. (16) Again we see that the secondary simple cokriging weights are zero under the assumption of intrinsic coregionalization. Partial Covariance To understand the mechanism generating the null secondary weights under intrinsic coregionalization, it is fruitful to consider the partial covariance of 6~(s0) and 62(s D given"
9815,unknown,"and 62(s D given all other measurements. For example, consider the partial co- variance between 6t(so) and 62(s ~) given all other observations. We first calculate the partial covariance without conditioning on 6~(s~), and then we calculate the partial covariance including conditioning on 6~(s~). Consider the vector 60 -= [b~(So), 6(sl) r, 6(s2) T, .... 6(s,,)r] T. Decompose 6 o into three parts, "
9816,unknown,"6 o into three parts, namely 6 0 ~ (hi(S0) , 6(SI) T, 6TI) T, where 8111 ~ (6(S2) T, .... 6(S,,)7) T. The corresponding partitioned covariance matrix is"
9817,unknown,"Universal Cokriging 211 I var (hi(So)) cov (6(s0, 51(So)) r coy (6m, 51(so)) r-] l ~1 -= |cov (6(s0. 51(so)) var (6(SI)) COV (611D 6(Sl)) T ] (17) 1 A LCOV (6m. 51(s~)) cov (fro. 6(s,)) var (6m) Now, under the assumption of intrinsic coregionalization we have --c(s~. so) ('(S O, Sl) @ [1, .07] r C(SI' ~Jl) @ 12 = C(,~I, Sill ) @ [[, p~]l C(Sl ' Sill ) I where ill P7 03' 2 C(SIll, SIll) ""@ ;:1 C(So"
9818,unknown,"where ill P7 03' 2 C(SIll, SIll) ""@ ;:1 C(So, Snl ) ~ COV (51(S0) , [61(S2) ..... (~l(Sn)]T) T C(sl, sin) -= coy (5ds0, Ill(s2) ..... 5ds,,)]r) r C(sIH, s~H) ----- coy ([61(s2) ..... 6ds,,)] r, [hds~) ..... 6r(sD] r) (18) ~,,,, - ~,,h ~ ~, t Eh,, I c(s,~, so) C(So, s,) | [1, pT] [ c(s~ C(Sl' sl) @ I IL P')/ P')/] 17]/2 3] - .. / 1 P7 C(s I, Sill) 7 @ C(Sll 1, snl) @ L P""Y 3 fl IC(s(I, SIIIIT | [l,"
9819,unknown,"., | [-1 P~l/ C(Si' LP~ 2 J-J Sin) / I Io; 71] (20) Y~aa - J, = The other entries of the matrix are given by symmetry. First calculate the partial covariance matrix for 5.(So). 5~(s~). and 5._(s,). given all other observations, which we will denote 12.. b- Consider the partition I2 =- (19) 9 12b. 2:bhJ where 22.,, is a (3  3) matrix of covariance terms corresponding to 5~(so), 6t(sl), and 5z(sl)."
9820,unknown,"212 If we further decompose Helterbrand and Cressie I Ogll ('012 r 1 I2,.,.b-- 1o921 0)22 o9231 (21) L_ o931 (.-o32 o933d then ( E' 71) O911 = C(So, So) -- (C(So, Sill) T @ [1, p'y]) C(S[n, snl ) @ 03' 9 (C(so, SIll) r @ [1, pT]) T = C(So, So) -- IC(so, SIu)TC(Sul, SlIl)-IC(So, Sill) F 1 1 p~ [1, | [1, p-y] p3'] T 03, 3,- J = C(So, sl)) -- {C(so, Slu)rC(siil, Sul)-IC(so, sul) } (22) Similarly, [O9"
9821,unknown,"Similarly, [O921"" O931] T = (C(So"" Sl) -- C(SI, Sill)re(sill , Sill) It(so, Sill)) @ [1, p-y]T and (23) [ O9~'~ 0323 ] -- = (C(Sl, Sl) -- C(Sl ' Sill)TC(siii, snl)-IC(sl, Sill)) L ~ O933 [ 1 @ (24) 03, ~y- j We can therefore piece together .Y..,,. h- We note that O931 = (C(So"" SI) -- C(SI, Slll)/C(SlI[, SIIl)-lC(so, sIII))P-Y (25) which need not be zero in general. That is, the partial covariance "
9822,unknown,"which need not be zero in general. That is, the partial covariance between 8~(So) and 82(s,), conditioned on all other measurements except 81(Sl), may not be ZerO. Now consider what happens when we also condition on 8~(s,). Let R = coy (8,(So), 82(st)lfl(si), 6(s2) T, 6(s3) T, .... fi(S,,)T), where coy (d, elf) denotes the partial covariance of d and e given f. Then,"
9823,unknown,"Universal Cokriging 213 09210)32 R =- 0)31 - -- (26) 0)22 From our results in the preceding text, 0)31 = (c(so, sl) - C(sl, sm)rc(sH[, stH)-IC(so, s.0)p3' 0)_~1 = (c(s~, sl) - C(sl, siH)rc(sm, siH) IC(so, stH)) 0)32 = (C(SI, SI) -- C(Sl, Slll)/C(S[ll , Sl[l)-IC(Sl , sIII))P'Y 0)22 = (C(Sl, st) - C(si, SIll)TC(sIII 9 SIIl)-If(st , Sill)) and hence R = 0. Thus, by also conditioning on 6~(s~), the pa"
9824,unknown,"and hence R = 0. Thus, by also conditioning on 6~(s~), the partial covariance becomes zero. This appears analogous to the screen effect encountered in uni- variate spatial prediction (see, e.g., Journel and Huijbregts, 1978, p. 346, or Cressie, 1991, p. 133). The screen effect says that the influence of a datum is reduced when it is hidden by another datum. Under our current assumptions, we see th"
9825,unknown,"we see that the secondary datum at a sample location is completely hidden by the corresponding primary datum. UNIVERSAL COKRIGING Recall the decomposition Zi(s) = #i(s) + 6i(s); s ~ D, i = 1, 2 (27) where #i(s) = Xi(s)I~i (28) [5s is a (p~  1) vector of unknown large-scale variation parameters, and 6i(s) represents small-scale variation; i = 1, 2. We have shown in a variety of ways that when [5o "
9826,unknown,"that when [5o is fixed and known, simple cokriging on {6s(sk): k = 1, 2 ..... n: i = 1, 2} yields a predictor for 3j(so) that gives null weightings to the secondary information. Then, the simple cokriging predictor for Z~(so) is Zl(s o) = /zl(s o) + ~l(so) (29) where #l(So) = Xl(so)/P~l ~. The simple cokriging predictor is always unbiased. Now consider the more general universal cokriging case (My"
9827,unknown,"Clark et al., 1987) when [5 is unknown. The large-scale parameter space can be written as f2 = {[5 =- ([5~-, [5~)r: [5 ~ Rt,}, where p = Pl + P2. Recall that we are currently assuming that p~ are large-scale parameters exclusive to the large- scale structure in Z~(-), and I~_~ are exclusive to the large-scale structure in Z2('). That is, there are no existing relationships or restrictions between "
9828,unknown,"To examine the generalized least-squares estimator of [5 concisely, we redefine Z = [Z(, Z~] r, where Z i =- (Z,(sl) ..... Zi(s,,))r; i = 1, 2. This is"
9829,unknown,"214 Helterbrand and Cressie simply a rearrangement of the elements of the original Z vector. We note that the optimal universal cokriging predictor for the vector Z (So) at location So can be written, Z(so) = CTT-'(Z - X~,.~) + X(so)~t~ (30) where Co is the (2n x 2) matrix cov(Z, Z(so) ), T is the (2n x 2n) matrix var (Z), X is a matrix of ""explanatory"" variables, X(so) is the (2 x p) matrix of ex"
9830,unknown,"of explanatory variables at location So, and J]~l.,- = (XTT-IX) -~XrT-IZ is the generalized least-squares estimator of the large-scale parameter vector. (For details, see VerHoef and Cressie, 1993). Under the model given in (3), we have Co = @ c o (31) PT T- 3 and we can rewrite T = | C (32) p'Y ""Y- 3 We note that CTT-~(Z - X~ls) is equivalent to simple cokriging on 6 --- Z - XfIx, Is, where the u"
9831,unknown,"Z - XfIx, Is, where the unknown large-scale parameters are replaced by their generalized least-squares estimators. We have shown that the secondary simple cokriging weights must be zero under the intrinsic coregionalization assumption with all measurements available at the n sample locations. Hence, it is only possible for nonzero secondary cokriging weights to occur when the estimation of large-s"
9832,unknown,"of large-scale parameters is necessary, such as in ordinary and universal co- kriging. To determine when secondary information is used to estimate Pt we con- sider two situations. First assume all explanatory variables are common to both component processes, i.e., Xi(s) = X2(s) -= X*(s); s e D and Pl = P2 = P*. This model is often encountered in geostatistical problems. In this situation, we note "
9833,unknown,"note that the matrix of explanatory variables can be written as X = (I2 | X*), where I k denotes the (k x k) identity matrix and X* is the (n x p*) matrix with i-th row X*(si). The generalized least squares estimator for p can then be written E (I )' ]' ~L, = (I 2 @ X*)r 1 p @ C (I 2 @ X*) PT T- A 9 (I_~ | X*) T ~) C Z (33) PT T"" 3 which reduces to ~gt.,- = [I2 | (X*rC-IX*) Ix*Tc-I]Z (34)"
9834,unknown,"Universal Cokriging 215 Thus we see only the primary observations are used to estimate p~ and only the secondary observations are used to estimate ~2- However, should a restriction be placed on the parameter space that involves parameters from I~ and ~2 jointly, all observations, both primary and secondary, are used to obtain the optimal linear large-scale parameter estimator of ~j. From (30), the"
9835,unknown,"predictor for the primary process is where A T is the first row of CorT -I, and Xt(so) T is the first row of X(so). The first term involves simple cokriging weights which we have shown are zero for the secondary component. The second term involves O~.~/.,- and it is here that nonzero weights on Z2 may enter. For example, if ~l(s) = ~ and ~2(s) = g2, and we have the restriction g~ = ~2, we note tha"
9836,unknown,"= ~2, we note that the full-rank reparameterization will lead to a model with one large-scale parameter, ~ =- ~ = ~2- This restriction might arise if the two processes represent two kinds of unbiased measurements of the same variable. The restricted generalized-least-squares estimator ~ will be a linear weighting of all 2n observations. Thus, in this situation, the secondary information will be us"
9837,unknown,"be used to estimate the mean of the primary process, ~, and so will receive nonzero universal cokriging weights in predicting Z~(s0). Now assume that some of the explanatory variables are unique to one of the component processes and that no restriction is placed on the large-scale parameter space that involves parameters from 1~ and p, jointly. In this case X~(s) ~ X2(s); s ~ D, but it is still tr"
9838,unknown,"estimator will reduce to the form = Z (36) D2 where D t and D2 are n x n matrices. Hence, secondary information will not be used to estimate I]~ and therefore the secondary universal cokriging weights will be zero. We can therefore conclude that when restrictions are placed on the large- scale parameter space that jointly involve parameters in I]~ and ~2, the secondary information will be used to "
9839,unknown,"information will be used to estimate the mean of the primary process and will thus receive non-zero universal cokriging weights in predicting Z~(so). Other- wise, the secondary cokriging weights will be zero. An Additional Primary Observation It is of interest to observe the behavior of the secondary cokriging weights when an additional primary observation, say Z~(s,,+~), is available without Z2(s"
9840,unknown,"216 Helterbrand and Cressie weights can occur when particular restrictions are placed on the large-scale parameter space, we may consider the simple cokriging problem and determine if nonzero secondary cokriging weights can occur when no restrictions are placed on the large-scale parameter space. Again, consider the simple cokriging equations vt Z m=l n Z m = ] {~lmCtl(Sk, Sm) -F ~k2mCi2(Sk, Sin) "
9841,unknown,"= Cll(Sk, SO), k = 1, 2 ..... n + 1 {XlmCI2(Sk, Sin) + ~k2mC22(Sk, Sin) } ""F"" ~'kl. n + I C21(Sk, S n + I) = c21(Sk, SO). k = 1, 2 ..... n (37) By the intrinsic coregionalization assumption, we can express these equations as n ~] h*c(st, s,,) + Xi.n+lc(sk, s,,+ I) = c(s k, So), k = 1, 2 ..... n + 1 m-- 1 n ~,, X,,**c(sk, s,,,) + h I .... iC(Sk, S,,+l) = C(Sk, S0), k = 1,2 ..... n (38) m-- I Again,"
9842,unknown,"m-- I Again, we see that two sets of equations are identical for k = 1, 2 ..... n, which implies h,,* = ~. .... m = 1, 2 ..... n, and hence the secondary cokriging weights are still zero despite the addition of an additional primary' observation. It can also be shown that the partial correlation between 6i(so) and 62(s~) remains zero when we condition additionally on tS~(s,, ~ t). An Additional Se"
9843,unknown,"We may also want to consider what happens to the secondary cokriging weights when an additional secondary observation Z2(s,, + ~) is available without ZI(s,, ~ I). Again, consider the simple cokriging equations n Z {~,l,,,cll(sk, s,,,) + ~,2,,,c~_(sk. s,,,)} + ~2.,,+lcl:(s~, s,,+l) m = I = c II(sk, So), k = 1. 2 ..... n tl {~kl,,,CI2(Sk, Sin) + ~2,,,C22(Sk, Sin)} + ~2 .... IC22(Sk, Sn+l) tit- [ = "
9844,unknown,"Universal Cokriging 217 By the intrinsic coregionalization assumption, we have n ~k~C(Sk, Sin) ""~- ~[p~k2,n+lC(Sk, Sn+l) : C(Sk, S0) , k = 1, 2 ..... n I/t= I II ** 3' X,~ C(Sk, Sm) +--X2,,+IC(Sk, S,,+I) = c(s~,so),k = 1,2 ..... n + 1 m=l ,O (40) We see that, with the inclusion of Z2(s . + i), it is no longer necessary for X,* = ** X,, , m = 1,2 ..... n. Thus, it is no longer necessary for the sec"
9845,unknown,"cokriging weights to be zero. It can also be shown that the partial correlation between 6~(So) and 62(st) need not be zero when we condition additionally on 62(Sn + I)"" AN EXAMPLE--THE NEVADA TEST SITE DATA In 1957, a device containing plutonium was blown apart by chemical ex- plosives at the Area 13 ""safety-shot"" location on the Nevada Test Site (NTS). This experiment was performed partly to test"
9846,unknown,"This experiment was performed partly to test for ""safety"" against fission re- actions in an accident situation involving an atomic weapon. A consequence of the test was the contamination of the immediate surrounding desert soil and vegetation with plutonium (Pu) and americium (Am). In 1971, the Nevada Applied Ecology Group (NAEG) began conducting environmental transuranic studies in this area by t"
9847,unknown,"studies in this area by taking field instrument surveys and collecting soil, veg- etation, and animal tissue samples. One goal of these studies was to predict the total amount and spatial distribution of 239'24~ and 24~Am in surface soil. The Pu concentrations (in/~Ci/m 2) were determined by wet chemistry on surface (top 5 cm) soil samples taken at random locations. The Am concentra- tions in surf"
9848,unknown,"tions in surface soil was obtained from Field Instrument for the Detection of Low Energy Radiation (FIDLER) readings (in 10 3 counts per minute (cpm)) at one foot above the surface. In this paper, we use a subset of the data accumulated in Area 13, considering 104 sample locations where measurements are available on both components (Gilbert, 1978). A map of these sample locations and ground zero i"
9849,unknown,"ground zero is displayed in Fig. 1. Previous analyses on the Area 13 data have concluded that there is a good overall correlation, on the log scale, between wet chemistry Pu analyses and Am FIDLER measurements (Church et al., 1975; Gilbert and Simpson, 1985). To consider the effects of cokriging with an in- trinsic coregionalization model, a restricted large-scale structure between com- ponents, a"
9850,unknown,"ponents, and measurements on both components at all sample locations, we consider the prediction of log Pu at unsampled locations based on the measure- ments at the locations displayed in Fig. 1. A prediction is made for each of 120 unsampled grid sites in the interior of the rectangular region outlined in Fig. 1."
9851,unknown,218 Helterbrand and Cressie : ~ .... g 720000 720500 721000 721500 722000 722500 723000 Easl (Feet) Fig. 1. The sample locations in the inner-fence region of Area 13 where measurements on both Pu and Am are available. The axes are in Nevada feet coordinates and the cross (X) denotes ground zero. Cokriging is very useful in the under-sampled problem where there are relatively few primary measuremen
9852,unknown,"relatively few primary measurements as compared to secondary measurements (see, e.g., Stein and Corsten, 1991; and Zhang et al., 1992). Indeed, in the NTS study, many more FIDLER readings were taken because the cost of a FIDLER reading was approximately 50 times less than that of a Pu analysis on a soil sample (Gilbert, 1978). However, in this intrinsic coregionalization study, we are only interes"
9853,unknown,"we are only interested in the effects of cokriging when measurements are avail- able for both components at all sample locations. Figure 2 displays a bivariate ray-glyph map of the raw log Pu and log Am measurements (Carr et al., 1992). The rays pointing to the right represent log Pu concentration trends and rays pointing to the left represent log Am concen- tration trends. The bivariate rap map i"
9854,unknown,"tration trends. The bivariate rap map is an effective technique for showing bivariate associations. The two rays at each location in Fig. 2 generally point down or up together, clearly indicating that the components are positively cor- related. To examine the effects of cokriging under intrinsic coregionalization and a restricted large-scale structure, the following spatial model will be considere"
9855,unknown,"for Zt -= log Pu, and ~ -= log Am, at spatial locations s E D. Assume Z,(s) = #, + /3 exp <-IIs - s~.ll/0) + a,(s) KZ2(s) = #2 + /3 exp (-IIs - s~=ll/0) + a2(s) (41) where sg: denotes ground zero, K and 0 are parameters that are assumed known, #l, #2, and ~ are unknown large-scale parameters, and ~51(s) and 6=(s) are as-"
9856,unknown,"Universal Cokriging 219 8 Q Observed Log Data Log Am Lo~Pu ~"" 6 85 '~ 970 5 22 ""e"" 708 ~,- 360 -8- 445 197 ~. 1 83 0 34 Q -80 -~ 4)- J& Ground Zero Q i i i i i i [ 720000 720500 721000 721500 722000 722500 723000 East (Nevada Feet Coordinales) Fig. 2. A bivariate ray-glyph map of the log Pu and Am measurements. Pu is measured in /,tCi/m-', and Am is measured in 10 3 counts per minute (cpm). sumed "
9857,unknown,"(cpm). sumed to be second-order stationary processes that are spatially cross-correlated; i.e., the vector 6(s) =- [61(s), 62(s)] T is a bivariate second-order stationary process. The large-scale structure for both components in Eq. (41) was selected to model the observed exponential decline away from ground zero. The parameter [3 is assumed to be shared by the two large-scale models. The paramete"
9858,unknown,"to transform the log Am data to an equivalent scale with log Pu. Though we assume this parameter is known, we actually estimated K externally by regressing log Pu on log Am, based on all 104 observations. Similarly, the exponential scale parameter 0 was estimated externally (for fixed K) using non-linear least squares. The sample variance of these estimates were relatively small and thus we consid"
9859,unknown,we consider K = 1.25 and 0 = 1880 fixed for the following analysis. The ordinary-least-squares residuals from the large-scale models above were used to assist in modeling an isotropic small-scale spatial dependence structure. The experimental (cross-) semivariograms were estimated using the robust fourth- root estimator proposed by Cressie and Hawkins (1980). (The derivation of the fourth-root cro
9860,unknown,fourth-root cross-variogram estimator is exactly as that for the variogram esti- mator under proper standardization.) In this example we chose to fit an isotropic exponential intrinsic coregion- alization model to the experimental (cross-) semivariograms. A bivariate gen- eralization of the univariate non-linear weighted-least-squares criterion of Cres- sie (1985) was used to obtain estimates for 
9861,unknown,"sie (1985) was used to obtain estimates for the parameters of the exponential variogram model. Since variograms and cross-variograms are interrelated, pa- rameter estimation requires simultaneous consideration of the experimental var-"
9862,unknown,"221111 Helterbrand and ('ressie iograms and cross-variograms (Heherhrand and Cressie, 1994). The multivariate spatial dependence stnwture (in tenns of covariograms) was estimated as 2.079 1.614 ( 1 ) C(sa, s,,,) = exp - Ils, - s,,,ll (42) 1.614 2.290 ~7~ The fit of this ',Mid model to the experimental (cross-) semivadograms is dis- played (standardized) in a matrix display in Fig. 3. The semivario"
9863,unknown,"played (standardized) in a matrix display in Fig. 3. The semivariograms for Pu and Am appear on the diagonal, with the cross-semivariogram displayed off the diagonal. Notice that the intrinsic coregionalizalion model does not have a nug- gcI c11~'c11 and flails to lit the cxperimental variogram for the primary component (Pu) at the shorter lags. When a nugget cfllect parameter is included for the "
9864,unknown,"variogram model, the estimalled bivariate spatial dependence model is 0.834 1.330 1.330 2.290 1.928 1.330 1.330 2.29(/ (' ) e -340 - its~ s,,, i|"" S,{ : Sm (43) This model is n~ longer of an intrinsic coregionalization Ionn. but provides a satisfactory fit to the experimental variograms (Fig. 4). To compare the gain in :~recision of cokriging relatiw; to kriging, the pre- I C, "". : .~-.7 ~ . ..~'"
9865,unknown,"q, ! -- 0 0 Z 2~i 4qO C,C') i)Ch FO~A 0 200 400 600 800 Feel --i i 1 ,, G 5 ' . 9 "" I] , ,,I ,Ii10 i_(;O i-;( Ii / C, 5 .---,.... r~ ,. Ii. r, _ ,o .m() ~300 8oo Fc,_.t Feel Fi~. 3. ]'he lit Lfl the cslhnutcd c~,poncnlial inlrmsh.' cnrugnmalilafinn model Io the r JcIOs~. - ) SCIlli~.;.IrioIZRIIll~."
9866,unknown,"Universal Cokrigin~ 221 oo -- 9 I O5 ""' 05 t O0 0 200 400 600 800 Feet B 0 200 400 600 800 Feet J O0 .~ ... ~ ] J n 200 400 600 800 0 200 400 600 800 Feel Feet Fig. 4. The fit of the exponential scnfi,.,ariogram model, ~i111 a nugget cfl~""..'l Ik~r the prima D component, to the expenmcnlal Icros~ I semi'.ario gr~tms. diction error variance was calculated fl~r the 120 prediction locations based on"
9867,unknown,"gr~tms. diction error variance was calculated fl~r the 120 prediction locations based on the universal kriging and cokriging predictors, Z'b,u~: and Zl,~,{v,-, respectively. The relative efficiency is calculated as var (Zl(sll) - 2i.u~-(S.)) (4-4.) var (Zl(s .) - Zj,u(-,~-(s.)) A kriging neighborhood with a search window of radius 560 feet was used for prediction. I[ more than 12 sample locations "
9868,unknown,"prediction. I[ more than 12 sample locations fell in the search window, only the measurements from the 12 nearest locations were used. If fewer than ten loca- tions fell in the search window, the search window was expanded so that each prediction was based on measurements from its nearest ten neighbors (e.g., Harper et al., 1988). Kriging neighborhoods are primarily used to reduce the computationa"
9869,unknown,"computational burden demanded by kriging, although Joumel and Rossi (1989) also point out that kriging neighborhoods can be used to protect the user lrom local large-scale model misspecification. Figures 5 through 7 display the kriging, cokriging, and cokriging variance maps, respectively, based on the fitted exponential intrinsic coregionalization model given in Eq. (42). Though non-zero secondar"
9870,unknown,"model given in Eq. (42). Though non-zero secondary weights occur and the kriging and cokriging predictions differ, the reduction in the prediction error variance is minimal. The mean relative efficiency for the 120 prediction locations is 1.004, with a maximum of 1.031 lk-~r one prediction location. This minimal reduction is not surprising when one notes that the prediction error variance"
9871,unknown,"222 Helterbrand and Cressie ~ o z ~ / s I ~ff 9 ~ . 9 - "" ~ i f , J , i i 720400 720600 720800 721000 721200 721400 721600 East (Feel) Fig. 5. The kriging map based on the exponential intrinsic coregionalization model. Contour units are in log gCi/m 2. The cross (X) denotes ground zero, u ~ o z / i i i i , i 720400 720600 720800 721000 721200 721400 721600 Easl (Feel) Fig. 6. The cokriging map bas"
9872,unknown,"Easl (Feel) Fig. 6. The cokriging map based on the exponential intrinsic coregional- ization model. Contour units are in log /~Ci/m 2. The cross (X) denotes ground zero. matrix corresponding to Eq. (30) can be decomposed into the sum of two terms, var (Z(so) - CorT-'Z), and vat [(X(so) - CoTT-IX)~gt}. (For this example, P =- (tzl, ~2, ilL) Under an intrinsic coregionalization model with measuremen"
9873,unknown,"on both components at all sample locations, CorT -' = 12 | corC-', and the first variance term is the same for both the kriging and cokriging predictor of"
9874,unknown,Universal Cokriging 223 A LL v g i i i i t i i 720400 720600 720800 721000 721200 721400 721600 Easl (Feel) Fig. 7. The cokriging variance map based on the exponential intrinsic co- regJonalizatJon model. Contour units are in (log /~Ci/m2) 2. The cross (X) denotes ground zero. u_ o ~ o z m i i J i i ~ i 720400 720600 720800 721000 721200 721400 721600 Easl (Feel) Fig. 8. The kriging variance map b
9875,unknown,"Easl (Feel) Fig. 8. The kriging variance map based on the exponential variogram model with a nugget effect for Pu. Contour units are in (log #Ci/m2f. The cross (X) denotes ground zero. the primary [secondary] component. Thus, the reduction in the prediction error variance due to cokriging is from the second variance term, so that any additional efficiency attained by cokriging is due to the additi"
9876,unknown,224 Helterbrand and Cressie Y. o i i i i i ~ i 720400 720600 720800 721000 721200 721400 721600 Easl (Feet) Fig. 9. The cokriging variance map based on the exponential variogram model with a nugget effect for Pu. Contour units are in dog/~Ci/m-')-'. The cross (X) denotes ground zero. Recall that the intrinsic coregionalization model fails to fit the experimental variogram for the primary component
9877,unknown,"variogram for the primary component (Pu) at the shorter lags. Figures 8 and 9 display, respectively, the kriging variance and cokriging variance maps based on the fitted exponential variogram model with a nugget effect for the Pu var- iogram. For this model, the first term of the prediction error variance is not the same for kriging and cokriging. The mean relative efficiency for the 120 pre- dict"
9878,unknown,"diction locations under this more appropriate spatial dependence model is 1.072, with a minimum of 1.013 and maximum of 1.214. Thus, for this example, a reduction of up to 17.6% in the prediction error variance is obtained using cokriging even when measurements are available on both components at all sample locations. CONCLUSION We have shown that, under the assumption of intrinsic coregionalizati"
9879,unknown,"with observations available on both components at each sample location, the large-scale parameter space for uniform unbiasedness determines the allowable values for the secondary cokriging weights. If we require uniform unbiasedness on a parameter space where the large-scale parameters for the primary mean and the large-scale parameters for the secondary mean are restricted to depend on each other"
9880,unknown,"on each other in some manner, the secondary information will be used in esti- mating primary large-scale parameters and thus nonzero secondary cokriging weights will occur. However, the reduction in the prediction error variance may"
9881,unknown,"Universal Cokriging 225 be minimal. The results here also extend to the m-variate (where m > 2) cokriging problem when we assume m-variate intrinsic coregionalization. We began by arguing that the intrinsic coregionalization assumption is restrictive and often inappropriate. Realistic multivariate spatial statistical models are typically more complex than intrinsic coregionalization, resulting in "
9882,unknown,"multivariate universal cokriging predictors (Eq. 30). In general, secondary in- formation is crucial in determining optimal predictors. ACKNOWLEDGMENTS This research was supported by the National Science Foundation (DMS- 9001862 and DMS-9204521), the National Security Agency (MDA904-92-H- 3021), and an Iowa State University Research Grant (Carver Grant). The authors are grateful to Richard O. Gilb"
9883,unknown,"are grateful to Richard O. Gilbert of Battelle, Pacific Northwest Laboratories and the Nevada Field Office of the U.S. Department of Energy for use of the Nevada Test Site data. REFERENCES Carr, D., OIsen, A., and White, D., 1992, Hexagon mosaic maps for display of univariate and bivariate geographical data: Cartogr. Geogr. Inf. Syst., v. 19, p. 228-236. Church, B., Medling, E., and Brady, D., 197"
9884,unknown,"Church, B., Medling, E., and Brady, D., 1975, A different look at area 13 [=IDLER survey data, in M. White and P, Dunaway (Eds.), The Radioecology of Plutonium and Other Transuranies in Desert Environments: USERDA Report, NVO-153, p. 231-235~ Clark, 1., Basinger, K., and Harper, W., 1987, MUCK--A novel approach to cokriging, in B. Buxton (Ed.), Proceedings of the Conference on Geostatstical, Sensi"
9885,unknown,"Buxton (Ed.), Proceedings of the Conference on Geostatstical, Sensitivit3,, and Uncertainty Methods for Ground-Water Flow and Radionuclide Transport Modeling: Battelle Press, Co- lumbus, p. 473~493. Cressie, N., 1991, Statistics for Spatial Data: Wiley, New York. Cressie, N., 1985, Fitting variogram models by weighted least squares: Math. Geol., v. 17, p. 563-586. Cressie, N., and Hawkins, D., 198"
9886,unknown,"p. 115-125. Gilbert, R., 1978, On the estimation of spatial pattern for environment contaminants, in M. White and P. Dunaway (Eds.), Selected Environmental Plutonium Research Reports of the NAEG: Nevada Applied Ecology Group, U.S. Department of Energy, Las Vegas, Nevada, p. 319- 360. Gilbert, R., and Simpson, J., 1985. Kriging for estimating spatial pattern of contaminants: Potential and problems:"
9887,unknown,"and problems: Env. Monitor. Assess., v. 5, p. 113-135. Harper, W., Basinger, K., and Fun-, J., 1988, Geostatistical analysis of potentiometric data in the Pennsylvanian aquifer of the Palo Duro Basin, Texas: Tech. Rep BMI/ONW1-680, Office of Nuclear Waste Isolation, Battelle Memorial Institute, Hereford, Texas. Helterbrand, J., and Cressie, N., 1994, Models and inference for multivariate spatial p"
9888,unknown,"preparation. Joumel, A., and Huijbregts, C., 1978, Mining Geostatistics: Academic Press, London. Joumel, A., and Rossi, M., 1989, When do we need a trend model in kriging? Math. Geol., v. 21, p. 715-739."
9889,unknown,"226 Helterbrand and Cressie Myers, D., 1982, Matrix formulation of co-kriging: Math. Geol., v. 14, p. 249-257. Stein. A., and Corsten, L., 1991, Universal kriging and cokriging as a regression procedure: Biometrics, v. 47, p, 575-587. Ver Hoef, J., and Cressie, N., 1993. Multivariable spatial prediction: Math. Geol,. v. 25, p. 219- 240. Zhang. R., Yates, S., and Shouse, P,, 1992, Prediction of soi"
9890,unknown,"Zhang. R., Yates, S., and Shouse, P,, 1992, Prediction of soil salinity using cokriging with non- symmetric pseudo-cross-variograms, in First Conference of the Working Group on Pedometrics of the International Society of Soil Science: Pedometrics-92: Developments in Spatial Statistics for Soil Science, International Agricultural Centre, Wageningen, The Netherlands, p. 145- 166."
