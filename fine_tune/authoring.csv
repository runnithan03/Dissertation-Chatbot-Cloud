question_id,question,chunk_ids,answer
q1,What is Multiple-Response Regression?,,Multiple-Response Regression models several responses against a set of predictor variables while considering response intercorrelations.
q2,What is an equity fund?,,An equity fund is a pooled investment that puts money predominantly in stocks listed on major exchanges.[1]
q3,What is return on equity?,,ROE is a measure of an equity fund’s profitability and how efficiently it generates those profits.[3] It is calculated by dividing net income by shareholders’ equity.[3]
q4,What is the sustainability score?,,"The sustainability score measures an equity fund’s sustainability performance and how effectively it mitigates environmental, social, and governance (ESG) risks across its portfolio."
q5,How was imputation carried out on the equity fund dataset?,,Random forest imputation was used to fill in the missing categorical and ordinal data. Linear model imputation was used to fill in missing numerical features because it uses the relationships between variables rather than relying on simpler approaches such as mean imputation.
q6,What is Mardia's Test for Multivariate Normality?,,"Mardia’s test assesses multivariate skewness and kurtosis. The null and alternative hypotheses of Mardia’s test are that the responses are and are not multivariate normally distributed, respectively. Therefore, if either the kurtosis or skewness statistic deviates significantly from their expected values under multivariate normality, then the null hypothesis is rejected.[8]"
q7,What is single-response regression?,,Single-Response Regression models one response against a set of predictor variables while considering response intercorrelations.
q8,What assumptions does single-response linear regression have?,,"They are linearity, homoscedasticity, normality, and independence of its variables."
q9,Why can single-response linear regression not be extended to cover multiple correlated responses?,,"If this method were extended to model m responses using m independent single-response regression models, it would ignore the covariance between responses, leading to sub-optimal predictions when responses are correlated."
q10,What is the difference between single-response linear regression and multiple-response linear regression?,,"The first key difference between MRLR and SRLR lies in the structures of the responses, coefficients and errors. These are now matrices rather than vectors, reflecting that the responses are modelled together. The second key difference between SRLR and MRLR lies in the structure of the error terms.
MRLR allows for correlation between errors across different responses within the same observation."
q11,What assumptions does multiple-response linear regression have?,,"It comes with a few underlying assumptions, which can be extended from SRLR. However now there is a covariance matrix of errors."
q12,What is forward stepwise selection?,,"Forward stepwise selection starts with a model having no predictors and adds the predictor that produces the largest improvement in model fit. Model fit is usually determined using a selection criterion, for example, the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), or adjusted R2 . At each step, the variable that contributes most significantly to this criterion is added."
q13,What is backward stepwise selection?,,"Backward stepwise selection begins with the full model as the initial model, and successively removes the predictor that makes the smallest contribution to the model according to the same criterion."
q14,What is bi-directional stepwise selection?,,"Bi-directional stepwise selection incorporates both forward and backward stepwise selection, with inclusion or exclusion of predictors at every step based on their impact on the model fit as a whole."
q15,What is Analysis of Variance (ANOVA)?,,ANOVA primarily analyses the differences among group means and their associated variances. It helps determine whether there is a significant difference between the means of different groups by comparing the variance within groups to the variance between groups.
q16,What is Sequential Analysis of Variance (ANOVA)?,,Sequential ANOVA is a type of ANOVA which decomposes the regression component of an ANOVA table by considering a sequence of m nested models. This approach allows an assessment of how the unexplained variation in the response changes as predictors are added to the model one at a time.
q17,What is Multiple Analysis of Variance?,,Multivariate Analysis of Variance (MANOVA) extends ANOVA to multiple responses. It comes with the assumption that the responses need to have a multivariate normal distribution.
q18,Why is Multiple Analysis of Variance suitable for multiple-response regression?,,MANOVA is suitable for multiple-response regression because the total SSCP matrix is an unscaled version of the sample response covariance matrix
q19,How does the Wilks' Lambda Test work?,,"The Wilks' Lambda test measures the proportion of variance not explained by the independent variables. A small Λ (close to 0) indicates the predictors explain a large proportion of response variation, and a
large Λ (close to 1) indicates the predictors explain a small proportion of response variation."
q20,What is Sequential Multiple Analysis of Variance (ANOVA)?,,"Sequential MANOVA extends sequential ANOVA to cover multiple responses. It works the same as sequential ANOVA, but at each step, the Wilks’ Lambda value (Λj ) is computed for each model."
q21,What is the Average Normalised Root Mean Squared Error?,,"While the root mean square error (RMSE) measures model error for each response variable separately, it retains the original units, making direct comparisons difficult. In contrast, the normalised RMSE (NRMSE) scales the error by the standard deviation of each response, returning a unitless and comparable measure of model performance. Since two response variables were considered, the NRMSE was computed for each and then averaged to obtain a single summary statistic, the ANRMSE."
q22,How is the Average Normalised Root Mean Squared Error measured?,,"The ANRMSE gives the proportion of natural variability in the responses not accounted for by the model. An excellent model has a 0-0.5 ANRMSE, a good model has a 0.5-0.6 ANRMSE, a satisfactory model has a 0.6-0.7 ANRMSE, and an unsatisfactory model has an ANRMSE greater than 0.7."
q23,What is Ridge Regression?,,"Ridge regression shrinks estimated regression coefficients towards zero by minimising the following penalised loss function: L(β) = ∥Y − Xβ∥_2^2 + λ∥β∥_2^2 ,"
q24,When is Ridge Regression useful?,,"Ridge regression is particularly useful when the predictors exhibit high multicollinearity or when the number of predictors, p, is close to or exceeds the number of observations, n."
q25,What does the tuning parameter do in Shrinkage Methods?,,"The tuning parameter is crucial as it controls the strength of the penalty, balancing the trade-off between model fit and coefficient magnitude. Therefore, as λ increases, the penalty term gets larger, so to minimise the entire loss function, the regression coefficients get smaller."
q26,How is the Ridge Regression cost function solved?,,"Ridge regression minimises the cost function, and this can be shown by differentiating the cost
function with respect to β and setting it equal to zero. This gives the ridge regression closed-form
coefficient estimator."
q27,How does Lasso Regression work?,,"Lasso regression removes irrelevant features by shrinking their coefficients exactly to zero. Instead of penalising the squared magnitude of the coefficients like ridge, lasso penalises their absolute values. It shares the same aim of minimising a loss function as ridge, but with a different penalty term: L(β) = ∥Y − Xβ∥_2^2 + λ∥β∥_1"
q28,How is the Lasso cost function solved?,,"Unlike ridge regression, lasso has no closed-form solution because the ℓ1-norm penalty, ∥β∥1, is not differentiable at zero. This makes the optimisation problem non-smooth and means lasso requires iterative algorithms, such as coordinate descent, to solve it."
q29,How does Ridge Regression extend to multiple-responses?,,"Ridge regression extends to multiple responses by adding a Frobenius norm penalty to the coefficient matrix. 
In single-response ridge regression, the penalty is given by the squared ℓ2-norm of the coefficient
vector. See Equation 4.1. However, since the standard ℓ2-norm applies only to vectors, it cannot
directly penalise all the entries in the coefficient matrix. The Frobenius norm replaces it because it
extends the ℓ2-norm to matrices, summing the squared entries across all rows and columns in B."
q30,How does Lasso Regression extend to multiple-response regression?,,"Multivariate Lasso also uses coordinate descent to solve its optimisation problem. However, this slightly changes because now there is a coefficient matrix to estimate rather than a coefficient vector."
q31,What is Reduced Rank Ridge Regression?,,Reduced rank ridge regression (RRRR) builds upon this by modifying the estimation of the coefficient matrix using multiple-response ridge regression and a rank constraint.
q32,What is Multivariate Regression with Covariance Estimation?,,Multivariate Regression with Covariance Estimation (MRCE) builds upon multiple-response linear regression by jointly estimating the regression coefficient matrix B and the inverse error covariance matrix Ω.
q33,Why does Multivariate Response with Covariance Estimation qualify for multiple-response regression?,,"Since multivariate response with covariance estimation estimates both the coefficient matrix and inverse matrix simultaneously, the coefficient matrix is optimised considering response covariance. This reasoning follows from the explanation for why MRLR qualifies as an
multiple-response regression."
q34,What are random forests?,,"Random forests are tree-based models built upon classification and regression trees (CARTs) as a building block. A CART partitions the training data into groups with similar response values, and then predicts the same category for all data within a given subgroup."
q35,What are Classification and Regression Trees (CARTs) built up off?,,"The CART starts with a root node containing all the objects, and is then divided into “leaf” nodes by recursive binary splitting.[34] This recursive splitting means that CARTs are able to detect patterns that occur only within specific regions of the predictor space, including non-linear relationships and interactions that other models may miss. Each leaf node relates to a hyper-rectangle in the feature space, Rj , j = {1, . . . , J}. In other words, the feature space defined by X1, X2, . . . , Xp is split into J distinct regions which do not overlap."
q36,How do Classification and Regression Trees (CARTs) grow?,,"To grow the tree, CARTs use a greedy fitting algorithm, which means it selects the best split at each step by evaluating all possible features and thresholds. This decision is made locally and recursively, without considering future splits. The algorithm chooses
the split that minimises the RSS."
q37,What is pruning and why is it necessary?,,"The greedy fitting algorithm provides good predictions for the training data but may overfit said data, resulting in poor performance on the test dataset. However, pruning can mitigate this issue. Pruning involves removing branches that contribute little to the tree’s prediction accuracy to control the bias–variance trade-off. This is either based on a validation set or a complexity penalty. A common approach to pruning is the weakest link pruning algorithm"
q38,What is bootstrap aggregating?,,"Bootstrap aggregating, also known as bagging, improves the stability and accuracy of CART algorithms by averaging models. Bagging helps reduce variance and avoid over-fitting, but the model becomes more challenging to interpret"
q39,What are the advantages and disadvantages of bagging?,,"Bagging is advantageous when there is a large amount of data, as is the case here for the equity fund dataset. This is because the empirical distribution will be closer to the actual underlying population’s distribution. Additionally, it is the best option when the aim is to minimise the variance of a predictor. However, bagging also brings some disadvantages. Firstly, interpretability is lost as the final estimate is not a tree. Secondly, it is an ensemble prediction, i.e. multiple trees are combined to make the final prediction.[35] Also, bagged trees are inherently correlated. This is problematic because the more correlated the random variables are, the smaller the variance reduction of their average, which can undermine one of the key advantages of bagging."
q40,What is the Random Forests Algorithm?,,"The Random Forests algorithm is used in the following way: a bootstrap resample (y ∗ i , x∗ i ) n i=1 is taken of the training data, like for bagging. Then, the tree is built and each time a split in a tree is considered, m predictors are randomly selected out of the full set of p predictors as split candidates, and the best split within those m predictors is found. Finally, the first two steps are repeated, averaging the prediction of all the regression forest trees."
q41,What are out-of-bag samples?,,The observations that were not included in the bootstrap sample used to train that tree.
q42,What are Multivariate Regression Trees?,,Multivariate Regression Trees (MRTs) generalise Classification and Regression Trees by modelling all responses simultaneously and hence accounting for multiple correlated responses.[38]
q43,How are Classification and Regression Trees typically pruned?,,"Therefore, trees are pruned using CV, typically selecting the smallest tree within one standard error of the minimum cross-validated error.[39] This is known as the 1-standard error rule."
q44,What is Covariance Regression with Random Forests?,,"Covariance Regression with Random Forests (CRRFs) extend MRFs by incorporating a splitting criterion, which maximises differences in sample covariance estimates between child nodes."
q45,What is the Bag of Observations for Prediction (BOP)?,,"For a new observation, nearest neighbour observations are used to estimate the response mean and final covariance matrix, ensuring both the predicted value and its uncertainty are captured.[41]"
q46,How does Gradient Tree Boosting work?,,"Gradient tree boosting builds a model by sequentially adding regression trees to minimise a predefined loss function. Each new tree is trained on the negative gradient of the current model, enabling the ensemble to iteratively refine its predictions.[50]"
q47,What is Extreme Gradient Boosting?,,"Extreme Gradient Boosting, otherwise known as XGBoost, improves gradient boosting by including regularisation explicitly in the loss function to address over-fitting and improve model predictions."
q48,What is Cholesky decomposition?,,"To ensure positive definiteness of the response covariance matrix, ΣY, a common and efficient approach is to use the Cholesky decomposition, which factorises the matrix as follows: ΣY = LL^⊤, where L ∈ R D×D is a lower triangular matrix.[51]"
q49,Why does Cholesky Decomposition work with Extreme Gradient Boosting?,,"Cholesky decomposition can be used to remove intercorrelation between response variables by applying the transformation L −1Y⊤, which produces decorrelated responses.[52] This decorrelation allows XGBoost to be applied independently to each transformed response. After prediction, the original correlated response space is recovered by reapplying L, which reintroduces the covariance structure."
q50,How did each model perform?,,"The MRLR model using all predictors performed poorly, and all the stepwise selection methods (forwards, backwards and bi-directional) failed to improve performance significantly. Backward selection performed the worst, suggesting that eliminating variables here led to worse model performance. However, the introduction of interaction terms, alongside bi-directional stepwise selection, led to a huge improvement, which resulted in a “Good” model fit. Similarly, the addition of non-linear terms, also selected through bi-directional stepwise selection, improved upon the MRLR model (with just the standard predictors), and achieved a “Satisfactory” fit. The shrinkage methods aimed to improve regression stability by using penalisation techniques. However, the MRCE and RRRR models, with the regular features, performed similarly to their counterparts in standard MRLR, again returning an “Unsatisfactory” fit. Introducing polynomial terms in MRCE improved performance, but for RRRR, this was less effective. While adding interaction terms improved MRCE, but did not improve RRRR as much. The performance of shrinkage methods was also noteworthy. This is because, despite being specifically designed to handle multiple responses, particularly under multicollinearity or high-dimensional settings, both MRCE and RRRR performed as poorly as, or even worse than, the MRLR models. This can be attributed to the pre-processing of the dataset: after pre-processing (see Chapter 2), the predictors were not highly correlated, and the data was not high-dimensional (p = 14, n = 1269). In this case, the advantages that regularisation and rank reduction bring are less impactful. Tree-based models significantly improved upon linear and shrinkage-based methods. The CRRF model was significantly better than previous models and was evaluated as a “Good” fit model. Meanwhile, the XGBoost gave further improvements, being the only “Excellent” model. The strong performance of XGBoost and CRRF, particularly in comparison to the limited performance of the models in Chapters 3 and 4 with only the given features, indicates that the equity fund responses, sustainability score and ROE, are driven more by conditional and non-linear interactions among predictors. This notion is further supported by the improved performance of every model in Chapters 3 and 4 with interaction or non-linear terms introduced. Mardia’s test also supports this through its borderline p-values of 0.0732 for skewness and 0.0862 for kurtosis (see Subsection 2.3.5 for further details). While these values are not low enough to reject the null hypothesis of multivariate normality at standard significance levels, they do indicate a potential deviation from normality. This means that the response structure is unlikely to be adequately captured by linear models with additive assumptions (like MRLR, MRCE and RRRR), thereby justifying the use of more flexible models (like Cholesky-Decomposed XGBoost and CRRFs), that can consider extra predictor effects. Overall, the findings suggest that the most effective modelling approaches are those that accommodate flexible, non-linear and interaction effects, as opposed to approaches based on rigid parametric or distributional assumptions."
