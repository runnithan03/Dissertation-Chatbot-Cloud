question_id,question,chunk_ids,answer,scores,chunk_sources,chunk_preview,needs_review
q1,What is Multiple-Response Regression?,3940;3970;29,Multiple-Response Regression models several responses against a set of predictor variables while considering response intercorrelations.,0.686;0.597;0.567,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Chapter 7.7.pdf,[3940] Chapter 1 Introduction 1.1 Background 1.1.1 Multiple-Response Regression Single-response regression models one response against a set of predictor variables. Th… || [3970] 3.1.2 Multiple-Response Linear Regression Multiple-response linear regression (MRLR) is the most basic model in MRR. Its general form is: Y(n×m) = X(n×p) B(p×m)… || [29] Multivariate Multiple Regression Predictions from Multivariate Multiple Regressions Suppose the model with normal errors has been fit and checked for any inadeq…,no
q2,What is an equity fund?,3941;3952;4110,An equity fund is a pooled investment that puts money predominantly in stocks listed on major exchanges.[1],0.781;0.578;0.523,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[3941] An equity fund is a pooled investment that puts money predominantly in stocks listed on major exchanges.1 They provide investors with several benefits, such as … || [3952] management fees was removed because it is a significant component of ongoing cost. 2.2 Variable Explanation category gives the type of each equity fund. The cle… || [4110] with independent observations, multivariate normality between responses, full rank in the predictor matrix, and a moderate degree of correlation between respons…",no
q3,What is return on equity?,3789;3941;3955,ROE is a measure of an equity fund’s profitability and how efficiently it generates those profits.[3] It is calculated by dividing net income by shareholders’ equity.[3],0.592;0.465;0.452,Profitability and Sustainability.pdf;Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[3789] shown in Table 1. The main statistical features of the firms considered are summed up in Table2. The definitions of the variables used is provided below:  Retu… || [3941] An equity fund is a pooled investment that puts money predominantly in stocks listed on major exchanges.1 They provide investors with several benefits, such as … || [3955] eventually.12 Therefore, a lower equity style score means the stock is more of a value stock, and a higher equity style score indicates more of a growth stock. …",no
q4,What is the sustainability score?,3943,"The sustainability score measures an equity fund’s sustainability performance and how effectively it mitigates environmental, social, and governance (ESG) risks across its portfolio.",1,Raul_Unnithan_Dissertation.pdf,"[3943] CHAPTER 1. INTRODUCTION distribution costs.4 The higher the ROE, the more efficiently an equity fund is generating income from its stocks it has invested in. 5 …",no
q5,How was imputation carried out on the equity fund dataset?,4111;3950;3899,Random forest imputation was used to fill in the missing categorical and ordinal data. Linear model imputation was used to fill in missing numerical features because it uses the relationships between variables rather than relying on simpler approaches such as mean imputation.,0.615;0.571;0.589,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Random Forest Imputation.pdf,"[4111] CHAPTER 7. CONCLUSION Imputing missing data presented another challenge. In this report, linear model imputation was used for numerical variables and random for… || [3950] Chapter 2 Exploratory Data Analysis This chapter prepares the equity fund dataset for modelling. It begins by describing the master dataset and the initial prep… || [3899] 25% across observations. MAR was introduced by setting the probability of X being missing to 25% according to a standard right-tailed logistic function in Y, th…",no
q6,What is Mardia's Test for Multivariate Normality?,4129;2489;3963,"Mardia’s test assesses multivariate skewness and kurtosis. The null and alternative hypotheses of Mardia’s test are that the responses are and are not multivariate normally distributed, respectively. Therefore, if either the kurtosis or skewness statistic deviates significantly from their expected values under multivariate normality, then the null hypothesis is rejected.[8]",0.709;0.708;0.606,Raul_Unnithan_Dissertation.pdf;Mardia's Test for MVN.pdf;Raul_Unnithan_Dissertation.pdf,"[4129] APPENDIX A. APPENDICES Finally, apply the Pearson Correlation Formula. The Pearson correlation coefficient r is given by r = P(xi − ¯x)(yi − ¯y)pP(xi − ¯x)2 P(y… || [2489] Psychology Science, Volume 46, 2004 (2), p. 243-258 Testing the assumption of multivariate normality ALEXANDER VON EYE 1 , G. ANNE BOGAT Abstract Methods of ass… || [3963] CHAPTER 2. EXPLORATORY DATA ANALYSIS Finally, this left a cleaned equity fund dataset with 1248 rows, each with individual equity fund information. In addition,…",no
q7,What is single-response regression?,3940;3970;3966,Single-Response Regression models one response against a set of predictor variables while considering response intercorrelations.,0.629;0.524;0.515,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[3940] Chapter 1 Introduction 1.1 Background 1.1.1 Multiple-Response Regression Single-response regression models one response against a set of predictor variables. Th… || [3970] 3.1.2 Multiple-Response Linear Regression Multiple-response linear regression (MRLR) is the most basic model in MRR. Its general form is: Y(n×m) = X(n×p) B(p×m)… || [3966] Chapter 3 Linear Regression This chapter introduces the methodology of the first MRR model, multiple-response linear regression, by starting with single-respons…",no
q8,What assumptions does single-response linear regression have?,3968,"They are linearity, homoscedasticity, normality, and independence of its variables.",1,Raul_Unnithan_Dissertation.pdf,"[3968] ˆβOLS = (X⊤X)−1X⊤Y. The model’s accuracy can then be evaluated using metrics such as the R2-value, which measures the proportion of variance in the dependent va…",no
q9,Why can single-response linear regression not be extended to cover multiple correlated responses?,3940;3975;3312,"If this method were extended to model m responses using m independent single-response regression models, it would ignore the covariance between responses, leading to sub-optimal predictions when responses are correlated.",0.585;0.510;0.529,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Multi-Target XGBoost.pdf,"[3940] Chapter 1 Introduction 1.1 Background 1.1.1 Multiple-Response Regression Single-response regression models one response against a set of predictor variables. Th… || [3975] CHAPTER 3. LINEAR REGRESSION Normality also holds here, but it differs slightly. If we look at using several independent singleresponse models, each error term … || [3312] 2While XGBoost (Chen and Guestrin, 2016) and CatBoost (Prokhorenkova et al., 2018; Dorogush et al., 2017) allow modelling of several responses, with a separate …",no
q10,What is the difference between single-response linear regression and multiple-response linear regression?,3940;3970;3966,"The first key difference between MRLR and SRLR lies in the structures of the responses, coefficients and errors. These are now matrices rather than vectors, reflecting that the responses are modelled together. The second key difference between SRLR and MRLR lies in the structure of the error terms.
MRLR allows for correlation between errors across different responses within the same observation.",0.619;0.574;0.557,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[3940] Chapter 1 Introduction 1.1 Background 1.1.1 Multiple-Response Regression Single-response regression models one response against a set of predictor variables. Th… || [3970] 3.1.2 Multiple-Response Linear Regression Multiple-response linear regression (MRLR) is the most basic model in MRR. Its general form is: Y(n×m) = X(n×p) B(p×m)… || [3966] Chapter 3 Linear Regression This chapter introduces the methodology of the first MRR model, multiple-response linear regression, by starting with single-respons…",no
q11,What assumptions does multiple-response linear regression have?,2020;3975;3940,"It comes with a few underlying assumptions, which can be extended from SRLR. However now there is a covariance matrix of errors.",0.640;0.584;0.568,Machine Learning III Notes.pdf;Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[2020] 3.6. MUL TIPLE LINEAR REGRESSION: INTRODUCTION 65 Multiple Linear Regression Assumptions • Linearity: F or each set of values, 𝑥1, 𝑥2, … , 𝑥𝑘, of the predictor … || [3975] CHAPTER 3. LINEAR REGRESSION Normality also holds here, but it differs slightly. If we look at using several independent singleresponse models, each error term … || [3940] Chapter 1 Introduction 1.1 Background 1.1.1 Multiple-Response Regression Single-response regression models one response against a set of predictor variables. Th…",no
q12,What is forward stepwise selection?,3978;2043;2044,"Forward stepwise selection starts with a model having no predictors and adds the predictor that produces the largest improvement in model fit. Model fit is usually determined using a selection criterion, for example, the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), or adjusted R2 . At each step, the variable that contributes most significantly to this criterion is added.",0.674;0.667;0.649,Raul_Unnithan_Dissertation.pdf;Machine Learning III Notes.pdf;Machine Learning III Notes.pdf,"[3978] CHAPTER 3. LINEAR REGRESSION methods were used because they are more computationally efficient than best subset selection, as they consider a smaller set of mod… || [2043] though they might not have any predictive power on future data. F orward stepwise selection is a computationally eﬀicient alternative to best subset selection. … || [2044] 5.3. BACKW ARDS STEPWISE SELECTION 83 𝑀0 ∶ intercept only (null model) 𝐶1 ∶ ⎧{ ⎨{⎩ 𝑋1 𝑋2 → lowest training 𝑅𝑆𝑆 within 𝐶1 ∶ make this 𝑀1 𝑋3 𝐶2 ∶ {𝑋1, 𝑋2 → lowest…",no
q13,What is backward stepwise selection?,3978,"Backward stepwise selection begins with the full model as the initial model, and successively removes the predictor that makes the smallest contribution to the model according to the same criterion.",1,Raul_Unnithan_Dissertation.pdf,"[3978] CHAPTER 3. LINEAR REGRESSION methods were used because they are more computationally efficient than best subset selection, as they consider a smaller set of mod…",no
q14,What is bi-directional stepwise selection?,3978,"Bi-directional stepwise selection incorporates both forward and backward stepwise selection, with inclusion or exclusion of predictors at every step based on their impact on the model fit as a whole.",1,Raul_Unnithan_Dissertation.pdf,"[3978] CHAPTER 3. LINEAR REGRESSION methods were used because they are more computationally efficient than best subset selection, as they consider a smaller set of mod…",no
q15,What is Analysis of Variance (ANOVA)?,3980,ANOVA primarily analyses the differences among group means and their associated variances. It helps determine whether there is a significant difference between the means of different groups by comparing the variance within groups to the variance between groups.,1,Raul_Unnithan_Dissertation.pdf,[3980] different predictor combinations. 21 3.1.4 Analysis of Variance ANOVA primarily analyses the differences among group means and their associated variances. It he…,no
q16,What is Sequential Analysis of Variance (ANOVA)?,3981,Sequential ANOVA is a type of ANOVA which decomposes the regression component of an ANOVA table by considering a sequence of m nested models. This approach allows an assessment of how the unexplained variation in the response changes as predictors are added to the model one at a time.,1,Raul_Unnithan_Dissertation.pdf,"[3981] ANOVA results are displayed as a table which generates an F statistic, F = MSR MSE , where MSR = SSR p−1 and MSE = SSE n−p, where n is the number of observation…",no
q17,What is Multiple Analysis of Variance?,3980;2470;288,Multivariate Analysis of Variance (MANOVA) extends ANOVA to multiple responses. It comes with the assumption that the responses need to have a multivariate normal distribution.,0.526;0.539;0.502,Raul_Unnithan_Dissertation.pdf;MANOVA.pdf;Correlation Covariance.pdf,"[3980] different predictor combinations. 21 3.1.4 Analysis of Variance ANOVA primarily analyses the differences among group means and their associated variances. It he… || [2470] Newsom Psy 522/622 Multiple Regression and Multivariate Quantitative Methods, Winter 2024 1 Multivariate Analysis of Variance Multivariate analysis of variance … || [288] different aspects of the variability in the data. The generalized variance plays an important role in maxim um likelihood estimation (Chapter 5) and the total v…",yes
q18,Why is Multiple Analysis of Variance suitable for multiple-response regression?,3979;3940;17,MANOVA is suitable for multiple-response regression because the total SSCP matrix is an unscaled version of the sample response covariance matrix,0.657;0.594;0.548,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Chapter 7.7.pdf,"[3979] When using all of these selection methods in single-response regression, the “best” model is chosen for every number of predictors, and then the single best mod… || [3940] Chapter 1 Introduction 1.1 Background 1.1.1 Multiple-Response Regression Single-response regression models one response against a set of predictor variables. Th… || [17] Multivariate Multiple Regression Result 7.10. Let the multivariate multiple regression model in (7-23) hold with full rank and let the errors have a normal dist…",no
q19,How does the Wilks' Lambda Test work?,3991;3986;2477,"The Wilks' Lambda test measures the proportion of variance not explained by the independent variables. A small Λ (close to 0) indicates the predictors explain a large proportion of response variation, and a
large Λ (close to 1) indicates the predictors explain a small proportion of response variation.",0.574;0.560;0.582,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;MANOVA.pdf,"[3991] or exclusion depends on the change in Wilks’ Lambda (Λ): ∆Λ = Λ current −Λnew, where Λcurrent is the Wilks’ Lambda from the current model, and Λ new is it after… || [3986] as sequential ANOVA, but at each step, the Wilks’ Lambda value (Λ j) is computed for each model: Λ1 ≥ Λ2 ≥ ··· ≥Λm, where the difference: Λ j − Λj+1 is the addi… || [2477] When the result is significant, it indicates that differences existing among the groups on the dependent variables taken together. Wilks' lambda is the simplest…",no
q20,What is Sequential Multiple Analysis of Variance (ANOVA)?,3981;3982;3980,"Sequential MANOVA extends sequential ANOVA to cover multiple responses. It works the same as sequential ANOVA, but at each step, the Wilks’ Lambda value (Λj ) is computed for each model.",0.636;0.593;0.582,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[3981] ANOVA results are displayed as a table which generates an F statistic, F = MSR MSE , where MSR = SSR p−1 and MSE = SSE n−p, where n is the number of observation… || [3982] CHAPTER 3. LINEAR REGRESSION Mj+1 : E[Y | X] = b1 + b2X2 + ··· + bpj Xpj + bpj+1 Xpj+1 . At each step, a hypothesis test is performed to assess whether the newl… || [3980] different predictor combinations. 21 3.1.4 Analysis of Variance ANOVA primarily analyses the differences among group means and their associated variances. It he…",no
q21,What is the Average Normalised Root Mean Squared Error?,3987,"While the root mean square error (RMSE) measures model error for each response variable separately, it retains the original units, making direct comparisons difficult. In contrast, the normalised RMSE (NRMSE) scales the error by the standard deviation of each response, returning a unitless and comparable measure of model performance. Since two response variables were considered, the NRMSE was computed for each and then averaged to obtain a single summary statistic, the ANRMSE.",1,Raul_Unnithan_Dissertation.pdf,[3987] assessed using the average normalised root mean square error (ANRMSE). While the root mean square error (RMSE) measures model error for each response variable s…,no
q22,How is the Average Normalised Root Mean Squared Error measured?,3987;2593;2915,"The ANRMSE gives the proportion of natural variability in the responses not accounted for by the model. An excellent model has a 0-0.5 ANRMSE, a good model has a 0.5-0.6 ANRMSE, a satisfactory model has a 0.6-0.7 ANRMSE, and an unsatisfactory model has an ANRMSE greater than 0.7.",0.579;0.498;0.491,Raul_Unnithan_Dissertation.pdf;MOGP Actual.pdf;MRR Article.pdf,"[3987] assessed using the average normalised root mean square error (ANRMSE). While the root mean square error (RMSE) measures model error for each response variable s… || [2593] root mean squared errors (MRMSE) averaged over ten random train/test splits of the data. Dataset R-Baxter F-Baxter Kuka Sarcos MuJoCo Model LL MRMSE LL MRMSE LL… || [2915] 92 6 Univariate Statistics Sample Variance can be negative, and squaring gets rid of the negative sign. The variance is the approximate average of the squared d…",no
q23,What is Ridge Regression?,4004;4005;4006,"Ridge regression shrinks estimated regression coefficients towards zero by minimising the following penalised loss function: L(β) = ∥Y − Xβ∥_2^2 + λ∥β∥_2^2 ,",0.711;0.664;0.605,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[4004] by minimising a loss function that combines model fit with a penalty on coefficient size. 4.1.1 Ridge Regression Ridge regression shrinks estimated regression c… || [4005] coefficient estimator as: ˆβ Ridge =  X⊤X + λIp −1 X⊤Y. (4.2) See Subsection A.2.4 for the full derivation of the closed-form estimator. Ridge regression is p… || [4006] CHAPTER 4. SHRINKAGE METHODS Ridge regression estimates are obtained by solving the least squares problem, subject to a constraint on the sum of squared coeffic…",no
q24,When is Ridge Regression useful?,4005,"Ridge regression is particularly useful when the predictors exhibit high multicollinearity or when the number of predictors, p, is close to or exceeds the number of observations, n.",1,Raul_Unnithan_Dissertation.pdf,[4005] coefficient estimator as: ˆβ Ridge =  X⊤X + λIp −1 X⊤Y. (4.2) See Subsection A.2.4 for the full derivation of the closed-form estimator. Ridge regression is p…,no
q25,What does the tuning parameter do in Shrinkage Methods?,4004,"The tuning parameter is crucial as it controls the strength of the penalty, balancing the trade-off between model fit and coefficient magnitude. Therefore, as λ increases, the penalty term gets larger, so to minimise the entire loss function, the regression coefficients get smaller.",1,Raul_Unnithan_Dissertation.pdf,[4004] by minimising a loss function that combines model fit with a penalty on coefficient size. 4.1.1 Ridge Regression Ridge regression shrinks estimated regression c…,no
q26,How is the Ridge Regression cost function solved?,4004;2096;2099,"Ridge regression minimises the cost function, and this can be shown by differentiating the cost
function with respect to β and setting it equal to zero. This gives the ridge regression closed-form
coefficient estimator.",0.707;0.663;0.651,Raul_Unnithan_Dissertation.pdf;Machine Learning III Notes.pdf;Machine Learning III Notes.pdf,[4004] by minimising a loss function that combines model fit with a penalty on coefficient size. 4.1.1 Ridge Regression Ridge regression shrinks estimated regression c… || [2096] 6.2. RIDGE REGRESSION 107 across models of differing dimensionality (number of predictors). Shrinkage methods offer a continuous analogue (which is much faster)… || [2099] 6.2. RIDGE REGRESSION 109 # Number of iterations for the simulation study. iter <- 50 # Number of training points. n <- 10 # lambda coefficient - feel free to e…,no
q27,How does Lasso Regression work?,4008;4010;4009,"Lasso regression removes irrelevant features by shrinking their coefficients exactly to zero. Instead of penalising the squared magnitude of the coefficients like ridge, lasso penalises their absolute values. It shares the same aim of minimising a loss function as ridge, but with a different penalty term: L(β) = ∥Y − Xβ∥_2^2 + λ∥β∥_1",0.667;0.608;0.608,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[4008] Unlike ridge regression, lasso has no closed-form solution because the ℓ1-norm penalty, ∥β∥1, is not differentiable at zero. This makes the optimisation problem… || [4010] prediction accuracy. Lasso is particularly useful in high dimensions, as it is scalable and can perform automatic feature selection by shrinking some coefficien… || [4009] CHAPTER 4. SHRINKAGE METHODS where X−j denotes the design matrix,X, with thej-th column removed, andβ−j is the coefficient vector, β, with the j-th entry remove…",no
q28,How is the Lasso cost function solved?,4007;4008,"Unlike ridge regression, lasso has no closed-form solution because the ℓ1-norm penalty, ∥β∥1, is not differentiable at zero. This makes the optimisation problem non-smooth and means lasso requires iterative algorithms, such as coordinate descent, to solve it.",1.000;1.000,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[4007] zero, it does not set any of them exactly to zero, even those corresponding to irrelevant predictors. This inability to create sparse models, i.e. a model in wh… || [4008] Unlike ridge regression, lasso has no closed-form solution because the ℓ1-norm penalty, ∥β∥1, is not differentiable at zero. This makes the optimisation problem…",no
q29,How does Ridge Regression extend to multiple-responses?,4004;4005;4012,"Ridge regression extends to multiple responses by adding a Frobenius norm penalty to the coefficient matrix. 
In single-response ridge regression, the penalty is given by the squared ℓ2-norm of the coefficient
vector. See Equation 4.1. However, since the standard ℓ2-norm applies only to vectors, it cannot
directly penalise all the entries in the coefficient matrix. The Frobenius norm replaces it because it
extends the ℓ2-norm to matrices, summing the squared entries across all rows and columns in B.",0.626;0.603;0.585,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[4004] by minimising a loss function that combines model fit with a penalty on coefficient size. 4.1.1 Ridge Regression Ridge regression shrinks estimated regression c… || [4005] coefficient estimator as: ˆβ Ridge =  X⊤X + λIp −1 X⊤Y. (4.2) See Subsection A.2.4 for the full derivation of the closed-form estimator. Ridge regression is p… || [4012] vector. See Equation 4.1. However, since the standard ℓ2-norm applies only to vectors, it cannot directly penalise all the entries in the coefficient matrix. Th…",no
q30,How does Lasso Regression extend to multiple-response regression?,4010;4008;4013,"Multivariate Lasso also uses coordinate descent to solve its optimisation problem. However, this slightly changes because now there is a coefficient matrix to estimate rather than a coefficient vector.",0.576;0.538;0.515,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[4010] prediction accuracy. Lasso is particularly useful in high dimensions, as it is scalable and can perform automatic feature selection by shrinking some coefficien… || [4008] Unlike ridge regression, lasso has no closed-form solution because the ℓ1-norm penalty, ∥β∥1, is not differentiable at zero. This makes the optimisation problem… || [4013] CHAPTER 4. SHRINKAGE METHODS which follows from single-response ridge regression (see Equation 4.2). Similarly, Lasso Regression can also be extended to MRR usi…",no
q31,What is Reduced Rank Ridge Regression?,4014;4004;2143,Reduced rank ridge regression (RRRR) builds upon this by modifying the estimation of the coefficient matrix using multiple-response ridge regression and a rank constraint.,0.705;0.609;0.635,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Machine Learning III Notes.pdf,"[4014] reduced rank ridge regression and multivariate regression with covariance estimation. 4.1.4 Reduced Rank Ridge Regression In MRLR, predictions are generated usi… || [4004] by minimising a loss function that combines model fit with a penalty on coefficient size. 4.1.1 Ridge Regression Ridge regression shrinks estimated regression c… || [2143] 130 CHAPTER 6. SHRINKAGE METHODS Sparsifying the ridge coeﬀicients? One thing mentioned in Chapter 6.2 is that one can potentially consider some kind of posthoc…",no
q32,What is Multivariate Regression with Covariance Estimation?,1292;3509;1291,Multivariate Regression with Covariance Estimation (MRCE) builds upon multiple-response linear regression by jointly estimating the regression coefficient matrix B and the inverse error covariance matrix Ω.,0.618;0.575;0.550,CovRegRF.pdf;MV Linear Regression.pdf;CovRegRF.pdf,"[1292] Page 3 of 19 Alakus et al. BMC Bioinformatics (2023) 24:258 the data variation in the projected space. The model assumes that in the eigendecompo - sition of co… || [3509] Multivariate regression First, we’ll review multiple (univariate) regression with ﬁxed x variables. For this model, we have y1 = β0 + p∑ j=1 βj x1j + ε1 y2 = β0… || [1291] ings obtained with a latent factor model. The conditional covariance matrix is a quadratic function of these factor loadings. The method is limited to data sets…",no
q33,Why does Multivariate Response with Covariance Estimation qualify for multiple-response regression?,3940;1288;29,"Since multivariate response with covariance estimation estimates both the coefficient matrix and inverse matrix simultaneously, the coefficient matrix is optimised considering response covariance. This reasoning follows from the explanation for why MRLR qualifies as an
multiple-response regression.",0.565;0.583;0.583,Raul_Unnithan_Dissertation.pdf;CovRegRF.pdf;Chapter 7.7.pdf,"[3940] Chapter 1 Introduction 1.1 Background 1.1.1 Multiple-Response Regression Single-response regression models one response against a set of predictor variables. Th… || [1288] Page 2 of 19Alakus et al. BMC Bioinformatics (2023) 24:258 between multiple responses, e.g. for microbiome data, the goal is to estimate net - work changes with… || [29] Multivariate Multiple Regression Predictions from Multivariate Multiple Regressions Suppose the model with normal errors has been fit and checked for any inadeq…",no
q34,What are random forests?,4046,"Random forests are tree-based models built upon classification and regression trees (CARTs) as a building block. A CART partitions the training data into groups with similar response values, and then predicts the same category for all data within a given subgroup.",1,Raul_Unnithan_Dissertation.pdf,[4046] with similar response values. 5.1.1 Classification and Regression Trees Random forests are tree-based models built upon classification and regression trees (CAR…,no
q35,What are Classification and Regression Trees (CARTs) built up off?,4046;4045;4054,"The CART starts with a root node containing all the objects, and is then divided into “leaf” nodes by recursive binary splitting.[34] This recursive splitting means that CARTs are able to detect patterns that occur only within specific regions of the predictor space, including non-linear relationships and interactions that other models may miss. Each leaf node relates to a hyper-rectangle in the feature space, Rj , j = {1, . . . , J}. In other words, the feature space defined by X1, X2, . . . , Xp is split into J distinct regions which do not overlap.",0.738;0.646;0.599,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[4046] with similar response values. 5.1.1 Classification and Regression Trees Random forests are tree-based models built upon classification and regression trees (CAR… || [4045] Chapter 5 Random Forests This chapter delves into Random Forests. It starts with classification and regression trees, from which we can create an ensemble of de… || [4054] CARTs are limited to single-response regression, making them unsuitable for modelling relationships between multiple correlated responses. Multivariate Regressi…",no
q36,How do Classification and Regression Trees (CARTs) grow?,4046;4045;4054,"To grow the tree, CARTs use a greedy fitting algorithm, which means it selects the best split at each step by evaluating all possible features and thresholds. This decision is made locally and recursively, without considering future splits. The algorithm chooses
the split that minimises the RSS.",0.726;0.578;0.558,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf,"[4046] with similar response values. 5.1.1 Classification and Regression Trees Random forests are tree-based models built upon classification and regression trees (CAR… || [4045] Chapter 5 Random Forests This chapter delves into Random Forests. It starts with classification and regression trees, from which we can create an ensemble of de… || [4054] CARTs are limited to single-response regression, making them unsuitable for modelling relationships between multiple correlated responses. Multivariate Regressi…",no
q37,What is pruning and why is it necessary?,4047,"The greedy fitting algorithm provides good predictions for the training data but may overfit said data, resulting in poor performance on the test dataset. However, pruning can mitigate this issue. Pruning involves removing branches that contribute little to the tree’s prediction accuracy to control the bias–variance trade-off. This is either based on a validation set or a complexity penalty. A common approach to pruning is the weakest link pruning algorithm",1,Raul_Unnithan_Dissertation.pdf,"[4047] CHAPTER 5. RANDOM FORESTS Note: the threshold is the value used to split the data on a feature. This decision is made locally and recursively, without consideri…",no
q38,What is bootstrap aggregating?,4048,"Bootstrap aggregating, also known as bagging, improves the stability and accuracy of CART algorithms by averaging models. Bagging helps reduce variance and avoid over-fitting, but the model becomes more challenging to interpret",1,Raul_Unnithan_Dissertation.pdf,[4048] Pruning involves removing branches that contribute little to the tree’s prediction accuracy to control the bias–variance trade-off. This is either based on a va…,no
q39,What are the advantages and disadvantages of bagging?,4050;4049;2296,"Bagging is advantageous when there is a large amount of data, as is the case here for the equity fund dataset. This is because the empirical distribution will be closer to the actual underlying population’s distribution. Additionally, it is the best option when the aim is to minimise the variance of a predictor. However, bagging also brings some disadvantages. Firstly, interpretability is lost as the final estimate is not a tree. Secondly, it is an ensemble prediction, i.e. multiple trees are combined to make the final prediction.[35] Also, bagged trees are inherently correlated. This is problematic because the more correlated the random variables are, the smaller the variance reduction of their average, which can undermine one of the key advantages of bagging.",0.689;0.645;0.656,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Machine Learning III Notes.pdf,"[4050] However, bagging also brings some disadvantages. Firstly, interpretability is lost as the final estimate is not a tree. Secondly, it is an ensemble prediction, … || [4049] improves the stability and accuracy of CART algorithms by averaging models. Bagging helps reduce variance and avoid over-fitting, but the model becomes more cha… || [2296] set. Such kind of Out Of Bag (OOB) error estimation is approximately equal to a 3-fold cross validation. Pros and Cons The advantages of bagging are: • If you h…",no
q40,What is the Random Forests Algorithm?,2634;3681;4190,"The Random Forests algorithm is used in the following way: a bootstrap resample (y ∗ i , x∗ i ) n i=1 is taken of the training data, like for bagging. Then, the tree is built and each time a split in a tree is considered, m predictors are randomly selected out of the full set of p predictors as split candidates, and the best split within those m predictors is found. Finally, the first two steps are repeated, averaging the prediction of all the regression forest trees.",0.720;0.704;0.689,MRFs.pdf;Parallelisation.pdf;RF OG Paper.pdf,"[2634] and sand, and it is dominated by the spider species Pardosa lugubris. CONCLUSION Random forests have emerged as a forefront classiﬁcation and regression techniq… || [3681] The Random Forest (RF) algorithm [9] is a suitable data mining algorithm for big data. It is an ensemble learning algorithm using feature sub-space to construct… || [4190] 2 1. Random Forests 1.1 Introduction Significant improvements in classification accuracy have resulted from growing an ensemble of trees and letting them vote f…",no
q41,What are out-of-bag samples?,4206;2296;4242,The observations that were not included in the bootstrap sample used to train that tree.,0.584;0.525;0.508,RF OG Paper.pdf;Machine Learning III Notes.pdf;RF OG Paper.pdf,"[4206] used out-of-bag estimates of variance to estimate generalization error for arbitrary classifiers. The study of error estimates for bagged classifiers in Breiman… || [2296] set. Such kind of Out Of Bag (OOB) error estimation is approximately equal to a 3-fold cross validation. Pros and Cons The advantages of bagging are: • If you h… || [4242] 23 Data Set Nr. Inputs #Training #Test Boston Housing 12 506 10% Ozone 8 330 10% Servo 4 167 10% Abalone 8 4177 25% Robot Arm 12 15,000 5000 Friedman#1 10 200 2…",no
q42,What are Multivariate Regression Trees?,3091;3089;4123,Multivariate Regression Trees (MRTs) generalise Classification and Regression Trees by modelling all responses simultaneously and hence accounting for multiple correlated responses.[38],0.705;0.668;0.638,MRTs OG.pdf;MRTs OG.pdf;Raul_Unnithan_Dissertation.pdf,"[3091] Ecology, 83(4), 2002, pp. 1105-1117 ? 2002 by the Ecological Society of America MULTIVARIATE REGRESSION TREES: A NEW TECHNIQUE FOR MODELING SPECIES-ENVIRONMENT … || [3089] Multivariate Regression Trees: A New Technique for Modeling Species-Environment Relationships Author(s): Glenn De'ath Source: Ecology , Apr., 2002, Vol. 83, No.… || [4123] 39. G. De’ath. Multivariate regression trees: A new technique for modelling species-environment relationships. Ecological Society of America, 88:2783–2792, 2002…",no
q43,How are Classification and Regression Trees typically pruned?,4048;2276;4047,"Therefore, trees are pruned using CV, typically selecting the smallest tree within one standard error of the minimum cross-validated error.[39] This is known as the 1-standard error rule.",0.624;0.641;0.592,Raul_Unnithan_Dissertation.pdf;Machine Learning III Notes.pdf;Raul_Unnithan_Dissertation.pdf,"[4048] Pruning involves removing branches that contribute little to the tree’s prediction accuracy to control the bias–variance trade-off. This is either based on a va… || [2276] 5) of training observations in each hyper-rectangle (box). Note that such a greedy fitting algorithm does NOT guarantee an optimal tree since it is constructed … || [4047] CHAPTER 5. RANDOM FORESTS Note: the threshold is the value used to split the data on a feature. This decision is made locally and recursively, without consideri…",no
q44,What is Covariance Regression with Random Forests?,4059,"Covariance Regression with Random Forests (CRRFs) extend MRFs by incorporating a splitting criterion, which maximises differences in sample covariance estimates between child nodes.",1,Raul_Unnithan_Dissertation.pdf,"[4059] CHAPTER 5. RANDOM FORESTS As a result, when responses are highly correlated, MRFs fail to consider the correlations optimally when building trees, potentially l…",no
q45,What is the Bag of Observations for Prediction (BOP)?,4064;1298;4063,"For a new observation, nearest neighbour observations are used to estimate the response mean and final covariance matrix, ensuring both the predicted value and its uncertainty are captured.[41]",0.575;0.504;0.460,Raul_Unnithan_Dissertation.pdf;CovRegRF.pdf;Raul_Unnithan_Dissertation.pdf,"[4064] which is key for response prediction. 41 The response mean and covariance matrix are then estimated using the sample mean and covariance matrix of the observati… || [1298] to the ‘nearest neighbour forest weights’ [15, 16], was introduced in [17] and later used in [18–21]. [19] called this set of observations the Bag of Observatio… || [4063] CHAPTER 5. RANDOM FORESTS The final covariance matrices are estimated using random forests. For a new observation, nearest neighbour observations are used to es…",no
q46,How does Gradient Tree Boosting work?,4074;3411;3585,"Gradient tree boosting builds a model by sequentially adding regression trees to minimise a predefined loss function. Each new tree is trained on the negative gradient of the current model, enabling the ensemble to iteratively refine its predictions.[50]",0.729;0.685;0.677,Raul_Unnithan_Dissertation.pdf;Multivariate XGBoost.pdf;OG XGBoost.pdf,"[4074] 6.1.1 Introduction Gradient tree boosting builds a model by sequentially adding regression trees to minimise a predefined loss function. Each new tree is traine… || [3411] statistics community, starting from the introduction in the 90s of the famous AdaBoost classiﬁcation algorithm (Freund and Schapire, 1997). Originally introduce… || [3585] literatures in gradient boosting. Specicially the second order method is originated from Friedman et al. [12]. We make minor improvements in the reguralized obj…",no
q47,What is Extreme Gradient Boosting?,4077,"Extreme Gradient Boosting, otherwise known as XGBoost, improves gradient boosting by including regularisation explicitly in the loss function to address over-fitting and improve model predictions.",1,Raul_Unnithan_Dissertation.pdf,"[4077] training and represent the model’s prediction for any inputs that fall into their corresponding leaves. Building on Equations 6.1 and 6.3, each regression tree …",no
q48,What is Cholesky decomposition?,150;151;159,"To ensure positive definiteness of the response covariance matrix, ΣY, a common and efficient approach is to use the Cholesky decomposition, which factorises the matrix as follows: ΣY = LL^⊤, where L ∈ R D×D is a lower triangular matrix.[51]",0.750;0.711;0.695,Cholesky-Decomposition Algorithm.pdf;Cholesky-Decomposition Algorithm.pdf;Cholesky-Decomposition Algorithm.pdf,[150] data. Introduction The Cholesky decomposition of a symmetric matrix A = A′ of full-rank has numerous applications. Amongst these is the solution of linear equat… || [151] the diagonal elements of D. An important use of the Cholesky decomposition is in solving the equation Ab = y when A is symmetric. A further specialisation arise… || [159] CHOLESKY DECOMPOSITION AND SEASONAL ADJUSTMENT =   d0 0 0 0 d0 m11 d1 0 0 d0 m22 d1 m21 d2 0 {d0 m33 } d1 m32 d2 m31 d3     1 m11 m22 {m33 } 0 1 m21 m3…,no
q49,Why does Cholesky Decomposition work with Extreme Gradient Boosting?,4087;4090;1487,"Cholesky decomposition can be used to remove intercorrelation between response variables by applying the transformation L −1Y⊤, which produces decorrelated responses.[52] This decorrelation allows XGBoost to be applied independently to each transformed response. After prediction, the original correlated response space is recovered by reapplying L, which reintroduces the covariance structure.",0.665;0.643;0.672,Raul_Unnithan_Dissertation.pdf;Raul_Unnithan_Dissertation.pdf;Gradient and Hessian.pdf,"[4087] CHAPTER 6. XG BOOST 6.1.3 Cholesky Decomposition Although a prior distribution is not required to apply Cholesky decomposition, assuming a multivariate normal d… || [4090] CHAPTER 6. XG BOOST 6.2 Application 6.2.1 Small-Scale Example We will work with the familiar dataset, evaluating Mathematics and Science Scores: X1: Hours Studi… || [1487] i be the Cholesky decomposition of Σ i, where Ci is a lower triangular matrix for i = 1, . . . , K, and C = blockdiag(C1, . . . , CK). For the parametrization λ…",no
q50,How did each model perform?,3333;3949;3196,"The MRLR model using all predictors performed poorly, and all the stepwise selection methods (forwards, backwards and bi-directional) failed to improve performance significantly. Backward selection performed the worst, suggesting that eliminating variables here led to worse model performance. However, the introduction of interaction terms, alongside bi-directional stepwise selection, led to a huge improvement, which resulted in a “Good” model fit. Similarly, the addition of non-linear terms, also selected through bi-directional stepwise selection, improved upon the MRLR model (with just the standard predictors), and achieved a “Satisfactory” fit. The shrinkage methods aimed to improve regression stability by using penalisation techniques. However, the MRCE and RRRR models, with the regular features, performed similarly to their counterparts in standard MRLR, again returning an “Unsatisfactory” fit. Introducing polynomial terms in MRCE improved performance, but for RRRR, this was less effective. While adding interaction terms improved MRCE, but did not improve RRRR as much. The performance of shrinkage methods was also noteworthy. This is because, despite being specifically designed to handle multiple responses, particularly under multicollinearity or high-dimensional settings, both MRCE and RRRR performed as poorly as, or even worse than, the MRLR models. This can be attributed to the pre-processing of the dataset: after pre-processing (see Chapter 2), the predictors were not highly correlated, and the data was not high-dimensional (p = 14, n = 1269). In this case, the advantages that regularisation and rank reduction bring are less impactful. Tree-based models significantly improved upon linear and shrinkage-based methods. The CRRF model was significantly better than previous models and was evaluated as a “Good” fit model. Meanwhile, the XGBoost gave further improvements, being the only “Excellent” model. The strong performance of XGBoost and CRRF, particularly in comparison to the limited performance of the models in Chapters 3 and 4 with only the given features, indicates that the equity fund responses, sustainability score and ROE, are driven more by conditional and non-linear interactions among predictors. This notion is further supported by the improved performance of every model in Chapters 3 and 4 with interaction or non-linear terms introduced. Mardia’s test also supports this through its borderline p-values of 0.0732 for skewness and 0.0862 for kurtosis (see Subsection 2.3.5 for further details). While these values are not low enough to reject the null hypothesis of multivariate normality at standard significance levels, they do indicate a potential deviation from normality. This means that the response structure is unlikely to be adequately captured by linear models with additive assumptions (like MRLR, MRCE and RRRR), thereby justifying the use of more flexible models (like Cholesky-Decomposed XGBoost and CRRFs), that can consider extra predictor effects. Overall, the findings suggest that the most effective modelling approaches are those that accommodate flexible, non-linear and interaction effects, as opposed to approaches based on rigid parametric or distributional assumptions.",0.495;0.408;0.421,Multi-Target XGBoost.pdf;Raul_Unnithan_Dissertation.pdf;Multi-Output Neural Networks.pdf,"[3333] 5 multivariate models. For conducting the experiments, we create 11 randomly shuﬄed folds for all datasets. Each is split into train and test, where 80% is used… || [3949] XGBoost model is then evaluated on the equity fund dataset. Finally, Chapter 7 gives an overall comparison of the performance of each modelling approach applied… || [3196] operating point instantaneously, which is a main feature of our approach and sets it apart from other static approaches that opt for a fixed performance point l…",yes
